.. -*- mode: rst -*-
.. include:: ../definitions.txt
.. include:: regexp-defns.txt
.. sectnum::

===================================
6. Structured Programming in Python
===================================

------------
Introduction
------------

In this chapter we pick up where we left off in |Programming|.  We
assume you are confident with the material presented there and have
tried lots of exercises in the intervening chapters.  Here we will
consolidate your knowledge of the key programming concepts and explain
many of the minor points that could easily trip you up.  We also
introduce new topics in structured programming and algorithmic problem
solving.  The chapter contains many working program fragments and
exercises which you should try yourself.

-----------------
More Fundamentals
-----------------

Sequences: Strings, Lists and Tuples
------------------------------------

In Part I we saw three kinds of sequence object: strings, lists, and tuples.
As sequences they have some common properties: they can be indexed and they have
a length:

    >>> string = 'I turned off the spectroroute'
    >>> words = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> pair = (6, 'turned')
    >>> string[2], words[3], pair[1]
    ('t', 'the', 'turned')
    >>> len(string), len(words), len(pair)
    (29, 5, 2)

|nopar|
We can iterate over a sequence ``s`` using the construct ``for item in s``.
We can iterate in sorted order using ``for item in sorted(s)``,
and we can eliminate duplicates using ``for item in set(s)`` or
``for item in sorted(set(s))``.
For convenience, we can convert between these types, e.g.
``tuple(s)`` converts a sequence into a tuple, and
``list(s)`` converts a sequence into a list.
We can convert a list of strings to a single string using the
``join`` function, e.g. ``':'.join(words)``.

Notice in the above code sample that we computed multiple values
on a single line, separated by commas.  Lines of comma-separated
expressions are actually just tuples.  We use tuples to spontaneously
aggregate items.  In the next example we use tuples to re-arrange the
contents of our list:

    >>> words[2], words[3], words[4] = words[3], words[4], words[2]
    >>> words    
    ['I', 'turned', 'the', 'spectroroute', 'off']

|nopar|
This is an idiomatic and readable way to move items inside a list.
It is equivalent to the following (traditional) way of doing such a
task that does not use tuples:

    >>> tmp = words[2]
    >>> words[2] = words[3]
    >>> words[3] = words[4]
    >>> words[4] = tmp

Another common way to use tuples is in the return values of functions,
in those situations where we want to return multiple items of
different types.  Here we define a function that returns a tuple
consisting of the average word length of a sentence, and the inventory
of letters used in the sentence.

    >>> def proc_words(words):
    ...     avg_wordlen = sum(len(word) for word in words)/len(words)
    ...     chars_used = ''.join(sorted(set(''.join(words))))
    ...     return avg_wordlen, chars_used
    >>> proc_words(words)
    (5, 'Icdefhnoprstu')

Notice that Python is usually able to detect the presence of a tuple simply by
seeing a comma.  In such cases the parentheses are optional.
For example, the return line of the above function does not contain
parentheses, nor does the tuple assignment statement in the previous
example (i.e. comma has higher `precedence`:dt: than assignment).
However, when we print a tuple, the parentheses are always displayed.

List Comprehensions
-------------------

Many language processing tasks involve applying the same operation to
every item in a list.  `List comprehensions`:dt: are a convenient Python
construct for doing this.  Here we lowercase each word:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> [word.lower() for word in sent]
    ['the', 'dog', 'gave', 'john', 'the', 'newspaper']

|nopar|
This syntax might be reminiscent of the notation used for building
sets, e.g. {(x,y) | x\ :superscript:`2` + y\ :superscript:`2` = 1}.
Just as this set definition incorporates a constraint, list
comprehensions can constrain the items they include.  In the next
example we remove all determiners from a list of words:

    >>> def is_lexical(word):
    ...     return word.lower() not in ('a', 'an', 'the', 'that', 'to')
    >>> [word for word in sent if is_lexical(word)]
    ['dog', 'gave', 'John', 'newspaper']

|nopar|
Now we can combine the two ideas, to normalize a subset of the words:

    >>> [word.lower() for word in sent if is_lexical(word)]
    ['dog', 'gave', 'john', 'newspaper']

List comprehensions can build nested structures too.  For example,
the following code builds a list of tuples, where each tuple consists
of a word and its length.

    >>> [(x, len(x)) for x in sent]
    [('The', 3), ('dog', 3), ('gave', 4), ('John', 4),
     ('the', 3), ('newspaper', 9)]

Putting it Together
-------------------

Let's combine our knowledge of the three sequence types and list
comprehensions to perform the task of sorting the words in a string by
their length.

    >>> words = 'I turned off the spectroroute'.split()
    >>> wordlens = [(len(word), word) for word in words]
    >>> wordlens.sort()
    >>> ' '.join(word for count, word in wordlens)
    'I off the turned spectroroute'

Each of the above lines of code contains a significant feature.
The first line demonstrates that a simple string is actually
an object with methods defined on it, such as ``split()``.
The second line shows the construction of a list of tuples,
where each tuple consists of a number (the word length) and the
word, e.g. ``(3, 'the')``.  The third line sorts the list,
modifying the list in-place.  The last line of code discards
the length information then joins the words back into a single string.
    
We began by talking about the commonalities in these sequence types,
but the above program illustrates important differences in their
roles.  First, strings appear at the beginning and the end: this is
typical in the context where our program is reading in some text and
producing output for us to read.  Lists and tuples are used in the
middle, but for different purposes.  A list is typically a sequence of
objects all having the *same type*, of *arbitrary length*.  We often
use lists to hold sequences of words.  In contrast,
a tuple is typically a collection of objects of *different types*, of
*fixed length*.  We often use a tuple to hold a `record`:dt:,
a collection of different `fields`:dt: relating to some entity.

This distinction between the use of lists and tuples is a bit subtle,
so here is another example:

    >>> lexicon = [
    ...     ('the', 'DT', ['Di:', 'D@']),
    ...     ('off', 'IN', ['Qf', 'O:f'])
    ... ]

|nopar|
Here, a lexicon is represented as a list because it is a collection of objects
of a single type |mdash| lexical entries |mdash| of no predetermined
length.  An individual entry is represented as a tuple because it is
a collection of objects with different interpretations, such as
the orthographic form, the part of speech, and the pronunciations
represented in the SAMPA system.
Note that these pronunciations are stored using a list (why?).

The distinction between lists and tuples has been described in terms of
usage.  However, there is a more fundamental difference: in Python,
lists are `mutable`:dt:, while tuples are `immutable`:dt:.  In other
words, lists can be modified, while tuples cannot.  Here are some of
the operations on lists which do in-place modification of the list.
None of these operations is permitted on a tuple, a fact you should
confirm for yourself.

    >>> lexicon.sort()
    >>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])
    >>> del lexicon[0]


More List Comprehensions and Related Methods
--------------------------------------------

List comprehensions have a very wide range of uses in natural language
processing.  In this section we will see some more examples.
The first one implements a sliding window to create a list of n-grams:

    >>> n = 3
    >>> [sent[i:i+n] for i in range(len(sent)-n+1)]
    [['The', 'dog', 'gave'],
     ['dog', 'gave', 'John'],
     ['gave', 'John', 'the'],
     ['John', 'the', 'newspaper']]

|nopar|
This code takes successive overlapping slices of size ``n`` from ``sent``.
Pay particular attention to the range of the variable ``i``.

Another clever use of list comprehensions is for a kind of multiplication.
Here we generate all combinations of two determiners, two adjectives, and two nouns.
The list comprehension is split across three lines for readability.

    >>> [(dt,jj,nn) for dt in ('two', 'three')
    ...             for jj in ('old', 'blind')
    ...             for nn in ('men', 'mice')]
    [('two', 'old', 'men'), ('two', 'old', 'mice'), ('two', 'blind', 'men'),
     ('two', 'blind', 'mice'), ('three', 'old', 'men'), ('three', 'old', 'mice'),
     ('three', 'blind', 'men'), ('three', 'blind', 'mice')]

|nopar|
The above example contains three independent loops.  We can also have loops
with shared variables.  The next example iterates over all sentences in a
section of the Brown Corpus, and for each sentence, iterates over each word.

    >>> from nltk_lite.corpora import brown
    >>> [word for sent in brown.raw('a') for word in sent if len(word) == 17]
    ['September-October', 'Sheraton-Biltmore', 'anti-organization',
     'anti-organization', 'Washington-Oregon', 'York-Pennsylvania',
     'misunderstandings', 'Sheraton-Biltmore', 'neo-stagnationist',
     'cross-examination', 'bronzy-green-gold', 'Oh-the-pain-of-it',
     'Secretary-General', 'Secretary-General', 'textile-importing',
     'textile-exporting', 'textile-producing', 'textile-producing']

Another way to use loop variables is to ignore them!  This is
the standard method to build multidimensional structures.
For example, to build an array with *m* rows and *n* columns,
where each cell is a set, we would do the following:

    >>> from pprint import pprint
    >>> m, n = 4, 7
    >>> array = [[set() for i in range(n)] for j in range(m)]
    >>> array[3][5].add('foo')
    >>> pprint(array)
    [[set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set(['foo']), set([])]]

Sometimes we use a list comprehension as part of a larger aggregation task.
Here we calculate the average length of words in part of the Brown Corpus.
Notice that we don't bother storing the list comprehension.

    >>> from numpy import average
    >>> average([len(w) for sent in brown.raw('a') for w in sent])
    4.40154543827

Generator expressions: when a list comprehension occurs as the argument
of a function, we can often leave out the brackets.

Earlier we saw an example of filtering out some items in a list comprehension.
Thus we can restrict the list to just the lexical words, using
``[word for word in sent if is_lexical(word)]``.  This is a little cumbersome
as it mentions the ``word`` variable three times.  A more compact way to say
the same thing is as follows:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> filter(is_lexical, sent)
    ['dog', 'gave', 'John', 'newspaper']

|nopar|
Here we have used the function itself as an argument.  Functions are ultimately
just another kind of object that can be passed around a program.  The ``filter()``
function applies its first argument (a function) to its second (a sequence),
applying the function to each item in turn, only passing it through if the function
returns true for that item.  Thus ``filter(f, seq)`` is equivalent to
``[item for item in seq if apply(f,item) == True]``.
    
Python provides some other helpful functions like ``filter()``.  Here is a simple
way to find the average length of a sentence in a section of the Brown Corpus:

    >>> average([map(len, brown.raw('a'))])
    21.7461072664

|nopar|
This code applies the function (in this case ``len()``) to each item in the
sequence, and builds a new sequence containing the results.

Anonymous Functions and Lambda Expressions
------------------------------------------

In the above example we used the ``map()`` function to apply ``len()`` to
each item in a sequence.  Instead of ``len()`` we could have passed in any
other function we liked:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> def is_vowel(letter):
    ...     return letter in "AEIOUaeiou"
    >>> def vowelcount(word):
    ...     return len(filter(is_vowel, word))
    >>> map(vowelcount, sent)
    [1, 1, 2, 1, 1, 3]

|nopar|
Instead of using filter to call a named function ``is_vowel``, we can
define an `anonymous function`:dt: using a `lambda expression`:dt: as
follows: ``filter(lambda c: c in "AEIOUaeiou", word)``.

    >>> map(lambda w: len(filter(lambda c: c in "AEIOUaeiou", w)), sent)
    [1, 1, 2, 1, 1, 3]

Making Copies
-------------

Variables as named locations:

    >>> a = 'cat'
    >>> b = a
    >>> b
    'cat'

|nopar|
If we modify ``a``, ``b`` is unchanged:

    >>> a = 'dog'
    >>> b
    'cat'

What happens with lists?  

    >>> sent1 = ['the', 'cat']
    >>> sent2 = sent1
    >>> sent2
    ['the', 'cat']

|nopar|
So far so good, that's exactly what we expected.
Now let's modify ``a``:

    >>> sent1[1] = 'dog'
    >>> sent1
    ['the', 'dog']
    >>> sent2
    ['the', 'dog']

Why has ``b`` changed, when we didn't touch it?  To understand why, we need to
know how lists are stored in the computer's memory.  [Arrays]

.. _array-memory:
.. figure:: ../images/array-memory.png
   :scale: 20

   Arrays and Computer Memory

|nopar|
Deep copy:

    >>> sent2 = sent1[:]

Strings and Formats
-------------------

We have seen that there are two ways to display the contents of an object:

    >>> word = 'cat'
    >>> print word
    cat
    >>> word
    'cat'

The first of these is Python's attempt to produce the most human-readable form of an object.
The second method |mdash| naming the variable at a prompt |mdash| shows us a string
that can be used to recreate this object.  It is important to keep in mind that both of
these are just strings, displayed for the benefit of you the user.  They do not give
us any clue as to the actual internal representation of the object.

There are many other useful ways to display an object as a string of
characters.  This may be for the benefit of a human reader, or because
we want to `export`:dt: our data to a particular file format for use
in an external program.

Formatted output typically contains a combination of variables and
pre-specified strings, e.g. given a dictionary ``wordcount``
consisting of words and their frequencies we could do:

    >>> wordcount = {'cat':3, 'dog':4, 'snake':1}
    >>> for word in wordcount:
    ...     print word, '->', wordcount[word], ';',
    dog -> 4 ; cat -> 3 ; snake -> 1

Apart from the problem of unwanted whitespace, such print statements
alternating variables and constants can be difficult to read and
maintain.  Instead:

    >>> for word in wordcount:
    ...    print '%s->%d;' % (word, wordcount[word]),
    dog->4; cat->3; snake->1
    
Example: Tabulation
-------------------

An important use of formatting strings is for tabulating data.  The
following code iterates over the fifteen genres of the Brown Corpus
(accessed using ``brown.items()``).  Each of these is tokenized in
turn.  The next step is to check if the token has the ``md`` tag.  For
each of these words we increment a count.  This uses the conditional
frequency distribution, where the condition is the current genre, and
the event is the modal.

    >>> from nltk_lite.probability import ConditionalFreqDist
    >>> cfdist = ConditionalFreqDist()
    >>> for genre in brown.items:                  # each genre
    ...     for sent in brown.tagged(genre):       # each sentence
    ...         for (word,tag) in sent:            # each tagged token
    ...             if tag == 'md':                # found a modal
    ...                  cfdist[genre].inc(word.lower())

The conditional frequency distribution is nothing more than a mapping
from each genre to the distribution of modals in that genre.  The
following code fragment identifies a small set of modals of interest,
and processes the data structure to output the required counts.

    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> print '%-18s' % 'Genre', ' '.join([('%6s' % m) for m in modals])
    Genre                 can  could    may  might   must   will
    >>> for genre in cfdist.conditions():    # generate rows
    ...     print '%-18s' % brown.item_name[genre],
    ...     for modal in modals:
    ...         print '%6d' % cfdist[genre].count(modal),
    ...     print
    press: reportage       94     86     66     36     50    387
    press: reviews         44     40     45     26     18     56
    press: editorial      122     56     74     37     53    225
    skill and hobbies     273     59    130     22     83    259
    religion               84     59     79     12     54     64
    belles-lettres        249    216    213    113    169    222
    popular lore          168    142    165     45     95    163
    miscellaneous: gov    115     37    152     13     99    237
    fiction: general       39    168      8     42     55     50
    learned               366    159    325    126    202    330
    fiction: science       16     49      4     12      8     16
    fiction: mystery       44    145     13     57     31     17
    fiction: adventure     48    154      6     58     27     48
    fiction: romance       79    195     11     51     46     43
    humor                  17     33      8      8      9     13

There are some interesting patterns in this table.  For instance,
compare the rows for government literature and adventure literature;
the former is dominated by the use of ``can, may, must, will`` while
the latter is characterised by the use of ``could`` and ``might``.
With some further work it might be possible to guess the genre of a
new text automatically, according to its distribution of modals.

Example: Concordance Display
----------------------------

In this section we see another example of formatted output.  So far our
formatting strings have contained specifications of fixed width, such
as ``%6s``, a string that is padded to width 6.  Let's experiment
with this for a moment to make sure we remember the details:

    >>> '%6s' % 'dog'
    '   dog'
    >>> '%-6s' % 'dog'
    'dog   '
    >>> '%6s' % 'giraffe'
    'giraffe'

Sometimes we don't know how wide a displayed value should be in advance.
We can replace the digit specifier with an asterisk:

    >>> width = 6
    >>> '%-*s' % (width, 'dog')
    'dog   '

We use the left/right justification of strings, and the variable width, in the
following example, to get vertical alignment of a variable-width window:

    >>> context = 32
    >>> word = 'line'
    >>> for sent in brown.raw('a'):
    ...     try:
    ...         pos = sent.index(word)
    ...         left = ' '.join(sent[:pos])
    ...         right = ' '.join(sent[pos+1:])
    ...         print '%*s %s %-*s' % (context, left[-context:], word, context, right[:context])
    ...     except ValueError:
    ...         pass
    ce , is today closer to the NATO line .                               
    n more activity across the state line in Massachusetts than in Rhode I
     , gained five yards through the line and then uncorked a 56-yard touc
                     `` Our interior line and out linebackers played excep
    k then moved Cooke across with a line drive to left .                 
    chal doubled down the rightfield line and Cooke singled off Phil Shart
                  -- Billy Gardner's line double , which just eluded the d
               -- Nick Skorich , the line coach for the football champion 
                         Maris is in line for a big raise .               
    uld be impossible to work on the line until then because of the large 
             Murray makes a complete line of ginning equipment except for 
        The company sells a complete line of gin machinery all over the co
    tter Co. of Sherman makes a full line of gin machinery and equipment .
    fred E. Perlman said Tuesday his line would face the threat of bankrup
     sale of property disposed of in line with a plan of liquidation .    
     little effort spice up any chow line .                               
    es , filed through the cafeteria line .                               
    l be particularly sensitive to a line between first and second class c
    A skilled worker on the assembly line , for example , earns $37 a week

Exercises
---------

#. Find out more about sequence objects using Python's help facility.
   In the interpreter, type ``help(str)``, ``help(list)``, and ``help(tuple)``.
   This will provide a full list of the functions supported by each type.
   Some functions have special names flanked with underscore; as the
   help documentation shows, each such function corresponds to something
   more familiar.  For example ``x.__getitem__(y)`` is just a long-winded
   way of saying ``x[y]``.

#. Write code that removes whitespace at the beginning and end of a
   string, and normalizes whitespace between words to be a single
   space character.

   #) do this task using ``split()`` and ``join()``

   #) do this task using regular expression substitutions


#. Write code that takes a list of words (containing duplicates) and
   returns a list of words (with no duplicates) sorted by decreasing frequency.
   E.g. if the input list contained 10 instances of the word ``table`` and 9 instances
   of the word ``chair``, then ``table`` would appear before ``chair`` in the output
   list.

#. In this section we saw examples of some special functions such as ``filter()`` and
   ``map()``.  Other functions in this family are ``zip()`` and ``reduce()``.
   Find out what these do, and write some code to try them out.
   What uses might they have in language processing?

#. Dictionaries in formatting strings...


---------
Functions
---------

Once you have been programming for a while, you will find that you need
to perform a task that you have done in the past.  In fact, over time,
the number of completely novel things you have to do in creating a program
decreases significantly.  Half of the work may involve simple tasks that
you have done before.  Thus it is important for you code to be *re-usable*.
One effective way to do this is to abstract commonly used sequences of steps
into a `function`:dt:.

For example, suppose we find that we often want to read text from an HTML file.
This involves several steps: opening the file, reading it in, normalizing
whitespace, and stripping HTML markup.  We can collect these steps into a
function, and give it a name such as ``get_text()``:

    >>> import re
    >>> def get_text(file):
    ...     "Read text from a file, normalizing whitespace and stripping HTML markup"
    ...     text = open(file).read()
    ...     text = re.sub('\s+', ' ', text)
    ...     text = re.sub(r'<.*?>', ' ', text)
    ...     return text

Now, any time we want to get cleaned-up text from an HTML file, we can just call
``get_text()`` with the name of the file as its only argument.  It will return
a string, and we can assign this to a variable, e.g.:
``contents = get_text("test.html")``.  Each time we want to use this series of
steps we only have to call the function.

Using functions has the benefit of saving space in our program.  More
importantly, our choice of name for the function helps make the program *readable*.
In the case of the above example, whenever our program needs to read cleaned-up
text from a file we don't have to clutter the program with 4 lines of code, we
simply need to call ``get_text()``.  This naming helps to provide some "semantic
interpretation" |mdash| it helps a reader of our program to see what the program "means".

Notice that the above function definition contains a string.  The first string inside
a function definition is called a `docstring`:dt:.  Not only does it document the
purpose of the function to someone reading the code, it is accessible to a programmer
who has loaded the code from a file:

    >>> help(get_text)
    get_text(file)
        Read text from a file, normalizing whitespace and stripping HTML markup

We have seen that functions help to make our work reusable and readable.  They
also help make it *reliable*.  When we re-use code that has already been developed
and tested, we can be more confident that it handles a variety of cases correctly.
We also remove the risk that we forget some important step, or introduce a bug.
The program which calls our function also has increased reliability.  The author
of that program is dealing with a shorter program, and its components have a
transparent semantics.

Named Arguments
---------------

One of the difficulties in re-using functions is remembering the order of arguments.
Consider the following function, which finds the ``n`` most frequent words that are
at least ``min_len`` characters long:

    >>> from nltk_lite.probability import FreqDist
    >>> from nltk_lite import tokenize
    >>> def freq_words(file, min, num):
    ...     freqdist = FreqDist()
    ...     text = open(file).read()
    ...     for word in tokenize.wordpunct(text):
    ...         if len(word) >= min:
    ...             freqdist.inc(word)
    ...     return freqdist.sorted_samples()[:num]
    >>> freq_words('programming.txt', 4, 10)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']

This function has three arguments.  It follows the convention of listing the most
basic and substantial argument first (the file).  However, it might be hard to remember
the order of the second and third arguments on subsequent use.  We can make this function
more readable by using `keyword arguments`:dt:.  These appear in the function's argument
list with an equals sign and a default value:

    >>> def freq_words(file, min=1, num=10):
    ...     freqdist = FreqDist()
    ...     text = open(file).read()
    ...     for word in tokenize.wordpunct(text):
    ...         if len(word) >= min:
    ...             freqdist.inc(word)
    ...     return freqdist.sorted_samples()[:num]

|nopar|
Now there are several equivalent ways to call this function:

    >>> freq_words('programming.txt', 4, 10)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']
    >>> freq_words('programming.txt', min=4, num=10)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']
    >>> freq_words('programming.txt', num=10, min=4)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']

|nopar|
When we use IDLE, simply typing the name of a function at the command prompt will
list the arguments.  Using named arguments helps someone to re-use the code...

A side-effect of having named arguments is that they permit optionality.  Thus we
can leave out any arguments for which we are happy with the default value.

    >>> freq_words('programming.txt', min=4)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']
    >>> freq_words('programming.txt', 4)
    ['string', 'word', 'that', 'this', 'phrase', 'Python', 'list', 'words', 'very', 'using']

Another common use of optional arguments is to permit a flag, e.g.:

    >>> def freq_words(file, min=1, num=10, trace=False):
    ...     freqdist = FreqDist()
    ...     if trace: print "Opening", file
    ...     text = open(file).read()
    ...     if trace: print "Read in %d characters" % len(file)
    ...     for word in tokenize.wordpunct(text):
    ...         if len(word) >= min:
    ...             freqdist.inc(word)
    ...             if trace and freqdist.N() % 100 == 0: print "."
    ...     if trace: print
    ...     return freqdist.sorted_samples()[:num]


Accumulative Functions
----------------------

These functions start by initializing some storage, and iterate over
input to build it up, before returning some final object (a large structure
or aggregated result).  The standard way to do this is to initialize an
empty list, accumulate the material, then return the list:

    >>> def find_nouns(tagged_text):
    ...     nouns = []
    ...     for word, tag in tagged_text:
    ...         if tag[:2] == 'NN':
    ...             nouns.append(word)
    ...     return nouns

|nopar|
We can apply this function to some tagged text to extract the nouns:

    >>> tagged_text = [('the', 'DT'), ('cat', 'NN'), ('sat', 'VBD'),
    ...                ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]
    >>> find_nouns(tagged_text)
    ['cat', 'mat']

However, a superior way to do this is to define a `generator`:dt:

    >>> def find_nouns(tagged_text):
    ...     for word, tag in tagged_text:
    ...         if tag[:2] == 'NN':
    ...             yield word

|nopar|
The first time this function is called, it gets as far as the ``yield``
statement and stops.  The calling program gets the first word and does
any necessary processing.  Once the calling program is ready for another
word, execution of the function is continued from where it stopped, until
the next time it encounters a ``yield`` statement.

Let's see what happens when we call the function:

    >>> find_nouns(tagged_text)
    <generator object at 0x14b2f30>

|nopar|
We cannot call it directly.  Instead, we can convert it to a list.

    >>> list(find_nouns(tagged_text))
    ['cat', 'mat']

|nopar|
We can also iterate over it in the usual way:

    >>> for noun in find_nouns(tagged_text):
    ...     print noun,
    cat mat

Some Important Subtleties
-------------------------


* call by reference vs call by name

    >>> def set_up(word, properties):
    ...     word = 'cat'
    ...     properties['pos'] = 'noun'
    >>> w = ''
    >>> p = {}
    >>> set_up(w, p)
    >>> w
    ''
    >>> p
    {'pos': 'noun'}


* return
* need for consistent type
* avoid deeply nested returns


* function pointers

    >>> words = 'I turned off the spectroroute'.split()
    >>> words.sort(cmp)
    >>> words
    ['I', 'off', 'spectroroute', 'the', 'turned']
    >>> words.sort(lambda x, y: cmp(len(y), len(x)))
    >>> words
    ['spectroroute', 'turned', 'off', 'the', 'I']

This is inefficient when the list of items gets long, as
we compute ``len()`` twice for every comparison (about 2nlog(n) times).
The following is more efficient:

    >>> [pair[1] for pair in sorted((len(w), w) for w in words)[::-1]]
    ['spectroroute', 'turned', 'the', 'off', 'I']

This technique is called `decorate-sort-undecorate`:dt:.



NOW: doing this with functions...

Recursive Functions
-------------------

Iterative solution:

    >>> def selection(a):
    ...     for i in range(len(a) - 1):
    ...         min = i
    ...         for j in range(i+1, len(a)):
    ...             if a[j] < a[min]:
    ...                 min = j
    ...         a[min],a[i] = a[i],a[min]
    ...     return count


    >>> def factorial(n):
    ...     result = 1
    ...     for i in range(n+1):
    ...         result *= i
    ...     return result

Recursive solution (base case, induction step)

    >>> def factorial(n):
    ...     if n == 1:
    ...         return n
    ...     else:
    ...         return n * factorial(n-1)

Generating all permutations of words, to check which ones are
grammatical:

    >>> def perms(seq):
    ...     if len(seq) <=1:
    ...         yield seq
    ...     else:
    ...         for perm in perms(seq[1:]):
    ...             for i in range(len(perm)+1):
    ...                 yield perm[:i] + seq[0:1] + perm[i:]
    >>> list(perms(['police', 'fish', 'cream']))
    [['police', 'fish', 'cream'], ['fish', 'police', 'cream'],
     ['fish', 'cream', 'police'], ['police', 'cream', 'fish'],
     ['cream', 'police', 'fish'], ['cream', 'fish', 'police']]

Deeply Nested Objects
---------------------

We can use recursive functions to build deeply-nested objects.

Building a letter trie.


    >>> def insert(trie, key, value):
    ...     if key:
    ...         first, rest = key[0], key[1:]
    ...         if first not in trie:
    ...             trie[first] = {}
    ...         insert(trie[first], rest, value)
    ...     else:
    ...         trie['value'] = value


Initialize and insert, then display

    >>> trie = {}
    >>> insert(trie, 'chat', 'cat')
    >>> insert(trie, 'chien', 'dog')
    >>> trie['c']['h']
    {'a': {'t': {'value': 'cat'}}, 'i': {'e': {'n': {'value': 'dog'}}}}
    >>> trie['c']['h']['a']['t']['value']
    'cat'

Also pretty-print entire object:

    >>> pprint(trie)
    {'c': {'h': {'a': {'t': {'value': 'cat'}},
                 'i': {'e': {'n': {'value': 'dog'}}}}}}




Functional Decomposition
------------------------

Well-structured programs often make extensive use of functions.  Often
when a block of program code grows longer than a screenful, it is a
great help to readability if it is decomposed into one or more
functions.


Example -- function does two tasks: creates freqdist and returns list.
Function should do one thing only.

    >>> def freq_words(file, freqdist, n):
    ...     text = open(file).read()
    ...     for word in text.split():
    ...         freqdist.inc(word)
    ...     return freqdist.sorted_samples()[:n]



Errors -- how a function reports the fact that it could not accomplish its task.



Exercises
---------

#. Write a recursive function ``lookup(trie, key)`` that looks up a key in a trie,
   and returns the value it finds.

#. Write a recursive function that pretty prints a trie in alphabetically
   sorted order, as follows

   chat: 'cat'
   --ien: 'dog'
   -???: ???


-----
Trees
-----

In this part of the book we will be investigating the constituent
structure of sentences, and we will see how these can be represented
using syntactic trees.  Here we will look at tree structures in NLTK.

A tree is a set of connected nodes, each of which is labeled with a
category.  It common to use a 'family' metaphor to talk about the
relationships of nodes in a tree: for example, `S`:gc: is the
`parent`:dt: of `VP`:gc:; conversely `VP`:gc: is a `daughter`:dt: (or
`child`:dt:) of `S`:gc:.  Also, since `NP`:gc: and `VP`:gc: are both
daughters of `S`:gc:, they are also `sisters`:dt:. 
Here is an example of a tree:

.. ex::
  .. tree:: (S (NP Lee) (VP (V saw) (NP the dog)))

Although it is helpful to represent trees in a graphical format, for
computational purposes we usually need a more text-oriented
representation. One standard method is to use a combination of bracket
and labels to indicate the structure, as shown here:

.. doctest-ignore::
      (S 
         (NP  'Lee')
         (VP 
            (V 'saw')
            (NP 
               (Det 'the')
               (N  'dog'))))

The conventions for displaying trees in NLTK are similar:

.. doctest-ignore::
      (S: (NP: 'Lee') (VP: (V: 'saw') (NP: 'the' 'dog')))

In such trees, the node value is a string containing the tree's
constituent type (e.g., `NP`:gc: or `VP`:gc:), while the children encode
the hierarchical contents of the tree [#]_.

.. [#] Although the ``Tree`` class is usually used for encoding
   syntax trees, it can be used to encode *any* homogeneous hierarchical
   structure that spans a text (such as morphological structure or
   discourse structure).  In the general case, leaves and node values do
   not have to be strings.

The Tree Interface
------------------

Trees are created with the ``Tree`` constructor, which takes a
node value and a list of zero or more children.  Here's an example of
a simple NLTK tree with a single child node, where the latter is a token:

    >>> from nltk_lite.parse.tree import Tree
    >>> tree1 = Tree('NP', ['John'])
    >>> print tree1
    (NP: 'John')

Here is an example with two children:

    >>> tree2 = Tree('NP', ['the', 'man'])
    >>> print tree2
    (NP: 'the' 'man')

Finally, here is a more complex example, where one of the
children is itself a tree:

    >>> tree3 = Tree('VP', ['saw', tree2])
    >>> print tree3
    (VP: 'saw' (NP: 'the' 'man'))

A tree's root node value is accessed with the ``node`` property, and
its leaves are accessed with the ``leaves()`` method:

    >>> tree3.node
    'VP'
    >>> tree3.leaves()
    ['saw', 'the', 'man']

One common way of defining the subject of a sentence `S`:gc: in
English is as *the noun phrase that is the daughter of* `S`:gc: *and
the sister of* `VP`:gc:. Although we cannot access subjects directly,
in practice we can get something similar by using `tree
positions`:dt:. Consider ``tree4`` defined as follows:

    >>> tree4 = Tree('S', [tree1, tree3])
    >>> print tree4
    (S: (NP: 'John') (VP: 'saw' (NP: 'the' 'man')))

Now we can just use indexing to access the subtrees of this tree:

    >>> print tree4[0]
    (NP: 'John')
    >>> print tree4[1]
    (VP: 'saw' (NP: 'the' 'man'))

Since the value of ``tree4[1]`` is itself a tree, we can index into that as well:

    >>> print tree4[1][0]
    saw
    >>> print tree4[1][1]
    (NP: 'the' 'man')

The printed representation for complex trees can be difficult to read.
In these cases, the ``draw`` method can be very useful. 

.. doctest-ignore::
    >>> tree3.draw()

This method opens a new window, containing a graphical representation
of the tree:

.. image:: ../images/parse_draw.png
   :scale: 70

The tree display window allows you to zoom in and out;
to collapse and expand subtrees; and to print the graphical
representation to a postscript file (for inclusion in a document).

To compare multiple trees in a single window, we can use the
``draw_trees()`` method:

.. doctest-ignore::
    >>> from nltk_lite.draw.tree import draw_trees
    >>> draw_trees(tree1, tree2, tree3)

The ``Tree`` class implements a number of other useful methods.  See
the ``Tree`` help documentation for more details, i.e. import
the Tree class and then type ``help(Tree)``.

The ``nltk_lite.corpora`` module defines the ``treebank`` corpus,
which contains a collection of hand-annotated parse trees for English
text, derived from the Penn Treebank.

    >>> from nltk_lite.corpora import treebank, extract
    >>> print extract(0, treebank.parsed())
    (S:
      (NP-SBJ:
        (NP: (NNP: 'Pierre') (NNP: 'Vinken'))
        (,: ',')
        (ADJP: (NP: (CD: '61') (NNS: 'years')) (JJ: 'old'))
        (,: ','))
      (VP:
        (MD: 'will')
        (VP:
          (VB: 'join')
          (NP: (DT: 'the') (NN: 'board'))
          (PP-CLR:
            (IN: 'as')
            (NP: (DT: 'a') (JJ: 'nonexecutive') (NN: 'director')))
          (NP-TMP: (NNP: 'Nov.') (CD: '29'))))
      (.: '.'))

We will take a closer look at treebanks in |Parsing|.


Recursion on Trees
------------------

1. recurse over tree to display in some useful way (e.g. whitespace formatting)

    >>> def indent_tree(t, level=0, first=False, width=8):
    ...     if not first:
    ...         print ' '*(width+1)*level,
    ...     try:
    ...         print "%-*s" % (width, t.node),
    ...         indent_tree(t[0], level+1, first=True)
    ...         for child in t[1:]:
    ...             indent_tree(child, level+1, first=False)
    ...     except AttributeError:
    ...         print t
    >>> t = extract(0, treebank.parsed())
    >>> indent_tree(t)
     S        NP-SBJ   NP       NNP      Pierre
                                NNP      Vinken
                       ,        ,
                       ADJP     NP       CD       61
                                         NNS      years
                                JJ       old
                       ,        ,
              VP       MD       will
                       VP       VB       join
                                NP       DT       the
                                         NN       board
                                PP-CLR   IN       as
                                         NP       DT       a
                                                  JJ       nonexecutive
                                                  NN       director
                                NP-TMP   NNP      Nov.
                                         CD       29
              .        .


2. recurse over tree to look for coordinate constructions (cf 4th
   example in chapter 1.1)

   (possible extension: callback function for Tree.subtrees())

3. generate new dependency tree from a phrase-structure tree



Exercises
---------

#. Write a recursive function to traverse a tree and return the
   depth of the tree, such that a tree with a single node would have
   depth zero.  (Hint: the depth of a subtree is the maximum depth
   of its children, plus one.)

#. Write a recursive function that produces a nested bracketing for
   a tree, leaving out the leaf nodes, and displaying the non-terminal
   labels after their subtrees.  So the above example about Pierre
   Vinken would produce:
   ``[[[NNP NNP]NP , [ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR [NNP CD]NP-TMP]VP .]S``
   Consecutive categories should be separated by space.

#. Write a recursive function to produce an XML representation for a
   tree, with non-terminals represented as XML elements, and leaves represented
   as text content, e.g.:

|   <S>
|     <NP type="SBJ">
|       <NP>
|         <NNP>Pierre</NNP>
|         <NNP>Vinken</NNP>
|       </NP>
|       <COMMA>,</COMMA>
|       <ADJP>
|         <NP>
|           <CD>61</CD>
|           <NNS>years</NNS>
|         </NP>
|         <JJ>old</JJ>
|       <COMMA>,</COMMA>
|     </NP>
|     <VP>
|       <MD>will</MD>
|       <VP>
|         <VB>join</VB>
|         <NP>
|           <DT>the</DT>
|           <NN>board</NN>
|         </NP>
|         <PP type="CLR">
|           <IN>as</IN>
|           <NP>
|             <DT>a</DT>
|             <JJ>nonexecutive</JJ>
|             <NN>director</NN>
|           </NP>
|         </PP>
|         <NP type="TMP">
|           <NNP>Nov.</NNP>
|           <CD>29</CD>
|         </NP>
|       </VP>
|     </VP>
|     <PERIOD>.</PERIOD>
|   </S>


-------------------
XML and ElementTree
-------------------

* inspecting and processing XML
* example: find nodes matching some criterion and add an attribute

------------
Dictionaries
------------

* persistent storage
* dictionary keys and immutability
* indexing and space-time trade-offs (more efficient concordancing)

-------------------------
Writing Complete Programs
-------------------------

Classifying Words Automatically
-------------------------------

A tagged corpus can be used to *train* a simple classifier, which can
then be used to guess the tag for untagged words.  For each word, we
can count the number of times it is tagged with each tag.  For
instance, the word ``deal`` is tagged 89 times as ``nn`` and 41 times
as ``vb``.  On this evidence, if we were asked to guess the tag for
``deal`` we would choose ``nn``, and we would be right over two-thirds
of the time.  The following program performs this tagging task, when
trained on the "g" section of the Brown Corpus (so-called *belles
lettres*, creative writing valued for its aesthetic content).

    >>> from nltk_lite.corpora import brown
    >>> cfdist = ConditionalFreqDist()
    >>> for sentence in brown.tagged('g'):
    ...     for token in sentence:
    ...         word = token[0]
    ...         tag = token[1]
    ...         cfdist[word].inc(tag)
    >>> for word in "John saw 3 polar bears".split():
    ...     print word, cfdist[word].max()
    John np
    saw vbd
    3 cd-tl
    polar jj
    bears vbz
    
Note that ``bears`` was incorrectly tagged as the 3rd person singular
form of a verb, since this word appears more frequently as a verb than
a noun in esthetic writing.

A problem with this approach is that it creates a huge model, with an
entry for every possible combination of word and tag.  For certain
tasks it is possible to construct reasonably good models which are
tiny in comparison.  For instance, let's try to guess whether a verb
is a noun or adjective from the last letter of the word alone.  We can
do this as follows:

    >>> tokens = []
    >>> for sent in brown.tagged('g'):
    ...     for (word,tag) in sent:
    ...         if tag in ['nn', 'jj'] and len(word) > 3:
    ...             char = word[-1]
    ...             tokens.append((char,tag))
    >>> split = len(tokens)*9/10
    >>> train, test = tokens[:split], tokens[split:]
    >>> cfdist = ConditionalFreqDist()
    >>> for (char,tag) in train:
    ...     cfdist[char].inc(tag)
    >>> correct = total = 0
    >>> for (char,tag) in test:
    ...     if tag == cfdist[char].max():
    ...         correct += 1
    ...     total += 1
    >>> print correct*100/total
    71

This result of 71% is marginally better than the result of 65% that we
get if we assign the ``nn`` tag to every word.  We can inspect the
model to see which tag is assigned to a word given its final letter.
Here we learn that words which end in ``c`` or ``l`` are more likely
to be adjectives than nouns::

    >>> print [(c, cfdist[c].max()) for c in cfdist.conditions()]
    [('%', 'nn'), ("'", None), ('-', 'jj'), ('2', 'nn'), ('5', 'nn'),
     ('A', 'nn'), ('D', 'nn'), ('O', 'nn'), ('S', 'nn'), ('a', 'nn'),
     ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'), ('g', 'nn'),
     ('f', 'nn'), ('i', 'nn'), ('h', 'nn'), ('k', 'nn'), ('m', 'nn'),
     ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ('s', 'nn'),
     ('r', 'nn'), ('u', 'nn'), ('t', 'nn'), ('w', 'nn'), ('y', 'nn'),
     ('x', 'nn'), ('z', 'nn')]

Exercises
---------

#. **Classifying words automatically:**
   The program for classifying words as nouns or adjectives scored 71%.
   Try to come up with better conditions, to get the system to score 80% or better.

   a) Revise the condition to use a longer suffix of the word, such as
      the last two characters, or the last three characters.  What happens
      to the performance?  Which suffixes are diagnostic for adjectives?

   #) Explore other conditions, such as variable length prefixes of a
      word, or the length of a word, or the number of vowels in a word.

   #) Finally, combine multiple conditions into a tuple, and explore
      which combination of conditions gives the best result.

#. **Exploring text genres:**
   Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

---------------
Further Reading
---------------

.. include:: footer.txt
