.. -*- mode: rst -*-
.. include:: ../definitions.txt
.. include:: regexp-defns.txt

.. _chap-structured-programming:

===================================
6. Structured Programming in Python
===================================

------------
Introduction
------------

In Part I you had an intensive introduction to Python (Chapter chap-programming_)
followed by chapters on words, tags, and chunks containing may examples and
exercises that should have helped you apply and consolidate your Python
programming skills.
Here we will review key programming concepts and explain
many of the minor points that could easily trip you up.  More fundamentally,
we will introduce important concepts in structured programming that
help you write readable, well-organized programs that you and others will
be able to re-use.
As before, this chapter contains many working program fragments and
exercises which you should try yourself.

------------------
Back to the Basics
------------------

Let's begin by revisiting some of the fundamental operations and data structures
required for natural language processing in Python.  It is important to appreciate
several finer points in order to use them correctly.  We'll start with the most
innocuous operation of all, namely assignment.  Then we will look at sequence types
in detail.

.. _sec-assignment:

Assignment
----------

Python's assignment statement operates on `values`:em:.  Consider the following
code fragment:

    >>> word1 = 'Monty'
    >>> word2 = word1              # [_assignment1]
    >>> word1 = 'Python'           # [_assignment2]
    >>> word2
    'Monty'
    
This code shows that when we write ``word2 = word1`` in line assignment1_,
the value of ``word1`` (the string ``'Monty'``) is assigned to ``word2``.
This makes a copy, and when we overwrite
``word1`` with a new string ``'Python'`` in line assignment2_, the value
of ``word2`` is not affected.

An important subtlety of Python is that the "value" of a structured object
(such as a list) is actually a `reference`:em: to the object.  Thus line
assignment3_ assigns the reference of ``words1`` to the new variable ``words2``.
When we modify something inside ``words1`` on line assignment4_, we can see
that the contents of ``words2`` has also been changed.

    >>> words1 = ['Monty', 'Python']
    >>> words2 = words1            # [_assignment3]
    >>> words1[1] = 'Bodkin'       # [_assignment4]
    >>> words2
    ['Monty', 'Bodkin']
    
Thus line assignment3_ does not copy the contents of the
variable, only its "object reference".
To understand why, we need to
know how lists are stored in the computer's memory.
In Figure array-memory_, we see that a list ``sent1`` is
a reference to an object stored at location 3133 (which is
itself a series of pointers to other locations holding strings).
When we assign ``sent2 = sent1``, it is just the object reference
3133 that gets copied across.

.. _array-memory:
.. figure:: ../images/array-memory.png
   :scale: 20

   Lists and Computer Memory

Sequences: Strings, Lists and Tuples
------------------------------------

We have seen three kinds of sequence object: strings, lists, and tuples.
As sequences they have some common properties: they can be indexed and they have
a length:

    >>> string = 'I turned off the spectroroute'
    >>> words = ['I', 'turned', 'off', 'the', 'spectroroute']
    >>> pair = (6, 'turned')
    >>> string[2], words[3], pair[1]
    ('t', 'the', 'turned')
    >>> len(string), len(words), len(pair)
    (29, 5, 2)

|nopar|
We can iterate over a sequence ``s`` using the construct ``for item in s``.
We can iterate in sorted order using ``for item in sorted(s)``,
and we can eliminate duplicates using ``for item in set(s)`` or
``for item in sorted(set(s))``.
For convenience, we can convert between sequence types, e.g.
``tuple(s)`` converts a sequence into a tuple, and
``list(s)`` converts a sequence into a list.
We can convert a list of strings to a single string using the
``join`` function, e.g. ``':'.join(words)``.

Notice in the above code sample that we computed multiple values
on a single line, separated by commas.  Lines of comma-separated
expressions are actually just tuples.  We use tuples to spontaneously
aggregate items.  In the next example we use tuples to re-arrange the
contents of our list:

    >>> words[2], words[3], words[4] = words[3], words[4], words[2]
    >>> words    
    ['I', 'turned', 'the', 'spectroroute', 'off']

|nopar|
This is an idiomatic and readable way to move items inside a list.
It is equivalent to the following (traditional) way of doing such a
task that does not use tuples:

    >>> tmp = words[2]
    >>> words[2] = words[3]
    >>> words[3] = words[4]
    >>> words[4] = tmp

Notice that Python is usually able to detect the presence of a tuple simply by
seeing a comma.  In such cases the parentheses are optional.
For example, the tuple assignment statement in the previous
example does not contain parentheses
(because comma has higher `precedence`:dt: than assignment).
However, when we print a tuple, the parentheses are always displayed.

Combining different sequence types
----------------------------------

Let's combine our knowledge of these three sequence types, together with list
comprehensions, to perform the task of sorting the words in a string by
their length.

    >>> words = 'I turned off the spectroroute'.split()     # [_sequence1]
    >>> wordlens = [(len(word), word) for word in words]    # [_sequence2]
    >>> wordlens.sort()                                     # [_sequence3]
    >>> ' '.join(word for count, word in wordlens)          # [_sequence4]
    'I off the turned spectroroute'

Each of the above lines of code contains a significant feature.
Line sequence1_ demonstrates that a simple string is actually
an object with methods defined on it, such as ``split()``.
Line sequence2_ shows the construction of a list of tuples,
where each tuple consists of a number (the word length) and the
word, e.g. ``(3, 'the')``.  Line sequence3_ sorts the list,
modifying the list in-place.  Finally, line sequence4_ discards
the length information then joins the words back into a single string.
    
We began by talking about the commonalities in these sequence types,
but the above code illustrates important differences in their
roles.  First, strings appear at the beginning and the end: this is
typical in the context where our program is reading in some text and
producing output for us to read.  Lists and tuples are used in the
middle, but for different purposes.  A list is typically a sequence of
objects all having the *same type*, of *arbitrary length*.  We often
use lists to hold sequences of words.  In contrast,
a tuple is typically a collection of objects of *different types*, of
*fixed length*.  We often use a tuple to hold a `record`:dt:,
a collection of different `fields`:dt: relating to some entity.
This distinction between the use of lists and tuples is a bit subtle,
so here is another example:

    >>> lexicon = [
    ...     ('the', 'DT', ['Di:', 'D@']),
    ...     ('off', 'IN', ['Qf', 'O:f'])
    ... ]

|nopar|
Here, a lexicon is represented as a list because it is a collection of objects
of a single type |mdash| lexical entries |mdash| of no predetermined
length.  An individual entry is represented as a tuple because it is
a collection of objects with different interpretations, such as
the orthographic form, the part of speech, and the pronunciations
represented in the SAMPA system.
Note that these pronunciations are stored using a list (why?).

The distinction between lists and tuples has been described in terms of
usage.  However, there is a more fundamental difference: in Python,
lists are `mutable`:dt:, while tuples are `immutable`:dt:.  In other
words, lists can be modified, while tuples cannot.  Here are some of
the operations on lists which do in-place modification of the list.
None of these operations is permitted on a tuple, a fact you should
confirm for yourself.

    >>> lexicon.sort()
    >>> lexicon[1] = ('turned', 'VBD', ['t3:nd', 't3`nd'])
    >>> del lexicon[0]


More List Comprehensions
------------------------

List comprehensions are a convenient and readable way to express
list operations in Python, and they have a wide range of uses in natural language
processing.  In this section we will see some more examples.
The first one implements a sliding window to create a list of n-grams:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> n = 3
    >>> [sent[i:i+n] for i in range(len(sent)-n+1)]
    [['The', 'dog', 'gave'],
     ['dog', 'gave', 'John'],
     ['gave', 'John', 'the'],
     ['John', 'the', 'newspaper']]

|nopar|
This code takes successive overlapping slices of size ``n`` from ``sent``.
Pay particular attention to the range of the variable ``i``.

Another clever use of list comprehensions is for a kind of multiplication.
Here we generate all combinations of two determiners, two adjectives, and two nouns.
The list comprehension is split across three lines for readability.

    >>> [(dt,jj,nn) for dt in ('two', 'three')
    ...             for jj in ('old', 'blind')
    ...             for nn in ('men', 'mice')]
    [('two', 'old', 'men'), ('two', 'old', 'mice'), ('two', 'blind', 'men'),
     ('two', 'blind', 'mice'), ('three', 'old', 'men'), ('three', 'old', 'mice'),
     ('three', 'blind', 'men'), ('three', 'blind', 'mice')]

|nopar|
The above example contains three independent loops; we could have put them
in any order.  We can also have loops
with shared variables.  The next example iterates over all sentences in a
section of the Brown Corpus, and for each sentence, iterates over each word.

    >>> from nltk_lite.corpora import brown
    >>> [word for sent in brown.raw('a')
    ...     for word in sent
    ...     if len(word) == 17]
    ['September-October', 'Sheraton-Biltmore', 'anti-organization',
     'anti-organization', 'Washington-Oregon', 'York-Pennsylvania',
     'misunderstandings', 'Sheraton-Biltmore', 'neo-stagnationist',
     'cross-examination', 'bronzy-green-gold', 'Oh-the-pain-of-it',
     'Secretary-General', 'Secretary-General', 'textile-importing',
     'textile-exporting', 'textile-producing', 'textile-producing']

Another way to use loop variables is to ignore them!  This is
the standard method to build multidimensional structures.
For example, to build an array with *m* rows and *n* columns,
where each cell is a set, we would do the following:

    >>> from pprint import pprint
    >>> m, n = 4, 7
    >>> array = [[set() for i in range(n)] for j in range(m)]
    >>> array[3][5].add('foo')
    >>> pprint(array)
    [[set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set([]), set([])],
     [set([]), set([]), set([]), set([]), set([]), set(['foo']), set([])]]

Sometimes we use a list comprehension as part of a larger aggregation task.
In the following example we calculate the average length of words in part of the Brown Corpus.
Notice that we don't bother storing the list comprehension in a temporary
variable, but just pass it as an argument to the ``average()`` function.

    >>> from numpy import average
    >>> average(len(w) for sent in brown.raw('a') for w in sent)
    4.40154543827

[Generator expressions: when a list comprehension occurs as the argument
of a function we can leave out the brackets.]

Dictionaries
------------

* access by position vs access by name (tuples vs dictionaries)
* dictionary keys and immutability
* indexing and space-time trade-offs (more efficient concordancing)
* persistent storage (shelve)
* catching exceptions (dictionary addition)
* dictionary of sets (Shakespeare corpus)

Exercises
---------

#. |easy| Find out more about sequence objects using Python's help facility.
   In the interpreter, type ``help(str)``, ``help(list)``, and ``help(tuple)``.
   This will give you a full list of the functions supported by each type.
   Some functions have special names flanked with underscore; as the
   help documentation shows, each such function corresponds to something
   more familiar.  For example ``x.__getitem__(y)`` is just a long-winded
   way of saying ``x[y]``.

#. |easy| Find out how to create a tuple consisting of a single item.
   Hint: it is not ``(item)``.

#. |soso| Create a list of words and store it in a variable ``sent1``.
   Now assign ``sent2 = sent1``.  Modify one of the items in ``sent1``
   and verify that ``sent2`` has changed.
   
   a) Now try the same exercise but instead assign ``sent2 = sent1[:]``.
      Modify ``sent1`` again and see what happens to ``sent2``.  Explain.
   b) Now define ``text1`` to be a list of lists of strings (e.g. to
      represent a text consisting of multiple sentences.  Now assign
      ``text2 = text1[:]``, assign a new value to one of the words,
      e.g. ``text1[1][1] = 'Monty'``.  Check what this did to ``text2``.
      Explain.
   c) Load Python's ``deepcopy()`` function (i.e. ``from copy import deepcopy``),
      consult its documentation, and test that it makes a fresh copy of any
      object.

#. |soso| Write code that takes a list of words (containing duplicates) and
   returns a list of words (with no duplicates) sorted by decreasing frequency.
   E.g. if the input list contained 10 instances of the word ``table`` and 9 instances
   of the word ``chair``, then ``table`` would appear before ``chair`` in the output
   list.
   
#. |soso| We saw code to initialize a two-dimensional array of sets.  Write code
   to initialize an array ``word_vowels`` and process a list of words, adding each
   word to ``word_vowels[l][v]`` where ``l`` is the length of the word and ``v`` is
   the number of vowels it contains.

#. |soso| Initialize an n-by-m list of lists of empty strings using list
   multiplication, e.g. ``word_table = [[''] * n] * m``.  What happens
   when you set one of its values, e.g. ``word_table[1][2] = "hello"``?
   Explain why this happens.  Now write an expression using ``range()``
   to construct a list of lists, and show that it does not have this problem.

#. |soso| Write code that builds a dictionary of dictionaries of sets...

#. |easy| Does the method for creating a sliding window of n-grams
   behave correctly for the two limiting cases: `n`:math: = 1, and `n`:math: = ``len(sent)``?

[More exercises involving corpora]

------------------
Presenting Results
------------------

Often we write a program to report a single datum, such as a particular element
in a corpus that meets some complicated criterion, or a single summary statistic
such as a word-count or the performance of a tagger.  More often, we write a program
to produce a structured result, such as a tabulation of numbers or linguistic forms,
or a reformatting of the original data.  When the results to be presented are linguistic,
textual output is usually the most natural choice.  However, when the results are numerical,
it may be preferrable to produce graphical output.  In this section you will learn about
a variety of ways to present program output.

Strings and Formats
-------------------

We have seen that there are two ways to display the contents of an object:

    >>> word = 'cat'
    >>> print word
    cat
    >>> word
    'cat'

The first of these is Python's attempt to produce the most human-readable form of an object.
The second method |mdash| naming the variable at a prompt |mdash| shows us a string
that can be used to recreate this object.  It is important to keep in mind that both of
these are just strings, displayed for the benefit of you, the user.  They do not give
us any clue as to the actual internal representation of the object.

There are many other useful ways to display an object as a string of
characters.  This may be for the benefit of a human reader, or because
we want to `export`:dt: our data to a particular file format for use
in an external program.

Formatted output typically contains a combination of variables and
pre-specified strings, e.g. given a dictionary ``wordcount``
consisting of words and their frequencies we could do:

    >>> wordcount = {'cat':3, 'dog':4, 'snake':1}
    >>> for word in wordcount:
    ...     print word, '->', wordcount[word], ';',
    dog -> 4 ; cat -> 3 ; snake -> 1

Apart from the problem of unwanted whitespace, such print statements
alternating variables and constants can be difficult to read and
maintain.  Instead:

    >>> for word in wordcount:
    ...    print '%s->%d;' % (word, wordcount[word]),
    dog->4; cat->3; snake->1
    
Lining things up
----------------

So far our formatting strings have contained specifications of fixed width, such
as ``%6s``, a string that is padded to width 6.  Let's experiment
with this for a moment to make sure we remember the details:

    >>> '%6s' % 'dog'
    '   dog'
    >>> '%-6s' % 'dog'
    'dog   '
    >>> '%6s' % 'giraffe'
    'giraffe'

An important use of formatting strings is for tabulating data.
The code in Listing modal-tabulate_ iterates over the fifteen genres of the Brown Corpus
(accessed using ``brown.items()``).  Each of these is tokenized in
turn.  The next step is to check if the token has the ``md`` tag.  For
each of these words we increment a count.  This uses the conditional
frequency distribution, where the condition is the current genre, and
the event is the modal.
The conditional frequency distribution is nothing more than a mapping
from each genre to the distribution of modals in that genre.
Line modals-of-interest_
identifies a small set of modals of interest, and calls
the function ``tabulate()`` which
processes the data structure to output the required counts.

.. pylisting:: modal-tabulate
   :caption: Frequency of Modals in Different Sections of the Brown Corpus

    from nltk_lite.probability import ConditionalFreqDist
    def count_words_by_tag(t, genres):
        cfdist = ConditionalFreqDist()
        for genre in genres:                       # each genre
            for sent in brown.tagged(genre):       # each sentence
                for (word,tag) in sent:            # each tagged token
                    if tag == t:                   # found a word tagged t
                         cfdist[genre].inc(word.lower())
        return cfdist

    def tabulate(cfdist, words):
        print '%-18s' % 'Genre', ' '.join([('%6s' % w) for w in words])
        for genre in cfdist.conditions():               # for each genre
            print '%-18s' % brown.item_name[genre],     # print row heading
            for w in words:                             # for each word
                print '%6d' % cfdist[genre].count(w),   # print table cell
            print                                       # end the row

    >>> genres = ['a', 'd', 'e', 'h', 'n']
    >>> cfdist = count_words_by_tag('md', genres)
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']   # [_modals-of-interest]
    >>> tabulate(cfdist, modals)
    Genre                 can  could    may  might   must   will
    press: reportage       94     86     66     36     50    387
    skill and hobbies     273     59    130     22     83    259
    religion               84     59     79     12     54     64
    miscellaneous: gov    115     37    152     13     99    237
    fiction: adventure     48    154      6     58     27     48

There are some interesting patterns in the table produced by
Listing modal-tabulate_.  For instance,
compare the rows for government literature and adventure literature;
the former is dominated by the use of ``can, may, must, will`` while
the latter is characterised by the use of ``could`` and ``might``.
With some further work it might be possible to guess the genre of a
new text automatically, according to its distribution of modals.

Sometimes we don't know how wide a displayed value should be in advance.
We can replace the digit specifier with an asterisk:

    >>> width = 6
    >>> '%-*s' % (width, 'dog')
    'dog   '

We use the left/right justification of strings, and the variable width, in
Listing concordance_, to get vertical alignment of a variable-width window:

.. NOTE: the following would be better if the corpus was an argument, and if
   brown.raw() behaved like other raw() methods and generated words not lists

.. pylisting:: concordance
   :caption: Simple Concordance Display

    def concordance(word, context)
        for sent in brown.raw('a'):
            try:
                pos = sent.index(word)
                left = ' '.join(sent[:pos])
                right = ' '.join(sent[pos+1:])
                print '%*s %s %-*s' %\\
                    (context, left[-context:], word, context, right[:context])
            except ValueError:
                pass

    >>> concordance('line', 32)
    ce , is today closer to the NATO line .                               
    n more activity across the state line in Massachusetts than in Rhode I
     , gained five yards through the line and then uncorked a 56-yard touc
                     `` Our interior line and out linebackers played excep
    k then moved Cooke across with a line drive to left .                 
    chal doubled down the rightfield line and Cooke singled off Phil Shart
                  -- Billy Gardner's line double , which just eluded the d
               -- Nick Skorich , the line coach for the football champion 
                         Maris is in line for a big raise .               
    uld be impossible to work on the line until then because of the large 
             Murray makes a complete line of ginning equipment except for 
        The company sells a complete line of gin machinery all over the co
    tter Co. of Sherman makes a full line of gin machinery and equipment .
    fred E. Perlman said Tuesday his line would face the threat of bankrup
     sale of property disposed of in line with a plan of liquidation .    
     little effort spice up any chow line .                               
    es , filed through the cafeteria line .                               
    l be particularly sensitive to a line between first and second class c
    A skilled worker on the assembly line , for example , earns $37 a week

.. _sec-graphics:

Format Conversion
-----------------

* reading and writing files

Graphical Presentation
----------------------

So far we have focussed on textual presentation and the use of formatted print
statements to get output lined up in columns.  It is often very useful to display
numerical data in graphical form, since this often makes it easier to detect
patterns.  For example, in Listing modal-tabulate_ we saw a table of numbers
showing the frequency of particular modal verbs in the Brown Corpus, classified
by genre.  In Listing modal-plot_ we present the same information in graphical
format.  The output is shown in Figure modal_genre_ (a color figure in the online version).

[Explain pylab]

.. pylisting:: modal-plot
   :caption: Frequency of Modals in Different Sections of the Brown Corpus

    from pylab import *
    from nltk_lite.corpora import brown

    def bar_chart(cfdist, genres, words):
        ind = arange(len(words))
        colors = 'rgbcmyk' * 3
        width = 1.0 / (len(genres)+1)
        bar_groups = []
        for g in range(len(genres)):
            counts = [cfdist[genres[g]].count(w) for w in words]
            bars = bar(ind+g*width, counts, width, color=colors[g])
            bar_groups.append(bars)
        xticks(ind+width, words)
        legend([b[0] for b in bar_groups], [brown.item_name[g][:18] for g in genres], loc='upper left')
        ylabel('Frequency')
        title('Frequency of Six Modal Verbs by Genre')
        show()
        
    >>> genres = ['a', 'd', 'e', 'h', 'n']
    >>> cfdist = count_words_by_tag('md', genres)
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> bar_chart(cfdist, genres, modals)

.. _modal_genre:
.. figure:: ../images/modal_genre.png
   :scale: 30

   Bar Chart Showing Frequency of Modals in Different Sections of Brown Corpus

From the bar chart it is immediately obvious that `may`:lx: and `must`:lx: have
almost identical relative frequencies.  The same goes for `could`:lx: and `might`:lx:.

Exercises
---------

#. |easy| Write code that removes whitespace at the beginning and end of a
   string, and normalizes whitespace between words to be a single
   space character.

   #) do this task using ``split()`` and ``join()``

   #) do this task using regular expression substitutions

#. (Dictionaries in formatting strings...)

#. |soso| Listing baseline_tagger_ in Chapter chap-tag_ plotted a curve showing
   change in the performance of a lookup tagger as the model size was increased.
   Plot the performance curve for a unigram tagger, as the amount of training
   data is varied.

---------------------
Functions and Modules
---------------------

Once you have been programming for a while, you will find that you need
to perform a task that you have done in the past.  In fact, over time,
the number of completely novel things you have to do in creating a program
decreases significantly.  Half of the work may involve simple tasks that
you have done before.  Thus it is important for your code to be `re-usable`:em:.
One effective way to do this is to abstract commonly used sequences of steps
into a `function`:dt:.

For example, suppose we find that we often want to read text from an HTML file.
This involves several steps: opening the file, reading it in, normalizing
whitespace, and stripping HTML markup.  We can collect these steps into a
function, and give it a name such as ``get_text()``:

    >>> import re
    >>> def get_text(file):
    ...     "Read text from a file, normalizing whitespace and stripping HTML markup"
    ...     text = open(file).read()
    ...     text = re.sub('\s+', ' ', text)
    ...     text = re.sub(r'<.*?>', ' ', text)
    ...     return text

Now, any time we want to get cleaned-up text from an HTML file, we can just call
``get_text()`` with the name of the file as its only argument.  It will return
a string, and we can assign this to a variable, e.g.:
``contents = get_text("test.html")``.  Each time we want to use this series of
steps we only have to call the function.

Using functions has the benefit of saving space in our program.  More
importantly, our choice of name for the function helps make the program *readable*.
In the case of the above example, whenever our program needs to read cleaned-up
text from a file we don't have to clutter the program with four lines of code, we
simply need to call ``get_text()``.  This naming helps to provide some "semantic
interpretation" |mdash| it helps a reader of our program to see what the program "means".

Notice that the above function definition contains a string.  The first string inside
a function definition is called a `docstring`:dt:.  Not only does it document the
purpose of the function to someone reading the code, it is accessible to a programmer
who has loaded the code from a file:

    >>> help(get_text)
    get_text(file)
        Read text from a file, normalizing whitespace and stripping HTML markup

We have seen that functions help to make our work reusable and readable.  They
also help make it *reliable*.  When we re-use code that has already been developed
and tested, we can be more confident that it handles a variety of cases correctly.
We also remove the risk that we forget some important step, or introduce a bug.
The program which calls our function also has increased reliability.  The author
of that program is dealing with a shorter program, and its components have a
transparent semantics.

Checking Arguments
------------------

* defensive programming
* assert
* raising errors

Return Values
-------------

* return
* need for consistent type
* avoid deeply nested returns

Another common way to use tuples is in the return values of functions,
in those situations where we want to return multiple items of
different types.  Here we define a function that returns a tuple
consisting of the average word length of a sentence, and the inventory
of letters used in the sentence.

    >>> def proc_words(words):
    ...     avg_wordlen = sum(len(word) for word in words)/len(words)
    ...     chars_used = ''.join(sorted(set(''.join(words))))
    ...     return avg_wordlen, chars_used
    >>> proc_words(words)
    (5, 'Icdefhnoprstu')


An Important Subtlety
---------------------

Back in Section sec-assignment_ you saw that in Python, assignment works on values,
but that the value of a structured object is a reference to that object.  The same
is true for functions.  Python interprets function parameters as values (this is
known as `call-by-value`:dt:.  Consider the following function; it has two parameters,
both of which are modified inside the function.  We begin by assigning an empty string
to ``w`` and an empty dictionary to ``p``.  After calling the function, ``w`` is unchanged,
while ``p`` is changed:

    >>> def set_up(word, properties):
    ...     word = 'cat'
    ...     properties['pos'] = 'noun'
    >>> w = ''
    >>> p = {}
    >>> set_up(w, p)
    >>> w
    ''
    >>> p
    {'pos': 'noun'}
    
To understand why ``w`` was not changed, it is necessary to understand call-by-value.
When we called ``set_up(w, p)``, the value of ``w`` (an empty string) was assigned to
a new local variable ``word``.  Inside the function, the value of ``word`` was modified.
However, that had no affect on the external value of ``w``.  This parameter passing is
identical to the following sequence of assignments:

    >>> w = ''
    >>> word = w
    >>> word = 'cat'
    >>> w
    ''
 
Now, in the case of the structured object, matters are quite different.
When we called ``set_up(w, p)``, the value of ``p`` (an empty
dictionary) was assigned to a new local variable ``properties``.  Since the
value of ``p`` was an object reference, both variables reference the same
memory location.  Modifying something inside ``properties`` will also
change ``p``, just as if we had done the following sequence of assignments:
 
    >>> p = {}
    >>> properties = p
    >>> properties['pos'] = 'noun'
    >>> p
    {'pos': 'noun'}

Passing a Function as an Argument
---------------------------------

So far the arguments we have passed into functions have been simple objects like
strings, or structured objects like lists.  These arguments allow us to parameterise
the behavior of a function.  As a result, functions are very flexible and powerful
abstractions, permitting us to repeatedly apply the `same operation`:em: on `different data`:em:.
Python also lets us pass a function as
an argument to another function.  Now we can abstract out the operation, and apply
a `different operation`:em: on the `same data`:em:.  As the following examples show,
we can pass the built-in function ``len()`` or a user-defined function ``last_letter()``
as parameters to another function:

    >>> def extract_property(prop):
    ...     words = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    ...     return [prop(word) for word in words]
    >>> extract_property(len)
    [3, 3, 4, 4, 3, 9]
    >>> def last_letter(word):
    ...     return word[-1]
    >>> extract_property(last_letter)
    ['e', 'g', 'e', 'n', 'e', 'r']
    
Surprisingly, ``len`` and ``last_letter`` are objects that can be
passed around like lists and dictionaries.  Notice that parentheses
are only used after a function name if we are invoking the function;
when we are simply passing the function around as an object these are
not used.

Python provides us with one more way to define functions as arguments
to other functions, so-called `lambda expressions`:dt:.  Supposing there
was no need to use the above ``last_letter()`` function in multiple places,
we can equivalently write the following:
    
    >>> extract_property(lambda w: w[-1])
    ['e', 'g', 'e', 'n', 'e', 'r']
    
Here is an example of passing a function to the list sort method.
When we call ``sorted()`` with a single argument (the list to be sorted),
it uses the built-in lexicographic comparison function ``cmp()``.
However, we can supply our own sort function, e.g. to sort by decreasing
length.

    >>> words = 'I turned off the spectroroute'.split()
    >>> sorted(words)
    ['I', 'off', 'spectroroute', 'the', 'turned']
    >>> sorted(words, cmp)
    ['I', 'off', 'spectroroute', 'the', 'turned']
    >>> sorted(words, lambda x, y: cmp(len(y), len(x)))
    ['spectroroute', 'turned', 'off', 'the', 'I']

Earlier we saw an example of filtering out some items in a list comprehension.
Thus we can restrict the list to just the lexical words, using
``[word for word in sent if is_lexical(word)]``.  This is a little cumbersome
as it mentions the ``word`` variable three times.  A more compact way to say
the same thing is as follows:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> filter(is_lexical, sent)
    ['dog', 'gave', 'John', 'newspaper']

|nopar|
Here we have used the function itself as an argument.  Functions are ultimately
just another kind of object that can be passed around a program.  The ``filter()``
function applies its first argument (a function) to its second (a sequence),
applying the function to each item in turn, only passing it through if the function
returns true for that item.  Thus ``filter(f, seq)`` is equivalent to
``[item for item in seq if apply(f,item) == True]``.
    
Python provides some other helpful functions like ``filter()``.  Here is a simple
way to find the average length of a sentence in a section of the Brown Corpus:

    >>> average(map(len, brown.raw('a')))
    21.7461072664

|nopar|
This code applies the function (in this case ``len()``) to each item in the
sequence, and builds a new sequence containing the results.

Above we used the ``map()`` function to apply ``len()`` to
each item in a sequence.  Instead of ``len()``, we could have passed in any
other function we liked:

    >>> sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']
    >>> def is_vowel(letter):
    ...     return letter in "AEIOUaeiou"
    >>> def vowelcount(word):
    ...     return len(filter(is_vowel, word))
    >>> map(vowelcount, sent)
    [1, 1, 2, 1, 1, 3]

|nopar|
Instead of using filter to call a named function ``is_vowel``, we can
define a lambda expression as follows:

    >>> map(lambda w: len(filter(lambda c: c in "AEIOUaeiou", w)), sent)
    [1, 1, 2, 1, 1, 3]

Functional Decomposition
------------------------

Well-structured programs often make extensive use of functions.  Often
when a block of program code grows longer than a screenful, it is a
great help to readability if it is decomposed into one or more
functions.


Example -- function does two tasks: creates freqdist and returns list.
Function should do one thing only.

    >>> def freq_words(file, freqdist, n):
    ...     text = open(file).read()
    ...     for word in text.split():
    ...         freqdist.inc(word)
    ...     return freqdist.sorted_samples()[:n]



Errors -- how a function reports the fact that it could not accomplish its task.

Defensive programming: assert


.. MORE: block structure

Modules
-------

[Overview of Python modules]

Documentation
-------------

* some guidelines for literate programming (e.g. variable, function, module naming)
* documenting functions
* documenting modules

Scope
-----

[Explanation of variable scope in functions and modules]

* local variables
* global variables: introduces dependency on context, limits the reusability of a function

Exercises
---------

#. |soso| In this section we saw examples of some special functions such as ``filter()`` and
   ``map()``.  Other functions in this family are ``zip()`` and ``reduce()``.
   Find out what these do, and write some code to try them out.
   What uses might they have in language processing?
   
#. |soso| Import the ``itemgetter()`` function from the ``operator`` module in Python's
   standard library (i.e. ``from operator import itemgetter``).  Create a list
   ``words`` containing several words.  Now try calling:
   ``sorted(words, key=itemgetter(1))``, and ``sorted(words, key=itemgetter(-1))``.
   Explain what ``itemgetter()`` is doing.


-----
Trees
-----

In Part II we will be investigating the constituent
structure of sentences, and we will see how these can be represented
using syntactic trees.  Here we will look at tree structures.

A tree is a set of connected nodes, each of which is labeled with a
category.  It common to use a 'family' metaphor to talk about the
relationships of nodes in a tree: for example, `S`:gc: is the
`parent`:dt: of `VP`:gc:; conversely `VP`:gc: is a `daughter`:dt: (or
`child`:dt:) of `S`:gc:.  Also, since `NP`:gc: and `VP`:gc: are both
daughters of `S`:gc:, they are also `sisters`:dt:. 
Here is an example of a tree:

.. ex::
  .. tree:: (S (NP Lee) (VP (V saw) (NP the dog)))

Although it is helpful to represent trees in a graphical format, for
computational purposes we usually need a more text-oriented
representation. One standard method (used in the Penn Treebank)
is to use a combination of bracket
and labels to indicate the structure, as shown here:

.. doctest-ignore::
      (S 
         (NP  'Lee')
         (VP 
            (V 'saw')
            (NP 
               (Det 'the')
               (N  'dog'))))

|nopar|
The conventions for displaying trees in NLTK are similar:

.. doctest-ignore::
      (S: (NP: 'Lee') (VP: (V: 'saw') (NP: 'the' 'dog')))

In such trees, the node value is a string containing the tree's
constituent type (e.g., `NP`:gc: or `VP`:gc:), while the children encode
the hierarchical contents of the tree.

Although we will focus on syntactic trees, trees can be used to encode
`any`:em: homogeneous hierarchical structure that spans a sequence
of linguistic forms (e.g. morphological structure, discourse structure).
In the general case, leaves and node values do not have to be strings.

Trees in NLTK
-------------

Trees are created with the ``Tree`` constructor, which takes a
node value and a list of zero or more children.  Here's a couple of
simple trees:

    >>> from nltk_lite.parse import Tree
    >>> tree1 = Tree('NP', ['John'])
    >>> print tree1
    (NP: 'John')
    >>> tree2 = Tree('NP', ['the', 'man'])
    >>> print tree2
    (NP: 'the' 'man')

|nopar|
We can incorporate these into successively larger trees as follows:

    >>> tree3 = Tree('VP', ['saw', tree2])
    >>> tree4 = Tree('S', [tree1, tree3])
    >>> print tree4
    (S: (NP: 'John') (VP: 'saw' (NP: 'the' 'man')))

|nopar|
Here are some of the methods available for tree objects:

    >>> tree4[1]
    (VP: 'saw' (NP: 'the' 'man'))
    >>> tree4[1].node
    'VP'
    >>> tree.leaves()
    ['John', 'saw', 'the', 'man']
    >>> tree[1,1,0]
    'saw'

The printed representation for complex trees can be difficult to read.
In these cases, the ``draw`` method can be very useful. 
It opens a new window, containing a graphical representation
of the tree.  The tree display window allows you to zoom in and out;
to collapse and expand subtrees; and to print the graphical
representation to a postscript file (for inclusion in a document).

.. doctest-ignore::
    >>> tree3.draw()


.. image:: ../images/parse_draw.png
   :scale: 70

Treebanks
---------

The ``nltk_lite.corpora`` module defines the ``treebank`` corpus,
which contains a collection of hand-annotated parse trees for English
text, derived from the Penn Treebank.

    >>> from nltk_lite.corpora import treebank, extract
    >>> print extract(0, treebank.parsed())
    (S:
      (NP-SBJ:
        (NP: (NNP: 'Pierre') (NNP: 'Vinken'))
        (,: ',')
        (ADJP: (NP: (CD: '61') (NNS: 'years')) (JJ: 'old'))
        (,: ','))
      (VP:
        (MD: 'will')
        (VP:
          (VB: 'join')
          (NP: (DT: 'the') (NN: 'board'))
          (PP-CLR:
            (IN: 'as')
            (NP: (DT: 'a') (JJ: 'nonexecutive') (NN: 'director')))
          (NP-TMP: (NNP: 'Nov.') (CD: '29'))))
      (.: '.'))

We will take a closer look at treebanks in Chapter chap-parse_.

[Chinese Treebank example]


XML and ElementTree
-------------------

* inspecting and processing XML
* example: find nodes matching some criterion and add an attribute
* Shakespeare XML corpus example

Exercises
---------

#. |easy| The ``Tree`` class implements a variety of other useful methods.
   See the ``Tree`` help documentation for more details, i.e. import
   the Tree class and then type ``help(Tree)``.

#. |soso| To compare multiple trees in a single window, we can use the
   ``draw_trees()`` method.  Define some trees and try it out:

    >>> from nltk_lite.draw.tree import draw_trees
    >>> draw_trees(tree1, tree2, tree3)

#. |hard| One common way of defining the subject of a sentence `S`:gc: in
   English is as *the noun phrase that is the daughter of* `S`:gc: *and
   the sister of* `VP`:gc:.   Write a function that takes the tree for
   a sentence and returns the subtree corresponding to the subject of the
   sentence.  What should it do if the root node of the tree passed to
   this function is not `S`:gc:, or it lacks a subject?


---------
Recursion
---------

We first saw recursion in Chapter chap-words_, in a function that navigated
the hypernym hierarchy of WordNet...

Iterative solution:

    >>> def factorial(n):
    ...     result = 1
    ...     for i in range(n+1):
    ...         result *= i
    ...     return result

Recursive solution (base case, induction step)

    >>> def factorial(n):
    ...     if n == 1:
    ...         return n
    ...     else:
    ...         return n * factorial(n-1)

Generating all permutations of words, to check which ones are
grammatical:

    >>> def perms(seq):
    ...     if len(seq) <=1:
    ...         yield seq
    ...     else:
    ...         for perm in perms(seq[1:]):
    ...             for i in range(len(perm)+1):
    ...                 yield perm[:i] + seq[0:1] + perm[i:]
    >>> list(perms(['police', 'fish', 'cream']))
    [['police', 'fish', 'cream'], ['fish', 'police', 'cream'],
     ['fish', 'cream', 'police'], ['police', 'cream', 'fish'],
     ['cream', 'police', 'fish'], ['cream', 'fish', 'police']]

Deeply Nested Objects
---------------------

We can use recursive functions to build deeply-nested objects.

Building a letter trie, Listing trie_.

.. pylisting:: trie
   :caption: Building a Letter Trie

    def insert(trie, key, value):
        if key:
            first, rest = key[0], key[1:]
            if first not in trie:
                trie[first] = {}
            insert(trie[first], rest, value)
        else:
            trie['value'] = value

    >>> trie = {}
    >>> insert(trie, 'chat', 'cat')
    >>> insert(trie, 'chien', 'dog')
    >>> trie['c']['h']
    {'a': {'t': {'value': 'cat'}}, 'i': {'e': {'n': {'value': 'dog'}}}}
    >>> trie['c']['h']['a']['t']['value']
    'cat'
    >>> pprint(trie)
    {'c': {'h': {'a': {'t': {'value': 'cat'}},
                 'i': {'e': {'n': {'value': 'dog'}}}}}}


Recursion on Trees
------------------

1. recurse over tree to display in some useful way (e.g. whitespace formatting)

    >>> def indent_tree(t, level=0, first=False, width=8):
    ...     if not first:
    ...         print ' '*(width+1)*level,
    ...     try:
    ...         print "%-*s" % (width, t.node),
    ...         indent_tree(t[0], level+1, first=True)
    ...         for child in t[1:]:
    ...             indent_tree(child, level+1, first=False)
    ...     except AttributeError:
    ...         print t
    >>> t = extract(0, treebank.parsed())
    >>> indent_tree(t)
     S        NP-SBJ   NP       NNP      Pierre
                                NNP      Vinken
                       ,        ,
                       ADJP     NP       CD       61
                                         NNS      years
                                JJ       old
                       ,        ,
              VP       MD       will
                       VP       VB       join
                                NP       DT       the
                                         NN       board
                                PP-CLR   IN       as
                                         NP       DT       a
                                                  JJ       nonexecutive
                                                  NN       director
                                NP-TMP   NNP      Nov.
                                         CD       29
              .        .


2. recurse over tree to look for coordinate constructions (cf 4th
   example in chapter 1.1)

   (possible extension: callback function for Tree.subtrees())

3. generate new dependency tree from a phrase-structure tree



Exercises
---------

#. Write a recursive function to traverse a tree and return the
   depth of the tree, such that a tree with a single node would have
   depth zero.  (Hint: the depth of a subtree is the maximum depth
   of its children, plus one.)

#. Write a recursive function that produces a nested bracketing for
   a tree, leaving out the leaf nodes, and displaying the non-terminal
   labels after their subtrees.  So the above example about Pierre
   Vinken would produce:
   ``[[[NNP NNP]NP , [ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR [NNP CD]NP-TMP]VP .]S``
   Consecutive categories should be separated by space.

#. Write a recursive function to produce an XML representation for a
   tree, with non-terminals represented as XML elements, and leaves represented
   as text content, e.g.:

|   <S>
|     <NP type="SBJ">
|       <NP>
|         <NNP>Pierre</NNP>
|         <NNP>Vinken</NNP>
|       </NP>
|       <COMMA>,</COMMA>
|       <ADJP>
|         <NP>
|           <CD>61</CD>
|           <NNS>years</NNS>
|         </NP>
|         <JJ>old</JJ>
|       <COMMA>,</COMMA>
|     </NP>
|     <VP>
|       <MD>will</MD>
|       <VP>
|         <VB>join</VB>
|         <NP>
|           <DT>the</DT>
|           <NN>board</NN>
|         </NP>
|         <PP type="CLR">
|           <IN>as</IN>
|           <NP>
|             <DT>a</DT>
|             <JJ>nonexecutive</JJ>
|             <NN>director</NN>
|           </NP>
|         </PP>
|         <NP type="TMP">
|           <NNP>Nov.</NNP>
|           <CD>29</CD>
|         </NP>
|       </VP>
|     </VP>
|     <PERIOD>.</PERIOD>
|   </S>

#. Write a recursive function ``lookup(trie, key)`` that looks up a key in a trie,
   and returns the value it finds.

#. Write a recursive function that pretty prints a trie in alphabetically
   sorted order, as follows

   chat: 'cat'
   --ien: 'dog'
   -???: ???

-----------
Probability
-----------


-------------------------
Writing Complete Programs
-------------------------

Classifying Words Automatically
-------------------------------

A tagged corpus can be used to *train* a simple classifier, which can
then be used to guess the tag for untagged words.  For each word, we
can count the number of times it is tagged with each tag.  For
instance, the word ``deal`` is tagged 89 times as ``nn`` and 41 times
as ``vb``.  On this evidence, if we were asked to guess the tag for
``deal`` we would choose ``nn``, and we would be right over two-thirds
of the time.  The following program performs this tagging task, when
trained on the "g" section of the Brown Corpus (so-called *belles
lettres*, creative writing valued for its aesthetic content).

    >>> from nltk_lite.corpora import brown
    >>> cfdist = ConditionalFreqDist()
    >>> for sentence in brown.tagged('g'):
    ...     for token in sentence:
    ...         word = token[0]
    ...         tag = token[1]
    ...         cfdist[word].inc(tag)
    >>> for word in "John saw 3 polar bears".split():
    ...     print word, cfdist[word].max()
    John np
    saw vbd
    3 cd-tl
    polar jj
    bears vbz
    
Note that ``bears`` was incorrectly tagged as the 3rd person singular
form of a verb, since this word appears more frequently as a verb than
a noun in esthetic writing.

A problem with this approach is that it creates a huge model, with an
entry for every possible combination of word and tag.  For certain
tasks it is possible to construct reasonably good models which are
tiny in comparison.  For instance, let's try to guess whether a verb
is a noun or adjective from the last letter of the word alone.  We can
do this as follows:

    >>> tokens = []
    >>> for sent in brown.tagged('g'):
    ...     for (word,tag) in sent:
    ...         if tag in ['nn', 'jj'] and len(word) > 3:
    ...             char = word[-1]
    ...             tokens.append((char,tag))
    >>> split = len(tokens)*9/10
    >>> train, test = tokens[:split], tokens[split:]
    >>> cfdist = ConditionalFreqDist()
    >>> for (char,tag) in train:
    ...     cfdist[char].inc(tag)
    >>> correct = total = 0
    >>> for (char,tag) in test:
    ...     if tag == cfdist[char].max():
    ...         correct += 1
    ...     total += 1
    >>> print correct*100/total
    71

This result of 71% is marginally better than the result of 65% that we
get if we assign the ``nn`` tag to every word.  We can inspect the
model to see which tag is assigned to a word given its final letter.
Here we learn that words which end in ``c`` or ``l`` are more likely
to be adjectives than nouns::

    >>> print [(c, cfdist[c].max()) for c in cfdist.conditions()]
    [('%', 'nn'), ("'", None), ('-', 'jj'), ('2', 'nn'), ('5', 'nn'),
     ('A', 'nn'), ('D', 'nn'), ('O', 'nn'), ('S', 'nn'), ('a', 'nn'),
     ('c', 'jj'), ('b', 'nn'), ('e', 'nn'), ('d', 'nn'), ('g', 'nn'),
     ('f', 'nn'), ('i', 'nn'), ('h', 'nn'), ('k', 'nn'), ('m', 'nn'),
     ('l', 'jj'), ('o', 'nn'), ('n', 'nn'), ('p', 'nn'), ('s', 'nn'),
     ('r', 'nn'), ('u', 'nn'), ('t', 'nn'), ('w', 'nn'), ('y', 'nn'),
     ('x', 'nn'), ('z', 'nn')]

Exercises
---------

#. |soso| **Classifying words automatically:**
   The program for classifying words as nouns or adjectives scored 71%.
   Try to come up with better conditions, to get the system to score 80% or better.

   a) Revise the condition to use a longer suffix of the word, such as
      the last two characters, or the last three characters.  What happens
      to the performance?  Which suffixes are diagnostic for adjectives?

   #) Explore other conditions, such as variable length prefixes of a
      word, or the length of a word, or the number of vowels in a word.

   #) Finally, combine multiple conditions into a tuple, and explore
      which combination of conditions gives the best result.

#. |soso| **Exploring text genres:**
   Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?
   
#. |soso| Revisit your solutions to exercises in earlier chapters, and try to improve
   them, applying your new understanding of structured programming

---------------
Further Reading
---------------

.. include:: footer.txt
