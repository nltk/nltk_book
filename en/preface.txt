.. -*- mode: rst -*-
.. include:: ../definitions.txt

=======
Preface
=======

Most human knowledge |mdash| and most human communication |mdash| is
represented and expressed using language.  Language technologies
permit computers to process human language automatically.  Handheld
computers support predictive text and handwriting recognition.  Web
search engines give access to information locked up in unstructured
text.  By providing more natural human-machine interfaces, and more
sophisticated access to stored information, language processing has
come to play a central role in the multilingual information society.
This textbook provides a comprehensive introduction to the field of
natural language processing, covering the major techniques and theories.

--------
Audience
--------

This book is intended for people in the language sciences and the
information sciences who want to learn how to write programs that
analyze written language.  No prior knowledge of linguistics or
computer science is assumed.  Readers having a background in either
area can simply skip over some of the discussions.  The book provides
numerous worked examples and exercises, and can serve as the main text
for undergraduate and introductory graduate courses on natural
language processing or computational linguistics.

.. Once having finished this book, the reader will be ready to proceed
   with one of the more advanced NLP textbooks.

-----
Goals
-----

programming: if you don't program already, this book will teach you how

practical: learning by example, writing real programs, seeing the
  benefits of being able to test a grammar

principles: going beyond regular expression hackery, housekeeping etc,
  to cover algorithms, linguistic analyses, etc.

pragmatic: careful balance...

phun!: many applications and examples are interesting and entertaining


---------
Structure
---------

Part 1:

* introduces programming in Python
* sequence of chapters about recognising structure in text,
  starting with individual words, identifying parts of speech,
  and simple syntactic constituents
* final chapter of this part, on chunk parsing, can be skipped as
  it is not presumed in later chapters

Part 2:

* another chapter on more Python, as required for subsequent chapters
* focus on trees, grammars, parsing
* last chapter on probabilistic parsing can be skipped

Part 3:

* another chapter on programming
* a series of chapters on advanced topics, with no inter-dependencies
* to be used to fill out a course

Individual chapters have a common structure:

* exercises for each major section
* summary section, further reading
* later sections are sometimes optional (as marked -- how?)

---------
Exercises
---------

* inline examples are exercises
* end of each major section has exercises
* difficulty scheme

-----------------------
Typographic Conventions
-----------------------


--------------------
Electronic Resources
--------------------

NLTK website: layout; software downloads, documentation types,
downloadable examples, lecture slides, 3rd party contributions



------------
Teaching NLP
------------

Natural Language Processing (NLP) is often taught within the confines
of a single-semester course at advanced undergraduate level
or postgraduate level.  Unfortunately, it turns out to be rather
difficult to cover both the theoretical and practical sides of the
subject in such a short span of time.  Some courses focus on theory to
the exclusion of practical exercises, and deprive students of the
challenge and excitement of writing programs to automatically process
language.  Other courses are simply designed to teach
programming for linguists, and do not manage to cover any significant
NLP content.  The *Natural Language Toolkit*
was developed to address this problem, making
it feasible to cover a substantial amount of theory and practice
within a single-semester course, even if students have no prior
programming experience.
    
A significant fraction of any NLP syllabus covers fundamental
data structures and algorithms.  These are usually taught with the
help of formal notations and complex diagrams.  Large trees and charts
are copied onto the board and edited in tedious slow motion, or
laboriously prepared for presentation slides.  It is more effective
to use live demonstrations in which those diagrams are generated
and updated automatically.  NLTK provides interactive graphical user
interfaces, making it possible to view program state and to study
program execution step-by-step.  Most NLTK components have a
demonstration mode, and will perform an interesting task without
requiring any special input from the user.  It is even possible to
make minor modifications to programs in response to "what if"
questions.  In this way, students learn the mechanics of NLP quickly,
gain deeper insights into the data structures and algorithms, and
acquire new problem-solving skills.
    
NLTK supports assignments of varying difficulty and scope.  In the
simplest assignments, students experiment with existing components to
perform a wide variety of NLP tasks.  This may involve no programming
at all, in the case of the existing demonstrations, or simply changing
a line or two of program code.  As students become more familiar with
the toolkit they can be asked to modify existing components or to
create complete systems out of existing components.  NLTK also
provides students with a flexible framework for advanced projects,
such as developing a multi-component system, by integrating and
extending NLTK components, and adding on entirely new components.
Here NLTK helps by providing standard implementations of all the basic
data structures and algorithms, interfaces to standard corpora,
substantial corpus samples, and a flexible and extensible
architecture.  Thus, as we have seen, NLTK offers a fresh approach to
NLP pedagogy, in which theoretical content is tightly integrated with
application.
    
NLTK was originally created as part of a computational linguistics
course in the Department of Computer and Information Science at the
University of Pennsylvania in 2001.  Since then it has been developed
and expanded with the help of dozens of contributors.  It has now been
adopted in courses in dozens of universities, and serves as the basis
of many research projects.  Here we will expand on
the benefits of learning (and teaching) NLP using NLTK.

**Note on NLTK-Lite:**
Recently, the NLTK developers have been creating a lightweight version
NLTK, called NLTK-Lite.  NLTK-Lite is simpler and faster than NLTK.
Once it is complete, NLTK-Lite will provide the same functionality
as NLTK.  However, unlike NLTK, NLTK-Lite does not impose such a heavy
burden on the programmer.  Wherever possible, standard Python objects
are used instead of custom NLP versions, so that students learning to
program for the first time will be learning to program in Python with
some useful libraries, rather than learning to program in NLTK.

-------------------------------
The Python Programming Language
-------------------------------

NLTK is written in the Python language, a simple yet powerful
scripting language with excellent functionality for processing
linguistic data.  Python can be downloaded for free from
``http://www.python.org/``.  Here is a five-line Python program which takes
text input and prints all the words ending in ``ing``:

.. doctest-ignore::
    >>> import sys                          # load the system library
    >>> for line in sys.stdin.readlines():  # for each line of input
    ...     for word in line.split():       # for each word in the line
    ...         if word.endswith('ing'):    # does the word end in 'ing'?
    ...             print word              # if so, print the word

This program illustrates some of the main features of Python.  First,
whitespace is used to *nest* lines of code, thus the line starting
with ``if`` falls inside the scope of the previous line starting with
``for``, so the ``ing`` test is performed for each word.  Second,
Python is *object-oriented*; each variable is an entity which has
certain defined attributes and methods.  For example, ``line`` is more
than a sequence of characters.  It is a string object that has a
`method`:dt: (or operation) called ``split`` that we can use to break a line
into its words.  To apply a method to an object, we give the object
name, followed by a period, followed by the method name.  Third,
methods have *arguments* expressed inside parentheses.  For instance,
``split`` had no argument because we were splitting the string
wherever there was white space.  To split a string into sentences
delimited by a period, we could write ``split('.')``.  Finally, and
most importantly, Python is highly readable, so much so that it is
fairly easy to guess what the above program does even if you have
never written a program before.

We chose Python as the implementation language for NLTK because it has
a shallow learning curve, its syntax and semantics are transparent,
and it has good string-handling functionality.  As a scripting
language, Python facilitates interactive exploration.  As an
object-oriented language, Python permits data and methods to be
encapsulated and re-used easily.  Python comes with an extensive
standard library, including components for graphical programming,
numerical processing, and web data processing.

NLTK defines a basic infrastructure that can be used to build NLP
programs in Python.  It provides:

* Basic classes for representing data relevant to natural language
  processing.

* Standard interfaces for performing tasks, such
  as tokenization, tagging, and parsing.

* Standard implementations for each task, which
  can be combined to solve complex problems.

* Extensive documentation, including tutorials
  and reference documentation.

------------------
The Design of NLTK
------------------

NLTK was designed with six requirements in mind:

1. *Ease of use:* The primary purpose of the toolkit is to allow
   students to concentrate on building natural language processing
   systems.  The more time students must spend learning to use the
   toolkit, the less useful it is.  We have provided software
   distributions for several platforms, along with platform-specific
   instructions, to make the toolkit easy to install.
2. *Consistency:* We have made a significant effort to ensure that all
   the data structures and interfaces are consistent, making it easy to
   carry out a variety of tasks using a uniform framework.
3. *Extensibility:* The toolkit easily accommodates new components,
   whether those components replicate or extend existing functionality.
   Moreover, the toolkit is organized so that it is usually obvious where
   extensions would fit into the toolkit's infrastructure.
4. *Simplicity:* We have tried to provide an intuitive and appealing
   framework along with substantial building blocks, for students to gain
   a practical knowledge of NLP without getting bogged down in the
   tedious house-keeping usually associated with processing annotated
   language data.
5. *Modularity:* The interaction between different components of the
   toolkit uses simple, well-defined interfaces.  It is
   possible to complete individual projects using small parts of the
   toolkit, without needing to understand how they interact with the rest
   of the toolkit.  This allows students to learn how to use the toolkit
   incrementally throughout a course.  Modularity also makes it easier to
   change and extend the toolkit.
6. *Well-Documented:* The toolkit comes with substantial
   documentation, including nomenclature, data structures, and
   implementations.
    
Contrasting with these requirements are three non-requirements
|mdash| potentially useful features that we have deliberately avoided.
First, while the toolkit provides a wide range of functions, it is not
intended to be encyclopedic.  There should be a wide variety of ways
in which students can extend the toolkit.  Second, while the toolkit
should be efficient enough that students can use their NLP systems to
perform meaningful tasks, it does not need to be highly optimized for
runtime performance.  Such optimizations often involve more complex
algorithms, and sometimes require the use of C or C++.  This would make the
toolkit less accessible and more difficult to install.  Third, we have
avoided clever programming tricks, since clear implementations are far
preferable to ingenious yet indecipherable ones.
    
**NLTK Organization:**
NLTK is organized into a collection of task-specific packages.  Each
package is a combination of data structures for representing a
particular kind of information such as trees, and implementations of
standard algorithms involving those structures such as parsers.  This
approach is a standard feature of *object-oriented design*, in which
components encapsulate both the resources and methods needed to
accomplish a particular task.
    
The most fundamental NLTK components are for identifying and
manipulating individual words of text.  These include: ``tokenize``,
for breaking up strings of characters into word tokens; ``tag``, for
adding part-of-speech tags, including regular-expression taggers,
n-gram taggers and Brill taggers; and the Porter stemmer.
    
The second kind of module is for creating and manipulating structured
linguistic information.  These components include: ``tree``, for
representing and processing parse trees; ``featurestructure``, for
building and unifying nested feature structures (or attribute-value
matrices); ``cfg``, for specifying free grammars; and ``parse``, for
creating parse trees over input text, including chart parsers, chunk
parsers and probabilistic parsers.

Several utility components are provided to facilitate processing and
visualization.  These include: ``draw``, to visualize NLP structures
and processes; ``probability``, to count and collate events, and
perform statistical estimation; and ``corpora``, to access tagged
linguistic corpora.
    
.. Finally, several advanced components are provided, mostly
   demonstrating NLP applications of machine learning techniques.  These
   include: ``clusterer``, for discovering groups of similar items within
   a large collection, including k-means and expectation maximisation;
   ``classifier``, for categorising text into different types, including
   naive Bayes and maximum entropy; and ``hmm``, for Hidden Markov
   Models, useful for a variety of sequence classification tasks.
    
A further group of components is not part of NLTK proper.  These
are a wide selection of third-party contributions, often developed as
student projects at various institutions where NLTK is used, and
distributed in a separate package called *NLTK Contrib*.  Several of
these student contributions, such as the Brill tagger and the HMM
module, have now been incorporated into NLTK.  Although these
contributed components are not maintained, they may serve as a useful starting
point for future student projects.
    
In addition to software and documentation, NLTK provides substantial
corpus samples, listed below.  Many of these can be accessed using the
``corpora`` module, avoiding the need to write specialized file
parsing code before you can do NLP tasks.
    
=========================  =====================  ============================================================
Corpora and Corpus Samples Distributed with NLTK (starred items with NLTK-Lite)
--------------------------------------------------------------------------------------------------------------
Corpus                     Compiler               Contents
=========================  =====================  ============================================================
Brown Corpus               Francis, Kucera        15 genres, 1.15M words, tagged
CoNLL 2000 Chunking Data   Tjong Kim Sang         270k words, tagged and chunked
Genesis Corpus             Misc web sources       6 texts, 200k words, 6 languages
Project Gutenberg (sel)    Hart, Newby, et al     14 texts, 1.7M words
NIST 1999 Info Extr (sel)  Garofolo               63k words, newswire and named-entity SGML markup
Lexicon Corpus                                    Words, tags and frequencies from Brown Corpus and WSJ
Names Corpus               Kantrowitz, Ross       8k male and female names
PP Attachment Corpus       Ratnaparkhi            28k prepositional phrases, tagged as noun or verb modifiers
Presidential Addresses     Ahrens                 485k words, formatted text
Roget's Thesaurus          Project Gutenberg      200k words, formatted text
SEMCOR                     Rus, Mihalcea          880k words, part-of-speech and sense tagged
SENSEVAL 2 Corpus          Ted Pedersen           600k words, part-of-speech and sense tagged
Stopwords Corpus           Porter et al           2,400 stopwords for 11 languages
Penn Treebank (sel)        LDC                    40k words, tagged and parsed
TIMIT Corpus (sel)         NIST/LDC               audio files and transcripts for 16 speakers
Wordlist Corpus            OpenOffice.org et al   960k words and 20k affixes for 8 languages
=========================  =====================  ============================================================

**NLTK Website:**
All software, corpora, and documentation are freely downloadable from
``http://nltk.sourceforge.net/``.  Distributions are provided for
Windows, Macintosh and Unix platforms.  An ISO CD-ROM image,
containing all NLTK distributions, plus Python and WordNet
distributions, is also downloadable.

-----------------------------------
Relationship to Other NLP Textbooks
-----------------------------------

A variety of excellent NLP textbooks are available.  What sets our
materials apart from the others is the tight coupling of the chapters
and exercises with a toolkit, giving students |mdash| even those with no
prior programming experience |mdash| a practical introduction to NLP.
Once completing these materials, students will be ready to attempt
the more advanced textbook
*Foundations of Statistical Natural Language Processing*,
by Manning and Sch√ºtze (MIT Press,
2000).  Two other recent textbooks cover NLP together with speech processing:
*Speech and Language Processing*, by
Jurafsky and Martin (Prentice Hall, 2000),
and *Introducing Speech and Language Processing*
by Coleman (Cambridge, 2005).  While impressive for their
coverage, neither provides a uniform computational framework
so important for newcomers to NLP.
Hammond's book
*Programming for Linguists: Perl for Language Researchers*,
(Blackwell, 2003)
and a Java version, cover elementary programming but do not address NLP.
There are many older textbooks which opened the field to earlier generations of
students; these are mostly of historical interest:
*Natural Language Understanding*
(Allen, Addison Wesley, 1995);
*Statistical Language Learning*
(Charniak, MIT Press, 1993);
*Natural Language Processing for Prolog Programmers*
(Covington, 1993);
*Natural Language Processing in Prolog*
(Gazdar and Mellish, Addison Wesley, 1989);
*Prolog and Natural-Language Analysis*
(Pereira and Shieber, CSLI, 1987);
*Computational Linguistics*
(Grishman, Cambridge, 1986).

----------------
Acknowledgements
----------------

James Curran: feedback, advice, draft of programming chapter.


-----------------
About the Authors
-----------------

  +------------------+------------------+------------------+
  | |StevenBirdPic|  | |EwanKleinPic|   | |EdwardLoperPic| |
  |                  |                  |                  |
  | Steven Bird      | Ewan Klein       | Edward Loper     |
  +------------------+------------------+------------------+

.. |StevenBirdPic|  image:: ../images/StevenBird.png
                    :scale: 180

.. |EwanKleinPic|   image:: ../images/EwanKlein.png
                    :scale: 120

.. |EdwardLoperPic| image:: ../images/EdwardLoper.png
                    :scale: 100

**Steven Bird** is an Associate Professor in the
Department of Computer Science and Software Engineering
at the University of Melbourne,
and a Senior Research Associate in the
Linguistic Data Consortium
at the University of Pennsylvania.
After completing a PhD at the University of Edinburgh on
computational phonology (1990), Steven moved to Cameroon to
conduct fieldwork on tone and orthography.  Later he spent
four years as Associate Director of the Linguistic Data Consortium
where he developed models and tools for linguistic annotation.
His current research interests are in linguistic databases and
query languages.

**Ewan Klein** is a Professor in the
School of Informatics
at the University of Edinburgh. ...

**Edward Loper** is a doctoral student in the
Department of Computer and Information Sciences
at the University of Pennsylvania. ...

--------------------------------------------------------------------------


.. include:: footer.txt
