.. -*- mode: rst -*-
.. include:: ../definitions.txt

===========================
13. Semantic Interpretation
===========================

--------------
 Introduction
--------------


One of the goals of linguistic theory is to provide a systematic
correspondence between form and meaning. One widely adopted approach
to representing meaning |mdash| or at least, some aspects of meaning
|mdash| involves translating expressions of natural language in to
|fol|. From a computational point of view, a strong
argument in favour of |fol| is that it strikes a
reasonable balance between expressiveness and logical
tractability. On the one hand, it is flexible enough to represent many
aspects of the logical structure of natural language. On the other
hand, automated theorem proving for |fol| has received
much attention, and although inference in |fol| is not
decidable, in practice many reasoning problems are efficiently
solvable using modern theorem provers.

Standard textbooks on |fol| often contain exercises in
which the reader is required to translate between English and logic,
as illustrated in km_ and wq_.\ [#]_

.. [#] These examples come, respectively, from D. Kalish and
       R. Montague (1964) *Logic: Techniques of Formal Reasoning*,
       Harcourt, Brace and World, p94, and W. v. Quine (1952) *Methods
       of Logic*, Routledge and Kegan Paul, p121.


.. _km:
.. ex::

   .. ex:: If all whales are mammals, then Moby Dick is not a fish.

   .. ex:: |forall|\ `x`:math:\ (whale(`x`:math:) |rarr| mammal(`x`:math:)) |rarr| |neg|\ fish(MD)

.. _wq:
.. ex::

   .. ex:: There is a painting that all critics admire.

   .. ex:: |exists|\ `y`:math:\ (painting(`y`:math:) |wedge| |forall|\ `x`:math:\ (critic(`x`:math:) |rarr|  admire(`x`:math:, `y`:math:)))


Although there are numerous subtle and difficult issues about how this
translation should be carried out in particular cases, we will put
these to one side. The main focus of our discussion will be on a
different problem: how can we systematically construct a semantic
representation for a sentence which proceeds in step with the
process of parsing that sentence?


Unfortunately, it is not within the scope of this chapter to introduce
the syntax and semantics of |fol|, so if you don't already
have some familiarity with it, we suggest you consult an appropriate
source.

-------------------
The lambda-calculus
-------------------

In a functional programming language, computation can be carried out
by reducing an expression `E`:math: according to specified rewrite
rules. This reduction is carried out on subparts of `E`:math:, and
terminates when no further subexpressions can be reduced. The
resulting expression `E*`:math: is called the `Normal Form`:dt: of
`E`:math:. Here is an example of reduction involving a simple Python
expression (where '|rtharr|' means 'reduces to'):

.. ex::	
 +------------------------+---------------------------------------------------------+
 |                        | ``len(max(['cat', 'zebra', 'rabbit'] + ['gopher']))``   |
 +------------------------+---------------------------------------------------------+
 |        |rtharr|        | ``len(max(['cat', 'zebra', 'rabbit', 'gopher']))``      |
 +------------------------+---------------------------------------------------------+
 |        |rtharr|        | ``len('zebra')``                                        |
 +------------------------+---------------------------------------------------------+
 |      |rtharr|          |  ``5``                                                  |
 +------------------------+---------------------------------------------------------+

The final expression, ``5``, is considered to be the output of the program.
This fundamental notion of computation is modeled in an abstract way by
the |lambda|-calculus.

The first basic concept in the |lambda|-calculus is `application`:dt:,
represented by an expression of the form `(F A)`:math:, where
`F`:math: is considered to be a function, and `A`:math: is considered
to be an argument (or input) for `F`:math:.  For example, `(walk
x)`:math: is an application. Application expressions can be applied to
other expressions. For example, in a functional framework, binary
addition might represented as `((+ x) y)`:math: rather than `(x +
y)`:math:. Note that `+`:math: is being treated as a function which is
applied to its first argument `x`:math: to yield a function `(+
x)`:math: that is then applied to the second argument `y`:math:.

The second basic concept in the |lambda|-calculus is 
`abstraction`:dt:. If `M[x]`:math: is an expression containing the free
variable `x`:math:, then |lambda|\ `x.M[x]`:math: denotes the function
`x`:math: |mapsto| `M[x]`:math:. Abstraction and application are
combined in the expression (|lambda|\ `x.((+ x) 3) 4)`:math:, which
denotes the function `x`:math: |mapsto| `x + 3`:math: applied to 4,
giving 4 + 3, which is 7. In general, we have (|lambda|\ `x.M[x] N) =
M[N]`:math:, where `M[N]`:math: is the result of replacing all
occurences of `x`:math: in `M`:math: by `N`:math:. This axiom of the
lambda calculus is known as |beta|-conversion. |beta|-conversion is
the primary form of reduction in the |lambda|-calculus.

The module ``logic`` can parse expressions of the
|lambda|-calculus. The |lambda| symbol is represented as ``'\'``, which
needs to be escaped by a second ``'\'`` in parsable expressions.

    >>> from nltk_lite.contrib.logic import *
    >>> Parser().parse('(walk x)')
    ApplicationExpression('walk', 'x')
    >>> Parser().parse('\\x.(walk x)')
    LambdaExpression('x', '(walk x)')
    >>>

An ``ApplicationExpression`` has subparts consisting of the function
and the argument; a ``LambdaExpression`` has subparts consisting of
the variable (e.g., ``x``) that is bound by the |lambda| and the body
of the expression (e.g., ``walk``).

The |lambda|-calculus is a calculus of functions; by itself, it says
nothing about logical structure. Although it is possible to define
logical operators within the |lambda|-calculus, we shall adopt a
hybrid approach which supplements the |lambda|-calculus with logical
and non-logical constants as primitives.
In order to show how this is done, we turn next to the language of
propositional logic.




Propositional  Logic
--------------------

The language `L`:subscript:`prop` of propositional logic represents
certain aspects of natural language, but at a high level of
abstraction. The only structure that is made explicit involves
`logical connectives`:dt:\; these correspond to 'logically
interesting' expressions such as `and`:lx: and `not`:lx:. The basic
expressions of the language are `propositional variables`:dt:, usually
written `p`:math:, `q`:math:, `r`:math:, etc. Let `A`:math: be a
finite set of such variables. The set |Omega| of logical connectives
contains the unary operator |neg|, and binary operators |wedge|,
|vee|, |rarr| and |iff|. 

The set of formulas of `L`:subscript:`prop` is described inductively.

   1. Every element of `A`:math: is a formula of `L`:subscript:`prop`.

   2. If |phi| is a formula, then so is |neg| |phi|.

   3. If |phi| and |psi| are formulas, then so are
      (|phi| |wedge| |psi|),
      (|phi| |vee| |psi|),
      (|phi| |rarr| |psi|) and
      (|phi| |iff| |psi|).

   4. Nothing else is a formula of `L`:subscript:`prop`.

The Boolean connectives of propositional logic are supported by
``logic``, and are parsed as objects of the class
``ApplicationExpression`` (i.e., function expressions). However, infix
notation is also allowed as an input format. The connectives
themselves belong to the ``Operator`` class of expressions.

   >>> Parser().parse('(and p q)')
   ApplicationExpression('(and p)', 'q')
   >>> Parser().parse('(p and q)')
   ApplicationExpression('(and p)', 'q')
   >>> Parser().parse('and')
   Operator('and')
   >>> 

Since a negated proposition is syntactically an application, the unary
operator ``not`` and its argument *must* be surrounded by parentheses.

   >>> Parser().parse('(not (p and q))')
   ApplicationExpression('not', '(and p q)')
   >>> 

Using the `print` function on an ``ApplicationExpression`` just
outputs a string without any parse structure. To make the output
even easier to read, we can invoke the ``infixify`` method, which
places binary Boolean operators in infix position.

   >>> e = Parser().parse('(and p (not a))')
   >>> e
   ApplicationExpression('(and p)', '(not a)')
   >>> print e
   (and p (not a))
   >>> print e.infixify()
   (p and (not a))
   >>>

First-Order Logic
-----------------

In |fol| (|FOL|), propositions are further analysed into
predicates and arguments, which takes us a step closer to the
structure of natural languages. The standard construction rules for
|FOL| recognize `terms`:dt: such as individual variables and individual
constants, and `predicates`:dt: which take differing numbers of
arguments. For example, `John walks`:lx: might be formalized as
`walk(john)`:math: and `John loves Mary`:lx: as
`love(john, mary)`:math:.

However, the |NLTK| ``logic`` module is based on an implementation of the untyped
lambda calculus, and by default all identifiers are considered to be
variables. This means, for example, that the identifier ``walk`` can
be bound by |lambda|: 

   >>> Parser().parse('walk')
   VariableExpression('walk')
   >>> Parser().parse('\\walk.(walk x)')
   LambdaExpression('walk', '(walk x)')
   >>>

In addition, because the language is untyped, there is no formal
distinction between predicate expressions and individual expressions;
anything can be applied to anything. Indeed, functions can be applied
to themselves:

   >>> Parser().parse('(walk walk)')
   ApplicationExpression('walk', 'walk')
   >>> 


However, most standard approaches to natural language semantics forbid
self-application (e.g., applications such as `(walk walk)`:math:) by
assigning types to expressions of the semantic representation
language.

.. [#] When combined with logic, unrestricted self-application leads
       to Russell's Paradox.

It is also standard to allow constants as basic
expressions of the language. We use a mixture of convention and
supplementary stipulations in the ``logic`` module to bring our
practice closer to this more standard framework for natural language
semantics.  In particular, we shall use expressions like `x`:math:,
`y`:math:, `z`:math:, or `x`:subscript:`0`, `x`:subscript:`1`, `x`:subscript:`2`, ... to
indicate individual variables.  As an extension over the 'pure'
|lambda|-calculus syntax of ``logic``, we can assign such strings to
the class ``IndVariableExpression``.

   >>> Parser().parse('x')
   IndVariableExpression('x')
   >>> Parser().parse('x01')
   IndVariableExpression('x01')
   >>> 

English-like expressions such as `dog`:math:,
`walk`:math: and `john`:math: will be non-logical constants
(non-logical in contrast to logical constants such as `not`:math: and
`and`:math:). In order to force ``logic.Parser()`` to recognize
non-logical constants, we can initialize the parser with a list of
identifiers.

   >>> lp = Parser(constants=['dog', 'walk', 'love'])
   >>> lp.parse('walk')
   ConstantExpression('walk')
   >>> 

To sum up our discussion so far, while the |lambda|-calculus only
recognizes one kind of basic expression other than |lambda|, namely
the class of variables (the class ``VariableExpression`` in
``logic``), our extended language adds to ``logic`` three further
classes of basic expression: ``IndVariableExpression``,
``ConstantExpression`` and ``Operator`` (Boolean connectives plus the
equality relation ``=``). 

However, predication will be formalized using function application, as
discussed earlier. In standard |FOL|, predication is treated in a
'relational' style. For example, the proposition that John loves Mary
is formalized as `love(j, m)`:math:. Semantically, `love`:math: is
modeled as a relation, i.e., a set of pairs, and the proposition is
true in a situation just in case the pair |langle|\ `j, m`:math:\ |rangle|
belongs to this set. By contrast, in the functional style of the
|lambda| calculus, `John loves Mary`:lx: is formalized as `((love m)
j)`:math:. Rather than being modeled as a relation, `love`:math:
denotes a function. Before going into detail about this function,
let's first look at a simpler case, namely the different styles of
interpreting a unary constant such as `walk`:math:.

In the relational approach, `walk`:math: denotes some set `S`:math: of
individuals. The formula `walk(j)`:math: is true in a situation if and
only if the individual denoted by `j`:math: belongs to `W`:math:. Now,
corresponding to every set is something called the `characteristic
function`:dt: of that set. If `f`:subscript:`S` is the characteristic
function of some set `S`:math:, then for any individual `a`:math:,
`f`:subscript:`S`\ `(a) = True`:math: if and only if `a`:math: belongs
to `S`:math:. To be more specific, suppose in some situation our
domain of discourse is the set containing the individuals `j`:math:
(John), `m`:math: (Mary) and `f`:math: (Fido); and the set of
individuals that walk is `W = {j, f}`:math:. So in this situation, the
formulas `walk(j)`:math: and `walk(f)`:math: are both true, while
`walk(m)`:math: is false.  Now we can use the characteristic function
`f`:subscript:`W` as the interpretation of `walk`:math: in the
functional style.  The diagram cf01_ gives a graphical representation
of the mapping `f`:subscript:`W`.

.. _cf01:
.. ex::
   .. image:: ../images/models_walk_cf.png
      :scale: 25

Binary relations can be converted into functions in a very similar
fashion. Suppose for example that on the relational style of
interpretation, `love`:math: denotes the following set of pairs:

.. _loverel:
.. ex:: {|langle|\ `j, m`:math:\ |rangle|, |langle|\ `m, f`:math:\ |rangle|,  
         |langle|\ `f, j`:math:\ |rangle|}

That is, John loves Mary, Mary loves Fido, and Fido loves John. One
option on the functional style would be to treat `love`:math: as the
expected characteristic function of this set, i.e., a
function from pairs to truth values. This mapping is illustrated in cf02_.

.. _cf02:
.. ex::
   .. image:: ../images/models_love_cf01.png
      :scale: 20

However, a more uniform approach can be achieved through a technique
known as 'currying' (named after Haskell B. Curry), in which
`n`:math:\ -ary functions are converted into functions of one
argument. This idea can be illustrated with
|lambda|-abstraction. Suppose we have an application `love(x, y)`:math:
which depends on two arguments. In currying, we define

.. _L1:
.. ex:: `L`:subscript:`1` = |lambda|\ `x.love(x, y)`:math:
.. _L2:
.. ex:: `L`:subscript:`2` = |lambda|\ `y.`:math:\ `L`:subscript:`1` =
        |lambda|\ `y.`:math:\ |lambda|\ `x.love(x, y)`:math:

That is, `L`:subscript:`1` = |lambda|\ `x.love(x, y)`:math: is a
function of only one argument, namely `x`:math:, and `L`:subscript:`2`
is also a function of only one argument, this time `y`:math:. Just as
importantly, the *value* of `L`:subscript:`2` is a function, namely
`L`:subscript:`1`. If we think of `love(x, y)`:math: as the
formalization of `x loves y`:lx:, then  `L`:subscript:`1` = |lambda|\ `x.love(x, y)` corresponds
intuitively to the property of being an `x`:lx: that loves `y`:lx:.
According to these definitions (and using |beta|-conversion), we have:

.. ex:: ((`L`:subscript:`2` m) j)  = (`L`:subscript:`1` j) = `love(j, m)`:math:

or equivalently,

.. ex::  ((|lambda|\ `y.`:math:\ |lambda|\ `x.love(x, y) m) j)`:math:
         = |lambda|\ `x.love(x, m) j)`:math: 
         = `love(j, m)`:math:

Diagram cf03_ shows the curryed counterpart of cf02_. According to
this, `love`:math: denotes a function which, for example, given the
argument `j`:math: yields a characteristic function that maps
`m`:math: to `True`:math: and `j`:math: and `f`:math: to
`False`:math:.
(While there are 2\ :sup:`3` = 8 characteristic functions from
our domain of three individuals into `{True, False}`:math:, we have
only shown the functions which are in the range of the function `love`:math:.)

.. _cf03:
.. ex::
   .. image:: ../images/models_love_cf02.png
      :scale: 20


Of course, we could have done things slightly differently, and defined
`L`:subscript:`1` = |lambda|\ `y.love(x, y)`:math:, and
`L`:subscript:`2` = |lambda|\ `x.`:math:\ `L`:subscript:`1`. And L1_
and L2_ might strike you as a bit perverse since we consume the
arguments in an order that appears to be opposite to `love(x,
y)`:math:. Yet there is something very attractive about L1_ and L2_,
since (`L`:subscript:`2` `m`:math:) is just the right function to serve as the
denotation of the `VP`:gc: `loves Mary`:lx:. That is, if `loves`:lx: is
interpreted as |lambda|\ `y.`:math:\ |lambda|\ `x.love(x, y)`:math:,
then combining the verb with an object `NP`:gc: such as `Mary`:lx:
yields a `VP`:gc: interpretation
(|lambda|\ `y.`:math:\ |lambda|\ `x.love(x, y) m)`:math: = |lambda|\
`x.love(x, m)`:math:, which can then combine with a subject `NP`:gc:
such as `John`:lx: to yield the interpretation `love(j, m)`:math:. In
fact, doing things according to  L1_
and L2_ allows the semantic construction rules to mirror the syntactic
construction rules, and thus provides an extremely elegant basis for a
compositional approach to semantic interpretation.  



.. +-----------------+---------------+----------------+
   |   English       | Relational    |  Functional    |
   +=================+===============+================+
   |     John walks  |``walk(j)``    |``(walk j)``    |
   +-----------------+---------------+----------------+
   |John loves Mary  |``love(j, m)`` |``((love m) j)``|
   +-----------------+---------------+----------------+


In |NLTK|, |lambda|-abstraction can be carried out on Boolean combinations. For
example, the following can be thought of as the property of being an
`x`:math: who walks and talks:

  >>> Parser().parse('\\x.((walk x) and (talk x))')
  LambdaExpression('x', '(and (walk x) (talk x))')
  >>>

|beta|-conversion can be invoked with the ``simplify`` method of
``ApplicationExpression``\ s.

  >>> WT = Parser().parse('(\\x.((walk x) and (talk x)) john)')
  >>> WT
  ApplicationExpression('\x.(and (walk x) (talk x))', 'john')
  >>> WT.simplify()
  ApplicationExpression('(and (walk john))', '(talk john)')
  >>> 

Up to this point, we have restricted ourselves to looking at formulas
where all the arguments are individual terms, corresponding to proper
names such as `John`:lx:, `Mary`:lx: and `Fido`:lx:. Yet a crucial
ingredient of |Fol| is the ability to make general
statements involving quantified expressions such as `all dogs`:lx: and
`some cats`:lx:. We turn to this topic in the next section.


Quantification and Scope
------------------------

|Fol| standardly offers us two quantifiers, `every`:lx: (or `all`:lx:) and
`some`:lx:. These are formally written as |forall| and |exists|,
respectively. The following two triples show an a simple English
example, a logical representation, and the encoding which is accepted
by the |NLTK| `logic` module.

.. _forall1:
.. ex::

   .. _forall1a:
   .. ex:: Every dog barks.

   .. _forall1b:
   .. ex:: |forall|\ `x`:math:.\ ((dog `x`:math:) |rarr| (bark `x`:math:))

   .. _forall1c:
   .. ex:: `all x. ((dog x) implies (bark x))`

.. _exists1:
.. ex::

   .. ex:: Some cat sleeps.

   .. ex:: |exists|\ `x`:math:.\ ((cat `x`:math:) |wedge| (sleep `x`:math:))

   .. ex:: `some x. ((cat x) and (sleep x))`

One important property of forall1b_ often trips people up. The logical
rendering in effects says that *if* something is a dog, then it barks,
but makes no commitment to the existence of dogs. So in a situation
where nothing is a dog, forall1b_ will still come out true. (Although
it might be felt that forall1b_ does presuppose the existence of dogs,
contrary to the logic formalization, other examples such as `all students
with more than 10 body piercings are prohibited from taking this
class`:lx:: lacks any such presupposition.) 

What happens when we want to give a formal rendering of a sentence
with *two* quantifiers, such as the following.

.. _scope1:
.. ex:: Every girl chases a dog.

There are (at least) two ways of expressing scope1_ in |FOL|:

.. _scope2:
.. ex::

   .. _scope2a:
   .. ex:: |forall|\ `x`:math:.\ ((girl `x`:math:) |rarr|\ |exists|\
           `y`:math:.\ ((dog `y`:math:) |wedge| (chase `y x`:math:)))

   .. _scope2b:
   .. ex:: |exists|\ `y`:math:.\ ((dog `y`:math:) |wedge| |forall|\
           `x`:math:.\ ((every `x`:math:) |rarr|\  (chase `y x`:math:))) 

Can we use both of these? Then answer is Yes, but they have different
meanings. scope2b_ is logically stronger than scope2a_: it claims that
there is a unique dog, say Fido, which is chased by every girl.
scope2a_, on the other hand, just requires that for every girl
`g`:math:, we can find some dog which `d`:math: chases; but this could
be a different dog in each case. We distinguish between scope2a_ and
scope2b_ in terms of the `scope`:dt: of the quantifiers. In the first,
|forall| has wider scope than |exists|, while in scope2b_, the scope ordering
is reversed. So now we have two ways of representing the meaning of
scope1_, and they are both quite legitimate. In other words, we are
claiming that scope1_ is *ambiguous* with respect to quantifier scope,
and the formulas in scope2_ give us a formal means of making the two
readings explicit. However, we are not just interested in associating
two distinct representations with scope1_. We also want to show in
detail how the two representations lead to different conditions for
truth in a formal model. This will be taken up in the next section.




----------------
Formal Semantics
----------------

A `model`:dt: for |FOL| is a pair `<D,V>`:math:, where `D`:math: is a
domain of discourse and `V`:math: is a valuation function for the
non-logical constants of a first-order language. 
As a first approximation, non-logical constants are interpreted
by `V`:math: as follows (note that `e`:math: is the type of
entities and `t`:math: is the type of truth values):

  - if |alpha| is an individual constant, then `V`:math:\ (|alpha|)
    is an element of the domain `D`:math:.
  - If |gamma| is a functor of type 
    (`e`:math: x ... x `e`:math:) |rarr| `t`:math:, then
    `V`:math:(|gamma|) is a function `f`:math: from  
    `D`:math: x ... x `D`:math: to {True, False}.

However, since we are basing our language on the |lambda|-calculus, a
binary relation such as the interpretation of `like`:lx: will not in
fact be associated with the relational type (`e`:math: x `e`:math:)
|rarr| `t`:math:, but rather the type (`e`:math: |rarr| (`e`:math:
|rarr| `t`:math:)); i.e., a function from entities to a function from
entities to truth values. In other words, as explained earlier,
functor expressions are assigned 'curryed' functions as their values. It should
also be noted that expressions of the language are not explicitly
typed.  We leave it to the grammar writer to assign 'sensible' values
to expressions rather than enforcing any type-to-denotation
consistency.

Characteristic Functions
------------------------

Within the ``models`` module, curryed characteristic functions are
implemented as a subclass of dictionaries, using the ``CharFun``
constructor.

   >>> cf = CharFun({'d1' : CharFun({'d2': True}), 'd2' : CharFun({'d1': True})})

Values of a ``CharFun`` are accessed by indexing in the usual way:

   >>> cf['d1']
   {'d2': True}
   >>> cf['d1']['d2']
   True
   >>>

``CharFun``\ s are 'sparse' data structures in the sense that they omit
key-value pairs of the form ``(e: False)``. In fact, they
behave just like ordinary dictionaries on keys which are
out of their domain, rather than yielding the value ``False``:

   >>> cf['not in domain']
   Traceback (most recent call last):
   ...
   KeyError: 'not in domain'

The assignment of ``False`` values is delegated to a wrapper method
``app`` of the ``Model`` class. ``app`` embodies the Closed World
assumption; i.e., where ``m`` is an instance of ``Model``:

   >>> m.app(cf,'not in domain')
   False
   >>>


In practise, it is often be more convenient to specify interpretations
as `n`:math:-ary relations (i.e., sets of `n`:math:-tuples) rather
than as `n`:math:-ary functions. ``CharFun`` provides a ``parse``
method which will convert such relations into curryed characteristic
functions:

   >>> s = set([('d1', 'd2'), ('d3', 'd4')])
   >>> cf = CharFun()
   >>> cf.parse(s)
   >>> cf
   {'d2': {'d1': True}, 'd4': {'d3': True}}
   >>>

``cf.parse`` will raise an exception if the set is not in fact a
relation (i.e., if it contains tuples of different lengths):

  >>> wrong = set([('d1', 'd2'), ('d2', 'd1', 'd3')])
  >>> cf.parse(wrong)
  Traceback (most recent call last):
  ...
  ValueError: Set contains sequences of different lengths

However, unary relations can be parsed to characteristic functions.

  >>> unary = set(['d1', 'd2'])
  >>> cf.parse(unary)
  >>> cf
  {'d2': True, 'd1': True}
  >>>

The function ``flatten`` returns a set of the entities used as keys in
a ``CharFun`` instance. The same information can be accessed via the
``domain`` attribute of ``CharFun``.

   >>> cf = CharFun({'d1' : {'d2': True}, 'd2' : {'d1': True}})
   >>> flatten(cf)
   set(['d2', 'd1'])
   >>> cf.domain
   set(['d2', 'd1'])
   >>>



Valuations
----------

A `Valuation`:dt: is a mapping from non-logical constants to
appropriate semantic values in the model. Valuations are created using
the ``Valuation`` constructor.

   >>> val = Valuation({'Fido' : 'd1', 'dog' : {'d1' : True, 'd2' : True}})
   >>> val
   {'Fido': 'd1', 'dog': {'d2': True, 'd1': True}}
   >>>

As with ``CharFun``, an instance of ``Valuation`` will parse
valuations using relations rather than characteristic functions as
interpretations.

   >>> setval = [('adam', 'b1'), ('betty', 'g1'),\
   ('girl', set(['g2', 'g1'])), ('boy', set(['b1', 'b2'])),\
   ('love', set([('b1', 'g1'), ('b2', 'g2'), ('g1', 'b1'), ('g2', 'b1')]))]
   >>> val = Valuation()
   >>> val.parse(setval)
   >>> print val
   {'adam': 'b1',
   'betty': 'g1',
   'boy': {'b1': True, 'b2': True},
   'girl': {'g2': True, 'g1': True},
   'love': {'b1': {'g2': True, 'g1': True},
            'g1': {'b1': True},
            'g2': {'b2': True}}}
    >>>

Valuations have a ``domain`` attribute, like ``CharFun``, and also a
``symbols`` attribute.

   >>> val.domain
   set(['g1', 'g2', 'b2', 'b1'])
   >>> val.symbols
   ['boy', 'girl', 'love', 'adam', 'betty']
   >>>


Assignments
-----------

A variable `Assignment`:dt: is a mapping from individual variables to
entities in the domain. Individual variables are indicated with the
letters ``'x'``, ``'y'``, ``'w'`` and ``'z'``, optionally followed by
an integer (e.g., ``'x0'``, ``'y332'``).  Assignments are created
using the ``Assignment`` constructor, which also takes the model's
domain of discourse as a parameter.

   >>> dom = set(['u1', 'u2', 'u3', 'u4'])
   >>> g = Assignment(dom, {'x': 'u1', 'y': 'u2'})
   >>> g
   {'y': 'u2', 'x': 'u1'}
   >>>

In addition, there is also a ``print`` format for assignments which
uses a notation closer to that in logic textbooks:
   
   >>> print g
   g[u2/y][u1/x]
   >>>

Initialization of an ``Assignment`` instance checks that the variable
really is an individual variable and also that the value belongs to
the domain of discourse:

    >>> Assignment(dom, {'xxx': 'u1', 'y': 'u2'})
    Traceback (most recent call last):
    ...
    AssertionError: Wrong format for an Individual Variable: 'xxx'
    >>> Assignment(dom, {'x': 'u5', 'y': 'u2'})
    Traceback (most recent call last):
    ...
    AssertionError: 'u5' is not in the domain: set(['u4', 'u1', 'u3', 'u2'])

It is possible to update an assignment using the ``add`` method, with
similar restrictions:

    >>> dom = set(['u1', 'u2', 'u3', 'u4'])
    >>> g = models.Assignment(dom, {})
    >>> g.add('u1', 'x')
    {'x': 'u1'}
    >>> g.add('u1', 'xyz')
    Traceback (most recent call last):
    ...
    AssertionError: Wrong format for an Individual Variable: 'xyz'
    >>> g.add('u2', 'x').add('u3', 'y').add('u4', 'x0')
    {'y': 'u3', 'x': 'u2', 'x0': 'u4'}
    >>> g.add('u5', 'x')
    Traceback (most recent call last):
    ...
    AssertionError: u5 is not in the domain set(['u4', 'u1', 'u3', 'u2'])

Variables (and their values) can be selectively removed from an
assignment with the ``purge`` method:

    >>> g
    {'y': 'u3', 'x': 'u2', 'x0': 'u4'}
    >>> g.purge('x')
    >>> g
    {'y': 'u3', 'x0': 'u4'}
    >>>

With no arguments,  ``purge`` is equivalent to ``clear`` on a dictionary:

    >>> g.purge()
    >>> g
    {}
    >>>

`evaluate` and `satisfy`
------------------------

The ``Model`` constructor takes two parameters, a ``set`` and a
``Valuation``.

   >>> m = Model(val.domain, val)

The top-level method of a ``Model`` instance is ``evaluate``, which
assigns a semantic value to expressions of the ``logic`` module, under
an assignment ``g``:

    >>> m.evaluate('all x. ((boy x) implies (not (girl x)))', g)
    True
    >>>

The function ``evaluate`` calls a recursive function ``satisfy``,
which in turn calls a function ``i`` to interpret non-logical
constants and individual variables. ``i`` first tries to call the
model's ``Valuation`` and if that fails, calls the variable assignment
``g``. Any atomic expression which cannot be assigned a value by ``i``
raises an ``Undefined`` exception; this is caught by ``evaluate``,
which returns the string 'Undefined'.

    >>> m.evaluate('(walk adam)', g, trace=2)
       ... checking whether 'walk' is an individual variable
    Expression 'walk' can't be evaluated by i and g.
    'Undefined'

..
 Boolean operators such as `not`:math:, `and`:math: and `implies`:math: are
 implemented as dictionaries. For example:

     >>> m.AND
     {False: {False: False, True: False}, True: {False: False, True: True}}


The ``satisfy`` function assigns semantic values to arbitrary expressions
according to their syntactic structure, as determined by ``decompose``.

Following the functional style of interpretation, Boolean connectives
in `models` are interpreted as truth functions; for example,
the connective `and` can be interpreted as the function
`AND`:

.. doctest-ignore::
     >>> AND = {True:  {True: True, 
     ...               False: False},
     ...       False: {True: False, 
     ...               False: False}}

We define `OPS` as a mapping between the Boolean connectives
and their associated truth functions. Then the simplified clause for
the satisfaction of Boolean formulas looks as follows:

.. doctest-ignore::
     >>> def satisfy(expr, g):
     ...   if parsed(expr) == (op, args) 
     ...      if args == (phi, psi):
     ...         val1 = self.satisfy(phi, g)
     ...         val2 = self.satisfy(psi, g)
     ...         return OPS[op][val1][val2]

A formula such as `(and p q)` is interpreted by indexing
the value of `and` with the values of the two propositional arguments,
in the following manner:

   >>> m.AND[m.evaluate('p', g)][m.evaluate('q', g)]


The `satisfy` clause for function application is similar to that for
the connectives. In order to handle type errors, application is
delegated to a wrapper function `app` rather than by directly indexing
the curryed characteristic function as described earlier. The
definition of `satisfy` started above continues as follows:

.. doctest-ignore::  
     ...   elif parsed(expr) == (fun, arg):
     ...      funval = self.satisfy(fun, g)
     ...      argval = self.satisfy(psi, g)
     ...      return app(funval, argval)

The first step in interpreting quantified formulas is to define the
`satisfiers`:dt: of a formula that is open in some variable. Formally,
given an open formula |phi|\ `[x]`:math: dependent on `x`:math: and a
model with domain `D`:math:, we define the set `sat`:mathit:\ (|phi|\
`[x], g)`:math: of satisfiers of |phi|\ `[x]`:math: to be: `{u`:math:
|in| `D:`:math: `satisfy`:mathit:\ (|phi|\ `[x], g[u/x]`) =
`True`:mathit:\ `}`:math:.  We use `g[u/x]`:math: to mean that
assignment which is just like `g`:math: except that `g(x) =
u`:math:. In Python, it is straightforward to build the set
`sat`:mathit:\ (|phi|\ `[x], g)`:math: with a `for` loop.

An existentially quantified formula |exists|\ `x.`:math:\ |phi|\
`[x]`:math: is held to be true if and only if `sat`:mathit:\ (|phi|\ `[x],
g)`:math: is nonempty. In Python, sets are a kind of collection (like
lists), and the length function `len` returns the cardinality
of a set.  

.. doctest-ignore::
     ...   elif parsed(expr) == (binder, body):
     ...      if binder = ('some', var):
     ...         sat = self.satisfiers(body, var, g)
     ...         return len(sat) > 0


In other words, a formula |exists|\ `x.`:math: |phi|\ `[x]`:math: has
the same value in model `M`:math: as the statement that the number of
satisfiers in `M`:math: of |phi|\ `[x]`:math: is greater than 0.


Quantifier Scope Revisited
--------------------------

You may recall that we discussed earlier an example of quantifier
scope ambiguity, repeated here as scope4_.

.. _scope4:
.. ex:: Every girl chases a dog.

The two readings are represented as follows.

   >>> sr1 = 'all x.((girl x) implies some z.((dog z) and (chase z x)))'
   >>> sr2 = 'some z.((dog z) and all x.((girl x) implies (chase z x)))'

In order to examine the ambiguity more closely, let's fix our valuation as
follows:

   >>> val = models.Valuation()
   >>> v = [('john', 'b1'),
   ... ('mary', 'g1'),
   ... ('suzie', 'g2'),
   ... ('fido', 'd1'),
   ... ('tess', 'd2'),
   ... ('noosa', 'n'),
   ... ('girl', set(['g1', 'g2'])),
   ... ('boy', set(['b1', 'b2'])),
   ... ('dog', set(['d1', 'd2'])),
   ... ('bark', set(['d1', 'd2'])),
   ... ('walk', set(['b1', 'g2', 'd1'])),
   ... ('chase', set([('b1', 'g1'), ('b2', 'g1'), ('g1', 'd1'), ('g2', 'd2')])),
   ... ('see', set([('b1', 'g1'), ('b2', 'd2'), ('g1', 'b1'),('d2', 'b1'), ('g2', 'n')])),
   ... ('in', set([('b1', 'n'), ('b2', 'n'), ('d2', 'n')])),
   ... ('with', set([('b1', 'g1'), ('g1', 'b1'), ('d1', 'b1'), ('b1', 'd1')]))]
   >>> val.parse(v)


Using a slightly different graph from before, we can also visualise the
`chase`:mathit: relation as in chasegraph_.

.. _chasegraph:
.. ex::
   .. image:: ../images/models_chase.png
      :scale: 20

In chasegraph_, an arrow between two individuals `x`:math: and
`y`:math: indicates that `x`:math: chases
`y`:math:. So ``b1`` and ``b2`` both chase ``g1``, while ``g1`` chases
``d1`` and ``g2`` chases ``d2``. In this model, formula ``sr1`` above
is true but ``sr2`` is false. One way of exploring these results is by
using the ``satisfiers`` method of ``Model`` objects.

   >>> dom = val.domain
   >>> m = model.Model(dom, val)
   >>> g = model.Assignment(dom)
   >>> fmla1 = '((girl x) implies some y.((dog y) and (chase y x)))'
   >>> m.satisfiers(fmla1, 'x', g)
   set(['g2', 'g1', 'n', 'b1', 'b2', 'd2', 'd1'])
   >>> 

This gives us the set of individuals that can be assigned as the value
of ``x`` in ``fmla1``. In particular, every girl is included in this
set. By contrast, consider the formula ``fmla2`` below; this has no
satisfiers for the variable ``y``.

   >>> fmla2 = '((dog y) and all x.((girl x) implies (chase y x)))'
   >>> m.satisfiers(fmla2, 'y', g)
   set([])
   >>> 

That is, there is no dog that is chased by both ``g1`` and
``g2``. Taking a slightly different open formula, ``fmla3``, we
can verify that there is a girl, namely ``g1``, who is chased by every boy.

   >>> fmla3 = '((girl y) and all x.((boy x) implies (chase y x)))'
   >>> m.satisfiers(fmla3, 'y', g)
   set(['g1'])
   >>> 

----------------------------
Evaluating English Sentences
----------------------------

So far, we have just taken for granted that we have some appropriate
logical formulas to interpret. However, ideally we would like to
derive these formulas from natural language input. One relatively easy
way of achieving this goal is to build on the grammar framework
developed in the chapter `Feature Based Grammar
<featgram.html>`__. Our first step is to introduce a new feature,
``sem``. Because values of ``sem`` generally need to be treated
differently from other feature values, we can use the convention of
enclosing them in angle brackets. sem1_ illustrates a first
approximation to the kind of analyses we would like to build.

.. _sem1:
.. ex::
   .. tree:: (S[sem=\<\(walk\ john\)\>] (NP[sem=\<\(john\)\>] John) (VP[sem=\<\walk\>] (V[sem=\<\walk\>] walks)))

Thus, the ``sem`` value at the root node shows a semantic
representation for the whole sentence, while the ``sem`` values at
lower nodes show semantic representations for constituents of the
sentence. So far, so good, but how do we write grammar rules which
will give us this kind of result? To be more specific, suppose we have
a `NP`:gc: and `VP`:gc: constituents with appropriate values for their
``sem`` nodes? If you reflect on the machinery that was introduced in
discussing the |lambda| calculus, you might guess that function
application will be central to composing semantic values. You will
also remember that our feature-based grammar framework gives us the
means to refer to `variable`:em: values. Putting this together, we can
postulate a rule like sem2_ for building the ``sem`` value of an
`S`:gc:.  (Observe that in the case where the value of ``sem`` is a
variable, we omit the angle brackets.)

.. _sem2:
.. ex::
   ::

     S[sem = <app(?vp,?subj)>] -> NP[sem=?subj] VP[sem=?vp]

sem2_ tells us that given some ``sem`` value ``?subj`` for the subject
`NP`:gc: and some ``sem`` value ``?vp`` for the `VP`:gc:, the ``sem``
value of the `S`:gc: mother is constructed by applying ``?vp`` as a
functor to ``?np``.  From this, we can conclude that ``?vp`` has to
denote a function which has the denotation of ``?np`` in its
domain; in fact, we are going to assume that ``?vp`` denotes a
characteristic function on individuals. sem2_ is a nice example of
building semantics using `the principle of compositionality`:dt:\:
that is, the principle that the semantics of a complex expression is a
function of the semantics of its parts.

To complete the grammar is very straightford; all we require are the
rules shown in sem3_.

.. _sem3:
.. ex::
   ::

     VP[sem=?v] -> IV[sem=?v]
     NP[sem=<john>] -> 'John'
     IV[sem=<walk>] -> 'walks'

The `VP`:gc: rule says that the mother's semantics is the same as the
head daughter's. The two lexical rules just introduce non-logical
constants to serve as the semantic values of `John`:lx: and
`walks`:lx: respectively. This grammar can be parsed using the chart
parser in ``nltk_lite.parse.featurechart``, and the trace in sem4_
shows how semantic values are derived by feature unification in the
process of building a parse tree.

.. _sem4:
.. ex::
   ::

     Predictor |> . .| S[sem='(?vp ?subj)'] -> * NP[sem=?subj] VP[sem=?vp] 
     Scanner   |[-] .| [0:1] 'John' 
     Completer |[-> .| S[sem='(?vp john)'] -> NP[sem='john'] * VP[sem=?vp] 
     Predictor |. > .| VP[sem=?v] -> * IV[sem=?v] 
     Scanner   |. [-]| [1:2] 'walks' 
     Completer |. [-]| VP[sem='walk'] -> IV[sem='walk'] * 
     Completer |[===]| S[sem='(walk john)'] -> NP[sem='john'] VP[sem='walk'] * 
     Completer |[===]| [INIT] -> S * 

You might be thinking this is all too easy |mdash| surely there is a
bit more to building compositional semantics. What about quantifiers,
for instance? Right, this is a crucial issue. For example, we want
sem5a_ to be given a semantic representation like sem5b_. How can this
be accomplished? 

.. _sem5: 
.. ex::
   .. _sem5a:
   .. ex:: 
      A dog barks.
   .. _sem5b:
   .. ex:: 
      some x. ((dog x) and (bark x)) 

Let's make the assumption that our `only`:em: operation for building
complex semantic representations is ``app`` (corresponding to function
application). Then our problem is this: how do we give a semantic
representation to quantified `NP`:gc:\s such as `a dog`:lx: so that
they can be combined with something like `walk`:math: to give a result
like sem5b_? As a first step, let's make the subject's ``sem`` value
act as the functor rather than the argument in ``app``. Now we are
looking for way of instantiating ``?np`` so that sem6_ holds.

.. _sem6: 
.. ex::
   ::

     [sem=<app(?np, bark)>] ->> [sem=<some x.((dog x) and (bark x))>]

This is where |lambda| abstraction comes to the
rescue; doesn't sem6_ look a bit reminiscent of carrying out |beta|-reduction in the
|lambda|-calculus? In other words, can we come up with a |lambda| term `M`:math:
to replace ``?np`` so that applying `M`:math: to ``bark`` yields
sem5b_? Indeed we can; we replace the occurence of ``bark`` in sem5b_
by a variable ``P``, and bind the variable with |lambda|, as shown in sem7_.

.. _sem7: 
.. ex::
   ::
    
     \\ P. some x. ((dog x) and (P x))

As a point of interest, we have used a different style of variable in
sem7_, that is ``P`` rather than ``x`` or ``y``. This is to signal
that we are abstracting over a different kind of thing |mdash| not an
individual, but a unary predicate expression. 

We are pretty much done now, except that we also want to carry out a
further abstraction plus application for the process of combining the
semantics of the determiner `a`:lx: with the semantics of `dog`:lx:.
Applying sem7_ as a functor to ``bark`` gives us ``(\ P. some x. ((dog
x) and (P x)) bark)``, and carrying out |beta|\-reduction gives us
just what we wanted, namely sem5b_.

|NLTK| provides some utilities to make it easier to derive and inspect
 semantic interpretations.


   >>> from nltk_lite.contrib.grammartest import SemanticInterpreter
   >>> semint = SemanticInterpreter().text_interpret
   >>> from nltk_lite.contrib.grammarfile import GrammarFile
   >>> grammar = GrammarFile.read_file('sem1.cfg')
   >>> result  = semint(['a dog barks'], grammar)
   >>> (syntree, semrep) = result['a dog barks'][0]
   >>> print syntree
   ([INIT][]:
     (S[ sem = ApplicationExpression('(\Q P.some x.(and (Q x) (P x)) dog)', 'bark') ]:
       (NP[ sem = ApplicationExpression('\Q P.some x.(and (Q x) (P x))', 'dog') ]:
	 (Det[ sem = LambdaExpression('Q', '\P.some x.(and (Q x) (P x))') ]:
	   'a')
	 (N[ sem = VariableExpression('dog') ]: 'dog'))
       (VP[ sem = VariableExpression('bark') ]:
	 (IV[ sem = VariableExpression('bark') ]: 'barks'))))
   >>> print semrep
   (\Q P.some x.(and (Q x) (P x)) dog bark)

Semantic representations can also be reduced, using the ``simplify``
method, and Boolean connectives can be placed in infix position with
the ``infixify`` method.

   >>> print semrep.simplify()
   some x.(and (dog x) (bark x))
   >>> print semrep.simplify().infixify()
   some x.((dog x) and (bark x))


Our next challenge is to deal with sentences containing transitive
verbs, such as sem8_.

.. _sem8: 
.. ex::

     Suzie chases a dog.

The output semantics that we want to build is shown in sem9_.

.. _sem9: 
.. ex::
   :: 

   some x.((dog x) and (chase x suzie))

Let's look at how we can use |lambda|-abstraction to get this
result. An significant constraint on possible solutions is imposed by
the decision that the semantic representation of `a dog`:lx: is
independent of whether the `NP`:gc: acts as subject or object of the
sentence. In other words, we want to get sem9_ as our output while sticking to
sem7_ as the `NP`:gc: semantics. A second constraint is that
`VP`:gc:\s should have the same type of interpretation, regardless of
whether they consist of just an intransitive verb or a transitive verb
plus object. As before, `VP`:gc:\s will denote characteristic
functions on individuals. Here's a semantic representation for `chases
a dog`:lx: which does the trick.

.. _sem9: 
.. ex::
   :: 

   \ y.some x.((dog x) and (chase x y))

Think of sem9_ as the property of being a `y`:math: such that
for some dog `x`:math:, `y`:math: chases `x`:math:; or more
colloquially, being a `y`:math: who chases a dog. Our task now
resolves to designing a semantic representation for
`chases`:lx: which can combine via ``app`` with sem7_ so as to allow
sem9_ to be derived. Let ``X`` stand for sem7_. Then we are part
way to the solution if we can derive sem10_.

.. _sem10: 
.. ex::
   ::

   (X \z. (chase z y))

sem10_ is equivalent to sem11a, which reduces to sem9_.

.. _sem11: 
.. ex::
   ::

    (\\ P. some x. ((dog x) and (P x)) \z. (chase z y))

The full solution is reached by giving `chases`:lx: the semantic
representation shown in sem12_.

.. _sem12: 
.. ex::
   ::

    \X y. (X \x. (chase x y))

If sem12_ is applied to sem7_, the result after |beta|-reduction is
equivalent to sem9_, which is what we wanted all along. In order to
build a semantic representation for a sentence, we also need to
combine in the semantics of the subject `NP`:gc:. If the latter is a
quantified expression like `every girl`:lx:, everything proceeds in
the same way as we showed for `a dog barks`:lx: earlier on; the
subject is translated as a functor which is applied to the semantic
representation of the `VP`:gc:.  However,
we now seem to have created another problem for ourselves with proper
names. So far, these have been treated as simple individuals, and
these cannot be applied as functors to expressions like sem9_. What we
do in this case is re-intepret proper names so that they too are
functors, like quantified `NP`:gc:\ s. sem13_ shows the required
|lambda| expression for `Suzie`:lx:.

.. _sem12: 
.. ex::
   ::

    \ P. (P suzie)

sem12_ denotes the characteristi function corresponding to all
properties which are true of Suzie. Converting from an indiviual
constant to an expression like sem12_ is known as `type raising`:dt:,
and allows us to flip functors with arguments. That is, we can replace
a Boolean-valued application such as 
`(f a)`:math: with an equivalent application `(`:math:|lambda|`P. (P a) f)`:math:.


One important limitation of the approach we have presented here is
that it does not attempt to deal with scope ambiguity. Instead,
quantifier scope ordering directly reflects scope in the parse
tree. As a result, a sentence like scope1_, repeated here, will always
be translated as 

.. _scope11:
.. ex:: Every girl chases a dog.

.. _scope12:
.. ex::

   .. _scope12a:
   .. ex::
      :: 

       all x. ((girl x) implies some y. ((dog y) and (chase y x)))

   .. _scope12b:
   .. ex::  
      :: 

       some y. (dog y) and all x. ((girl x) implies (chase y x)))

This limiation can be overcome, for example using the hole semantics
described in Blackburn and Bos, but discussion would take us outside the scope of
the current chapter.


-----------------
 Further Reading
-----------------

Patrick Blackburn and Johan Bos
*Representation and Inference for Natural Language: A First Course in
Computational Semantics*, CSLI Publications


---------
Exercises
---------

1. 


.. include:: footer.txt
