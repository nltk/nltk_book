.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. standard global imports

    >>> import nltk, re, pprint



.. _chap-semantics:

=====================
12. Logical Semantics
=====================

.. _ch12-introduction:

------------
Introduction
------------

So far, this book has concentrated on relatively concrete aspects of
language, particularly on words, and the patterns in which they
occur. In the area of `syntax`:em:, we have studied the organization of
words into parts of speech, and from there we moved on to how
sequences of words are built up to make phrases and sentences. 

By contrast, we have said relatively little about `semantics`:dt:,
that is, about how language is used to convey meanings. However,
meaning is a very slippery notion, and there is much debate and
disagreement about what is involved in a computational model of
linguistic meaning. In this chapter, we will concentrate on one
particular approach to semantics, but at various points we shall
try to indicate alternative perspectives.

In order to pin down 'the meaning of meaning' more closely, let's
suppose that you are a native speaker of English, and have started to
learn German. I ask if you understand what ex-sem1_ means:

.. _ex-sem1:
.. ex::
   Die Kamera gef\ |aumlaut|\ llt Stefan. 

If you know the meanings of the individual words in ex-sem1_, and know
how these meanings are combined to make up the meaning of the whole
sentence, you might say that ex-sem1_ means the same as ex-sem2_:

.. _ex-sem2:
.. ex::
   Stefan likes the camera.

Now, I most likely will take this as evidence that you do grasp the meaning
of ex-sem1_. But isn't this because you have 
translated from one language, German, into another, namely English? If I
didn't already understand English, we would be no further forward! Nevertheless,
there are some important insights here, ones that we will return to shortly.

Here's another way of looking at things. Imagine there is a situation
*s* where there are two entities, Stefan and a specific camera, say Leica M3
Serial No. 700,000. In addition, there is a relation holding between the two
entities, which we will call the *like* relation. If you understand the
meaning of ex-sem1_, then you know that it is true in situation
*s*. In part, you know this because you know that `Stefan`:lx: refers
to Stefan, `Die Kamera`:lx: refers to Leica M3
Serial No. 700,000, and `gef\ |aumlaut|\ llt`:lx: refers to the *like*
relation. You also know that the grammatical subject of `gef\ |aumlaut|\ llt`:lx:
plays the role of the entity that is liked, while the object plays the
role of the entity who does the liking.


We have just presented two contrasting views of semantics. In the
first case, we tried to get at the meaning of ex-sem1_ by translating
it into another language which we already understood. Moreover, we
assumed that we could tell whether the translation was successful, in
the sense of being able to say that ex-sem1_ and ex-sem2_ 'have the
same meaning'. In the second case, rather than relating ex-sem1_ to
a string of formal symbols, we related it to a situation in the
world, with entities and relations between those entities. In both
cases, we talked informally about how the meaning of the parts of the
ex-sem1_ could be combined to make up the meaning of the whole. This
idea about how meanings should be constructed is so fundamental that
we will give it a special name, that is, the `Principle of Compositionality`:dt:.
(See [Partee1995LSC]_ for this formulation.)

Principle of Compositionality:
   The meaning of a whole is a function of the meanings of the parts
   and of the way they are syntactically combined.

So far, so good. But what about computational semantics? Can a
computer understand the meaning of a sentence? And how could we tell
if it did?  This is similar to asking 'Can a computer think?' Alan
Turing famously proposed to answer this by examining the ability of a
computer to hold sensible conversations with a human. Suppose you are
having a chat session with a person and a computer, but you are not
told at the outset which is which. If you cannot identify which of
your partners is the computer after chatting to each of them, then the
computer has successfully imitated a human. If a computer succeeds in
passing itself off as human in this 'imitation game', then according
to Turing, we should be prepared to say that the computer `can`:em:
think and can be said to be intelligent. So Turing sidestepped the
question of somehow examining the internal states of a computer by
instead using its `behaviour`:em: as evidence of intelligence. By the
same reasoning, we could argue that in order to say that a computer
understands English, it just needs to behave as though it did.  What
is important here is not so much the specifics of Turing's imitation
game, but rather the proposal to judge a capacity for natural language
understanding in terms of observable behaviour on the part of the
computer.

-------------------
Querying a Database
-------------------

There a variety of tasks that a computer could perform which might be
considered as involving some amount of natural language
understanding. Right now, we will look at one in particular: we type in
a query in English, and the computer responds with an appropriate (and
if we are lucky, correct) answer. For example:

.. _ex-dbq0:
.. ex::
   .. _ex-dbq01:
   .. ex:: Which country is Athens in?
   .. _ex-dbq02:
   .. ex:: Greece.

Building software that allows this task to be carried out in a general
way is extremely hard, but solving it for a particular case is close
to trivial. We start off by assuming that we have data about cities
and countries in a structured form. To be concrete, we will use a
database table whose first few rows are shown in Table cities_.

.. Note:: The data illustrated in Table cities_ is drawn from the Chat-80 system
   described by [Warren1982EEA]_, which will be  discussed in more detail
   later in the chapter.  Population figures are given in thousands,
   but note that the data used in these examples dates back at least
   to the 1980s, and was already somewhat out of date at the point
   when [Warren1982EEA]_ was published.

.. table:: cities

    ============    ===============	===========
    City            Country		Population
    ============    ===============	===========
    athens	    greece		1368
    bangkok	    thailand		1178
    barcelona	    spain		1280
    berlin	    east_germany	3481
    birmingham	    united_kingdom	1112
    ============    ===============	===========

    ``city_table``: A table of cities, countries and populations

The obvious way to retrieve answers from this tabular data involves
writing queries in a database query language such as SQL. 

.. Note::
     SQL (Structured Query Language) is a language designed for
     retrieving and managing data in relational databases.
     If you want to find out more about SQL,
     `<http://www.w3schools.com/sql/>`_ is a convenient online
     reference.

For example, executing the query ex-dbq1_ will pull out the value ``'greece'``:
  
.. _ex-dbq1:
.. ex::
   SELECT Country FROM city_table WHERE City = 'athens'

This specifies a result set consisting of all values for the column
``Country`` in data rows where the value of the ``City`` column is
``'athens'``.


  
Let's look at how we can get the same effect using English as our
input to the query system. Using the feature-based grammar formalism
developed in Chapter chap-featgram_ we can translate from English to
SQL. The grammar ``sql0.fcfg`` illustrates how to assemble a
meaning representation for a sentence in
tandem with parsing the sentence. Each phrase structure rule is
supplemented with a recipe for constructing a value for the feature
``sem``. As you will see, these recipes are extremely simple; in each
case, we use the string concatenation operation ``+`` (XREF to featgram
chapter) to splice the values for the daughter constituents to
make a value for the mother constituent. 
  
    >>> nltk.data.show_cfg('grammars/sql0.fcfg')
    % start S
    S[sem=(?np + WHERE + ?vp)] -> NP[sem=?np] VP[sem=?vp]
    VP[sem=(?v + ?pp)] -> IV[sem=?v] PP[sem=?pp]
    VP[sem=(?v + ?ap)] -> IV[sem=?v] AP[sem=?ap]
    NP[sem=(?det + ?n)] -> Det[sem=?det] N[sem=?n]
    PP[sem=(?p + ?np)] -> P[sem=?p] NP[sem=?np]
    AP[sem=?pp] -> A[sem=?a] PP[sem=?pp]
    NP[sem='Country="greece"'] -> 'Greece'
    NP[sem='Country="china"'] -> 'China'
    Det[sem='SELECT'] -> 'Which' | 'What'
    N[sem='City FROM city_table'] -> 'cities'
    IV[sem=''] -> 'are'
    A -> 'located'
    P[sem=''] -> 'in'

This allows us to parse a query into SQL.
  
    >>> from nltk.parse import load_earley
    >>> from string import join
    >>> cp = load_earley('grammars/sql0.fcfg')
    >>> query = 'What cities are located in China'
    >>> trees = cp.nbest_parse(query.split())
    >>> answer = trees[0].node['sem']
    >>> q = join(answer)
    >>> print q
    SELECT City FROM city_table WHERE Country="china"

.. note:: |TRY|
   Run the parser with maximum tracing on, i.e.,
   ``cp = load_earley('grammars/sql0.fcfg', trace=3)``, and examine
   how the values of ``sem`` are built up as complete edges are added
   to the chart.

Finally, we execute the query over the database ``city.db`` and
retrieve some results.

    >>> from nltk.sem import chat80
    >>> rows = chat80.sql_query('samples/city.db', q)
    >>> for r in rows: print "%s" % r,
    canton chungking dairen harbin kowloon mukden peking shanghai sian tientsin

Let's review what we've just learned. We claimed earlier that one way
of getting a computer to understand English was making it perform
tasks that seem to require some processing of meaning. The specific
task that we've just examined involves the computer giving useful data
back in response to a natural language query, and we implemented this
by translating a small subset of English into SQL. So to the extent
that our software already understands SQL, in the sense of being
able to execute SQL queries against a database, it also undertands
queries such as `What cities are located in China`:lx:. 

The grammar ``sql0.fcfg``, together with the NLTK Earley parser, is
instrumental in carrying out the translation. How adequate is this
grammar? On the positive side, it follows the Principle of Compositionality
ins assigning some kind of meaning to the parts
of sentences. For example, `what cities`:lx: receives the meaning
``SELECT City from city_table``. The parts can then be combined to
make a translation for the whole sentence, so one can imagine that the
approach could be adopted for other queries of a similar
sort. However, there are also several less satisfactory aspects to
``sql0.fcfg``. First off, we
have hard-wired an embarrassing amount of detail about the database:
we need to know the name of the relevant table (e.g., ``city_table``)
and the names of the fields. But our database could have contained
exactly the same rows of data yet used a different table name and
different field names, in which case the SQL queries would not be executable. Equally,
we could have stored our data in a different format, such as XML, in
which case retrieving the same results would require us to translate
our English queries into an XML query language rather than SQL. These
considerations suggest that we should be translating English into
something that is more abstract and generic than something like SQL.

In order to sharpen the point, let's consider another English query
and its translation:

.. _ex-dbq2:
.. ex::
   .. _ex-dbq21:
   .. ex:: What	cities are in China and have populations above 1,000,000?
   .. _ex-dbq22:
   .. ex:: SELECT City FROM city_table WHERE Country = 'china' AND
           Population > 1000

.. note:: |TRY|
   Extend the grammar ``sql0.fcfg`` so that it will translate
   ex-dbq21_ into ex-dbq22_, and check the values returned by
   the query. 

   You will probably find it easiest to first extend the grammar to
   handle queries like `What cities have populations above
   1,000,000`:lx: before tackling conjunction. After you have had a go
   at this task, you can compare your solution to ``grammars/sql1.fcfg`` in
   the NLTK data distribution.

As you can see, the `and`:lx: conjunction in ex-dbq21_ gets translated
into an ``AND`` in the SQL counterpart, ex-dbq22_. The latter tells us
to select results from rows where two conditions are true
together: the value of the ``Country`` column is ``'china'`` and the
value of the ``Population`` column is greater than 1000. This
interpretation for `and`:lx: invokes an idea we mentioned earlier,
namely it talks about what is true in some particular
situation. Moreover, although we haven't exhausted the meaning of
`and`:lx: in English, we have given it a meaning which is independent
of any query language. In fact, we have given it the standard
interpretation from standard logic.

--------------------------
Natural Language and Logic
--------------------------


A widely adopted approach to representing meaning |mdash| or at least,
some aspects of meaning |mdash| involves translating expressions of
natural language into |fol| (|FOL|). From a computational point of
view, a strong argument in favor of |FOL| is that it strikes a
reasonable balance between expressiveness and logical tractability. On
the one hand, it is flexible enough to represent many aspects of the
logical structure of natural language. On the other hand, automated
theorem proving for |FOL| has been well studied, and although
inference in |FOL| is not decidable, in practice many reasoning
problems are efficiently solvable using modern theorem provers
(cf. [Blackburn2005RIN]_ for discussion).



|nopar| There is an assumption here that the semantically relevant
parts of a complex expression will be determined by a theory of
syntax. Within this chapter, we will take it for granted that
expressions are parsed against a context-free grammar. However, this
is not entailed by the Principle of Compositionality.  To summarize,
we will be concerned with the task of systematically constructing a
semantic representation in a manner that can be smoothly integrated
with the process of parsing.

The overall framework we are assuming is illustrated in Figure semint_. Given
a syntactic analysis of a sentence, we can build one or more semantic
representations for the sentence. Once we have a semantic
representation, we can also check whether it is true in a model. 

.. _semint:
.. ex::
   .. image:: ../images/semint.png
      :scale: 30

|nopar| A `model`:dt: for a logical language is a set-theoretic
construction which provides a very simplified picture of how the world
is. For example, in this case, the model should contain individuals
(indicated in the diagram by small dots) corresponding to Suzie and
Fido, and it should also specify that these individuals belong to
the `chase`:math: relation.

The order of sections in this chapter is not what you might expect
from looking at the diagram. We will start off in the middle of semint_ by
presenting a logical language that will provide us with
semantic representations in |NLTK|. Next, we will show how formulas in
the language can be systematically evaluated in a model. At the end,
we will bring everything together and describe a simple method for
constructing semantic representations as part of the parse process in
|NLTK|.

..
   This chapter will only scratch the surface of how to carry out
   semantic analysis of natural language, and will be heavily slanted
   towards the approach currently implemented within
   |NLTK|. Consequently, it needs to be read in conjunction with a good
   overall introduction to formal semantics. [Eg., B&B, Gamut, Heim?]

--------------------
Propositional  Logic
--------------------

The language of propositional logic represents certain aspects of
natural language, but at a high level of abstraction. The only
structure that is made explicit involves `logical connectives`:dt:\;
these correspond to 'logically interesting' expressions such as
`and`:lx: and `not`:lx:. The basic expressions of the language are
`propositional variables`:dt:, usually written `p`:math:, `q`:math:,
`r`:math:, etc. Let `A`:math: be a finite set of such variables. There
is a disjoint set of logical connectives which contains the unary
operator |neg| (`not`:lx:), and binary operators |wedge| (`and`:lx:),
|vee| (`or`:lx:), |rarr| (`implies`:lx:) and |iff| (`iff`:lx:).

The set of formulas of `L`:subscript:`prop` is described inductively:

   1. Every element of `A`:math: is a formula of `L`:subscript:`prop`.

   2. If |phi| is a formula of `L`:subscript:`prop` , then so is |neg| |phi|.

   3. If |phi| and |psi| are formulas, then so are
      (|phi| |wedge| |psi|),
      (|phi| |vee| |psi|),
      (|phi| |rarr| |psi|) and
      (|phi| |iff| |psi|).

   4. Nothing else is a formula of `L`:subscript:`prop`.

|nopar| Within `L`:subscript:`prop`, we can construct formulas such as

.. _prop01:
.. ex:: `p`:math: |rarr| `q`:math: |vee| `r`:math:

There are many sentences of English which could be taken to have the 
logical structure shown in prop01_. Here's an example:

.. _prop02:
.. ex:: If it is raining, then Kim will take an umbrella or Lee will
        get wet. 

In order to explain the relation between prop01_ and prop02_, we need
to give a `key`:em: which maps between propositional variables and
English sentences:

.. _prop03:
.. ex::  `p`:math: stands for  `it is raining`:lx:,
          `q`:math: for `Kim will take an umbrella`:lx: and 
          `q`:math: for `Lee will get wet`:lx:.


The Boolean connectives of propositional logic are supported by the
``sem`` package, and are parsed into various kinds of
``Expression``. We use ``-``, ``&``, ``|``, ``->``, ``<->`` to stand,
respectively, for `not`:lx:, `and`:lx:, `or`:lx:, `implies`:lx: and
`iff`:lx:. In the following example, we start off by creating a new
instance ``lp`` of the NLTK ``LogicParser()``.

   >>> lp = nltk.LogicParser()
   >>> lp.parse('-(p & q)')
   <NegatedExpression -(p & q)>
   >>> lp.parse('p & q')
   <AndExpression (p & q)>
   >>> lp.parse('p | (r -> q)')
   <OrExpression (p | (r -> q))>
   >>> lp.parse('p <-> -- p')
   <IffExpression (p <-> --p)>


As the name suggests, propositional logic only studies the logical
structure of formulas made up of atomic propositions. We saw, for
example, that propositional variables stood for whole clauses in
English. In order to look at how predicates combine with arguments, we
need to look at a more complex language for semantic representation,
namely |fol|. In order to show how this new language interacts with
the |lambda|-calculus, it will be useful to introduce the notion of
types into our syntactic definition, in departure from the rather
simple approach to defining the clauses of `L`:subscript:`prop`.

In the general case, we interpret sentences of a logical language
relative to a model, which is a very simplified version of the
world. A model for propositional logic needs to assign the values
``True`` or ``False`` to every possible formula. We do this
inductively: first, every propositional variable is assigned a value,
and then we compute the value of complex formulas by consulting the
meanings of the Boolean connectives and applying them to the values of
the formula's components. Let's create a valuation:

    >>> val1 = nltk.sem.Valuation([('p', True), ('q', True), ('r', False)])

We initialize a ``Valuation`` with a list of pairs, each of which
consists of a semantic symbol and a semantic value. The resulting
object is essentially just a dictionary that maps logical expressions
(treated as strings) to appropriate values.

   >>> val1['p']
   True

The keys of the dictionary (sorted alphabetically) can also be
accessed via the property ``symbols``:

    >>> val1.symbols
    ['p', 'q', 'r']

As we will see later, our models need to be somewhat more complicated
in order to handle the more complicated expressions discussed in the
next section, so for the time being, just ignore the ``dom1`` and
``g1`` variables in the following declarations.

    >>> dom1 = set([])
    >>> g = nltk.sem.Assignment(dom1)

Now, let's create a model ``m that uses ``val1``:

    >>> m1 = nltk.sem.Model(dom1, val1, prop=True)

The ``prop=True`` is just a flag to say that our models are intended
for propositional logic.

Every instance of Model defines appropriate truth functions for the
Boolean connectives (and in fact they are implemented as functions
named ``AND()``, ``IMPLIES()`` and so on).

    >>> m1.AND
    <bound method Model.AND of (set([]), {'q': True, 'p': True, 'r': False})>

We can use these functions to create truth tables:

    >>> for first in [True, False]:
    ...     for second in [True, False]:
    ...	        print "%s %s => %s" % (first, second, m1.AND(first, second))
    True True => True
    True False => False
    False True => False
    False False => False


-----------------
First-Order Logic
-----------------

Predication
-----------

In |fol| (|FOL|), propositions are analyzed into predicates and
arguments, which takes us a step closer to the structure of natural
languages. The standard construction rules for |FOL| recognize
`terms`:dt: such as individual variables and individual constants, and
`predicates`:dt: which take differing numbers of arguments. For
example, `Adam walks`:lx: might be formalized as `walk(adam)`:mathit:
and `Adam sees Betty`:lx: as `see(adam, betty)`:mathit:. We will call
`walk`:mathit: a `unary predicate`:dt:, and `see`:mathit: a `binary
predicate`:dt:. Semantically, `see`:mathit: is usually modeled as a
relation, i.e., a set of pairs, and the proposition is true in a
situation just in case the ordered pair pair |langle|\ `a, b`:math:\
|rangle| belongs to this set.

There is an alternative
approach in which predication is treated as function application.  In
this functional style of representation, `Adam sees Betty`:lx: can be
formalized as `see(a)(b)`:mathit:. That is, rather than being modeled as a
relation, `see`:mathit: denotes a function which applies to one argument to
yield a new function that is then applied to the second argument. In
NLTK, we will in fact treat predications syntactically as function
applications, but we use a concrete syntax that allows them to
represented as `n`:math:-ary relations.

    >>> parsed = lp.parse('see(a, b)')
    >>> parsed.argument
    <IndividualVariableExpression b>   
    >>> parsed.function
    <ApplicationExpression see(a)>
    >>> parsed.function.function
    <VariableExpression see>

Relations are represented semantically in NLTK in the standard
set-theoretic way: as sets of tuples. For example, let's suppose we
have a domain of discourse consisting of the individuals Adam, Betty and Fido,
where Adam is a boy, Betty is a girl and Fido is a dog. For mnemonic
reasons, we use ``b1``, ``g1`` and ``d1`` as the corresponding labels
in the model. We can declare the domain as follows:

    >>> dom2 = set(['b1', 'g1', 'd1'])

As before, we are going to initialize a valuation with a list of (symbol,
value) pairs:

    >>> v = [('adam','b1'),('betty','g1'),('fido','d1'),
    ... ('boy',set(['b1'])),('girl',set(['g1'])), ('dog',set(['d1'])),
    ... ('walk',set(['g1', 'd1'])),
    ... ('see', set([('b1','g1'),('d1','b1'),('g1','d1'),]))]
    >>> val2 = nltk.sem.Valuation(v)
    >>> print val2
    {'adam': 'b1',
     'betty': 'g1',
     'boy': set([('b1',)]),
     'dog': set([('d1',)]),
     'fido': 'd1',
     'girl': set([('g1',)]),
     'see': set([('b1', 'g1'), ('d1', 'b1'), ('g1', 'd1')]),
     'walk': set([('d1',), ('g1',)])}

So according to this valuation, the value of ``see`` is a set of
tuples such that Adam sees Betty, Fido sees Adam, and
Betty sees Fido.

You may have noticed that our unary predicates (i.e, ``boy``, ``girl``,
``dog``) also come out represented as sets of singleton tuples, rather
than just sets of individuals. This is a convenience which allows us
to have a uniform treatment of relations of any arity. In order to
combine a unary relation with an argument, we use the function
``app()``. If the input relation is unary, then ``app()`` returns a
Boolean value; if the input is :mathit:`n`-ary, for `n`:mathit: > 1,
then ``app()`` returns an :mathit:`n`-1-ary relation.

    >>> from nltk.sem import app
    >>> boy = val2['boy']
    >>> app(boy, 'b1')
    True
    >>> app(boy, 'g1')
    False
    >>> see = val2['see']
    >>> app(see, 'g1')
    set([('d1',)])
    >>> app(app(see, 'g1'), 'd1')
    True

Individual Variables and Assignments
------------------------------------

In |FOL|, arguments of predicates can also be individual variables
such as ``x``, ``y`` and ``z``. These can be thought of as similar to
personal pronouns like `he`:lx:, `she`:lx: and `it`:lx:, in that we
need to know about the context of use in order to figure out their
denotation. In our models, the counterpart of a context of use is a
variable `Assignment`:dt:. This is a mapping from individual variables to
entities in the domain. 
Assignments are created using the ``Assignment`` constructor, which
also takes the model's domain of discourse as a parameter. We are not
required to actually enter any bindings, but if we do, they are in a
(variable, value) format similar to what we say earlier for valuations.

   >>> g = nltk.sem.Assignment(dom2, [('x', 'b1'), ('y', 'd1')])
   >>> g
   {'y': 'd1', 'x': 'b1'}

|nopar| In addition, there is a ``print()`` format for assignments which
uses a notation closer to that in logic textbooks:

   >>> print g
   g[d1/y][b1/x]

Let's now look at how we can evaluate an atomic formula of
|FOL|. First, we create a model, then we use the ``evaluate()`` method
to compute the truth value.

    >>> m2 = nltk.sem.Model(dom2, val2)
    >>> m2.evaluate('see(betty, y)', g)
    True

What's happening here? Essentially, we are making a call to
``app(app(see, 'g1'), 'd1')`` just as in our earlier example.
However, when the interpretation function encounters the variable ``'y'``,
rather than checking for a value in ``val2``, it asks the variable
assignment ``g`` to come up with a value:

  >>> g['y']
  'd1'

Since we already know that 'b1' and 'g1' stand in the `see`:mathit:
relation, the value ``True`` is what we expected. In this case, we can
say that assignment ``g`` `satisfies`:dt: the formula 'see(adam, y)'.
By contrast, the following formula evaluates to ``False`` relative to
``g`` |mdash| check that you see why this is.

    >>> m2.evaluate('see(x, y)', g)
    False

In our approach (though not in standard |fol|), variable assignments
are `partial`:em:. For example, ``g`` says nothing about any variables
apart from ``'x'`` and ``'y'''. The method ``purge()`` clears all
bindings from an assignment.

    >>> g.purge()
    >>> g
    {}

If we now try to evaluate a formula such as 'see(adam, y)' relative to
``g``, it is like trying to interpret a sentence containing a `she`:lx: when
we don't know what `she`:lx: refers to. In this case, the evaluation function
fails to deliver a truth value.

    >>> m2.evaluate('see(adam, y)', g)
    'Undefined'

Quantification and Scope
------------------------

|Fol| standardly offers us two quantifiers, `all`:lx: (or `every`:lx:)
and `some`:lx:. These are formally written as |forall| and |exists|,
respectively. At the syntactic level, quantifiers are used to bind
individual variables like ``'x'`` and ``'y'``. The following two sets
of examples show a simple English example, a logical representation,
and the encoding which is accepted by the |NLTK| ``logic`` module.

.. _forall1:
.. ex::

   .. _forall1a:
   .. ex:: Every dog barks.

   .. _forall1b:
   .. ex:: |forall|\ `x.(dog(x)`:mathit: |rarr| `bark(x))`:mathit:

   .. _forall1c:
   .. ex:: ``all x.(dog(x) -> bark(x))``

.. _exists1:
.. ex::

   .. ex:: Some girl walks.

   .. ex:: |exists|\ `x.(girl(x)`:mathit: |wedge| `walk(x))`:mathit:

   .. _existsc:
   .. ex:: ``some x.(girl(x) & walk(x))``

In the existsc_, the quantifier ``some`` binds both occurences of the
variable ``'x'``. As a result, existsc_ is said to be a `closed
formula`:dt:. By contrast, if we look at the body of existsc_,
the variables are unbound:

.. _exists2:
.. ex:: girl(x) & walk(x)

exists2_ is said to be an `open formula`:dt:. As we saw earlier, the
interpretation of open formulas depends on the particular variable
assignment that we are using. 

One of the crucial insights of modern
logic is that the notion of variable satisfaction can be used to
provide an interpretation to quantified formulas. Let's continue to
use existsc_ as an example. When is it true? Let's think about all the
individuals in our domain, i.e., in ``dom2``. We want to check whether
any of these individuals have the property of being a girl and
walking. In other words, we want to know if there is some ``u`` in
``dom2`` such that ``g[u/x]`` satisfies the open formula
exists2_. Consider the following:

    >>> m2.evaluate('exists x.(girl(x) & walk(x))', g)
    True

``evaluate()`` returns ``True`` here because there is some ``u`` in
``dom2`` such that exists2_ is satisfied by an assigment which binds
``'x'`` to ``u``. In fact, ``g1`` is such a ``u``:

    >>> m2.evaluate('girl(x) & walk(x)', g.add('x', 'g1'))
    True

One useful tool offered by NLTK is the ``satisfiers()`` method. This
lists all the individuals that satisfy an open formula. The method
parameters are a parsed formula, a variable, and an assignment. Here
are a few examples:

    >>> fmla1 = lp.parse('girl(x) | boy(x)')
    >>> m2.satisfiers(fmla1, 'x', g)
    set(['b1', 'g1'])
    >>> fmla2 = lp.parse('girl(x) -> walk(x)')
    >>> m2.satisfiers(fmla2, 'x', g)
    set(['b1', 'g1', 'd1'])
    >>> fmla3 = lp.parse('walk(x) -> girl(x)')
    >>> m2.satisfiers(fmla3, 'x', g)
    set(['b1', 'g1'])

It's useful to think about why ``fmla2`` and ``fmla3`` receive the
values they do. In particular, recall the truth conditions for ``->``
(encoded via the function ``IMPLIES()`` in every model):

    >>> for first in [True, False]:
    ...     for second in [True, False]:
    ...	        print "%s %s => %s" % (first, second, m2.IMPLIES(first, second))
    True True => True
    True False => False
    False True => True
    False False => True

This means that 
``fmla2`` is equivalent to this:

.. _fmla2or:
.. ex:: ``- girl(x) | walk(x)``

That is, fmla2or_ is satisfied by something which either isn't a girl
or walks. Since neither ``b1`` (Adam) nor ``d1`` (Fido)
are girls, according to model ``m2``, they both satisfy 
the whole formula. And of course ``g1`` satisfies the formula because ``g1``
satisfies both disjuncts. Now, since every member of the domain of
discourse satisfies ``fmla2``, the corresponding universally
quantified formula is also true.

    >>> m2.evaluate('all x.(girl(x) -> walk(x))', g)
    True

In other words, a universally quantified formula |forall|\
`x.`:mathit:\ |phi| is true with respect to ``g`` just in case for
every ``u``, |phi| is true with respect to ``g[u/x]``. 



.. 
   |nopar| The inclusion of first-order quantifiers motivates the final clause of
   the definition of our version of |fol|.

   7. If `x`:mathit: |element|  **Var**\ (**Ind**) and |phi| 
      |element|  **Term**\ (**Bool**), then |forall|\ `x.`:mathit:\
      |phi|, |exists|\ `x.`:mathit:\ |phi|  |element|  **Term**\
    (**Bool**).  

    One important property of forall1b_ often trips people up. The logical
    rendering in effect says that *if* something is a dog, then it barks,
    but makes no commitment to the existence of dogs. So in a situation
    where nothing is a dog, forall1b_ will still come out true. (Remember
    that ``'(p implies q)'`` is true when ``'p'`` is false.) Now you might
    argue that forall1b_ does presuppose the existence of dogs,
    and that the logic formalization is wrong. 
    But it is possible  to find other
    examples which lack such a presupposition.  For instance, we might
    explain that the value of the Python expression ``re.sub('ate', '8',
    astring)`` is the result of replacing all occurrences of ``'ate'`` in
    ``astring`` by ``'8'``, even though there may in fact be no such occurrences.


Quantifier Scope Ambiguity
------------------------------

What happens when we want to give a formal representation of a
sentence with *two* quantifiers, such as the following?

.. _scope1:
.. ex:: Every girl chases a dog.

There are (at least) two ways of expressing scope1_ in |FOL|:

.. _scope2:
.. ex::

   .. _scope2a:
   .. ex:: |forall|\ `x.(girl(x)`:mathit: |rarr| |exists|\
           `y.(dog(y)`:mathit: |wedge| `chase(x,y)))`:mathit:

   .. _scope2b:
   .. ex:: |exists|\ `y.(dog(y)`:mathit: |wedge| |forall|\
           `x.(girl(x)`:mathit: |rarr|  `chase(x,y)))`:mathit:

Can we use both of these? Then answer is Yes, but they have different
meanings. scope2b_ is logically stronger than scope2a_: it claims that
there is a unique dog, say Fido, which is chased by every girl.
scope2a_, on the other hand, just requires that for every girl
`g`:mathit:, we can find some dog which `d`:mathit: chases; but this could
be a different dog in each case. We distinguish between scope2a_ and
scope2b_ in terms of the `scope`:dt: of the quantifiers. In the first,
|forall| has wider scope than |exists|, while in scope2b_, the scope ordering
is reversed. So now we have two ways of representing the meaning of
scope1_, and they are both quite legitimate. In other words, we are
claiming that scope1_ is *ambiguous* with respect to quantifier scope,
and the formulas in scope2_ give us a formal means of making the two
readings explicit. However, we are not just interested in associating
two distinct representations with scope1_. We also want to show in
detail how the two representations lead to different conditions for
truth in a formal model.

|nopar| In order to examine the ambiguity more closely, let's fix our
valuation as follows:


   >>> v3 = [('john', 'b1'),
   ... ('mary', 'g1'),
   ... ('suzie', 'g2'),
   ... ('fido', 'd1'),
   ... ('tess', 'd2'),
   ... ('noosa', 'n'),
   ... ('girl', set(['g1', 'g2'])),
   ... ('boy', set(['b1', 'b2'])),
   ... ('dog', set(['d1', 'd2'])),
   ... ('bark', set(['d1', 'd2'])),
   ... ('walk', set(['b1', 'g2', 'd1'])),
   ... ('chase', set([('b1', 'g1'), ('b2', 'g1'), ('g1', 'd1'), ('g2', 'd2')])),
   ... ('see', set([('b1', 'g1'), ('b2', 'd2'), ('g1', 'b1'),
   ... ('d2', 'b1'), ('g2', 'n')])),
   ... ('in', set([('b1', 'n'), ('b2', 'n'), ('d2', 'n')])),
   ... ('with', set([('b1', 'g1'), ('g1', 'b1'), ('d1', 'b1'), ('b1', 'd1')]))]
   >>> val3 = nltk.sem.Valuation(v3)

|nopar| We can use the graph in chasegraph_ to visualize the
`chase`:mathit: relation.

.. _chasegraph:
.. ex::
   .. image:: ../images/models_chase.png
      :scale: 30

|nopar| In chasegraph_, an arrow between two individuals `x`:math: and
`y`:math: indicates that `x`:math: chases
`y`:math:. So ``b1`` and ``b2`` both chase ``g1``, while ``g1`` chases
``d1`` and ``g2`` chases ``d2``. In this model, formula ``scope2a_`` above
is true but ``scope2b_`` is false. One way of exploring these results is by
using the ``satisfiers()`` method of ``Model`` objects.

   >>> dom3 = val3.domain
   >>> m3 = nltk.sem.Model(dom3, val3)
   >>> g = nltk.sem.Assignment(dom3)
   >>> fmla1 = lp.parse('(girl(x) -> exists y.(dog(y) & chase(x, y)))')
   >>> m3.satisfiers(fmla1, 'x', g)
   set(['g2', 'g1', 'n', 'b1', 'b2', 'd2', 'd1'])
   >>> 

|nopar| This gives us the set of individuals that can be assigned as the value
of ``x`` in ``fmla1``. In particular, every girl is included in this
set. By contrast, consider the formula ``fmla2`` below; this has no
satisfiers for the variable ``y``.

   >>> fmla2 = lp.parse('(dog(y) & all x.(girl(x) -> chase(x, y)))')
   >>> m3.satisfiers(fmla2, 'y', g)
   set([])
   >>> 

|nopar| That is, there is no dog that is chased by both ``g1`` and
``g2``. Taking a slightly different open formula, ``fmla3``, we
can verify that there is a girl, namely ``g1``, who is chased by every boy.

   >>> fmla3 = lp.parse('(girl(y) & all x.(boy(x) -> chase(x, y)))')
   >>> m3.satisfiers(fmla3, 'y', g)
   set(['g1'])

..
   ADD something about the parse_valuation method

----------------------------
Evaluating English Sentences
----------------------------

Using the ``sem`` Feature
-------------------------

Until now, we have taken for granted that we have some appropriate
logical formulas to interpret. However, ideally we would like to
derive these formulas from natural language input. One relatively easy
way of achieving this goal is to build on the grammar framework
developed in Chapter chap-featgram_. Our first step is to introduce a new feature,
``sem``. Because values of ``sem`` generally need to be treated
differently from other feature values, we use the convention of
enclosing them in angle brackets. sem1_ illustrates a first
approximation to the kind of analyses we would like to build.

.. _sem1:
.. ex::
   .. tree:: (S[sem=\<\walk\(jane\)\>] (NP[sem=\<\(jane\)\>] Jane) (VP[sem=\<\walk\>] (V[sem=\<\walk\>] walks)))

|nopar| Thus, the ``sem`` value at the root node shows a semantic
representation for the whole sentence, while the ``sem`` values at
lower nodes show semantic representations for constituents of the
sentence. So far, so good, but how do we write grammar rules which
will give us this kind of result? To be more specific, suppose we have
a `np`:gc: and `vp`:gc: constituents with appropriate values for their
``sem`` nodes? If you reflect on the machinery that was introduced in
discussing the |lambda| calculus, you might guess that function
application will be central to composing semantic values. You will
also remember that our feature-based grammar framework gives us the
means to refer to `variable`:em: values. Putting this together, we can
postulate a rule like sem2_ for building the ``sem`` value of an
`s`:gc:.  (Observe that in the case where the value of ``sem`` is a
variable, we omit the angle brackets.)

.. _sem2:
.. ex::
   ::

     S[sem = <app(?vp,?subj)>] -> NP[sem=?subj] VP[sem=?vp]

sem2_ tells us that given some ``sem`` value ``?subj`` for the subject
`np`:gc: and some ``sem`` value ``?vp`` for the `vp`:gc:, the ``sem``
value of the `s`:gc: mother is constructed by applying ``?vp`` as a
functor to ``?np``.  From this, we can conclude that ``?vp`` has to
denote a function which has the denotation of ``?np`` in its
domain; in fact, we are going to assume that ``?vp`` denotes a
curryed characteristic function on individuals. sem2_ is a nice example of
building semantics using `the principle of compositionality`:dt:\:
that is, the principle that the semantics of a complex expression is a
function of the semantics of its parts.

To complete the grammar is very straightforward; all we require are the
rules shown in sem3_.

.. _sem3:
.. ex::
   ::

     VP[sem=?v] -> IV[sem=?v]
     NP[sem=<jane>] -> 'Jane'
     IV[sem=<walk>] -> 'walks'

|nopar| 
The `vp`:gc: rule says that the mother's semantics is the same as the
head daughter's. The two lexical rules just introduce non-logical
constants to serve as the semantic values of `Jane`:lx: and
`walks`:lx: respectively. This grammar can be parsed using the chart
parser in ``parse.featurechart``, and the trace in sem4_
shows how semantic values are derived by feature unification in the
process of building a parse tree.

.. _sem4:
.. ex::
   ::

     Predictor |> . .| S[sem='?vp(?subj)'] -> * NP[sem=?subj] VP[sem=?vp] 
     Scanner   |[-] .| [0:1] 'Jane' 
     Completer |[-> .| S[sem='?vp(john)'] -> NP[sem='john'] * VP[sem=?vp] 
     Predictor |. > .| VP[sem=?v] -> * IV[sem=?v] 
     Scanner   |. [-]| [1:2] 'walks' 
     Completer |. [-]| VP[sem='walk'] -> IV[sem='walk'] * 
     Completer |[===]| S[sem='walk(john)'] -> NP[sem='john'] VP[sem='walk'] * 
     Completer |[===]| [INIT] -> S * 

Quantified `np`:gc:\ s
----------------------

You might be thinking this is all too easy |mdash| surely there is a
bit more to building compositional semantics. What about quantifiers,
for instance? Right, this is a crucial issue. For example, we want
sem5a_ to be given a semantic representation like sem5b_. How can this
be accomplished? 

.. _sem5: 
.. ex::
   .. _sem5a:
   .. ex:: 
      A dog barks.
   .. _sem5b:
   .. ex:: 
      ``'exists x.(dog(x) & (bark(x))'``

|nopar| Let's make the assumption that our `only`:em: operation for building
complex semantic representations is function
application. Then our problem is this: how do we give a semantic
representation to quantified `np`:gc:\s such as `a dog`:lx: so that
they can be combined with something like ``'walk'`` to give a result
like sem5b_? As a first step, let's make the subject's ``sem`` value
act as the functor rather than the argument. Now we are
looking for way of instantiating ``?np`` so that sem6a_ is equivalent
to sem6b_.

.. _sem6: 
.. ex::

   .. _sem6a:
   .. ex:: | [sem=<bark(?np)>]

   .. _sem6b:
   .. ex:: | [sem=<exists x.(dog(x) & bark(x))>]


|nopar| This is where |lambda| abstraction comes to the rescue;
doesn't sem6_ look a bit reminiscent of carrying out |beta|-reduction
in the |lambda|-calculus? In other words, we want a |lambda| term
`M`:mathit: to replace ``'?np'`` so that applying `M`:mathit: to
``'bark'`` yields sem5b_.  To do this, we replace the occurence of
``'bark'`` in sem5b_ by a variable ``'P'``, and bind the variable with
|lambda|, as shown in sem7_.

.. _sem7: 
.. ex:: ``'\P.exists x.(dog(x) & P(x))'``

|nopar| As a point of interest, we have used a different style of variable in
sem7_, that is ``'P'`` rather than ``'x'`` or ``'y'``. This is to signal
that we are abstracting over a different kind of thing |mdash| not an
individual, but a function from **Ind** to **Bool**. So the type of
sem7_ as a whole is ((**Ind** |rarr| **Bool**) |rarr| **Bool**). We
will take this to be the type of `np`:gc:\ s in general. To illustrate
further, a universally quantified `np`:gc: will look like sem7univ_.

.. _sem7univ: 
.. ex:: ``'\P.all x.(dog(x) -> P(x))'``


We are pretty much done now, except that we also want to carry out a
further abstraction plus application for the process of combining the
semantics of the determiner `a`:lx: with the semantics of `dog`:lx:.
Applying sem7_ as a functor to ``'bark'`` gives us 
``'(\P.exists x.(dog(x) & P(x)))(bark)'``, and carrying out |beta|\-reduction 
yields just what we wanted, namely sem5b_.


|NLTK| provides some utilities to make it easier to derive and inspect
semantic interpretations. ``text_interpret()`` is intended for batch
interpretation of a list of input sentences. It builds a dictionary
``d`` where for each sentence ``sent`` in the input,
``d[sent]`` is a list of paired trees and semantic representations for
``sent``. The value is a list, since ``sent`` may be syntactically
ambiguous; in the following example, we just look at the first member
of the list.

    >>> grammar = nltk.data.load('grammars/sem1.fcfg')
    >>> result  = nltk.sem.text_interpret(['a dog barks'], grammar, beta_reduce=0)
    >>> (syntree, semrep) = result['a dog barks'][0]
    >>> print syntree
    (S[sem=<exists x.(dog(x) & bark(x))>]
      (NP[sem=<\P.exists x.(dog(x) & P(x))>]
        (Det[sem=<\Q.\P.exists x.(Q(x) & P(x))>] a)
         (N[sem=<dog>] dog))
       (VP[sem=<\x.bark(x)>] (IV[sem=<\x.bark(x)>] barks)))


|nopar| By default, the semantic representation that is produced by
``text_interpret()`` has already undergone |beta|-reduction, but in the
above example, we have overridden this.  Subsequent reduction is
possible using the ``simplify()`` method, and Boolean connectives can be
placed in infix position with the ``infixify()`` method.

   >>> print semrep.simplify()
   exists x.(dog(x) & bark(x))


Transitive Verbs
----------------

Our next challenge is to deal with sentences containing transitive
verbs, such as sem8_.

.. _sem8: 
.. ex:: Suzie chases a dog.

|nopar| The output semantics that we want to build is shown in sem9_.

.. _sem9: 
.. ex:: ``'exists x.(dog(x) & chase(suzie, x))'``

|nopar| Let's look at how we can use |lambda|-abstraction to get this
result. A significant constraint on possible solutions is to require
that the semantic representation of `a dog`:lx: be independent of
whether the `np`:gc: acts as subject or object of the sentence. In
other words, we want to get sem9_ as our output while sticking to
sem7_ as the `np`:gc: semantics. A second constraint is that
`vp`:gc:\s should have a uniform type of interpretation regardless
of whether they consist of just an intransitive verb or a transitive
verb plus object. More specifically, we stipulate that `vp`:gc:\s
always denote characteristic functions on individuals. Given these
constraints, here's a semantic representation for `chases a dog`:lx:
which does the trick.

.. _sem99: 
.. ex:: ``'\y.exists x.(dog(x) & chase(y, x))'``

|nopar| Think of sem99_ as the property of being a `y`:math: such that
for some dog `x`:math:, `y`:math: chases `x`:math:; or more
colloquially, being a `y`:math: who chases a dog. Our task now
resolves to designing a semantic representation for
`chases`:lx: which can combine via ``app`` with sem7_ so as to allow
sem99_ to be derived. 

Let's carry out a kind of inverse |beta|-reduction on sem99_,
giving rise to sem10_.

Then we are part
way to the solution if we can derive sem10_, where ``'X'`` is applied to
``'\z.chase(y, z)'``.

.. _sem10: 
.. ex:: ``'(\P.exists x.(dog(x) & P(x)))(\z.chase(y, z))'``

|nopar| sem10_ may be slightly hard to read at first; you need to see that
it involves applying the quantified `np`:gc: representation from 
sem7_ to ``'\z.chase(y,z))'``. sem10_ is of course 
equivalent to sem99_.

Now let's replace the functor in sem10_ by a variable ``'X'`` of the
same type as an `np`:gc:; that is, of type ((**Ind** |rarr| **Bool**)
|rarr| **Bool**). 

.. _sem11: 
.. ex:: ``'X(\z.chase(y, z))'``

|nopar| The representation of a transitive verb will have to apply to
an argument of the type of ``'X'`` to yield a functor of the type of
`vp`:gc:\ s, that is, of type (**Ind** |rarr| **Bool**). We can ensure
this by abstracting over both the ``'X'`` variable in sem11_ and also
the subject variable ``'y'``. So the full solution is reached by
giving `chases`:lx: the semantic representation shown in sem12_.

.. _sem12: 
.. ex:: ``'\X y.X(\x.chase(y, x))'``

|nopar| If sem12_ is applied to sem7_, the result after |beta|-reduction is
equivalent to sem99_, which is what we wanted all along:

.. ex:: | ``'(\X y.(X \x.(chase(y, x)) \P.exists x.(dog(x) & P(x))'``
        | |reduce|
        | ``'(\y.(\P.exists x.(dog(x) & P(x)) \x.chase(y, x))'``
	| |reduce|
        | ``'\y.(exists x.(dog(x) & chase(y, x)))'``

In order to build a semantic representation for a sentence, we also
need to combine in the semantics of the subject `np`:gc:. If the
latter is a quantified expression like `every girl`:lx:, everything
proceeds in the same way as we showed for `a dog barks`:lx: earlier
on; the subject is translated as a functor which is applied to the
semantic representation of the `vp`:gc:.  However, we now seem to have
created another problem for ourselves with proper names. So far, these
have been treated semantically as individual constants, and these
cannot be applied as functors to expressions like
sem99_. Consequently, we need to come up with a different semantic
representation for them. What we do
in this case is re-interpret proper names so that they too are
functors, like quantified `np`:gc:\ s. sem13_ shows the required
|lambda| expression for `Suzie`:lx:.

.. _sem13: 
.. ex:: ``'\P.P(suzie)'``

|nopar| sem13_ denotes the characteristic function corresponding to the set of
all properties which are true of Suzie. Converting from an individual
constant to an expression like sem12_ is known as `type raising`:dt:,
and allows us to flip functors with arguments. That is, type raising
means that we can replace a Boolean-valued application such as 
`f(a`:mathit:) with an equivalent application 
`(`:math:\ |lambda|\ `P.P(a))(f`:mathit:).


One important limitation of the approach we have presented here is
that it does not attempt to deal with scope ambiguity. Instead,
quantifier scope ordering directly reflects scope in the parse
tree. As a result, a sentence like scope1_, repeated here, will always
be translated as scope12a_, not scope12b_.

.. _scope11:
.. ex:: Every girl chases a dog.

.. _scope12:
.. ex::

   .. _scope12a:
   .. ex:: ``'all x.(girl(x) -> exists y. (dog(y) & chase(x,y)))'``

   .. _scope12b:
   .. ex:: ``'exists y. dog(y) & all x. (girl(x) -> chase(x,y)))'``

|nopar| This limitation can be overcome, for example using the hole semantics
described in [Blackburn2005RIN]_, but discussing the details would take
us outside the scope of the current chapter.

Now that we have looked at some slightly more complex constructions, we can
evaluate them in a model. In the following example, we derive two
parses for the sentence `every boy chases a girl in Noosa`:lx:, and
evaluate each of the corresponding semantic representations in the
model ``model0.py`` which we have imported.

    >>> grammar = nltk.data.load('grammars/sem2.fcfg')
    >>> val4 = nltk.data.load('grammars/valuation1.val')
    >>> dom4 = val4.domain
    >>> m4 = nltk.sem.Model(dom4, val4)
    >>> g = nltk.sem.Assignment(dom4)
    >>> sent = 'every boy chases a girl in Noosa'
    >>> result = nltk.sem.text_evaluate([sent], grammar, m4, g)
    >>> for (syntree, semrep, value) in result[sent]:
    ... 	print "'%s' is %s in Model m\n" % (semrep, value)
    'all x.(boy(x) -> (exists z2.(girl(z2) & chase(x,z2)) & in(x,noosa)))' is True in Model m
    <BLANKLINE>
    'all x.(boy(x) -> (exists z2.(girl(z2) & chase(x,z2)) & in(x,noosa)))' is True in Model m
    <BLANKLINE>
    'all x.(boy(x) -> exists z3.((girl(z3) & in(z3,noosa)) & chase(x,z3)))' is False in Model m
    <BLANKLINE>
    'all x.(boy(x) -> exists z3.((girl(z3) & in(z3,noosa)) & chase(x,z3)))' is False in Model m
    <BLANKLINE>

--------------
Hole Semantics
--------------

Hole Semantics in |NLTK| is handled by the
``nltk.sem.hole`` module, which uses a context free grammar to
generate an underspecified logical form.  Since the latter is itself a
formula of first order logic, we can continue to use the ``sem`` feature
in the context free grammar:

.. ex::
    N[sem=<\x h l.(PRED(l,dog,x) & LEQ(l,h) & HOLE(h) & LABEL(l))>] -> 'dog'

The Hole Semantics module uses a standard plugging algorithm to derive the
sentence's readings from the underspecified LF.

    >>> from nltk.sem import hole
    >>> readings = hole.main('every girl chases a dog')
    >>> for r in reading: print r
    exists z1.(dog(z1) & all z2.(girl(z2) -> chase(z1,z2)))
    all z2.(girl(z2) -> exists z1.(dog(z1) & chase(z1,z2)))


-------------------------------
Discourse Representation Theory
-------------------------------

The ``nltk.sem.drt`` module introduces
a ``DRS()`` constructor which takes lists of discourse referents
and conditions as initialization parameters:

.. ex::
    DRS([j,d],[John(j), dog(d), sees(j,d)])

On top of the functionality available for |FOL|
expressions, |DRT| expressions have a |DRS|\ -concatenation operator,
represented as the ``+`` symbol.  The concatenation of two |DRS|\ s
is a single |DRS| containing the merged discourse referents and the
conditions from both arguments.  |DRS|\ -concatenation automatically
|alpha|\ -converts bound variables to avoid name-clashes.  The
``+`` symbol is overloaded so that |DRT| expressions can be added
together easily.  The ``nltk.sem.drt`` parser allows |DRS|\ s to be
specified succinctly as strings.

    >>> from nltk.sem import drt
    >>> dp = drt.DrtParser()
    >>> d1 = dp.parse('([x],[walk(x)]) + ([y],[run(y)])')
    >>> print d1
    (([x],[walk(x)]) + ([y],[run(y)]))
    >>> print d1.simplify()
    ([x,y],[walk(x), run(y)])
    >>> d2 = dp.parse('([x,y],[Bill(x), Fred(y)])')
    >>> d3 = dp.parse("""([],[([u],[Porsche(u), own(x,u)])
    ...  ->  ([v],[Ferrari(v), own(y,u)])])""")
    >>> d4 = d2 + d3
    >>> print d4.simplify()
    ([x,y],[Bill(x), Fred(y), (([u],[Porsche(u), own(x,u)]) -> ([v],[Ferrari(v), own(y,u)]))])


.. _fig-drs:
.. figure:: ../images/drs.png
   :scale: 100

   DRS Screenshot


|DRT| expressions can be converted to their first order predicate
logic equivalents using the ``toFol()`` method and can be
graphically rendered on screen with the ``draw()`` method.

    >>> print d1.toFol()
    (exists x.walk(x) & exists y.run(y))
    >>> d4.simplify().draw()

Since the |lambda| operator can be combined with |DRT| expressions,
the ``nltk.sem.drt`` module can be used as a plug-in replacement for 
``nltk.sem.logic`` in building compositional semantics.

---------------
Inference tools
---------------

In order to perform inference over semantic representations, |NLTK|
can call both theorem provers and model builders.
The library includes a pure Python tableau-based first order theorem prover;
this is intended to allow students to study 
tableau methods for theorem proving, and provides an
opportunity for experimentation.  In addition, |NLTK| provides
interfaces to two state-of-the-art tools, namely the theorem prover Prover9, 
and the model builder Mace4  [McCune]_.  

The ``get_prover(G, A)`` method by default calls Prover9, and takes as
parameters a proof goal ``G`` and a list ``A`` of assumptions.
Here, we verify that if every dog barks, and Rover is a dog,
then it is true that Rover barks:

    >>> from nltk.inference import inference
    >>> a = lp.parse('all x.(dog(x) -> bark(x))')
    >>> b = lp.parse('dog(rover)')
    >>> c = lp.parse('bark(rover)')
    >>> prover = inference.get_prover(c, [a,b])
    >>> prover.prove()
    True


A theorem prover can also be used to check the logical equivalence of
expressions.  For two expressions *A* and *B*, we can pass (*A* |iff|
*B*) into a theorem prover and know that the theorem will be proved if
and only if the expressions are logically equivalent.  |NLTK|\ 's
standard equality operator for ``Expression``\ s (``==``) is able to
handle situations where two expressions are identical up to
|alpha|-conversion.  However, it would be impractical for |NLTK| to
invoke a wider range of logic rules every time we checked for equality
of two expressions. Consequently, both the ``logic`` and ``drt``
modules in |NLTK| have a separate method, ``tp_equals``, for checking
'equality' up to logical equivalence.

    >>> a = lp.parse('all x.walk(x)')
    >>> b = lp.parse('all y.walk(y)')
    >>> a == b
    True
    >>> c = lp.parse('-(P(x) & Q(x))')
    >>> d = lp.parse('-P(x) | -Q(x)')
    >>> c == d
    False
    >>> c.tp_equals(d)
    True


--------------------
Discourse Processing
--------------------

|NLTK| contains a discourse processing module,
``nltk.inference.discourse``, similar to the CURT program
presented in [Blackburn2005RIN]_.  This module processes sentences incrementally,
keeping track of all possible threads when there is ambiguity. For
simplicity, the following example ignores scope ambiguity.

    >>> from nltk.inference.discourse import DiscourseTester as DT
    >>> dt = DT(['A student dances', 'Every student is a person'])
    >>> dt.readings()
    <BLANKLINE>
    s0 readings:
    ------------------------------
    s0-r0: exists x.(student(x) & dance(x))
    <BLANKLINE>
    s1 readings:
    ------------------------------
    s1-r0: all x.(student(x) -> person(x))

When a new sentence is added to the current discourse, setting the
parameter ``consistchk=True`` causes consistency to be checked
by invoking the model checker for each 'thread', i.e., discourse sequence of
admissible readings. In this case, the user has the option
of retracting the sentence in question.

    >>> dt.add_sentence('No person dances', consistchk=True)
    Inconsistent discourse d0 ['s0-r0', 's1-r0', 's2-r0']:
    s0-r0: exists x.(student(x) & dance(x))
    s1-r0: all x.(student(x) -> person(x))
    s2-r0: -exists x.(person(x) & dance(x))
    >>> dt.retract_sentence('No person dances', quiet=False)
    Current sentences are 
    s0: A student dances
    s1: Every student is a person

In a similar manner, we use ``informchk=True`` to check whether
the new sentence is informative relative to the current discourse (by
asking the theorem prover to derive it from the discourse).

    >>> dt.add_sentence('A person dances', informchk=True)
    Sentence 'A person dances' under reading 'exists x.(person(x) & dance(x))':
    Not informative relative to thread 'd0'

It is also possible to pass in an additional set of assumptions as
background knowledge and use these to filter out inconsistent readings.

The ``discourse`` module can accommodate semantic 
ambiguity and filter out readings that are not admissable.
By invoking both Glue Semantics and |DRT|, the following example processes the 
two-sentence discourse `Every dog chases a boy.  He runs`lx:.  As
shown, the first sentence has two possible readings, while 
the second sentence contains an anaphoric pronoun, indicated as ``PRO(x)``.


    >>> from nltk.inference.discourse import DrtGlueReadingCommand as RC
    >>> dt = DT(['Every dog chases a boy', 'He runs'], RC())
    >>> dt.readings()
    <BLANKLINE>
    s0 readings:
    ------------------------------
    s0-r0: ([],[(([x],[dog(x)]) -> ([z15],[boy(z15), chase(x,z15)]))])
    s0-r1: ([z16],[boy(z16), (([x],[dog(x)]) -> ([],[chase(x,z16)]))])
    <BLANKLINE>
    s1 readings:
    ------------------------------
    s1-r0: ([x],[PRO(x), run(x)])

When we examine the two threads ``d0`` and ``d1``, we see
that that reading ``s0-r0``, where `every dog`:lx: out-scopes
``a boy``, is deemed inadmissable because the pronoun in the
second sentence cannot be resolved.  By contrast, in thread ``d1`` the
pronoun (relettered to ``z24``) has been bound \textit{via} the
equation ``(z24 = z20)``.  

Inadmissable readings are filtered out by passing the parameter
``filter=True``.

    >>> dt.readings(show_thread_readings=True)
    d0: ['s0-r0', 's1-r0'] : INVALID: AnaphoraResolutionException
    d1: ['s0-r1', 's1-r0'] : ([z20,z24],[boy(z20), (([x],[dog(x)]) -> 
    ([],[chase(x,z20)])), (z24 = z20), run(z24)])
    >>> dt.readings(filter=True, show_thread_readings=True)
    d1: ['s0-r1', 's1-r0'] : ([z26,z29],[boy(z26), (([x],[dog(x)]) -> 
    ([],[chase(x,z26)])), (z29 = z26), run(z29)])

    >>> dt.readings(show_thread_readings=True)
    d0: ['s0-r0', 's1-r0'] : INVALID: AnaphoraResolutionException
    d1: ['s0-r1', 's1-r0'] : ([z20,z24],[boy(z20), (([x],[dog(x)]) -> 
    ([],[chase(x,z20)])), (z24 = z20), run(z24)])




-------------------
Anaphora Resolution
-------------------

|NLTK| also includes a extension to the DRT module that allows the
user to resolve anaphoric pronouns, located in
``nltk.sem.drt\_resolve\_anaphora``.  For instance, the three sentence
discourse `John walks.  Bill runs.  He talks.`:lx: is shown below, along
with its resolution.  The anaphora resolution procedure used by |NLTK|
replaces any instance of the function ``PRO(x)`` with an expression
equating ``x`` to a list of possible antecedents of ``x``.  The list of
possible antecedents contains any discourse referent in an accessible
DRS.


    >>> d = drtparse(r'DRS([j,b,x],[(John=j), walk(j), (Bill=b), 
    run(b), PRO(x), talk(x)])')
    >>> print d.resolve_anaphora()
    DRS([j,b,x],[(John=j), walk(j), (Bill=b), run(b), (x=[j,b]), 
    talk(x)])


The anaphora resolution logic has been separated into a separate
module so that users may write their own anaphora resolution
procedures and swap them in.

----------------------------------------------
Case Study: Extracting Valuations from Chat-80
----------------------------------------------

Building ``Valuation`` objects by hand becomes rather tedious once we
consider larger examples. This raises the question of whether the
relation data in a ``Valuation`` could be extracted from some
pre-existing source. The ``corpora.chat80`` module
provides an example of extracting data from the Chat-80 Prolog
knowledge base (which included as part of the |NLTK| ``corpora``
distribution).

Chat-80 data is organized into collections of clauses, where each
collection functions as a table in a relational database. The
predicate of the clause provides the name of the table; the first
element of the tuple acts as the
'key'; and subsequent elements are further columns in the
table. 

In general, the name of the table provides a label for a unary
relation whose extension is all the keys. For example,
the table in ``cities.pl`` contains triples such as city_.

.. _city:
.. ex:: ``'city(athens,greece,1368).'``

|nopar| Here, ``'athens'`` is the key, and will be mapped to a member of the
unary relation `city`:mathit:.

The other two columns in the table are mapped to binary relations, where the first
argument of the relation is filled by the table key, and the
second argument is filled by the data in the relevant column. Thus, in
the ``city`` table illustrated by the tuple in 
city_, the data from the third column is extracted  into a binary predicate
`population_of`:mathit:, whose extension is a set of pairs such as
``'(athens, 1368)'``.

In order to encapsulate the results of the extraction, a class of
``Concept``\ s is introduced.  A ``Concept`` object has a number of
attributes, in particular a ``prefLabel`` and ``extension``, which
make it easier to inspect the output of the extraction. The
``extension`` of a ``Concept`` object is incorporated into a
``Valuation`` object.

As well as deriving unary and binary relations from the Chat-80 data,
we also create a set of individual constants, one for each entity in
the domain. The individual constants are string-identical to the
entities. For example, given a data item such as ``'zloty'``, we add
to the valuation a pair ``('zloty', 'zloty')``. In order to parse
English sentences that refer to these entities, we also create a
lexical item such as the following for each individual constant:

.. ex::
   ::

     PropN[num=sg, sem=<\P.P(zloty)>] -> 'Zloty'

The ``chat80`` module can be found in the ``examples`` package. The
attribute ``chat80.items`` gives us a list of Chat-80 relations:

   >>> from nltk.examples.semantics import chat80
   >>> chat80.items # doctest: +ELLIPSIS
    ('borders', 'circle_of_lat', 'circle_of_long', 'city', ...)

The ``concepts()`` method shows the list of ``Concept``\ s that can be
extracted from a ``chat80`` relation, and we can then inspect their extensions.

   >>> concepts = chat80.concepts('city')
   >>> concepts
   [Concept('city'), Concept('country_of'), Concept('population_of')]
   >>> rel = concepts[1].extension
   >>> list(rel)[:5]
   [('chungking', 'china'), ('karachi', 'pakistan'), 
   ('singapore_city', 'singapore'), ('athens', 'greece'), 
   ('birmingham', 'united_kingdom')]

|nopar| In order to convert such an extension into a valuation, we use the
``make_valuation()`` method; setting ``read=True`` creates and returns
a new ``Valuation`` object which contains the results.

   >>> val = nltk.corpus.chat80.make_valuation(concepts, read=True)
   >>> val['city']['calcutta']
   True
   >>> val['country_of']['india']
   {'hyderabad': True, 'delhi': True, 'bombay': True, 
   'madras': True, 'calcutta': True}
   >>> dom = val.domain
   >>> g = nltk.sem.Assignment(dom)
   >>> m = nltk.sem.Model(dom, val)
   >>> m.evaluate(r'\x .population_of(jakarta, x)', g)
   {'533': True}

.. Note:: Population figures are given in thousands. Bear in mind that
   the geographical data used in these examples dates back at least to
   the 1980s, and was already somewhat out of date at the point when
   [Warren1982EEA]_ was published.

-------
Summary
-------

* Semantic Representations (SRs) for English are constructed using a
  language based on the |lambda|-calculus, together with Boolean
  connectives, equality, and first-order quantifiers.

* |beta|-reduction in the |lambda|-calculus corresponds semantically
  to application of a function to an argument. Syntactically, it
  involves replacing a variable bound by |lambda| in the functor with
  the expression that provides the argument in the function
  application.

* If two |lambda|-abstracts differ only in the label of the variable
  bound by |lambda|, they are said to be |alpha|
  equivalents. Relabeling a variable bound by a |lambda| is called
  |alpha|-conversion.

* Currying of a binary function turns it into a unary function whose
  value is again a unary function.

* |FSRL| has both a syntax and a semantics. The semantics is
  determined by recursively evaluating expressions in a model.

* A key part of constructing a model lies in building a valuation
  which assigns interpretations to non-logical constants. These are
  interpreted as either curried characteristic functions or as
  individual constants. 

* The interpretation of Boolean connectives is handled by the
  model; these are interpreted as characteristic functions.

* An open expression is an expression containing one or more free
  variables. Open expressions only receive an interpretation when
  their free variables receive values from a
  variable assignment.

* Quantifiers are interpreted by constructing, for a formula |phi|\
  [`x`:mathit:\ ] open in variable `x`:mathit:, the set of individuals
  which make |phi|\ [`x`:mathit:\ ] true when an assignment *g*
  assigns them as the value of `x`:mathit:. The quantifier then places
  constraints on that set.

* A closed expression is one that has no free variables; that is, the
  variables are all bound. A closed sentence is true or
  false with respect to all variable assignments.

* Given a formula with two nested quantifiers *Q*\ :sub:`1` and *Q*\
  :sub:`2`, the outermost quantifier *Q*\ :sub:`1` is said to have wide
  scope (or scope over *Q*\ :sub:`2`). English sentences are frequently
  ambiguous with respect to the scope of the quantifiers they
  contain. 

* English sentences can be associated with an SR by treating ``sem``
  as a feature. The ``sem`` value of a complex expressions typically
  involves functional application of the ``sem`` values of the
  component expressions.

* Model valuations need not be built by hand, but can also be
  extracted from relational tables, as in the Chat-80 example.

---------------
Further Reading
---------------

For more examples of semantic analysis with |NLTK|, please see the
guides at
``http://nltk.org/doc/guides/sem.html`` and
``http://nltk.org/doc/guides/logic.html``.

The use of characteristic functions for interpreting expressions of
natural language was primarily due to Richard
Montague. [Dowty1981IMS]_ gives a comprehensive and reasonably
approachable introduction to Montague's grammatical framework.

A more recent and wide-reaching study of the use of a |lambda| based
approach to natural language can be found in [Carpenter1997TLS]_.

[Heim1998SGG]_ is a thorough application of formal semantics to
transformational grammars in the Government-Binding model.

[Blackburn2005RIN]_ is the first textbook devoted to computational
semantics, and provides an excellent introduction to the area.

The Turing Test was introduced by Alan M. Turing (1912-1954) as "the
imitation game" in his 1950 article (now available online) Computing
Machinery and Intelligence  (Mind, Vol. 59, No. 236,
pp. 433-460)http://cogprints.ecs.soton.ac.uk/archive/00000499/00/turing.html. 

http://www.fil.ion.ucl.ac.uk/~asaygin/tt/ttest.html

Natural language interfaces to databases - an
introduction. [I. Androutsopoulos, G. Ritchie, P. Thanisch]. Pp.29-81
in Journal of Natural Language Engineering 1(1), 1995. (Early version
in DAI Research Report 709).  

---------
Exercises
---------

1. |soso| Modify the ``sem.evaluate`` code so that it will
   give a helpful error message if an expression is not in the domain
   of a model's valuation function.

#. |hard| Specify and implement a typed functional language with quantifiers,
   Boolean connectives and equality. Modify
   ``sem.evaluate`` to interpret expressions of this language.

#. |hard| Extend the ``chat80`` code so that it will extract data from a
   relational database using SQL queries.

#. |hard| Taking [WarrenPereira1982] as a starting point, develop a technique
   for converting a natural language query into a form that can be
   evaluated more efficiently in a model. For example, given a query
   of the form ``'(P(x) & Q(x)'``, convert it to ``'(Q(x) & P(x)'`` if
   the extension of ``'Q'`` is smaller than the extension of 
   ``'P'``.

.. include:: footer.rst
