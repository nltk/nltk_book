.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. overview of book

.. _chap-advanced-topics:

=================================
12. Facing the Language Challenge
=================================

Natural language throws up some interesting computational challenges.
We've explored many of these in the preceding chapters, including
tokenization, tagging, classification, information extraction,
and building syntactic and semantic representations.
You should now be equipped to work with large datasets, to create
robust models of linguistic phenomena, and to extend them into
components for practical language technologies.  We hope that
the Natural Language Toolkit (|NLTK|) has served to open up the exciting
endeavor of practical natural language processing to a broader
audience than before.

In spite of all that has come before, language presents us with
far more than a temporary challenge for computation.  Consider the following
sentences which attest to the riches of language:

.. ex::
   .. ex:: Overhead the day drives level and grey, hiding the sun by a flight of grey spears.  (William Faulkner, *As I Lay Dying*, 1935)
   .. ex:: When using the toaster please ensure that the exhaust fan is turned on. (sign in dormitory kitchen)
   .. ex:: Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated activities with Ki values of 45.1-271.6 |mu|\ M (Medline, PMID: 10718780)
   .. ex:: Iraqi Head Seeks Arms (spoof news headline)
   .. ex:: The earnest prayer of a righteous man has great power and wonderful results. (James 5:16b)
   .. ex:: Twas brillig, and the slithy toves did gyre and gimble in the wabe (Lewis Carroll, *Jabberwocky*, 1872)
   .. ex:: There are two ways to do this, AFAIK :smile:  (internet discussion archive)
    
Other evidence for the riches of language is the vast array of disciplines
whose work centers on language.  Some obvious disciplines include
translation, literary criticism, philosophy, anthropology and psychology.
Many less obvious disciplines investigate language use, including
law, hermeneutics, forensics, telephony, pedagogy, archaeology, cryptanalysis and speech
pathology.  Each applies distinct methodologies to gather
observations, develop theories and test hypotheses.  All serve to
deepen our understanding of language and of the intellect that is
manifested in language.
          
In view of the complexity of language and the broad range of interest
in studying it from different angles, its clear that we have barely
scratched the surface here.  Additionally, within |NLP| itself,
there are many important methods and applications that we haven't
mentioned.  In this chapter we will take a broader view of |NLP|,
including its foundations and the further directions you might
want to explore.  Some of the topics are not well-supported by |NLTK|,
and you might like to rectify that problem by contributing new software
and data to the toolkit.

.. note::
   This chapter is still in the draft stage, and contains an assortment
   of topics that are not evenly balanced.  Several planned topics have
   not yet been added, including multi-word expressions, machine translation,
   and morphology.

----------------------------------------
Language Processing vs Symbol Processing
----------------------------------------

The very notion that natural language could be treated in a
computational manner grew out of a research program, dating back to
the early 1900s, to reconstruct mathematical reasoning using logic,
most clearly manifested in work by Frege, Russell, Wittgenstein,
Tarski, Lambek and Carnap.  This work led to the notion of language as
a formal system amenable to automatic processing.  Three later
developments laid the foundation for natural language processing.  The
first was `formal language theory`:dt:.  This defined a language as a set
of strings accepted by a class of automata, such as context-free
languages and pushdown automata, and provided the underpinnings for
computational syntax.

The second development was `symbolic logic`:dt:. This provided a
formal method for capturing selected aspects of natural language that
are relevant for expressing logical proofs. A formal calculus in
symbolic logic provides the syntax of a language, together with rules
of inference and, possibly, rules of interpretation in a set-theoretic
model; examples are propositional logic and First Order Logic.  Given
such a calculus, with a well-defined syntax and semantics, it becomes
possible to associate meanings with expressions of natural language by
translating them into expressions of the formal calculus. For example,
if we translate `John saw Mary`:lx: into a formula ``saw(j,m)``, we
(implicitly or explicitly) intepret the English verb `saw`:lx: as a
binary relation, and `John`:lx: and `Mary`:lx: as denoting
individuals.  More general statements like `All birds fly`:lx: require
quantifiers, in this case |forall|, meaning `for all`:lx:: |forall|\
`x (bird(x)`:math: |rarr| `fly(x))`:math:.  This use of logic provided
the technical machinery to perform inferences that are an important
part of language understanding.

A closely related development was the `principle of
compositionality`:dt:, namely that the meaning of a complex expression
is composed from the meaning of its parts and their mode of
combination (Chapter chap-semantics_). 
This principle provided a useful correspondence between
syntax and semantics, namely that the meaning of a complex expression
could be computed recursively.  Consider the sentence `It is not true
that`:lx: `p`:math:, where `p`:math: is a proposition.  We can
represent the meaning of this sentence as `not(p)`:math:.  Similarly, we
can represent the meaning of `John saw Mary`:lx: as `saw(j, m)`:math:.  Now we
can compute the interpretation of `It is not true that John saw Mary`:lx:
recursively, using the above information, to get
`not(saw(j,m))`:math:. 

The approaches just outlined share the premise that computing with
natural language crucially relies on rules for manipulating symbolic
representations. For a certain period in the development of |NLP|,
particularly during the 1980s, this premise provided a common starting
point for both linguists and practitioners of |NLP|, leading to a family
of grammar formalisms known as unification-based (or feature-based)
grammar, and to |NLP| applications implemented in the Prolog
programming language. Although grammar-based |NLP| is still a
significant area of research, it has become somewhat eclipsed in the
last 15\ |ndash|\ 20 years due to a variety of factors. One
significant influence came from automatic speech recognition. Although
early work in speech processing adopted a model that emulated the
kind of rule-based phonological `phonology`:topic: processing typified
by the *Sound Pattern of English* [ChomskyHalle68]_,
this turned out to be hopelessly inadequate in dealing
with the hard problem of recognizing actual speech in anything like
real time. By contrast, systems which involved learning patterns from
large bodies of speech data were significantly more accurate,
efficient and robust. In addition, the speech community found that
progress in building better systems was hugely assisted by the
construction of shared resources for quantitatively measuring
performance against common test data. Eventually, much of the |NLP|
community embraced a `data intensive`:dt: orientation to language
processing, coupled with a growing use of machine-learning techniques
and evaluation-led methodology.

----------------------------------
Contemporary Philosophical Divides
----------------------------------

The contrasting approaches to |NLP| described in the preceding section
relate back to early metaphysical debates about `rationalism`:dt:
versus `empiricism`:dt: and `realism`:dt: versus `idealism`:dt: that
occurred in the Enlightenment period of Western philosophy.  These
debates took place against a backdrop of orthodox thinking in which
the source of all knowledge was believed to be divine revelation.
During this period of the seventeenth and eighteenth centuries,
philosophers argued that human reason or sensory experience has
priority over revelation.  Descartes and Leibniz, amongst others, took
the rationalist position, asserting that all truth has its origins in
human thought, and in the existence of "innate ideas" implanted in our
minds from birth.  For example, they argued that the principles of
Euclidean geometry were developed using human reason, and were not the
result of supernatural revelation or sensory experience.  In contrast,
Locke and others took the empiricist view, that our primary source of
knowledge is the experience of our faculties, and that human reason
plays a secondary role in reflecting on that experience.  Prototypical
evidence for this position was Galileo's discovery |mdash| based on
careful observation of the motion of the planets |mdash| that the
solar system is heliocentric and not geocentric.  In the context of
linguistics, this debate leads to the following question: to what
extent does human linguistic experience, versus our innate "language
faculty", provide the basis for our knowledge of language?  In |NLP|
this matter surfaces as differences in the priority of corpus data
versus linguistic introspection in the construction of computational
models.  We will return to this issue later in the book.

A further concern, enshrined in the debate between realism and
idealism, was the metaphysical status of the constructs of a theory.
Kant argued for a distinction between phenomena, the manifestations we
can experience, and "things in themselves" which can never been
known directly.  A linguistic realist would take a theoretical
construct like `noun phrase`:dt: to be real world entity that exists
independently of human perception and reason, and which actually
*causes* the observed linguistic phenomena.  A linguistic idealist, on
the other hand, would argue that noun phrases, along with more
abstract constructs like semantic representations, are intrinsically
unobservable, and simply play the role of useful fictions.  The way
linguists write about theories often betrays a realist position, while
|NLP| practitioners occupy neutral territory or else lean towards the
idealist position.  Thus, in |NLP|, it is often enough if a theoretical
abstraction leads to a useful result; it does not matter whether this
result sheds any light on human linguistic processing.

These issues are still alive today, and show up in the distinctions
between symbolic vs statistical methods, deep vs shallow processing,
binary vs gradient classifications, and scientific vs engineering
goals.  However, such contrasts are now highly nuanced, and the debate
is no longer as polarized as it once was.  In fact, most of the
discussions |mdash| and most of the advances even |mdash| involve a
"balancing act".  For example, one intermediate position is to assume
that humans are innately endowed with analogical and memory-based
learning methods (weak rationalism), and to use these methods to identify
meaningful patterns in their sensory language experience (empiricism).
We have seen many examples of this methodology throughout this book.
Statistical methods inform symbolic models any time corpus statistics
are used to guide the selection of rules in a rule-based grammar.
Symbolic methods inform statistical models any time a corpus that
was created using rule-based methods is used to train a statistical
language model.  Now the circle is closed, and the dichotomy has virtually
disappeared.

---------------------
Probabilistic Parsing
---------------------

.. TODO: mention interest in having weights is because they can be learned.
   Without this it is mysterious why we would want to bother.
   Technical aspects follow, but this is important motivation (Steven)

As we pointed out in Chapter chap-parse_, dealing with ambiguity
is a key challenge in developing broad coverage parsers.
Chart parsers improve the efficiency of computing multiple
parses of the same sentences, but they are still overwhelmed by
the sheer number of possible parses.  Weighted grammars and
probabilistic parsing algorithms have provided an effective
solution to these problems.

Weighted Grammar
----------------

Before looking at these, we need to understand why the notion of
grammaticality could be gradient.  Considering the verb `give`:lx:.
This verb requires both a direct object (the thing being given)
and an indirect object (the recipient).
These complements can be given in either order, as
illustrated in dative_.  In the "prepositional dative" form in
dative-prepositional_, the direct object appears first, followed
by a prepositional phrase containing the indirect object.
 
.. _dative:
.. ex::
   .. _dative-prepositional:
   .. ex::
      Kim gave a bone to the dog
   .. _dative-double-object:
   .. ex::
      Kim gave the dog a bone

In the "double object" form in dative-double-object_,
the indirect object appears first, followed by the direct object.
In the above case, either order is acceptable.  However, if
the indirect object is a pronoun, there is a strong preference for
the double object construction:

.. ex::
   .. ex::
      *Kim gives the heebie-jeebies to me (*prepositional dative*)
   .. ex::
      Kim gives me the heebie-jeebies (*double object*)

Using the Penn Treebank sample, we can examine all instances of
prepositional dative and double object constructions involving
`give`:lx:, as shown in Figure give_.

.. pylisting:: give
   :caption: Usage of Give and Gave in the Penn Treebank sample

   def give(t):
       return t.node == 'VP' and len(t) > 2 and t[1].node == 'NP'\
              and (t[2].node == 'PP-DTV' or t[2].node == 'NP')\
              and ('give' in t[0].leaves() or 'gave' in t[0].leaves())
   def sent(t):
       return ' '.join(token for token in t.leaves() if token[0] not in '*-0')
   def print_node(t, width):
           output = "%s %s: %s / %s: %s" %\
               (sent(t[0]), t[1].node, sent(t[1]), t[2].node, sent(t[2]))
           if len(output) > width:
               output = output[:width] + "..."
           print output

   >>> for tree in nltk.corpus.treebank.parsed_sents():
   ...     for t in tree.subtrees(give):
   ...         print_node(t, 72)
   gave NP: the chefs / NP: a standing ovation
   give NP: advertisers / NP: discounts for maintaining or increasing ad sp...
   give NP: it / PP-DTV: to the politicians
   gave NP: them / NP: similar help
   give NP: them / NP: 
   give NP: only French history questions / PP-DTV: to students in a Europe...
   give NP: federal judges / NP: a raise
   give NP: consumers / NP: the straight scoop on the U.S. waste crisis
   gave NP: Mitsui / NP: access to a high-tech medical product
   give NP: Mitsubishi / NP: a window on the U.S. glass industry
   give NP: much thought / PP-DTV: to the rates she was receiving , nor to ...
   give NP: your Foster Savings Institution / NP: the gift of hope and free...
   give NP: market operators / NP: the authority to suspend trading in futu...
   gave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...
   give NP: the Transportation Department / NP: up to 50 days to review any...
   give NP: the president / NP: such power
   give NP: me / NP: the heebie-jeebies
   give NP: holders / NP: the right , but not the obligation , to buy a cal...
   gave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...
   give NP: the president / NP: line-item veto power

We can observe a strong tendency for the shortest complement to appear
first.  However, this does not account for a form like
``give NP: federal judges / NP: a raise``, where animacy may
play a role.  In fact there turn out to be a large number of contributing
factors, as surveyed by [Bresnan2006GG]_.
Such preferences can be represented in a weighted grammar.

A `probabilistic context free grammar`:dt: (or *PCFG*) is a context free
grammar that associates a probability with each of its productions.
It generates the same set of parses for a text that the corresponding
context free grammar does, and assigns a probability to each parse.
The probability of a parse generated by a PCFG is simply the product
of the probabilities of the productions used to generate it.

The simplest way to define a PCFG is to load it from a specially
formatted string consisting of a sequence of weighted productions,
where weights appear in brackets, as shown in Figure pcfg1_.

.. pylisting:: pcfg1
   :caption: Defining a Probabilistic Context Free Grammar (PCFG)

   grammar = nltk.parse_pcfg("""
       S    -> NP VP              [1.0]
       VP   -> TV NP              [0.4]
       VP   -> IV                 [0.3]
       VP   -> DatV NP NP         [0.3]
       TV   -> 'saw'              [1.0]
       IV   -> 'ate'              [1.0]
       DatV -> 'gave'             [1.0]
       NP   -> 'telescopes'       [0.8]
       NP   -> 'Jack'             [0.2]
       """)
   >>> print grammar
   Grammar with 9 productions (start state = S)
       S -> NP VP [1.0]
       VP -> TV NP [0.4]
       VP -> IV [0.3]
       VP -> DatV NP NP [0.3]
       TV -> 'saw' [1.0]
       IV -> 'ate' [1.0]
       DatV -> 'gave' [1.0]
       NP -> 'telescopes' [0.8]
       NP -> 'Jack' [0.2]
       

It is sometimes convenient to combine multiple productions into a single line,
e.g. ``VP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3]``.
In order to ensure that the trees generated by the grammar form a
probability distribution, PCFG grammars impose the constraint
that all productions with a given left-hand side must have
probabilities that sum to one.
The grammar in Figure pcfg1_ obeys this constraint: for ``S``,
there is only one production, with a probability of 1.0; for ``VP``,
0.4+0.3+0.3=1.0; and for ``NP``, 0.8+0.2=1.0.
The parse tree returned by ``parse()`` includes probabilities:

    >>> viterbi_parser = nltk.ViterbiParser(grammar)
    >>> print viterbi_parser.parse(['Jack', 'saw', 'telescopes'])
    (S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)

The next two sections introduce two probabilistic parsing algorithms
for PCFGs.  The first is an A* parser that uses dynamic
programming to find the single most likely parse for a given text.
Whenever it finds multiple possible parses for a subtree, it discards
all but the most likely parse.  The second is a bottom-up chart parser
that maintains a queue of edges, and adds them to the chart one at a
time.  The ordering of this queue is based on the probabilities
associated with the edges, allowing the parser to expand more likely
edges before less likely ones.  Different queue orderings are used to
implement a variety of different search strategies.  These algorithms
are implemented in the ``nltk.parse.viterbi`` and
``nltk.parse.pchart`` modules.

A* Parser
---------

An `A* Parser`:dt: is a bottom-up PCFG parser that uses
dynamic programming to find the single most likely parse for a text [Klein2003Astar]_.
It parses texts by iteratively filling in a 
`most likely constituents table`:dt:.  This table records the most likely tree for each
span and node value.  For example, after parsing the sentence "I saw the man with
the telescope" with the grammar ``cfg.toy_pcfg1``, the most likely constituents table
contains the following entries (amongst others):

.. table:: mlct

   ===== ==== ==================================================================  ============
   Span  Node Tree                                                                Prob
   ===== ==== ==================================================================  ============
   [0:1] NP   (NP I)                                                              0.15
   [6:7] NP   (NN telescope)                                                      0.5
   [5:7] NP   (NP the telescope)                                                  0.2
   [4:7] PP   (PP with (NP the telescope))                                        0.122
   [0:4] S    (S (NP I) (VP saw (NP the man)))                                    0.01365
   [0:7] S    (S (NP I) (VP saw (NP (NP the man) (PP with (NP the telescope)))))  0.0004163250
   ===== ==== ==================================================================  ============

   Fragment of Most Likely Constituents Table

Once the table has been completed, the parser
returns the entry for the most likely constituent that spans the
entire text, and whose node value is the start symbol.  For this
example, it would return the entry with a span of [0:6] and a node
value of "S".

Note that we only record the *most likely* constituent for any given
span and node value.  For example, in the table above, there are
actually two possible constituents that cover the span [1:6] and have
"VP" node values.

1. "saw the man, who has the telescope":

  (VP saw
     (NP (NP John)
          (PP with (NP the telescope))))

2. "used the telescope to see the man":

  (VP saw
     (NP John)
     (PP with (NP the telescope)))

|nopar|
Since the grammar we are using to parse the text indicates that the
first of these tree structures has a higher probability, the parser
discards the second one.

**Filling in the Most Likely Constituents Table:**
Because the grammar used by ``ViterbiParse`` is a PCFG, the
probability of each constituent can be calculated from the
probabilities of its children.  Since a constituent's children can
never cover a larger span than the constituent itself, each entry of
the most likely constituents table depends only on entries for
constituents with *shorter* spans (or equal spans, in the case of
unary and epsilon productions).

``ViterbiParse`` takes advantage of this fact, and fills in the most
likely constituent table incrementally.  It starts by filling in the
entries for all constituents that span a single element of text.
After it has filled in all the table entries for constituents that
span one element of text, it fills in the entries for constituents
that span two elements of text.  It continues filling in the entries
for constituents spanning larger and larger portions of the text,
until the entire table has been filled.

To find the most likely constituent with a given span and node value,
``ViterbiParse`` considers all productions that could produce that
node value.  For each production, it checks the most likely
constituents table for sequences of children that collectively cover
the span and that have the node values specified by the production's
right hand side.  If the tree formed by applying the production to the
children has a higher probability than the current table entry, then
it updates the most likely constituents table with the new tree.

**Handling Unary Productions and Epsilon Productions:**
A minor difficulty is introduced by unary productions and epsilon
productions: an entry of the most likely constituents table might
depend on another entry with the same span.  For example, if the
grammar contains the production ``V`` |rarr| ``VP``, then the table
entries for ``VP`` depend on the entries for ``V`` with the same span.
This can be a problem if the constituents are checked in the wrong
order.  For example, if the parser tries to find the most likely
constituent for a ``VP`` spanning [1:3] before it finds the most
likely constituents for ``V`` spanning [1:3], then it can't apply the
``V`` |rarr| ``VP`` production.

To solve this problem, ``ViterbiParse`` repeatedly checks each span
until it finds no new table entries.  Note that cyclic grammar
productions (e.g. ``V`` |rarr| ``V``) will *not* cause this procedure
to enter an infinite loop.  Since all production probabilities are
less than or equal to 1, any constituent generated by a cycle in the
grammar will have a probability that is less than or equal to the
original constituent; so ``ViterbiParse`` will discard it.

In NLTK, we create Viterbi parsers using ``ViterbiParse()``.
Note that since ``ViterbiParse`` only finds the single most likely
parse, that ``nbest_parse()`` will never return more than one parse.

.. pylisting:: viterbi-parse

   grammar = nltk.parse_pcfg('''
     NP  -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]
     NNS -> "cats" [0.1] | "dogs" [0.2] | "mice" [0.3] | NNS CC NNS [0.4]
     JJ  -> "big" [0.4] | "small" [0.6]
     CC  -> "and" [0.9] | "or" [0.1]
     ''')
   viterbi_parser = nltk.ViterbiParser(grammar)

   >>> sent = 'big cats and dogs'.split()
   >>> print viterbi_parser.parse(sent)
   (NP (JJ big) (NNS (NNS cats) (CC and) (NNS dogs))) (p=0.000864)    

The ``trace`` method can be used to set the level of tracing output
that is generated when parsing a text.  Trace output displays the
constituents that are considered, and indicates which ones are added
to the most likely constituent table.  It also indicates the
likelihood for each constituent.

    >>> viterbi_parser.trace(3)
    >>> print viterbi_parser.parse(sent)
    Inserting tokens into the most likely constituents table...
       Insert: |=...| big
       Insert: |.=..| cats
       Insert: |..=.| and
       Insert: |...=| dogs
    Finding the most likely constituents spanning 1 text elements...
       Insert: |=...| JJ -> 'big' [0.4]                 0.4000000000 
       Insert: |.=..| NNS -> 'cats' [0.1]               0.1000000000 
       Insert: |.=..| NP -> NNS [0.5]                   0.0500000000 
       Insert: |..=.| CC -> 'and' [0.9]                 0.9000000000 
       Insert: |...=| NNS -> 'dogs' [0.2]               0.2000000000 
       Insert: |...=| NP -> NNS [0.5]                   0.1000000000 
    Finding the most likely constituents spanning 2 text elements...
       Insert: |==..| NP -> JJ NNS [0.3]                0.0120000000 
    Finding the most likely constituents spanning 3 text elements...
       Insert: |.===| NP -> NP CC NP [0.2]              0.0009000000 
       Insert: |.===| NNS -> NNS CC NNS [0.4]           0.0072000000 
       Insert: |.===| NP -> NNS [0.5]                   0.0036000000 
      Discard: |.===| NP -> NP CC NP [0.2]              0.0009000000 
      Discard: |.===| NP -> NP CC NP [0.2]              0.0009000000 
    Finding the most likely constituents spanning 4 text elements...
       Insert: |====| NP -> JJ NNS [0.3]                0.0008640000 
      Discard: |====| NP -> NP CC NP [0.2]              0.0002160000 
      Discard: |====| NP -> NP CC NP [0.2]              0.0002160000 
    (NP (JJ big) (NNS (NNS cats) (CC and) (NNS dogs))) (p=0.000864)


A Bottom-Up PCFG Chart Parser
-----------------------------

The `A* parser`:idx: described in the previous section finds
the single most likely parse for a given text.  However, when parsers
are used in the context of a larger NLP system, it is often necessary
to produce several alternative parses.  In the context of an overall system,
a parse that is assigned low probability by the parser might still have the
best overall probability.

For example, a probabilistic parser might decide that the most likely
parse for "I saw John with the cookie" is is the structure with the
interpretation "I used my cookie to see John"; but that parse would be
assigned a low probability by a semantic system.  Combining the
probability estimates from the parser and the semantic system, the
parse with the interpretation "I saw John, who had my cookie" would be
given a higher overall probability.

A probabilistic bottom-up chart parser
maintains an `edge queue`:dt:, and adds these edges to the chart one at a time.
The ordering of this queue is based on the probabilities associated with the edges,
and this allows the parser to insert the most probable edges first.
Each time an edge is added to the chart, it may become possible
to insert further edges, so these are added to the queue.
The bottom-up chart parser continues adding the edges in the
queue to the chart until enough complete parses have been found, or
until the edge queue is empty.

Like an edge in a regular chart, a probabilistic edge
consists of a dotted production, a span, and a (partial) parse tree.
However, unlike ordinary charts, this time the tree is weighted
with a probability.  Its probability
is the product of the probability of the production that
generated it and the probabilities of its children.  For example, the
probability of the edge ``[Edge: S`` |rarr| ``NP``\ |dot|\ ``VP, 0:2]``
is the probability of the PCFG production ``S`` |rarr| ``NP VP``
multiplied by the probability of its `np`:gc: child.
(Note that an edge's tree only includes children for elements to the left
of the edge's dot.)

Bottom-Up PCFG Strategies
-------------------------

The `edge queue`:idx: is a sorted list of edges that can be added to the
chart.  It is initialized with a single edge for each token,
with the form ``[Edge: token`` |rarr| |dot|\ ``]``.
As each edge from the queue is added to the chart, it may
become possible to add further edges, according to two rules:
(i) the Bottom-Up Initialization Rule can be used to add a
self-loop edge whenever an edge whose dot is in position 0 is added to the chart; or
(ii) the Fundamental Rule can be used to combine a new edge
with edges already present in the chart.  These additional edges
are queued for addition to the chart.

By changing the sort order used by the queue, we can control the
strategy that the parser uses to explore the search space.  Since
there are a wide variety of reasonable search strategies,
``BottomUpChartParser()`` does not define any sort order.
Instead, different strategies are implemented in subclasses of ``BottomUpChartParser()``.

.. We should either explain "inside probabilities" or rename this parser (to
        ``LowestCostFirstParser``?). 

**Lowest Cost First:**
The simplest way to order the edge queue is to sort edges by the
probabilities of their associated trees.
This ordering concentrates the efforts of the parser on those edges
that are more likely to be correct analyses of their corresponding
input tokens.
Now, the probability of an edge's tree provides an upper bound on the
probability of any parse produced using that edge.  The probabilistic
"cost" of using an edge to form a parse is one minus its tree's
probability.  Thus, inserting the edges with the most likely trees
first results in a `lowest-cost-first search strategy`:dt:.
Lowest-cost-first search turns out to be optimal: the first
solution it finds is guaranteed to be the best solution
(cf the A* parser).

However, lowest-cost-first search can be rather inefficient.  Recall that a
tree's probability is the product of the probabilities of all the
productions used to generate it.  Consequently, smaller trees tend to have higher
probabilities than larger ones.  Thus, lowest-cost-first search tends to work
with edges having small trees before considering edges with larger trees.
Yet any complete parse of the text will necessarily have a
large tree, and so this strategy will tend to produce complete parses only
once most other edges are processed.

Let's consider this problem from another angle.
The basic shortcoming with lowest-cost-first search is that it ignores the
probability that an edge's tree will be part of a complete parse.  The parser will
try parses that are locally coherent even if they are unlikely to
form part of a complete parse.  Unfortunately, it can be quite
difficult to calculate the probability that a tree is part of a
complete parse.  However, we can use a variety of techniques to
approximate that probability.

**Best-First Search:**
This method sorts the edge queue in descending order of the edges'
span, no the assumption that edges having a larger span are more likely
to form part of a complete parse.  This is a `best-first search strategy`:dt:,
since it inserts the edges that are closest to producing
complete parses before trying any other edges.  However, best-first search is
*not* optimal: the first solution it finds is not
guaranteed to be the best solution.  However, it will usually find a
complete parse much more quickly than lowest-cost-first search.

**Beam Search:**
When large grammars are used to parse a text, the edge queue can grow
quite long.  The edges at the end of a long queue are
unlikely to be used.  Therefore, it is reasonable to remove
these edges from the queue.  This strategy is known as
`beam search`:dt:; it only keeps the best partial results.
The bottom-up chart parsers take an optional parameter ``beam_size``;
whenever the edge queue grows longer than this, it is pruned.
This parameter is best used in conjunction with ``InsideChartParser()``.
Beam search reduces the space requirements for lowest-cost-first
search, by discarding edges that are not likely to be used.  But beam
search also loses many of lowest-cost-first search's more useful
properties.  Beam search is not optimal: it is not guaranteed to find
the best parse first.  In fact, since it might prune a necessary edge,
beam search is not `complete`:idx:\ : it won't find every parse,
and it is not even guaranteed to return a parse if one exists.

The code in Figure bottom-up-chart-parsers_ demonstrates how
we define and use these probabilistic chart parsers in |NLTK|.

.. pylisting:: bottom-up-chart-parsers

   inside_parser = nltk.InsideChartParser(grammar)
   longest_parser = nltk.LongestChartParser(grammar)
   beam_parser = nltk.InsideChartParser(grammar, beam_size=20)

   >>> print inside_parser.parse(sent)
   (NP (JJ big) (NNS (NNS cats) (CC and) (NNS dogs))) (p=0.000864)
   >>> for tree in inside_parser.nbest_parse(sent):
   ...     print tree
   (NP
     (JJ big)
     (NNS (NNS cats) (CC and) (NNS dogs))) (p=0.000864)
   (NP
     (NP (JJ big) (NNS cats))
     (CC and)
     (NP (NNS dogs))) (p=0.000216)

The ``trace`` method can be used to set the level of tracing output
that is generated when parsing a text.  Trace output displays edges as
they are added to the chart, and shows the probability for each edges'
tree.

.. note:: |TRY|
   Run the above example using tracing, by calling
   ``inside_parser.trace(3)`` before running the parser.

Grammar Induction
-----------------

As we have seen, PCFG productions are just like CFG productions,
adorned with probabilities.  So far, we have simply specified these
probabilities in the grammar.  However, it is more usual to *estimate*
these probabilities from training data, namely a collection of parse
trees or *treebank*.

The simplest method uses *Maximum Likelihood Estimation*, so called
because probabilities are chosen in order to maximize the likelihood
of the training data.  The probability of a production
``VP`` |rarr| ``V NP PP`` is *p(V,NP,PP | VP)*.  We calculate this as
follows::

                        count(VP → V NP PP)
      P(V,NP,PP | VP) = -------------------
                        count(VP → ...)

Here is a simple program that induces a grammar from the first
three parse trees in the Penn Treebank corpus:

    >>> from itertools import islice
    >>> productions = []
    >>> S = nltk.Nonterminal('S')
    >>> for tree in nltk.corpus.treebank.parsed_sents('wsj_0002.mrg'):
    ...      productions += tree.productions()
    >>> grammar = nltk.induce_pcfg(S, productions)
    >>> for production in grammar.productions()[:10]:
    ...      print production
    CC -> 'and' [1.0]
    NNP -> 'Agnew' [0.166666666667]
    JJ -> 'industrial' [0.2]
    NP -> CD NNS [0.142857142857]
    , -> ',' [1.0]
    S -> NP-SBJ NP-PRD [0.5]
    VP -> VBN S [0.5]
    NNP -> 'Rudolph' [0.166666666667]
    NP -> NP PP [0.142857142857]
    NNP -> 'PLC' [0.166666666667]

Normal Forms
------------

Grammar induction usually involves normalizing the grammar
in various ways.  NLTK trees
support binarization (Chomsky Normal Form), parent annotation,
Markov order-N smoothing, and unary collapsing:

    >>> treebank_string = """(S (NP-SBJ (NP (QP (IN at) (JJS least) (CD nine) (NNS tenths)) )
    ...     (PP (IN of) (NP (DT the) (NNS students) ))) (VP (VBD passed)))"""
    >>> t = nltk.bracket_parse(treebank_string)
    >>> print t
    (S
      (NP-SBJ
        (NP (QP (IN at) (JJS least) (CD nine) (NNS tenths)))
        (PP (IN of) (NP (DT the) (NNS students))))
      (VP (VBD passed)))
    >>> t.collapse_unary(collapsePOS=True)
    >>> print t
    (S
      (NP-SBJ
        (NP+QP (IN at) (JJS least) (CD nine) (NNS tenths))
        (PP (IN of) (NP (DT the) (NNS students))))
      (VP+VBD passed))
    >>> t.chomsky_normal_form()
    >>> print t
    (S
      (NP-SBJ
        (NP+QP
          (IN at)
          (NP+QP|<JJS-CD-NNS>
            (JJS least)
            (NP+QP|<CD-NNS> (CD nine) (NNS tenths))))        
        (PP (IN of) (NP (DT the) (NNS students))))
      (VP+VBD passed))

These trees are shown in treetransforms_.

.. _treetransforms:
.. ex:: 
   .. ex::
      .. tree:: (S (NP-SBJ (NP (QP (IN at) (JJS least) (CD nine) (NNS tenths))) (PP (IN of) (NP (DT the) (NNS students)))) (VP (VBD passed)))
   .. ex::
      .. tree:: (S (NP-SBJ (NP+QP (IN at) (JJS least) (CD nine) (NNS tenths)) (PP (IN of) (NP (DT the) (NNS students)))) (VP+VBD passed))
   .. ex::
      .. tree:: (S (NP-SBJ (NP+QP (IN at) (NP+QP|\<JJS-CD-NNS\> (JJS least) (NP+QP|\<CD-NNS\> (CD nine) (NNS tenths)))) (PP (IN of) (NP (DT the) (NNS students)))) (VP+VBD passed))


------
Search
------

Many NLP tasks can be construed as search problems.
For example, the task of a parser is to identify one or more
parse trees for a given sentence.  As we saw in Part II,
there are several algorithms for parsing.  A `recursive descent parser`:idx:
performs `backtracking search`:dt:, applying grammar productions in turn
until a match with the next input word is found, and backtracking when
there is no match.  As we will see in Section sec-a-problem-of-scale_,
the space of possible parse trees is very large; a parser can be thought
of as providing a relatively efficient way to find the right solution(s)
within a very large space of candidates.

As another example of search, suppose we want to find the most complex
sentence in a text corpus.  Before we can begin we have to be explicit
about how the complexity of a sentence is to be measured: word count,
verb count, character count, parse-tree depth, etc.  In the context
of learning this is known as the `objective function`:dt:, the property
of candidate solutions we want to optimize.

In this section we will explore some other search methods that are
useful in NLP.  For concreteness we will apply them to the problem
of learning word segmentations in text, following the work of [Brent1995]_.
Put simply, this is the problem faced by a language learner in dividing
a continuous speech stream into individual words.  We will consider this
problem from the perspective of a child hearing utterances from a parent, e.g.

.. _kitty:
.. ex::
  .. ex:: doyouseethekitty
  .. ex:: seethedoggy
  .. ex:: doyoulikethekitty
  .. ex:: likethedoggy

Our first challenge is simply to represent the problem: we need to find
a way to separate the text content from the segmentation.  We will borrow
an idea from IOB-tagging (Chapter chap-chunk_), by annotating each character
with a boolean value to indicate whether or not a word-break appears after
the character.  We will assume that the learner is given the utterance breaks,
since these often correspond to extended pauses.  Here is a possible representation,
including the initial and target segmentations:

    >>> text = "doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"
    >>> seg1 = "0000000000000001000000000010000000000000000100000000000"
    >>> seg2 = "0100100100100001001001000010100100010010000100010010000"

Observe that the segmentation strings consist of zeros and ones.  They
are one character shorter than the source text, since a text of length
`n`:math: can only be broken up in `n-1`:math: places.

Now let's check that our chosen representation is effective.  We need to make
sure we can read segmented text from the representation.  The following function,
``segment()``, takes a text string and a segmentation string, and returns a list
of strings.

.. pylisting:: segment
   :caption: Program to Reconstruct Segmented Text from String Representation

   def segment(text, segs):
       words = []
       last = 0
       for i in range(len(segs)):
           if segs[i] == '1':
               words.append(text[last:i+1])
               last = i+1
       words.append(text[last:])
       return words
    
   >>> segment(text, seg1)
   ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']
   >>> segment(text, seg2)
   ['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you',
    'like', 'the', kitty', 'like', 'the', 'doggy']

Now the learning task becomes a search problem: find the bit string that causes
the text string to be correctly segmented into words.  Our first task is done: we have
represented the problem in a way that allows us to reconstruct the data, and to
focus on the information to be learned.

Now that we have effectively represented the problem we need to choose the objective
function.  We assume the learner is acquiring words and storing them in an internal lexicon.
Given a suitable lexicon, it is possible to reconstruct the source text as a sequence of
lexical items.  Following [Brent1995]_, we can use the size of the lexicon and the amount
of information needed to reconstruct the source text as the basis for an objective function, as shown
in Figure brent_.

.. _brent:
.. figure:: ../images/brent.png
   :scale: 30

   Calculation of Objective Function for Given Segmentation

It is a simple matter to implement this objective function, as shown in Listing
evaluate_.

.. pylisting:: evaluate
   :caption: Computing the Cost of Storing the Lexicon and Reconstructing the Source Text

   def evaluate(text, segs):
       import string
       words = segment(text, segs)
       text_size = len(words)
       lexicon_size = len(' '.join(list(set(words))))
       return text_size + lexicon_size

   >>> text = "doyouseethekittyseethedoggydoyoulikethekittylikethedoggy"
   >>> seg3 = "0000100100000011001000000110000100010000001100010000001"
   >>> segment(text, seg3)
   ['doyou', 'see', 'thekitt', 'y', 'see', 'thedogg', 'y', 'doyou', 'like',
    'thekitt', 'y', 'like', 'thedogg', 'y']
   >>> evaluate(text, seg3)
   47

Exhaustive Search
-----------------

* brute-force approach
* enumerate search space, evaluate at each point
* this example: search space size is 2\ `55`:superscript: = 36,028,797,018,963,968

For a computer that can do 100,000 evaluations per second, this
would take over 10,000 years!

Backtracking search -- saw this in the recursive descent parser.

Hill-Climbing Search
--------------------

Starting from a given location in the search space, evaluate nearby locations and move to
a new location only if it is an improvement on the current location.

.. pylisting:: hill-climb
   :caption: Hill-Climbing Search

   def flip(segs, pos):
       return segs[:pos] + `1-int(segs[pos])` + segs[pos+1:]
   def hill_climb(text, segs, iterations):
       for i in range(iterations):
           pos, best = 0, evaluate(text, segs)
           for i in range(len(segs)):
               score = evaluate(text, flip(segs, i))
               if score < best:
                   pos, best = i, score
           if pos != 0:
               segs = flip(segs, pos)
               print evaluate(text, segs), segment(text, segs)
       return segs

   >>> print evaluate(text, seg1), segment(text, seg1)
   63 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']
   >>> hill_climb(text, segs1, 20)
   61 ['doyouseethekittyseethedoggy', 'doyoulikethekitty', 'likethedoggy']
   59 ['doyouseethekittyseethedoggydoyoulikethekitty', 'likethedoggy']
   57 ['doyouseethekittyseethedoggydoyoulikethekittylikethedoggy']

Non-Deterministic Search
------------------------

* Simulated annealing

.. pylisting:: anneal
   :caption: Non-Deterministic Search Using Simulated Annealing

   def flip_n(segs, n):
       for i in range(n):
           segs = flip(segs, randint(0,len(segs)-1))
       return segs
   def anneal(text, segs, iterations, rate):
       distance = float(len(segs))
       while distance > 0.5:
           best_segs, best = segs, evaluate(text, segs)
           for i in range(iterations):
               guess = flip_n(segs, int(round(distance)))
               score = evaluate(text, guess)
               if score < best:
                   best = score
                   best_segs = guess
           segs = best_segs
           score = best
           distance = distance/rate
           print evaluate(text, segs),
       print
       return segs
    
   >>> anneal(text, segs, 5000, 1.2)
   60 ['doyouseetheki', 'tty', 'see', 'thedoggy', 'doyouliketh', 'ekittylike', 'thedoggy']
   58 ['doy', 'ouseetheki', 'ttysee', 'thedoggy', 'doy', 'o', 'ulikethekittylike', 'thedoggy']
   56 ['doyou', 'seetheki', 'ttysee', 'thedoggy', 'doyou', 'liketh', 'ekittylike', 'thedoggy']
   54 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'likethekittylike', 'thedoggy']
   53 ['doyou', 'seethekit', 'tysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']
   51 ['doyou', 'seethekittysee', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']
   42 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']


------------
NLTK Roadmap
------------

[Core NLP topics not covered by NLTK: morphology, multi-word expressions,
interfaces to high-end components (e.g. parsers)]


-------------------------------------
The Holy Grail: NLP-Complete Problems
-------------------------------------

[Discussion of SLDS, MT, QA; why they are hard; current methods; state-of-the-art]

---------------
Further Reading
---------------

Section 13.4 of [JurafskyMartin2008]_ covers chart parsing, and Chapter 14
contains a more formal presentation of statistical parsing.

* [Manning2003Probabilistic]_

* [Klein2003Astar]_

* [Charniak1997]_


.. include:: footer.rst
