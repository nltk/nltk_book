.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. standard global imports

    >>> import nltk, re, pprint
    
.. TODO: cover tf.idf

.. _chap-data-intensive:

=====================================
5. Data-Intensive Language Processing
=====================================

.. nomenclature note: training corpus/test corpus or training set/test set??
.. discuss qc corpus
.. explain that segmentation (e.g. tokenization, sentence segmentation) can
   be viewed as a classification task

.. _ch05-introduction:

------------
Introduction
------------

.. SB: Opening of chapter should follow pattern of previous chapters.

Language is full of patterns.
In Chapter chap-words_ we saw that frequent use of the modal verb
`will`:lx: is characteristic of news text, and
more generally, that we can use the frequency of a small number of diagnostic
words in order to automatically guess the genre of a text (Table brown-types_).
In Chapter chap-tag_ we saw that words ending in `ed`:lx: tend to
be past tense verbs, and more generally, that the internal structure of words
tells us something about their part of speech (section sec-regular-expression-tagging_).
Detecting and understanding such patterns is central to many NLP tasks,
particularly those that try to access the meaning of a text.

In order to study and model these linguistic patterns we need to be able to write
programs to process large quantities of annotated text.
In this chapter we will focus on data-intensive language processing,
covering manual approaches to exploring linguistic data in Section sec-exploratory-data-analysis_
and automatic approaches in Section sec-data-modeling_.

.. more motivation and overview of this simple ontology (manual vs automatic)

We have already seen a simple application of
classification in the case of part-of-speech tagging (Chapter chap-tag_).
Although this is a humble beginning, it actually holds the key for a range of more
difficult classification tasks, including those mentioned above.
Recall that adjectives (tagged ``JJ``) tend to precede nouns (tagged ``NN``),
and that we can use this information to predict that the word `deal`:lx: is
a noun in the context `good deal`:lx: (and not a verb, as in `to deal cards`:lx:).

.. _sec-exploratory-data-analysis:

-------------------------
Exploratory Data Analysis
-------------------------

As language speakers, we all have intuitions about how language works,
and what patterns it contains.  Unfortunately, those intuitions are
notoriously unreliable.  We tend to notice unusual words and constructions,
and to be oblivious to high-frequency cases.  Many public commentators go
further to make pronouncements about statistics and usage which turn out to
be false.  Many examples are documented on *LanguageLog*, e.g.

* Strunk and White's injunction against using adjectives and adverbs
  http://itre.cis.upenn.edu/~myl/languagelog/archives/001905.html
* Brizenden's claim that women use 20,000 words a day while men use 7,000
  http://itre.cis.upenn.edu/~myl/languagelog/archives/003420.html
* Payack's claim that vocabulary size of English stood at 986,120 on 2006-01-26
  http://itre.cis.upenn.edu/~myl/languagelog/archives/002809.html
  
In order to get an accurate idea of how language works, and what
patterns it contains, we must study langauge |mdash| in a wide
variety of forms and contexts |mdash| as impartial observers.
To help facilitate this endevour, researchers and
organizations have created many large collections of real-world
language, or `corpora`:dt:.  These corpora are collected from a wide
variety of sources, including literature, journalism, telephone
conversations, instant messaging, and web pages.

`Exploratory data analysis`:dt:, the focus of this section, is a technique
for learning about a specific linguistic pattern, or
`construction`:dt:.  It consists of four steps, illustrated in Figure
fig-exploration_.

.. XX add more examples of "constructions"

.. _fig-exploration:
.. figure:: ../images/exploration.png
   :scale: 20:25:25

   Exploratory Corpus Analysis
   
First, we must find the occurrences of the construction that we're
interested in, by searching the corpus.  Ideally, we would like to
find all occurrences of the construction, but sometimes that may not
be possible, and we have to be careful not to over-generalize our
findings.  In particular, we should be careful not to conclude that
something doesn't happen simply because we were unable to find any
examples; it's also possible that our corpus is deficient.

Once we've found the constructions of interest, we can then categorize
them, using two sources of information: content and context.  In some cases,
like identifying date and time expressions in text, we can simply write
a set of rules to cover the various cases.  In general, we can't just
enumerate the cases but we have to manually annotate a corpus of text
and then train systems to do the task automatically.

Having collected and categorized the constructions of interest, we can
proceed to look for patterns.  Typically, this involves describing
patterns as combinations of categories, and counting how often
different patterns occur.  We can check for both graded distinctions
and categorical distinctions...

* center-embedding suddenly gets bad after two levels
* examples from probabilistic syntax / gradient grammaticality

Finally, the information that we discovered about patterns in the
corpus can be used to refine our understanding of how constructions
work.  We can then continue to perform exploratory data analysis, both
by adjusting our characterizations of the constructions to better fit
the data, and by building on our better understanding of simple
constructions to investigate more complex constructions.

Although we have described exploratory data analysis as a cycle of
four steps, it should be noted that any of these steps may be skipped
or re-arranged, depending on the nature of the corpus and the
constructions that we're interested in understanding.  For example, we
can skip the search step if we already have a corpus of the relevant
constructions; and we can skip categorization if the constructions are
already labeled.

------------------
Selecting a Corpus
------------------

In exploratory data analysis, we learn about a specific linguistic
pattern by objectively examining how it is used.  We therefore must
begin by selecting a corpus (i.e., a collection of language data)
containing the pattern we are interested in.  Often, we can use one of
the many existing corpora that have been made freely-available by the
researchers who assembled them.  Sometimes, we may choose instead to
assemble a `derived corpus`:dt: by combining several existing corpora,
or by selecting out specific subsets of a corpus (e.g., only news
stories containing interviews).  Occasionally, we may decide to build
a new corpus from scratch (e.g., if we wish to learn about a
previously undocumented language).

The results of our analysis will be highly dependent on the corpus
that we select.  This should hardly be surprising, since many
linguistic phenomena pattern differently in different contexts -- for
example, <<add a good example -- child vs adult? written vs spoken?
some specific phenomenon?>>.  But when selecting the corpus for
analysis, it is important to understand how the characteristics of the
corpus will affect results of the the data analysis.

Source of Language Data
-----------------------

Language is used for many different purposes, in many different
contexts.  For example, language can be used in a novel to tell a
complex fictional story, or it can be used in an internet chat room to
exchange rumors about celebrities.  It can be used in a newspaper to
report a story about a sports team, or in a workplace to form a
collaberative plan for building a new product.  Although some
linguistic phenomena will act uniformly across these different
contexts, other phenomana may vary depending.

Therefore, one of the most important characteristics defining a corpus
is the source (or sources) from which its language data is drawn,
which will determine the types of language data it includes.
Attributes that characterize the type of language data contained in a
corpus include:

.. How should these be ordered?

* Domain: What subject matters does the language data talk about?

* Mode: Does the corpus contain spoken language data, written language
  data, or both?

* Number of Speakers: Does the corpus contain texts that are produced
  by a single speaker, such as books or news stories, or does it contain
  dialogues?

* Register: Is the language data in the corpus formal or informal?

* Communicative Intent: For what purpose was the langauge generated
   -- e.g., to communicate, to entertain, or to persuade?

* Dialect: Do the speakers use any specific dialects?

* Language: What language or languages are used?

When making conclusions on the basis of exploratory data analysis, it
is important to consider the extent to which those conclusions are
dependent on the type of language data included in the corpus.  For
example, if we discover a pattern in a corpus of newspaper articles,
we should not necessarily assume that the same pattern will hold in
spoken discourse.  

.. The sentence "For example..." would be stronger if "a pattern" were
   replaced with a specific example.

In order to allow more general conclusions to be drawn about
linguistic patterns, several `balanced corpora`:dt: have been created,
which include language data from a wide variety of different language
sources.  For example, the Brown Corpus contains documents ranging
from science fiction to howto guidebooks to legislative council
transcripts.  But it's worth noting that since language use is so
diverse, it would be almost impossible to create a single corpus that
includes *all* of the contexts in which language gets used.  Thus,
even balanced corpora should be considered to cover only a subset of
the possible linguistic sources (even if that subset is larger than
the subset covered by many other corpora).

.. Talk about the actual "balancing", i.e., picking how much of each
   type of language data to include.  And say something about
   sampling?

Information Content
-------------------

Corpora can vary in the amount of information they contain about the
language data they describe.  At a minimum, a corpus will typically
contain at least a sequence of sounds or orthographic symbols.  At the
other end of the spectrum, a corpus could contain a large amount of
information about the syntactic structure, morphology, prosody, and
semantic content of every sentence.  This extra information is called
:dt:`annotation`, and can be very helpful when performing exploratory data
analysis.  For example, it may be much easier to find a given
linguistic pattern if we can search for specific syntactic structures;
and it may be easier to categorize a linguistic pattern if every word
has been tagged with its word sense.

Corpora vary widely in the amount and types of annotation that they
include.  Some common types of information that can be annotated
include:

.. Is it useful to include a list like this here?  If yes, then it
   needs to be fleshed out.  If no, then delete "Some common types..."
   above, and merge the "Corpora vary widely..." sentence into the 
   next paragraph.

* Word Tokenization: In written English, word tokenization is often
   marked implicitly using whitespace.  However, it spoken English,
   and in some written languages, word tokenization may need to be
   explicitly marked.

* Sentence Segmentation: As we saw in Chapter chap-words_, sentence
  segmentation can be more difficult than it seems.  Some corpora 
  therefore use explicit annotations to mark sentence segmentation.

* Paragraph Segmentation: Paragraphs and other structural elements
  (headings, chapters, etc.) may be explicitly annotated.

* Part of Speech: The syntactic category of each word in a document.

* Syntactic Structure: A tree structure showing how each sentence
  is constructed from its constituent pieces.

* etc.


Unfortunately, there is not much consistency between existing corpora
in how they represent their annotations.  However, two general classes
of annotation representation should be distinguished.  `Inline
annotation`:dt: modifies the original document by inserting special
symbols or control sequences that carry the annotated information.
For example, when part-of-speech tagging a document, the string
``"fly"`` might be replaced with the string ``"fly/NN"``, to indicate
that the word *fly* is a noun in this context.  In contrast, `standoff
annotation`:dt: does not modify the original document, but instead
creates a new file that adds annotation information using pointers
into the original document.  For example, this new document might
contain the string ``"<word start=8 end=11 pos='NN'/>"``, to indicate
that the word starting at character 8 and ending at character 11 is a
noun.

Corpus Size
-----------

The size of corpora can vary widely, from tiny corpora containing just
a few hundred sentences up to enormous corpora containing a billion
words or more.  In general, we perform exploratory data analysis using
the largest appropriate corpus that's available.  This ensures that
the results of our analysis don't just reflect a quirk of the
particular language data contained in the corpus.  

However, in some circumstances, we may be forced to perform our
analysis using small corpora.  For example, if we are examining
linguistic patterns in a language that is not well studied, or if our
analysis requires specific annotations, then no large corpora may be
available.  In these cases, we should be careful when interpreting the
results of an exploratory data analysis.  In particular, we should
avoid concluding that a linguistic pattern or phenomenon never occurs,
just because we did not find it in our small sample of language data.
    
.. SB: xref data chapter

.. The following table needs to get filled in/completed.  

.. table:: example-corpora

   ============== ============ ======== =========== ===
   Corpus Name    Contents     Size     Annotations etc. 
   ============== ============ ======== =========== ===
   Penn Treebank  News stories 1m words etc.
   Web (google)   etc.
   ============== ============ ======== =========== ===

   Example Corpora.  This table summarizes some important properties
   of several popular corpora.

------
Search
------

Once we've selected or constructed a corpus, the next step is to
search that corpus for instances of the linguistic phenomenon we're
interested in.  The techniques that we use to search the corpus will
depend on whether the corpus is annotated or not.

Searching Unannotated Data
--------------------------

.. SB: Note that I've been using "text" to mean list of words and
   "raw text" to mean string in the preceding chapters.  More generally
   the concept of searching unannotated data has been covered
   comprehensively already.

Unannotated corpora consist of a large quantity of "raw text," with no
extra linguistic information.  We therefore typically rely on search
patterns to find the constructions of interest.  For example, if we
are investigating what adjectives can be used to describe the word
"man," we could use the following code to search for relevant phrases:

>>> gutenberg = nltk.corpus.gutenberg.raw()
>>> re.findall(r'a \w+ man', gutenberg.lower())
['a young man', 'a just man', 'a wild man', 'a young man', 'a dead man', 
 'a plain man', 'a hairy man', 'a smooth man', 'a certain man', ...]

For some linguistic phenomena, it can be fairly straightforward to
build appropriate search patterns |mdash| especially when the linguistic
phenomenon is tightly coupled with specific words.  However, for other
linguistic phenomena, it may be necessary to be very creative to think
up appropriate search patterns.  (For example, if we are interested in
learning about type-instance relations, we could try searching for the
pattern "*x* and other *y*\ s", which will match phrases such as "tea
and other refreshments.")  If we are unable to come up with an
appropriate search pattern for a given linguistic phenomenon, then we
should consider whether we might need to switch to a corpus containing
annotations that might help us to locate the occurrences of the
linguistic phenomenon we're interested in.

When dealing with an unannotated corpus, it's usually possible to
build search patterns that find some examples of the linguistic
phenomenon we're interested in.  However, we should keep two
limitations in mind.  First, most patterns that we write will
occasionally match examples that we are not interested in.  For
example, the search pattern "*x* and other *y*\ s" matches the string
"eyes and other limbs," where the larger context is "other eyes and
other limbs;" but we wouldn't want to conclude that an eye is a kind
of limb.

Second, we should keep in mind that it can be difficult to write a
pattern that finds every occurrence of a given phenomenon.  Thus, as
was the case when working with small corpora, we need to be very
careful about drawing any negative conclusions: just because we did
not find an example of a given pattern, does not mean that it never
happens.  We may have just not been creative enough to think of a
pattern that would match the context where it occurs.

Searching the Web
-----------------

.. Should I mention google by name here, or leave it vague?

The web can be thought of as a huge corpus of unannotated text.  Web
search engines provide an efficient means of searching this large
quantity of text for relevant linguistic examples.  The main advantage
that search engines hold, when it comes to exploratory data analysis,
is that of corpus size: since you are searching such a large set of
documents, you are more likely to find any linguistic phenomenon you
are interested in.  Furthermore, you can make use of very specific
patterns, which would only match one or two examples on a smaller
example, but which might match tens of thousands of examples when run
on the web.  A second advantage of web search engines is that they are
very easy to use.  Thus, they provide a very convenient tool for
quickly checking a theory, to see if it is reasonable.

However, search engines also have several significant disadvantages.
First, you are usually fairly severely restricted in the types of
patterns you can search for.  Unlike local corpora, where you can
build arbitrarily complex regular expressions, search engines
generally just allow you to search for individual words or strings of
words, and sometimes for fill-in-the-blank patterns.  Second, search
engines use advanced techniques to filter web pages, in an attempt to
block out spam and pornography.  These techniques may unintentionally
skew the results of a web-based exploratory data analysis.  

Finally, it should be noted that the web page counts provided by
search engines can be misleading.  Many of the pages may contain the
strings that you searched for, but they may be used in unexpected
ways.  For example, many web pages include non-language text that
might give you false matches.  Web pages that intersperse images with
text may cause unexpected matches.  And the web page count may include
pages with duplicated content, which do not actually represent
separate occurences of the search pattern.  As an example of these
problems, most search engines will return millions of hits for the
search pattern "the of", even though we know that this pair of words
should (almost) never occur together in English sentences.


.. SB: use of Google 5-gram corpus for some of these things?
   EL: elaborate?

.. EL: these would both make good exercises:

   .. SB: use "as * as x" to look for properties
          http://acl.ldc.upenn.edu/P/P07/P07-1008.pdf

   http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html

   - Use "give * a ball" to search for nouns that can receive
     concrete objects.

   - Look up the features of 2-3 search engines.  Compare how good
     they are for exporatory data analysis.


Searching annotated corpora
---------------------------

Large unannotated corpora are widely available; and it is fairly easy
to search these corpora for simple string patterns.  However, many of
the linguistic phenomena that we may be interested can not be captured
with simple string patterns.  For example, it would be very difficult
to write a search pattern that finds all verbs that take sentential
complements.  

Partially to help address these concerns, a large number of manually
annotated corpora have been created.  These corpora are augmented with
a wide variety of extra information about the language data they
contain.  The exact types and extents of this additional information
will vary from corpus to corpus, but common annotation types include
tokeniation and segmentation information; information about words'
parts of speech or word sense; and information about sentences'
syntactic structure.  We can make use of this extra information to
perform more advanced searches over the corpus.  Often, this extra
information will make it possible to find all of (or most of) the
occurences of a given linguistic phenomenon, especially if the
phenomenon is closely related to the type of information contained in
the corpus's annotations.

However, building annotated corpora is a very labor-intensive process,
and these corpora therefore tend to be significantly smaller than
corresponding unannotated corpora.  As we mentioned when discussing
corpus size, we should therefore be careful about concluding that a
linguistic pattern or phenomenon never occurs, just because we did not
find it in a small annotated corpus.
    
Because the format and information content of annotations can vary
widely from one corpus to the next, there is no single tool for
searching annotated corpora.  One solution is to use one of the many
specialized tools have been built for searching popular corporus
formats.  For example, the ``tgrep`` tool can be used to search for
specific syntactic patterns in a corpus that is annotated with the
syntactic structure of each sentence.  This can be a very efficient
solution if your corpus is in the required format, and if the tool
supports the search pattern you wish to construct.

.. These examples need to be walked through in more detail!  We haven't
   necessarily talked about parse trees yet. :-/

But a more general solution is to simply write a short Python script
that loads the corpus and then scans through it for any occurences of
the linguistic pattern you are interested in.  For example, if we are
interested in searching for occurences of the pattern ``"<Verb> to
<Verb>"`` in the Brown corpus, we could use the following short script:

    >>> from nltk.corpus import brown
    >>> for sent in brown.tagged_sents():
    ...     # Look at each 3-word window in the sentence.
    ...     for triple in nltk.trigrams(sent):
    ...         # Get the part-of-speech tags for this 3-word window.
    ...         tags = [t for (w,t) in triple]
    ...         # Check if they match our pattern.
    ...         if ( tags[0].startswith('V') and tags[1]=='TO' and
    ...              tags[2].startswith('V') ):
    ...             print triple
    [('combined', 'VBN'), ('to', 'TO'), ('achieve', 'VB')]
    [('continue', 'VB'), ('to', 'TO'), ('place', 'VB')]
    [('serve', 'VB'), ('to', 'TO'), ('protect', 'VB')]
    [('wanted', 'VBD'), ('to', 'TO'), ('wait', 'VB')]
    [('allowed', 'VBN'), ('to', 'TO'), ('place', 'VB')]
    [('expected', 'VBN'), ('to', 'TO'), ('become', 'VB')]
      ...
    [('seems', 'VBZ'), ('to', 'TO'), ('overtake', 'VB')]
    [('want', 'VB'), ('to', 'TO'), ('buy', 'VB')]

In Chapter chap-chunk_, we'll learn about the "regular expression
chunker," which can be used to make this search script even simpler:

    >>> cp = nltk.RegexpChunker("CHUNK: {<V.*> <TO> <V.*>}")
    >>> brown = nltk.corpus.brown
    >>> for sent in brown.tagged_sents():
    ...     tree = cp.parse(sent)
    ...     for subtree in tree.subtrees():
    ...         if subtree.node == 'CHUNK': print subtree
    ...
    (CHUNK combined/VBN to/TO achieve/VB)
    (CHUNK continue/VB to/TO place/VB)
    (CHUNK serve/VB to/TO protect/VB)
    (CHUNK wanted/VBD to/TO wait/VB)
    (CHUNK allowed/VBN to/TO place/VB)
    (CHUNK expected/VBN to/TO become/VB)
    ...
    (CHUNK seems/VBZ to/TO overtake/VB)
    (CHUNK want/VB to/TO buy/VB)

As a second example, the script show in Listing sentential_complement_
uses a simple filter to find all verbs in a given corpus that take
sentential complements.
        
.. pylisting:: sentential_complement

   def filter(tree):
       child_nodes = [child.node for child in tree
                      if isinstance(child, nltk.Tree)]
       return  (tree.node == 'VP') and ('S' in child_nodes)

   >>> treebank = nltk.corpus.treebank
   >>> for tree in treebank.parsed_sents()[:5]:
   ...     for subtree in tree.subtrees(filter):
   ...         print subtree
   (VP
     (VBN named)
     (S
       (NP-SBJ (-NONE- *-1))
       (NP-PRD
         (NP (DT a) (JJ nonexecutive) (NN director))
         (PP
           (IN of)
           (NP (DT this) (JJ British) (JJ industrial) (NN conglomerate))))))


  .. SB: NB a later discussion of XML will include XPath, another method for tree search

Searching Automatically Annotated Data
--------------------------------------

In some cases, we may find that the hand-annotated corpora that are
available are too small to perform the analysis we desire; but that
unannotated corpora do not contain the information we need to perform
a search.  One solution in these cases is to build an automatically
annotated corpus, and then to search that corpus.  

For example, if we wish to search for a relatively rare syntactic
configuration, we might first train an automatic parser using a corpus
that has been hand-annotated with syntactic parses (such as the
Treebank corpus); and then use that parser to generate parse trees for
a much larger unannotated corpus.  We could then use those
automatically generated parse trees to create a new annotated corpus,
and finally we could search this annotated corpus for the syntactic
configuration we're interested in.

  - is this safe?

    - yes, sometimes.
    - look at the automatic system's accuracy, and think about how it
      might affect your search

      - could the automatic system make a systematic error that would
        prevent you from finding an important class of instances?
      - the more closely tied the annotation & the phenomenon are, the
        more likely you are to get into trouble.

Categorizing
------------

Once we've found the occurrences we're interested in, the next step is
to categorize them.  In general, we're interested in two things:

  - features of the phenomenon itself
  - features of the context that we think are relevant to the phenomenon.

Categorization can be automatic or manual

  - automatic: when the decision can be made deterministically.  e.g.,
    what is the previous word?
    
  - manual: when the decision needs human judgement.  example.. animacy?

Encoding this information -- features.  We need to encode this info in
a concrete way.  Use a feature dictionary for each occurrence, mapping
feature names (eg 'prevword') to concrete values (eg a string, int).
Features are typically simple-valued, but don't necessarily need to
be.  (Though they will need to be for automatic methods.. coming up)


.. pylisting:: tagging

   def features(word):
       return dict( len = len(word),
                    last1 = word[-1:],
                    last2 = word[-2:],
                    last3 = word[-3:])

   >>> data = [(features(word), tag) for (word, tag) in nltk.corpus.brown.tagged_words('news')]
   >>> train = data[1000:]
   >>> test = data[:1000]
   >>> classifier = nltk.NaiveBayesClassifier.train(train)
   >>> nltk.classify.accuracy(classifier, test)
   0.8110




.. pylisting:: segmentation

   def preprocess(sents):
       tokens = []
       boundaries = []
       for sent in sents:
           for token in sent:
               tokens.append(token)
           boundaries.append(False)
           boundaries[-1] = True
       return (tokens, boundaries)

   def get_instances(tokens, boundaries):
       instances = []
       for i in range(len(tokens)):
           if tokens[i] in ".?!":
               try:
                   instances.append(
                       (dict( upper = tokens[i+1][0].isupper(),
                              abbrev = len(tokens[i-1]) == 1 ),
                        boundaries[i]))
               except IndexError:
                   pass
       return instances		

   >>> tokens, boundaries = preprocess(nltk.corpus.abc.sents())
   >>> data = get_instances(tokens, boundaries)
   >>> train = data[1000:]
   >>> test = data[:1000]
   >>> classifier = nltk.NaiveBayesClassifier.train(train)
   >>> nltk.classify.accuracy(classifier, test)
   0.9960

  
Counting
--------

Now that we've got our occurrences coded up, we want to look at how
often different combinations occur.

- we can look for both graded and categorical distictions

  - for categorical distinctions, we don't necessarily require that
    the counts be zero; every rule has its exception.

Example: what makes a name sound male or female?  Walk through it,
explain some features, do some counts using python.  


.. _sec-data-modeling:

-------------
Data Modeling
-------------

Exploratory data analysis helps us to understand the linguistic
patterns that occur in natural language corpora.  Once we have a basic
understanding of those patterns, we can attempt to create `models`:dt:
that capture those patterns.  Typically, these models will be
constructed automatically, using algorithms that attempt to select a
model that accurately describes an existing corpus; but it is also
possible to build analytically motivated models.  Either way, these
explicit models serve two important purposes: they help us to
understand the linguistic patterns, and they can be used to make
predictions about new language data.

The extent to which explicit models can give us insight into
linguistic patterns depends largely on what kind of model is used.
Some models, such as decision trees, are relatively transparent, and
give us direct information about which factors are important in making
decisions, and about which factors are related to one another.  Other
models, such as multi-level neural networks, are much more opaque --
although it can be possible to gain insight by studying them, it
typically takes a lot more work.

But all explicit models can make predictions about new "`unseen`:dt:"
language data that was not included in the corpus used to build the
model.  These predictions can be evaluated to assess the accuracy of
the model.  Once a model is deemed sufficiently accurate, it can then
be used to automatically predict information about new language
data.  These predictive models can be combined into systems that
perform many useful language processing tasks, such as document
classification, automatic translation, and question answering.

What do models tell us?
-----------------------

Before we delve into the mechanics of different models, it's important
to spend some time looking at exactly what automatically constructed
models can tell us about language.

One important consideration when dealing with language models is the
distinction between descriptive models and explanatory models.
Descriptive models capture patterns in the data but they don't
provide any information about *why* the data contains those patterns.
For example, as we saw in Table absolutely_, the synonyms
`absolutely`:lx: and `definitely`:lx: are not interchangeable:
we say `absolutely adore`:lx: not `definitely adore`:lx:,
and `definitely prefer`:lx: not `absolutely prefer`:lx:.
In contrast, explanatory models attempt
to capture properties and relationships that underlie the linguistic patterns.
For example, we might introduce the abstract concept of "polar adjective",
as one that has an extreme meaning, and categorize some adjectives
like `adore`:lx: and `detest`:lx: as polar.  Our explanatory model
would contain the constraint that `absolutely`:lx: can only combine with
polar adjectives, and `definitely`:lx: can only combine with non-polar adjectives.
In summary, descriptive models provide information about correlations
in the data, while explanatory models go further to postulate causal relationships.

Most models that are automatically constructed from a corpus are
descriptive models; in other words, they can tell us what features are
relevant to a given patterns or construction, but they can't
necessarily tell us how those features and patterns relate to one
another.  If our goal is to understand the linguistic patterns, then
we can use this information about which features are related as a
starting point for further experiments designed to tease apart the
relationships between features and patterns.  On the other hand, if
we're just interested in using the model to make predictions (e.g., as
part of a language processing system), then we can use the model to
make predictions about new data, without worrying about the precise
nature of the underlying causal relationships.

Supervised Classification
-------------------------

One of the most basic tasks in data modeling is `classification`:dt:.
In classification tasks, we wish to choose the correct `class
label`:dt: for a given input.  Each input is considered in isolation
from all other inputs, and set of labels is defined in advance.
Some examples of classification tasks are:

.. more compelling, interesting and/or more linguistically-oriented
   examples here:?

  - Classify an email as "spam" or "not spam."
  - Classify a news document as "sports," "technology," "politics," or "other."
  - Classify a name as "male" or "female."
  - Classify an occurrence of the word "bank" as "Noun-seaside,"
    "Noun-financial," "Verb-tilt," or "Verb-financial."

.. note:: The basic classification task has a number of interesting
  variants: for example, in multi-class classification, each instance
  may be assigned multiple labels; in open-class classification, the
  set of labels is not defined in advance; and in sequence
  classification, a list of inputs are jointly classified.

Classification models are typically trained using a corpus that
contains the correct label for each input.  This `training corpus`:dt:
is typically constructed by manually annotating each input with the
correct label, but for some tasks it is possible to automatically
construct training corpora.  Classification models that are built
based on training corpora that contain the correct label for each
input are called `supervised`:dt: classification models.

Feature Extraction
------------------

The first step in creating a model is deciding what information about
the input might be relevant to the classification task; and how to
encode that information.  In other words, we must decide which
`features`:dt: of the input are relevant, and how to `encode`:dt:
those features.  Most automatic learning methods restirct features to
have simple value types, such as booleans, numbers, and strings.  But
note that just because a feature has a simple type, does not
necessarily mean that the feature's value is simple to express or
compute; indeed, it is even possible to use very complex and
informative values, such as the output of a second supervised
classifier, as features.

.. _fig-supervised-classification:
.. figure:: ../images/supervised-classification.png
   :scale: 50:150:50

   Supervised Classification.  (a) During training, a feature
   extractor is used to convert each input value to a feature set.
   Pairs of feature sets and labels are fed into the machine learning
   algorithm to generate a model.  (b) During prediction, the same
   feature extractor is used to convert unseen inputs to feature sets.
   These feature sets are then fed into the model, which generates
   predicted labels.
   
For NLTK's classifiers, the features for each input are stored using a
dictionary that maps feature names to corresponding values.  Feature
names are case-sensitive strings that typically provide a short
human-readable description of the feature.  Feature values are
simple-typed values, such as booleans, numbers, and strings.  For
example, if we had built an ``animal_classifier`` model for
classifying animals, then we might provide it with the following
feature set:

>>> animal = {'fur': True, 'legs': 4, 
...           'size': 'large', 'spots': True}
>>> animal_classifier.classify(animal)
'leopard'

Generally, feature sets are constructed from inputs using a `feature
extraction`:dt: function.  This function takes an input value, and
possibly its context, as parameters, and returns a corresponding
feature set.  This feature set can then be passed to the machine
learning algorithm for training, or to the learned model for
prediction.  For example, we might use the following function to
extract features for a document classification task:

.. pylisting:: feature_extractor

    def extract_features(document):
       features = {}
       for word in document:
           features['contains(%s)' % word] = True
       return features

   >>> extract_features(nltk.corpus.brown.words('cj79'))
   {'contains(of)': True, 'contains(components)': True, 
    'contains(some)': True, 'contains(that)': True, 
    'contains(passage)': True, 'contains(table)': True, ...}

In addition to a feature extractor, we need to select or build a
training corpus, consisting of a list of examples and corresponding
class labels.  For many interesting tasks, appropriate corpora have
already been assembled.  Given a feature extractor and a training
corpus, we can train a classifier.  First, we run the feature
extractor on each instance in the training corpus, and building a list
of (featureset, label) tuples.  Then, we pass this list to the
classifier's constructor:

.. XX THIS IS CURRENTLY NOT USING THE CONSTRUCTOR

>>> train = [(extract_features(word), label)
...          for (word, label) in labeled_words]
>>> classifier = nltk.NaiveBayesClassifier.train(train)

The constructed model ``classifier`` can then be used to predict the
labels for unseen inputs:

>>> test_featuresets = [extract_features(word)
...                     for word in unseen_labeled_words]
>>> predicted = classifier.batch_classify(test)

.. note:: When working with large corpora, constructing a single list
   that contains the features of every instance can use up a large
   amount of memory.  In these cases, we can make use of the function
   ``nltk.classify.apply_features``, which returns an object that
   acts like a list but does not store all values in memory:

   >>> train = apply_features(extract_features, labeled_words)
   >>> test = apply_features(extract_features, unseen_words)

Selecting relevant features, and deciding how to encode them for the
learning method, can have an enormous impact on its ability to extract
a good model.  Much of the interesting work in modeling a phenomenon
is deciding what features might be relevant, and how we can represent
them.  Although it's often possible to get decent performance by using
a fairly simple and obvious set of features, there are usually
significant gains to be had by using carefully constructed features
based on an understanding of the task at hand.  

Typically, feature extractors are built through a process of
trial-and-error, guided by intuitions about what information is
relevant to the problem at hand.  It's often useful to start with a
"kitchen sink" approach, including all the features that you can think of,
and then checking to see which features actually appear to be helpful.
However, there are usually limits to the number of features that you
should use with a given learning algorithm -- if you provide too many
features, then the algorithm will have a higher chance of relying on
idiosyncracies of your training data that don't generalize well to new
examples.  This problem is known as `overfitting`:dt:, and can
especially problematic when working with small training sets.

Once a basic system is in place, a very productive method for refining
the feature set is `error analysis`:dt:.  First, the training corpus
is split into two pieces: a training subcorpus, and a `development`:dt:
subcorpus.  The model is trained on the training subcorpus, and then
run on the development subcorpus.  We can then examine individual cases in
the development subcorpus where the model predicted the wrong label, and
try to determine what additional pieces of information would allow it
to make the right decision (or which existing pieces of information
are tricking it into making the wrong decision).  The feature set can
then be adjusted accordingly, and the error analysis procedure can be
repeated, ideally using a different development/training split.

Example: Predicting Name Genders
--------------------------------

In section `Exploratory Data Analysis`_, we looked at some of the
factors that might influence whether an English name sounds more like
a male name or a female name.  Now we can build a simple model for
this classification task.  We'll use the same ``names`` corpus that we
used for exploratory data analysis, divided into a training set and an
evaluation set:

    >>> from nltk.corpus import names
    >>> import random
    >>>
    >>> # Construct a list of classified names, using the names corpus.
    >>> namelist = ([(name, 'male') for name in names.words('male')] + 
    ...             [(name, 'female') for name in names.words('female')])
    >>>
    >>> # Randomly split the names into a test & train set.
    >>> random.shuffle(namelist)
    >>> train = namelist[500:]
    >>> test = namelist[:500]

.. SB: NB I simplified the slice for testing in above example; was 5000:5500

Next, we'll build a simple feature extractor, using some of the
features that appeared to be useful in the exploratory data analysis.
We'll also throw in a number of features that seem like they might be
useful:

.. pylisting:: gender_features

   def gender_features(name):
       features = {} 
       features["firstletter"] = name[0].lower()
       features["lastletter"] = name[0].lower()
       for letter in 'abcdefghijklmnopqrstuvwxyz':
           features["count(%s)" % letter] = name.lower().count(letter)
           features["has(%s)" % letter] = (letter in name.lower())
       return features

    >>> gender_features('John')
    {'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}

Now that we have a corpus and a feature extractor, we can train a
classifier.  We'll use a "Naive Bayes" classifier, which will be
described in more detail in section `Naive Bayes Classifiers`_.

.. XX THIS IS CURRENTLY NOT USING THE CONSTRUCTOR

    >>> train_featuresets = [(gender_features(n), g) for (n,g) in train]
    >>> classifier = nltk.NaiveBayesClassifier.train(train_featuresets)

Now we can use the classifier to predict the gender for unseen names:

    >>> classifier.classify(gender_features('Blorgy'))
    'male'
    >>> classifier.classify(gender_features('Alaphina'))
    'female'

And using the test corpus, we can check the overall accuracy of the
classifier across a collection of unseen names with known labels:

    >>> test_featuresets = [(gender_features(n),g) for (n,g) in test]
    >>> print nltk.classify.accuracy(classifier, test_featuresets)
    0.688

Example: Predicting Sentiment
-----------------------------

Movie review domain; ACL 2004 paper by Lillian Lee and Bo Pang.
Movie review corpus included with NLTK.

.. pylisting:: movie-reviews

   import nltk, random

   TEST_SIZE = 500

   def word_features(doc):
       words = nltk.corpus.movie_reviews.words(doc)
       return nltk.FreqDist(words), doc[0]

   def get_data():
       featuresets = apply(word_features, nltk.corpus.movie_reviews.files())
       random.shuffle(featuresets)
       return featuresets[TEST_SIZE:], featuresets[:TEST_SIZE]

   >>> train_featuresets, test_featuresets = get_data()
   >>> c1 = nltk.NaiveBayesClassifier.train(train_featuresets)
   >>> print nltk.classify.accuracy(c1, test_featuresets)
   0.774
   >>> c2 = nltk.DecisionTreeClassifier.train(train_featuresets)
   >>> print nltk.classify.accuracy(c2, test_featuresets)
   0.576
    


Initial work on a classifier to use frequency of modal verbs to classify
documents by genre:

.. pylisting:: modals

   import nltk, math
   modals = ['can', 'could', 'may', 'might', 'must', 'will']

   def modal_counts(tokens):
       return nltk.FreqDist(word for word in tokens if word in modals)

   # just the most frequent modal verb
   def modal_features1(tokens):
       return dict(most_frequent_modal = model_counts(tokens).max())

   # one feature per verb, set to True if the verb occurs more than once
   def modal_features2(tokens):
       fd = modal_counts(tokens)
       return dict( (word,(fd[word]>1)) for word in modals)

   # one feature per verb, with a small number of scalar values
   def modal_features3(tokens):
       fd = modal_counts(tokens)
       features = {}
       for word in modals:
           try:
               features[word] = int(-math.log10(float(fd[word])/len(tokens)))
           except OverflowError:
               features[word] = 1000
       return features

   # 4 bins per verb based on frequency
   def modal_features4(tokens):
       fd = modal_counts(tokens)
       features = {}
       for word in modals:
           freq = float(fd[word])/len(tokens)
           for logfreq in range(3,7):
               features["%s(%d)" % (word, logfreq)] = (freq < 10**(-logfreq))
       return features

   >>> genres = ['hobbies', 'humor', 'science_fiction', 'news', 'romance', 'religion'] 
   >>> train = [(modal_features4(nltk.corpus.brown.words(g)[:2000]), g) for g in genres]
   >>> test = [(modal_features4(nltk.corpus.brown.words(g)[2000:4000]), g) for g in genres]
   >>> classifier = nltk.NaiveBayesClassifier.train(train)
   >>> print 'Accuracy: %6.4f' % nltk.classify.accuracy(classifier, test)


.. _fig-feature-extraction:
.. figure:: ../images/feature-extraction.png
   :scale: 25:35:25

   Feature Extraction


.. _fig-classification:
.. figure:: ../images/classification.png
   :scale: 25:35:25

   Document Classification


----------
Evaluation
----------

In order to decide whether a classification model is accurately
capturing a pattern, we must evaluate that model.  The result of this
evaluation is important for deciding how trustworthy the model is, and
for what purposes we can use it.  Evaluation can also be a useful tool
for guiding us in making future improvements to the model.

.. There are several techniques that can be used to measure how well a
   system does, and each has its pluses and minuses.

Evaluation Set
--------------

Most evaluation techniques calculate a score for a model by comparing
the labels that it generates for the inputs in an `evaluation set`:dt:
with the correct labels for those inputs.  This evaluation set
typically has the same format as the training corpus.  However, it is
very important that the evaluation set be distinct from the training
corpus: if we simply re-used the training corpus as the evaluation
set, then a model that simply memorized its input, without learning
how to generalize to new examples, would receive very high scores.
Similarly, if we use a development corpus, then it must be distinct
from the evaluation set as well.  Otherwise, we risk building a model
that does not generlize well to new inputs; and our evaluation scores
may be misleadingly high.

If we are actively developing a model, by adjusting the features that
it uses or any hand-tuned parameters, then we may want to make use of
two evaluation sets.  We would use the first evaluation set while
developing the model, to evaluate whether specific changes to the
model are beneficial.  However, once we've made use of this first
evaluation set to help develop the model, we can no longer trust that
it will give us an accurate idea of how well the model would perform
on new data.  We therefore save the second evaluation set until our
model development is complete, at which point we can use it to check
how well our model will perform on new input values.

When building an evaluation set, we must be careful to ensure that is
sufficiently different from the training corpus that it will
effectively evaluate the performance of the model on new inputs.  For
example, if our evaluation set and training corpus are both drawn from
the same underlying data source, then the results of our evaluation
will only tell us how well the model is likely to do on other texts
that come from the same (or a similar) data source.  

.. Which data should be used?  (eg random sampling vs single chunk)

Accuracy
--------

The simplest metric that can be used to evaluate a classifier,
`accuracy`:dt:, measures the percentage of inputs in the evaluation
set that the classifier correctly labeled.  For example, a name gender
classifier that predicts the correct name 60 times in an evaluation
set containing 80 names would have an accuracy of 60/80 = 75%.  The
function ``nltk.classify.accuracy`` can be used to calculate the
accuracy of a classifier model on a given evaluation set:

   >>> classifier = nltk.NaiveBayesClassifier.train(train)
   >>> print 'Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test)
   0.75

When interpreting the accuracy score of a classifier, it is important
to take into consideration the frequencies of the individual class
labels in the evaluation set.  For example, consider a classifier that
determines the correct word sense for each occurance of the word
"bank."  If we evaluate this classifier on financial newswire text,
then we may find that the ``financial-institution`` sense is used 19
times out of 20.  In that case, an accuracy of 95% would hardly be
impressive, since we could achieve that accuracy with a model that
always returns the ``finanical-institution`` sense.  However, if we
instead evaluate the classifier on a more balanced corpus, where the
most frequent word sense has a frequency of 40%, then a 95% accuracy
score would be a much more positive result.






- Simplest metric: accuracy.  Describe what it is, where it can be
  limited in usefulness.

.. SB: examples of meaningless accuracy scores, when irrelevant material
       is included; e.g. let X be the event that a document is on a particular
       topic; the presence of a large number of irrelevant documents can
       falsely exaggerate the accuracy score.  Even a majority class classifier
       that scores every document as irrelevant will get a high accuracy score.

Precision and Recall
--------------------

.. _fig-precision-recall:
.. figure:: ../images/precision-recall.png
   :scale: 25:120:25

   True and False Positives and Negatives

Consider Figure fig-precision-recall_.
The intersection of these sets defines four regions: the true
positives (TP), true negatives (TN), false positives (FP) or Type I errors, and false
negatives (FN) or Type II errors.  Two standard measures are
*precision*, the fraction of guessed chunks that were correct TP/(TP+FP),
and *recall*, the fraction of correct chunks that were identified
TP/(TP+FN).  A third measure, the *F measure*, is the harmonic mean
of precision and recall, i.e. 1/(0.5/Precision + 0.5/Recall).

.. 
   Confusion Matrices
   ------------------


Cross-Validation
----------------

To do evaluation, we need to keep some of the data back -- don't test
on train.  But that means we have less data available to train.  Also,
what if our training set has ideosyncracies?

Cross-validation: run training&testing multiple times, with different
training sets.

  - Lets us get away with smaller training sets
  - Lets us get a feel for how much the performance varies based on
    different training sets.

Error Analysis
--------------

The metrics above give us a general feel for how well a system does,
but doesn't tell us much about why it gets that performance .. are
there patterns in what it gets wrong?  If so, that can help us to
improve the system, or if we can't improve it, then at least make us
more aware of what the limitations of the system are, and what kind of
data it will produce more reliable or less reliable results for.

Talk some about how to do error analysis?

----------------------
Classification Methods
----------------------

In this section, we'll take a closer took at three machine learning
methods that can be used to automatically build classification models:
Decision Trees, Naive Bayes classifiers, and Maximum Entropy
classifiers.  As we've seen, it's possible treat these learning
methods as black boxes, simply training models and using them for
prediction without understanding how they work.  But there's a lot to be learned
from taking a closer look at how these learning methods select models
based on the data in a training corpus.  An understanding of these
methods can help guide our selection of appropriate features, and
especially our decisions about how those features should be encoded.
And an understanding of the generated models can allow us to extract
useful information about which features are most informative, and how
those features relate to one another.

--------------
Decision Trees
--------------

.. Note that they haven't necessarily seen syntax trees before this, so
   it may seem odd to them (or at least not obvious) that these "trees"
   are upside down.

A `decision tree`:dt: is a tree-structured flowchart used to choose
labels for input values.  This flowchart consists of `decision
nodes`:dt:, which check feature values, and `leaf nodes`:dt:, which
assign labels.  To choose the label for an input value, we begin at
the flowchart's initial decision node, known as its `root node`:dt:.
This node contains a condition that checks one of the input value's
features, and selects a branch based on that feature's value.
Following the branch that describes our input value, we arrive at a
new decision node, with a new condition on the input value's features.
We continue following the branch selected by each node's condition,
until we arrive at a leaf node, which provides a label for the input
value.  Figure fig-decision-tree_ shows an example decision tree model for
the name gender task.

.. _fig-decision-tree:
.. figure:: ../images/decision-tree.png
   :scale: 50:120:50

   Decision Tree model for the name gender task.  Note that tree
   diagrams are conventially drawn "upside down," with the root at the
   top, and the leaves at the bottom.

Once we have a decision tree, it is thus fairly streight forward to
use it to assign labels to new input values.  What's less streight
forward is how we can build a decision tree that models a given
training corpus.  But before we look at the learning algorithm for
building decision trees, we'll consider a simpler task: picking the
best "decision stump" for a corpus.  A `decision stump`:dt: is is a
decision tree with a single node, that decides how to classify inputs
based on a single feature.  It contains one leaf for each possible
feature value, specifying the class label that should be assigned to
inputs whose features have that value.  In order to build a decision
stump, we must first decide which feature should be used.  The
simplest method is to just build a decision stump for each possible
feature, and see which one achieves the highest accuracy on the
training data; but we'll discuss some other alternatives below.  Once
we've picked a feature, we can build the decision stump by assigning a
label to each leaf based on the most frequent label for the selected
examples in the training corpus (i.e., the examples where the selected
feature has that value).

Given the algorithm for choosing decision stumps, the algorithm for
growing larger decision trees is straightforward.  We
begin by selecting the overall best decision stump for the corpus.  We
then check the accuracy of each of the leaves on the training corpus.
Any leaves that do not achieve sufficiently good accuracy are then
replaced by new decision stumps, trained on the subset of the training
corpus that is selected by the path to the leaf.  For example, we
could grow the decision tree in Figure fig-decision-tree_ by replacing the
leftmost leaf with a new decision stump, trained on the subset of the
training corpus names that do not start with a "k" or end with a vowel
or an "l."

As we mentioned before, there are a number of methods that can be used
to select the most informative feature for a decision stump.  One
popular alternative is to use `information gain`:dt:, a measure of how
much more organized the input values become when we divide them up
using a given feature.  To measure how disorganized the original set
of input values are, we calculate entropy of their labels, which is
defined as:

Entropy(S) = sum_{label} freq(label) * log_2(freq(label))

**(how are we doing markup for math? -- also inline math?)**

If most input values have the same label, then the entropy of their
labels will be low.  In particular, labels that have low frequency
will not contribute much to the entropy (since the first term,
freq(label), will be low); and labels with high frequency will also
not contribute much to the entropy (since log_2(freq(label)) will be
low).  On the other hand, if the input values have a wide variety of
labels, then there will be many labels with a "medium" frequency,
where neither freq(label) nor log_2(freq(label)) is low, so the
entropy will be high.

Once we have calculated the entropy of the original set of input
values' labels, we can figure out how much more organized the labels
become once we apply the decision stump.  To do so, we calculate the
entropy for each of the decision stump's leaves, and take the average
of those leaf entropy values (weighted by the number of samples in
each leaf).  The information gain is then equal to the original
entropy minus this new, reduced entropy.  The higher the information
gain, the better job the decision stump does of dividing the input
values into coherent groups, so we can build decision trees by
selecting the decision stumps with the highest information gain.

Another consideration for decision trees is efficiency.  The simple
algorithm for selecting decision stumps described above must construct
a candidate decision stump for every possible feature; and this
process must be repeated for every node in the constructed decision
tree.  A number of algorithms have been developed to cut down on the
training time by storing and reusing information about previously
evaluated examples.  <<references>>.

Decision trees have a number of useful qualities.  To begin with,
they're simple to understand, and easy to interpret.  This is
especially true near the top of the decision tree, where it is usually
possible for the learning algorithm to find very useful features.
Decision trees are especially well suited to cases where many
hierarchical categorical distinctions can be made.  For example,
decision trees can be very effective at modelling phylogeny trees.

However, decision trees also have a few disadvantages.  One problem is
that, since each branch in the decision tree splits the training data,
the amount of training data available to train nodes lower in the tree
can become quite small.  As a result, these lower decision nodes may
`overfit`:dt: the training corpus, learning patterns that reflect
idiosynracies of the training corpus, rather than genuine patterns in
the underlying problem.  One solution to this problem is to stop
dividing nodes once the amount of training data becomes too small.
Another solution is to grow a full decision tree, but then to
`prune`:dt: decision nodes that do not improve performance on a
development corpus.

A second problem with decision trees is that they force features to be
checked in a specific order, even when features may act relatively
independently of one another.  For example, when classifying documents
into topics (such as sports, automotive, or murder mystery), features
such as ``hasword(football)`` are highly indicative of a specific
label, regardless of what other the feature values are.  Since there
is limited space near the top of the decision tree, most of these
features will need to be repeated on many different branches in the
tree.  And since the number of branches increases exponentially as we
go down the tree, the amount of repetition can be very large.

A related problem is that decision trees are not good at making use of
features that are weak predictors of the correct label.  Since these
features make relatively small incremental improvements, they tend to
occur very low in the decision tree.  But by the time the decision
tree learner has descended far enough to use these features, there is
not enough training data left to reliably determine what effect they
should have.  If we could instead look at the effect of these features
across the entire training corpus, then we might be able to make some
conclusions about how they should affect the choice of label.

The fact that decision trees require that features be checked in a
specific order limits their ability to make use of features that are
relatively independent of one another.  The Naive Bayes classification
method, which we'll discuss next, overcomes this limitation by
allowing all features to act "in parallel."

Naive Bayes Classifiers
-----------------------

In `Naive Bayes`:dt: classifiers, every feature gets a say in
determining which label should be assigned to a given input value.  To
choose a label for an input value, the Naive Bayes classifier begins
by calculating the `prior probability`:dt: of each label, which is
determined by checking frequency of each label in the training corpus.
The contribution from each feature is then combined with this prior
probability, to arrive at a likelihood estimate for each label.  The
label whose likelihood estimate is the highest is then assigned to the
input value.  Figure fig-naive-bayes-triangle_ illustrates this process.

.. I go back and forth on whether we should include a figure like this
   one.  I think it gives a good high-level feeling of what's going
   on, but the details don't really line up with the algorithm's 
   specifics, and it takes a good amount of work to explain the figure.

.. _fig-naive-bayes-triangle:
.. figure:: ../images/naive-bayes-triangle.png
   :scale: 30:100:30

   An abstract illustration of the procedure used by the Naive Bayes
   classifier to choose the topic for a document.  In the training
   corpus, most documents are automotive, so the classifier starts out
   at a pointer closer to the "automative" label.  But it then
   considers the effect of each feature.  In this example, the input
   document contains the word "dark," which is a weak indicator for
   murder mysteries; but it also contains the word "football," which
   is a strong indicator for sports documents.  After every feature
   has made its contribution, the classifier checks which label it is
   closest to, and assigns that label to the input.

Individual features make their contribution to the overall decision by
"voting against" labels that don't occur with that feature very often.
In particular, the likelihood score for each label is reduced by
multiplying it by the probability that an input value with that label
would have the feature.  For example, if the word "run" occurs in 12%
of the sports documents, 10% of the murder mystery documents, and 2%
of the automotive documents, then the likelihood score for the sports
label will be multiplied by 0.12; the likelihood score for the murder
mystery label will be multiplied by 0.1; and the likelihood score for
the automotive label will be multiplied by 0.02.  The overall effect
will be to reduce the score of the murder mystery label slightly more
than the score of the sports label; and to significantly reduce the
automotive label with respect to the other two labels.  This overall
process is illustrated in Figure fig-naive-bayes-bargraph_.

.. _fig-naive-bayes-bargraph:
.. figure:: ../images/naive_bayes_bargraph.png
   :scale: 30:120:30

   Calculating label likelihoods with Naive Bayes.  Naive Bayes begins
   by calculating the prior probability of each label, based on how
   frequently each label occurs in the training data.  Every feature
   then contributes to the likelihood estimate for each label, by
   multiplying it by the probability that input values with that label
   will have that feature.  The resulting likelihood score can be
   thought of as an estimate of the probability that a randomly
   selected value from the training corpus would have both the given
   label and the set of features, assuming that the feature
   probabilities are all independent.

Underlying Probabilistic Model
------------------------------

Another way of understanding the Naive Bayes classifier is that it
chooses the most likely label for an input, under the assumption that
every input value is generated by first choosing a class label for
that input value, and then generating each feature, entirely
independent of every other feature.  Of course, this assumption is
unrealistic: features are often highly dependent on one another in
ways that don't just reflect differences in the class label.  We'll
return to some of the consequences of this assumption at the end of
this section.  But making this simplifying assumption makes it much
easier to combine the contributions of the different features, since
we don't need to worry about how they should interact with one
another.

.. _fig-naive-bayes-graph:
.. figure:: ../images/naive_bayes_graph.png
   :scale: 30:120:30

   A `Bayesian Network Graph`:dt: illustrating the generative process
   that is assumed by the Naive Bayes classifier.  To generate a
   labeled input, the model first chooses a label for the input; and
   then it generates each of the input's features based on that label.
   Every feature is assumed to be entirely independent of every other
   feature, given the label.

Based on this assumption, we can calculate an expression for
`P(label|features)`, the probability that an input will have a
particular label, given that it has a particular set of features.  To
choose a label for a new input, we can then simply pick the label `l`
that maximizes `P(l|features)`.

To begin, we note that `P(label|features)` is equal to the probability
that an input has a particular label *and* the specified set of
features, divided by the probability that it has the specified set of
features:

`P(label|features) = P(features, label)/P(features)`

Next, we note that `P(features)` will be the same for every choice of
label, so if we are simply interested in finding the most likely
label, it suffices to calculate `P(features, label)`, which we'll call
the label likelihood.  

.. Note:: If we want to generate a probability estimate for each
   label, rather than just choosing the most likely label, then the
   easiest way to compute P(features) is to simply calculate the sum
   over labels of P(features, label):

   `P(features) = \sum_{l \in label} P(features, label)`

The label likelihood can be expanded out as the probability of the
label times the probability of the features given the label:

`P(features, label) = P(label) * P(features|label)`

Furthermore, since the features are all independent of one another
(given the label), we can seperate out the probability of each
individual feature:

`P(features, label) = P(label) * \prod_{f \in features} P(f|label)`

This is exactly the equation we discussed above for calculating the
label likelihood: P(label) is the prior probability for a given label,
and each P(f|label) is the contribution of a single feature to the
label likelihood.

Zero Counts and Smoothing
-------------------------

The simplest way to calculate `P(f|label)`, the contribution of a
feature `f` toward the label likelihood for a label `label`, is to
take the percentage of training instances with the given label that
also have the given feature:

`P(f|label) = count(f, label) / count(label)`

However, this simple approach can become problematic when a feature
*never* occurs with a given label in the training corpus.  In this
case, our calculated value for `P(f|label)` will be zero, which will
cause the label likelihood for the given label to be zero.  Thus, the
input will never be assigned this label, regardless of how well the
other features fit the label.

The basic problem here is with our calculation of `P(f|label)`, the
probability that an input will have a feature, given a label.  In
particular, just because we haven't seen a feature/label combination
occur in the training corpus, doesn't mean it's impossible for that
combination to occur.  For example, we may not have seen any murder
mystery documents that contained the word "football," but we wouldn't
want to conclude that it's completely impossible for such documents to
exist.  

Thus, although `count(f,label)/count(label)` is a good estimate for
`P(f|label)` when `count(f, label)` is relatively high, this
estimate becomes less reliable when `count(f)` becomes smaller.
Therefore, when building Naive Bayes models, we usually make use of
more sophisticated techniques, known as `smoothing`:dt: techniques,
for calculating `P(f|label)`, the probability of a feature given a
label.  For example, the "Expected Likelihood Estimation" for the
probability of a feature given a label basically adds 0.5 to each
`count(f,label)` value; and the "Heldout Estimation" uses a heldout
corpus to calculate the relationship between feature freequencies and
feature probabilities.  For more information on smoothing techniques,
see <<ref -- manning & schutze?>>.

Non-Binary Features
-------------------

We have assumed here that each feature is binary -- in other words
that each input either has a feature or does not.  Label-valued
features (e.g., a color feature which could be red, green, blue,
white, or orange) can be converted to binary features by replacing
them with features such as "color-is-red".  Numeric features can be
converted to binary features by `binning`:dt:, which replaces them with
features such as "4<x<6".  

Another alternative is to use regression methods to model the
probabilities of numeric features.  For example, if we assume that the
height feature is gaussian, then we could estimate P(height|label) by
finding the mean and variance of the heights of the inputs with each
label.  In this case, `P(f=v|label)` would not be a fixed value, but
would vary depending on the value of `v`.

The Naivite of Independence
---------------------------

The reason that Naive Bayes classifiers are called "naive" is that
it's unreasonable to assume that all features are independent of one
another (given the label).  In particular, almost all real-world
problems contain features with varying degrees of dependence on one
another.  If we had to avoid any features that were dependent on one
another, it would be very difficult to construct good feature sets
that provide the required information to the machine learning
algorithm.

So what happens when we ignore the independence assumption, and use
the Naive Bayes classifier with features that are not independent?
One problem that arises is that the classifier can end up
"double-counting" the effect of highly correlated features, pushing
the classifier closer to a given label than is justified.

To see how this can occur, consider a name gender classifier that
contains two identical features, `f_1` and `f_2`.  In other words,
`f_2` is an exact copy of `f_1`, and contains no new information.
Nevertheless, when the classifier is considering an input, it will
include the contribution of both `f_1` and `f_2` when deciding which
label to choose.  Thus, the information content of these two features
is given more weight than it should be.

Of course, we don't usually build Naive Bayes classifiers that contain
two identical features.  However, we do build classifiers that contain
features which are dependent on one another.  For example, the
features ``ends-with(a)`` and ``ends-with(vowel)`` are dependent on
one another, because if an input value has the first feature, then it
must also have the second feature.  For features like these, the
duplicated information may be given more weight than is justified by
the training corpus.

The Cause of Double-Counting
----------------------------

The basic problem that causes this double-counting issue is that
during training, feature contributions are computed seperately; but
when using the classifier to choose labels for new inputs, those
feature contributions are combined.  One solution, therefore, is to
consider the possible interactions between feature contributions
during training.  We could then use those interactions to adjust the
contributions that individual features make.

To make this more precise, we can rewrite the equation used to
calculate out the likelihood of a label, seperating out the
contribution made by each feature (or label):

.. _parameterized-naive-bayes:

   `P(features, label) = w[label] * \prod_{f \in features} w[f, label]`

Here, `w[label]` is the "starting score" for a given label; and `w[f,
label]` is the contribution made by a given feature towards a label's
likelihood.  We call these values `w[label]` and `w[f, label]` the
`parameters`:dt: or `weights`:dt: for the model.  Using the Naive
Bayes algorithm, we set each of these parameters independently:

`w[label] = P(label)`

`w[f, label] = P(f|label)`

However, in the next section, we'll look at a classifier that
considers the possible interactions between these parameters when
choosing their values.

---------------------------
Maximum Entropy Classifiers
---------------------------

The `Maximum Entropy`:dt: classifier uses a model that is very similar
to the model used by the Naive Bayes classifier.  But rather than
using probabilities to set the model's parameters, it uses search
techniques to find a set of parameters that will maximize the
performance of the classifier.  In particular, it looks for the set of
parameters that maximizes the `total likelihood`:dt: of the training
corpus, which is defined as:

``\sum_{(x) in corpus} P(label(x)|features(x))``

Where ``P(label|features)``, the probability that an input whose
features are ``features`` will have class label ``label``, is defined as:
                              
`P(label|features) = P(label, features) / sum_{label} P(label, features)`

Because of the potentially complex iteractions between the effects of
related features, there is no way to directly calculate the model
parameters that maximize the likelihood of the training corpus.
Therefore, Maximium Entropy classifiers choose the model paremeters
using `iterative optimization`:dt: techniques, which initialze the
model's parameters to random values, and then repeatedly refine those
parameters to bring them closer to the optimal solution.  The
iterative optimization techniques guarantee that each refinement of
the parameters will bring them closer to the optimal values; but do
not necessarily provide a means of determining when those optimal
values have been reached.  Because the parameters for Maximum Entropy
classifiers are seleced using iterative optimization techniques, they
can take a long time to train.  This is especially true when the size
of the training corpus, the number of features, and the number of
labels are all large.


.. The use of iterative optimization techniques is a 


.. note:: Some iterative optimization techniques are much faster than
   others.  When training Maximum Entropy models, avoid the use of
   Generalized Iterative Scaling (GIS) or Improved Iterative Scaling
   (IIS), which are both considerably slower than the Conjucate
   Gradient (CG) and the BFGS optimization methods.


- the technique, of fixing the form of the model, and searching for
  model parameters that optimize some evaluation metric is called
  optimization.

- a number of other machine learning algorithms can be thought of as 
  optimization systems.

..
  Input-Features vs Joint-Features
  --------------------------------

  - Another way that maxent is different from the other two classifiers
    we've considered is in the way that it gets info from its inputs

  - etc.

  Why Is It Called "Maximum Entropy"?
  -----------------------------------

  explain maximum entropy principle: least biased pdist consistent with
  knowledge

  ---------------------
  Classifier Properties
  ---------------------

  Generative vs Conditional
  -------------------------
  Another important difference between Naive Bayes and Maxent:

  - Naive Bayes tries to predict the probability of an (input,output) pair: P(x,y)
  - Maxent tries to predict the probability of an output, given an input: P(y|x)

  Generative models are more powerful that conditional models.
  - P(y|x) = P(x,y)/P(x)

  Generative models can answer questions that conditional models can not.

  E.g., how likely is a given input?

  Or how likely is an output, given that the input is either x1 or x2?

  So why would we want a conditional model?::

  Less powerful ->
		Less free parameters ->
			More data per parameter ->
				Easier to find correct parameter values

  - Finding a generative model is like trying to draw a topological map
    (x=input, y=output).
  - Finding a conditional model is like trying to draw a skyline
  - (include a picture comparing :) )

  Bias vs Variance
  ----------------

  The term bias is used in two ways:

  - ML bias & Statistical bias

  - ML bias: an assumption implicit in a ML technique that either rules out a class of models (absolute bias) or gives preference to a class of models (relative bias)

  - (when) do we need ML bias?

  Statistical bias:

    - Build a model from every possible training set (of a given size)
    - Average those models.
    - Statistical bias = error of this averaged model

  Variance:

    - Average (squared) difference between an individual model and the averaged model
    - Average model error = bias^2 + variance

  --------------------
  Joint Classification
  --------------------

  .. better name for this section/concept?

  - classification assigns a label to each input, independently of the 
    other inputs.

  - often, we're interested in doing several classification problems that
    are related to one another.

  - e.g.: part of speech tagging.

    - best tag in isolation, vs best tag in sequence.

  - in these cases, we can use `joint classification`:dt: models, which 
    find a good overall solution, taking into affect the constraints
    between output tags.

  - three examples: Greedy, HMMs and TBL.  Give a brief description of each.

    - Greedy: Do the first problem, then use our answer to that problem
      to generate features for the next problem.

    - HMMs: Use P(word|tag) to model the individual problems, just as we
      did for classifiers.  But also add in P(tag[i]|tag[i-1]), which is
      used to help capture the likelihood of different tag sequences.
      Pick the sequence of tags that maximizes all the P(word|tag) and 
      P(tag[i]|tag[i-1]) scores.

    - TBL: Start with an initial tagging (done locally), but then use
      transformations to "fix up" suspicious looking tag sequences.

.. ------------------------------------------------------------
   Include this section?

   Structured Prediction
   ---------------------

   - problems where we want to generate a single complex output value
     (e.g., a syntax tree).

   - break the problem down into smaller pieces, and use joint
     classification models to find a solution.
   ------------------------------------------------------------

.. ------------------------------------------------------------
   I don't think we'll have room for IR in this chapter; and we don't
   have much in the toolkit for it anyway.
   ---------------------
   Information Retrieval
   ---------------------
   ------------------------------------------------------------


..
  ---------------------
  Unsupervised Learning
  ---------------------

  - Even when there's no labeled training data, we can still use
    automatic methods to learn about the data.
  - What patterns tend to occur?
  - How do those patterns relate to one another?


  Example: Punkt sentence tokenizer.

  - Dividing a paragraph into sentences is hard. (western languages)
  - Mainly because we can't tell if "." is used as part of an
    abbreviation or not.
  - Punkt uses unsupervised methods to find common abbreviations.
  - If "xyz" is almost always followed by "." then it's probably an
    abbreviation.
  
  .. SB: evaluating segmentations, and windowdiff (if deleted from this chapter should go back in data chapter)

  It can be useful to know whether two words are "similar."

  - Search: don't just find the exact terms you specify.
  - Question Answering: handle synonyms.
  - Parsing: what words act similarly to one another?

  Different notions of "similarity":

  - What words tend to co-occur?
  - What words occur in similar contexts?

  Document clustering:

  - vocabulary (ignore words other than the n most frequent and m least frequent; or use tf.idf)
  - document vectors
  - WordNet
  - news collection?  unsupervised topic detection?


  ----------------------------
  Machine Learning in Python..
  ----------------------------

  - For machine learning, it can be very convenient to do feature
    extraction in Python.
  - Python has excellent text processing facilities
  - NLTK should make your job easier.
  - But when you actually build the model, you may want to use something
    written in C or Java.
  - Python does not perform numerically intensive calculations very
    quickly.
  - NLTK can interface w/ external ML systems.

  .. SB: I think numpy and other numerical libraries do their work in C, and
         are probably quite efficient.  Not clear about support for sparse arrays.
       
  .. SB: brief overview of machine learning methods that help cope with existence
         of small amounts of labeled data and larger amounts of unlabeled data.
       
  ------------
  Data Sources
  ------------

  Publishers: LDC, ELRA

  Annual competitions: CoNLL, TREC, CLEF


  -----------
  Conclusions
  -----------

  how it all fits together..

  -------
  Summary
  -------

  ---------------
  Further Reading
  ---------------

    - choosing prepositions http://itre.cis.upenn.edu/~myl/languagelog/archives/002003.html

    - Jurafsky and Martin, Manning and Schutze chapters


---------
Exercises
---------

#. |easy| Read up on one of the language technologies mentioned in this section, such as
   word sense disambiguation, semantic role labeling, question answering, machine translation,
   named entity detection.
   Find out what type and quantity of annotated data is required for developing such systems.
   Why do you think a large amount of data is required?

#. |easy|
   Exercise: compare the performance of different machine learning
   methods.  (they're still black boxes at this point)
   
#. |easy|
   The synonyms `strong`:lx: and `powerful`:lx: pattern
   differently (try combining them with `chip`:lx: and `sales`:lx:).

#. |soso|
   Accessing extra features from WordNet to augment those that appear
   directly in the text (e.g. hyperym of any monosemous word)

#. |hard|
   Task involving PP Attachment data; predict choice of preposition
   from the nouns.

#. |hard| Suppose you wanted to automatically generate a prose description of a scene,
   and already had a word to uniquely describe each entity, such as `the jar`:lx:,
   and simply wanted to decide whether to use `in`:lx: or `on`:lx: in relating
   various items, e.g. `the book is in the cupboard`:lx: vs `the book is on the shelf`:lx:.
   Explore this issue by looking at corpus data; writing programs as needed.

.. _prepositions:
.. ex::
   .. ex:: in the car *vs* on the train
   .. ex:: in town *vs* on campus
   .. ex:: in the picture *vs* on the screen
   .. ex:: in Macbeth *vs* on Letterman






.. include:: footer.rst


..   *** Old material ***
..   ------------
..   Introduction
..   ------------
..   
..   * grammar engineering, connection to Part II, test suites, regression testing
..   
..   ----------
..   Evaluation
..   ----------
..   
..   * basic tasks of segmentation and labeling
..   * accuracy: why it is not enough for a labeling task
..   
..   Precision and Recall
..   --------------------
..   
..   .. _precision-recall:
..   
..   .. figure:: ../images/precision-recall.png
..      :scale: 70
..   
..      True and False Positives and Negatives
..   
..   Consider Figure precision-recall_.
..   The intersection of these sets defines four regions: the true
..   positives (TP), true negatives (TN), false positives (FP), and false
..   negatives (FN).  Two standard measures are
..   *precision*, the fraction of guessed chunks that were correct TP/(TP+FP),
..   and *recall*, the fraction of correct chunks that were identified
..   TP/(TP+FN).  A third measure, the *F measure*, is the harmonic mean
..   of precision and recall, i.e. 1/(0.5/Precision + 0.5/Recall).
..   
..   Windowdiff: Evaluating Segmentations
..   ------------------------------------
..   
..   .. _windowdiff:
..   .. figure:: ../images/windowdiff.png
..      :scale: 30
..   
..      A Reference Segmentation and Two Hypothetical Segmentations
..   
..   A different method must be used for comparing segmentations.  In Figure windowdiff_ we see two
..   possible segmentations of a sequence of items (e.g. tokenization, chunking, sentence segmentation),
..   which might have been produced by two programs or annotators.  If we naively score S\ :subscript:`1` and
..   S\ :subscript:`2` for their alignment with the reference segmentation, both will score 0 as neither
..   got the correct alignment.  However, S\ :subscript:`1` is clearly better than S\ :subscript:`2`,
..   and so we need a corresponding measure, such as `Windowdiff`:dt:.  Windowdiff is a simple
..   algorithm for evaluating the quality of a segmentation, by running a sliding window over the
..   data and awarding partial credit for near misses.  The following code illustrates the algorithm
..   running on the segmentations from Figure windowdiff_ using a window size of 3:
..   
..       >>> ref = "00000001000000010000000"
..       >>> s1  = "00000010000000001000000"
..       >>> s2  = "00010000000000000001000"
..       >>> nltk.windowdiff(ref,ref,3)
..       0
..       >>> nltk.windowdiff(ref,s1,3)
..       4
..       >>> nltk.windowdiff(ref,s2,3)
..       16
..   
..   
..   -----------------
..   Language Modeling
..   -----------------
..   
..   * smoothing, estimation [EL]
..   
..   ----------------
..   Machine Learning
..   ----------------
..   
..   * feature selection, feature extraction
..   * text classification (question classification, language id, naive bayes etc)
..   * sequence classification (HMM, TBL)
..   * unsupervised learning (clusterers) 
..   
.. 
.. end of file
