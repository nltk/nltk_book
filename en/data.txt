.. -*- mode: rst -*-
.. include:: ../definitions.txt

.. standard global imports

    >>> from nltk.book import *

.. _chap-data:

==============================
13. Linguistic Data Management
==============================

.. TODO: Shoebox checking by chunking
.. TODO: random syllables according to a canon "%s%s" % (random.choice('ptkbdg'), random.choice('aeiou'))
.. TODO: linguistic annotation
.. TODO: paradigms

------------
Introduction 
------------

Language resources of all kinds are proliferating on the Web.  These include data such as
lexicons and annotated text, and software tools for creating and manipulating the data.
As we have seen in previous chapters, language resources are now essential in most areas of NLP.
This has been made possible by three significant developments over the past decade.
First, inexpensive mass storage technology permits large resources to be stored in 
digital form, while the Extensible Markup Language (XML) and Unicode provide flexible ways 
to represent structured data and give it good prospects for long-term survival.
Second, digital publication has been a practical and efficient means of sharing language 
resources. Finally, search engines, mailing lists, and online resource catalogues
make it possible for people to discover the existence of the resources they may be seeking.

Although language resources are central to NLP, we still face many difficulties in using them.
First, the resource we are looking for may not exist, and so we have to think about
*creating* a new language resource.
Second, a resource may exist but its creator didn't document its existence anywhere,
leaving us to recreate the resource; however, to save further wasted effort we
should learn about *publishing* metadata the documents the existence of a resource,
and even to publish the resource itself, in a form that is easy for others to re-use.
Third, the resource may exist and may be obtained, but is in an incompatible format,
and so we need to set about *converting* the data into a different format.
Finally, the resource may be in the right format, but the available software is unable
to perform the required analysis task, and so we need to develop our own program
for *analyzing* the data.  This chapter covers each of these issues in turn, using
many examples drawn from practical experience managing linguistic data.
Before embarking on this sequence of issues, we start by examining the structure of
the most important linguistic data types.

---------------------
Linguistic Data Types
---------------------


.. _datatypes:
.. figure:: ../images/datatypes.png
   :scale: 40

   Basic Linguistic Datatypes: Lexicons and Texts

Linguistic data management deals with a variety of data types, the most
important being lexicons, paradigms and texts. A lexicon is a database
of words, minimally containing part of speech information and
glosses. A paradigm, broadly construed, is any kind of rational
tabulation of words or phrases to illustrate contrasts and systematic
variation.  A text is essentially any larger unit such as a narrative
or a conversation.  In addition to these data types, empirical linguistic
investigation involves various kinds of description, such as
field notes, grammars and analytical papers.

These various kinds of data and description enter into a complex web
of relations. For example, the discovery of a new word in a text may
require an update to the lexicon and the construction of a new
paradigm (e.g. to correctly classify the word). Such updates may
occasion the creation of some field notes, the extension of a grammar
and possibly even the revision of the manuscript for an analytical
paper. Progress on description and analysis gives fresh insights about
how to organise existing data and it informs the quest for new data.
Whether one is sorting data, or generating tabulations, or gathering
statistics, or searching for a (counter-)example, or verifying the
transcriptions used in a manuscript, the principal challenge is
computational.

In the following we will consider various methods for linguistic
data management.

* language resources, OLAC, LDC, particularly important resources people should know about

* corpus curation, balance, life-cycle of linguistic data

* pragmatic issues in getting gold-standard data

* annotation, formats, models, 3-level model

* automatic learning of annotations (mention in passing)

* methods for processing data created with proprietary
  tools (e.g. Microsoft Office products).  The bulk of the discussion
  focusses on field data stored in the popular *Toolbox* format.

.. note:: Remember that our program samples assume you
   begin your interactive session or your program with: ``from nltk.book import *``
   
-------------
Creating Data
-------------

* crawling
* 3rd party tool

Conventional office software is widely used in computer-based language
documentation work, given its familiarity and ready availability.
This includes word processors and spreadsheets.

Spiders
-------

* what they do: basic idea is simple
* python code to find all the anchors, extract the href, and make an absolute URL for fetching
* many issues: starting points, staying within a single site, only getting HTML

CSV
---


XML and ElementTree
-------------------

* inspecting and processing XML
* example: find nodes matching some criterion and add an attribute
* Shakespeare XML corpus example


Creating Language Resources Using Word Processors
-------------------------------------------------

Word processing software is often used in creating dictionaries and
interlinear texts.  However, it is rather time-consuming to maintain
the consistency of the content and format.  Consider a dictionary in
which each entry has a part-of-speech field, drawn from a set of 20
possibilities, displayed after the pronunciation field, and rendered
in 11-point bold.  No convential word processor has search or macro
functions capable of verifying that all part-of-speech fields have
been correctly entered and displayed.  This task requires exhaustive
manual checking.  If the word processor permits the document to be
saved in a non-proprietary format, such as RTF, HTML, or XML, it may
be possible to write programs to do this checking automatically.

Consider the following fragment of a lexical entry:
"sleep [sli:p] **vi** *condition of body and mind...*\ ".
We can enter this in MSWord, then "Save as Web Page",
then inspect the resulting HTML::

    <p class=MsoNormal>sleep
      <span style='mso-spacerun:yes'> </span>
      [<span class=SpellE>sli:p</span>]
      <span style='mso-spacerun:yes'> </span>
      <b><span style='font-size:11.0pt'>vi</span></b>
      <span style='mso-spacerun:yes'> </span>
      <i>a condition of body and mind ...<o:p></o:p></i>
    </p>

Observe that the entry is represented as an HTML paragraph, using the
``<p>`` element, and that the part of speech appears inside a ``<span
style='font-size:11.0pt'>`` element.  The following program defines
the set of legal parts-of-speech, ``legal_pos``.  Then it extracts all
11-point content from the ``dict.htm`` file and stores it in the set
``used_pos``.  Observe that the search pattern contains a
parenthesized sub-expression; only the material that matches this
sub-expression is returned by ``re.findall``.  Finally, the program
constructs the set of illegal parts-of-speech as ``used_pos -
legal_pos``:

    >>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])
    >>> pattern = re.compile(r"'font-size:11.0pt'>([a-z.]+)<")
    >>> document = open("dict.htm").read()
    >>> used_pos = set(re.findall(pattern, document))
    >>> illegal_pos = used_pos.difference(legal_pos)
    >>> print list(illegal_pos)
    ['v.i', 'intrans']

This simple program represents the tip of the iceberg.  We can develop
sophisticated tools to check the consistency of word processor files,
and report errors so that the maintainer of the dictionary can correct
the original file *using the original word processor*.  We can write
other programs to *convert* the data into a different format.  For
example, the following program extracts the words and their
pronunciations and generates output in "comma-separated value" (CSV) format:

    >>> document = open("dict.htm").read()
    >>> document = re.sub("[\r\n]", "", document)
    >>> word_pattern = re.compile(r">([\w]+)")
    >>> pron_pattern = re.compile(r"\[.*>([a-z:]+)<.*\]")
    >>> for entry in document.split("<p"):
    ...     word_match = word_pattern.search(entry)
    ...     pron_match = pron_pattern.search(entry)
    ...     if word_match and pron_match:
    ...         lex = word_match.group(1)
    ...         pos = pron_match.group(1)
    ...         print '"%s","%s"' % (lex, pos)
    "sleep","sli:p"
    "walk","wo:k"
    "wake","weik"

We could also produce output in the Toolbox format (to be discussed
in detail later)::

    \lx sleep
    \ph sli:p
    \ps v.i
    \gl a condition of body and mind ...

    \lx walk
    \ph wo:k
    \ps v.intr
    \gl progress by lifting and setting down each foot ...

    \lx wake
    \ph weik
    \ps intrans
    \gl cease to sleep


Creating Language Resources using Spreadsheets and Databases
------------------------------------------------------------

**Spreadsheets.** These are often used for wordlists or paradigms.  A
comparative wordlist may be stored in a spreadsheet, with a row for
each cognate set, and a column for each language.  Examples are
available from ``www.rosettaproject.org``.  Programs such as Excel can
export spreadsheets in the CSV format, and we can write programs to
manipulate them, with the help of Python's ``csv`` module.  For
example, we may want to print out cognates having an edit-distance of
at least three from each other (i.e. 3 insertions, deletions, or
substitutions).

**Databases.** Sometimes lexicons are stored in a full-fledged
relational database.  When properly normalized, these databases can
implement many well-formedness constraints.  For example, we can
require that all parts-of-speech come from a specified vocabulary by
declaring that the part-of-speech field is an *enumerated type*.
However, the relational model is often too restrictive for linguistic
data, which typically has many optional and repeatable fields (e.g.
dictionary sense definitions and example sentences).
Query languages such as SQL cannot express many
linguistically-motivated queries, e.g. *find all words that appear
in example sentences for which no dictionary entry is provided*.
Now supposing that the database supports exporting data to CSV format,
and that we can save the data to a file ``dict.csv``::

    "sleep","sli:p","v.i","a condition of body and mind ..."
    "walk","wo:k","v.intr","progress by lifting and setting down each foot ..."
    "wake","weik","intrans","cease to sleep"

Now we can express this query in the following program:

    >>> import csv
    >>> lexemes = []
    >>> defn_words = []
    >>> for row in csv.reader(open("dict.csv")):
    ...     lexeme, pron, pos, defn = row
    ...     lexemes.append(lexeme)
    ...     defn_words += defn.split()
    >>> undefined = list(set(defn_words).difference(set(lexemes)))
    >>> undefined.sort()
    >>> print undefined
    ['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down', 'each',
    'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']

Creating Language Resources using Toolbox
-----------------------------------------

Over the last two decades, several dozen tools have been developed
that provide specialized support for linguistic data management.
(Please see [Bird2003Portability]_ for a detailed list of such tools.)
Perhaps the single most popular tool used by linguists for managing data
is *Toolbox* (previously known as *Shoebox*).
Toolbox uses a simple file format which we can easily
read and write, permitting us to apply computational methods to
linguistic field data.  In this section we discuss a variety of
techniques for manipulating Toolbox data in ways that are not supported
by the Toolbox software.

A Toolbox file consists of a collection of *entries* (or *records*),
where each record is made up of one or more *fields*.  Here is an
example of an entry taken from a Toolbox dictionary of Rotokas.
(Rotokas is an East Papuan language spoken on the island of
Bougainville; this data was provided by Stuart Robinson, and
is a sample from a larger lexicon)::

    \lx kaa
    \ps N
    \pt MASC
    \cl isi
    \ge cooking banana
    \tkp banana bilong kukim
    \pt itoo
    \sf FLORA
    \dt 12/Aug/2005
    \ex Taeavi iria kaa isi kovopaueva kaparapasia.
    \xp Taeavi i bin planim gaden banana bilong kukim tasol long paia.
    \xe Taeavi planted banana in order to cook it.

.. what does the other pt mean?

This lexical entry contains the following fields:
``lx`` lexeme;
``ps`` part-of-speech;
``pt`` part-of-speech;
``cl`` classifier;
``ge`` English gloss;
``tkp`` Tok Pisin gloss;
``sf`` Semantic field;
``dt`` Date last edited;
``ex`` Example sentence;
``xp`` Pidgin translation of example;
``xe`` English translation of example.
These field names are preceded by a backslash, and must always appear
at the start of a line.  The characters of the field names must be
alphabetic.  The field name is separated from the field's contents by
whitespace.  The contents can be arbitrary text, and can continue over
several lines (but cannot contain a line-initial backslash).

We can use the ``toolbox.parse_corpus()`` method to access a Toolbox
file and load it into an ``elementtree`` object.

    >>> from nltk.corpus import toolbox
    >>> lexicon = toolbox.parse_corpus('rotokas.dic')

There are two ways to access the contents of the lexicon object, by
indexes and by paths.  Indexes use the familiar syntax, thus
``lexicon[3]`` returns entry number 3 (which is actually the fourth
entry counting from zero).  And ``lexicon[3][0]`` returns its first
field:

    >>> lexicon[3][0] 
    <Element lx at 77bd28>
    >>> lexicon[3][0].tag
    'lx'
    >>> lexicon[3][0].text
    'kaa'

We can iterate over all the fields of a given entry:

    >>> print toolbox.to_sfm_string(lexicon[3])
    \lx kaa
    \ps N
    \pt MASC
    \cl isi
    \ge cooking banana
    \tkp banana bilong kukim
    \pt itoo
    \sf FLORA
    \dt 12/Aug/2005
    \ex Taeavi iria kaa isi kovopaueva kaparapasia.
    \xp Taeavi i bin planim gaden banana bilong kukim tasol long paia.
    \xe Taeavi planted banana in order to cook it.

The second way to access the contents of the lexicon object uses
paths.  The lexicon is a series of ``record`` objects, each containing
a series of field objects, such as ``lx`` and ``ps``.  We can
conveniently address all of the lexemes using the path ``record/lx``.
Here we use the ``findall()`` function to search for any matches to
the path ``record/lx``, and we access the text content of the element,
normalising it to lowercase.

    >>> [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]
    ['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',
    'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']

Adding and Removing Fields
--------------------------

It is often convenient to add new fields that are derived from
existing ones.  Such fields often facilitate analysis.
For example, let us define a function which maps a string of
consonants and vowels to the corresponding CV sequence,
e.g. ``kakapua`` would map to ``CVCVCVV``.

    >>> def cv(s):
    ...     s = s.lower()
    ...     s = re.sub(r'[^a-z]',     r'_', s)
    ...     s = re.sub(r'[aeiou]',    r'V', s)
    ...     s = re.sub(r'[^V_]',      r'C', s)
    ...     return (s)

This mapping has four steps.  First, the string is converted to lowercase,
then we replace any non-alphabetic characters ``[^a-z]`` with an underscore.
Next, we replace all vowels with ``V``.  Finally, anything that is not
a ``V`` or an underscore must be a consonant, so we replace it with a ``C``.
Now, we can scan the lexicon and add a new ``cv`` field after every ``lx``
field.

    >>> from nltk.etree.ElementTree import SubElement
    >>> for entry in lexicon:
    ...     for field in entry:
    ...	        if field.tag == 'lx':
    ...             cv_field = SubElement(entry, 'cv')
    ...             cv_field.text = cv(field.text)

Let's see what this does for a particular entry.  Note the last
line of output, which shows the new CV field:

    >>> print toolbox.to_sfm_string(lexicon[53])
    \lx kaeviro
    \ps V
    \pt A
    \ge lift off
    \ge take off
    \tkp go antap
    \sc MOTION
    \vx 1
    \nt used to describe action of plane
    \dt 03/Jun/2005
    \ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.
    \xp Pita i go antap na lukim haus win i bagarapim.
    \xe Peter went to look at the house that the wind destroyed.
    \cv CVVCVCV

(NB. To insert this field in a different position, we need to create
the new ``cv`` field using ``Element('cv')``, assign a text value to it
then use the ``insert()`` method of the parent element.)

This technique can be used to make copies of Toolbox data
that *lack* particular fields.  For example, we may want to sanitise our
lexical data before giving it to others, by removing unnecessary
fields (e.g. fields containing personal comments.)

    >>> retain = ('lx', 'ps', 'ge')
    >>> for entry in lexicon:
    ...     entry[:] = [f for f in entry if f.tag in retain]
    >>> print toolbox.to_sfm_string(lexicon[53])
    \lx kaeviro
    \ps V
    \ge lift off
    \ge take off


Generating Reports
------------------

Finally, we take a look at simple methods to generate summary reports,
giving us an overall picture of the quality and organisation of the data.

First, suppose that we wanted to compute the average number of fields for
each entry.  This is just the total length of the entries (the number
of fields they contain), divided by the number of entries in the
lexicon:

    >>> sum(len(entry) for entry in lexicon) / len(lexicon)
    13

Next, let's consider some methods for discovering patterns in the lexical
entries.  Which fields are the most frequent?

    >>> fd = nltk.FreqDist(field.tag for entry in lexicon for field in entry)
    >>> fd.sorted()[:10]
    ['xp', 'ex', 'xe', 'ge', 'tkp', 'lx', 'dt', 'ps', 'pt', 'rt']

Which sequences of fields are the most frequent?

    >>> fd = nltk.FreqDist(':'.join(field.tag for field in entry) for entry in lexicon)
    >>> top_ten = fd.sorted()[:10]
    >>> print '\n'.join(top_ten)
    lx:ps:pt:ge:tkp:dt:ex:xp:xe
    lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe
    lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe
    lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe
    lx:ps:pt:ge:tkp:nt:dt:ex:xp:xe:ex:xp:xe
    lx:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe
    lx:rt:ps:pt:ge:ge:tkp:dt:ex:xp:xe:ex:xp:xe
    lx:rt:ps:pt:ge:tkp:dt:ex:xp:xe:ex:xp:xe:ex:xp:xe
    lx:ps:pt:ge:tkp:nt:sf:dt:ex:xp:xe
    lx:ps:pt:ge:ge:tkp:dt:ex:xp:xe
    
Which pairs of consecutive fields are the most frequent?

    >>> fd = nltk.FreqDist()
    >>> for entry in lexicon:
    ...     previous = "0"
    ...     for field in entry:
    ...         current = field.tag
    ...         fd.inc("%s->%s" % (previous, current))
    ...         previous = current
    >>> fd.sorted()[:10]
    ['ex->xp', 'xp->xe', '0->lx', 'ps->pt', 'ge->tkp', 'pt->ge', 'dt->ex', 'xe->ex', 'lx->ps', 'rt->ps']

Once we have analyzed the sequences of fields, we might want to write
down a grammar for lexical entries, and look for entries which do not
conform to the grammar.  In general, toolbox entries have nested
structure.  Thus they correspond to a tree over the fields.  We can
check for well-formedness by parsing the field names.  We first set up
a putative grammar for the entries:

    >>> grammar = nltk.parse_cfg('''
    ...   S -> Head PS Glosses Comment Date Examples
    ...   Head -> Lexeme Root
    ...   Lexeme -> "lx"
    ...   Root -> "rt"
    ...   Root ->
    ...   PS -> "ps"
    ...   Glosses -> Gloss Glosses
    ...   Glosses ->
    ...   Gloss -> "ge" | "gp"
    ...   Date -> "dt"
    ...   Examples -> Example Ex_Pidgin Ex_English Examples
    ...   Examples ->
    ...   Example -> "ex"
    ...   Ex_Pidgin -> "xp"
    ...   Ex_English -> "xe"
    ...   Comment -> "cmt"
    ...   Comment ->
    ...   ''')
    >>> rd_parser = parse.RecursiveDescent(grammar)

Now we try to parse each entry.  Those that are accepted by the
grammar prefixed with a ``'+'``, and those that are rejected
are prefixed with a ``'-'``.

    >>> for entry in lexicon[10:20]:
    ...     marker_list = [field.tag for field in entry]
    ...     if rd_parser.get_parse_list(marker_list):
    ...         print "+", ':'.join(marker_list)
    ...     else:
    ...         print "-", ':'.join(marker_list)
    - lx:ps:ge:gp:sf:nt:dt:ex:xp:xe:ex:xp:xe
    - lx:rt:ps:ge:gp:nt:dt:ex:xp:xe:ex:xp:xe
    - lx:ps:ge:gp:nt:dt:ex:xp:xe:ex:xp:xe
    - lx:ps:ge:gp:nt:sf:dt
    - lx:ps:ge:gp:dt:cmt:ex:xp:xe:ex:xp:xe
    + lx:ps:ge:ge:ge:gp:cmt:dt:ex:xp:xe
    + lx:rt:ps:ge:gp:cmt:dt:ex:xp:xe:ex:xp:xe
    + lx:rt:ps:ge:ge:gp:dt
    - lx:rt:ps:ge:ge:ge:gp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe
    + lx:rt:ps:ge:gp:dt:ex:xp:xe

.. pylisting:: chunk_toolbox
   :caption: Chunking a Toolbox Lexicon

    from nltk_contrib import toolbox
    from nltk.corpus import find_corpus_file
    import os.path, sys

    grammar = r"""
          lexfunc: {<lf>(<lv><ln|le>*)*}
          example: {<rf|xv><xn|xe>*}
          sense:   {<sn><ps><pn|gv|dv|gn|gp|dn|rn|ge|de|re>*<example>*<lexfunc>*}
          record:   {<lx><hm><sense>+<dt>}
        """

    >>> from nltk.etree.ElementTree import ElementTree
    >>> db = toolbox.ToolboxData()
    >>> db.open(find_corpus_file('toolbox', 'iu_mien_samp.db'))
    >>> lexicon = db.chunk_parse(grammar, encoding='utf8')
    >>> toolbox.data.indent(lexicon)
    >>> tree = ElementTree(lexicon)
    >>> tree.write(sys.stdout, encoding='utf8')

Creating Metadata for Language Resources
----------------------------------------

OLAC metadata extends the *Dublin Core* metadata set with descriptors
that are important for language resources.

The container for an OLAC metadata record is the element ``<olac>``.
Here is a valid OLAC metadata record from the Pacific And Regional
Archive for Digital Sources in Endangered Cultures (PARADISEC)::

  <olac:olac xsi:schemaLocation="http://purl.org/dc/elements/1.1/ http://www.language-archives.org/OLAC/1.0/dc.xsd
    http://purl.org/dc/terms/ http://www.language-archives.org/OLAC/1.0/dcterms.xsd
    http://www.language-archives.org/OLAC/1.0/ http://www.language-archives.org/OLAC/1.0/olac.xsd">
    <dc:title>Tiraq Field Tape 019</dc:title>
    <dc:identifier>AB1-019</dc:identifier>
    <dcterms:hasPart>AB1-019-A.mp3</dcterms:hasPart>
    <dcterms:hasPart>AB1-019-A.wav</dcterms:hasPart>
    <dcterms:hasPart>AB1-019-B.mp3</dcterms:hasPart>
    <dcterms:hasPart>AB1-019-B.wav</dcterms:hasPart>
    <dc:contributor xsi:type="olac:role" olac:code="recorder">Brotchie, Amanda</dc:contributor>
    <dc:subject xsi:type="olac:language" olac:code="x-sil-MME"/>
    <dc:language xsi:type="olac:language" olac:code="x-sil-BCY"/>
    <dc:language xsi:type="olac:language" olac:code="x-sil-MME"/>
    <dc:format>Digitised: yes;</dc:format>
    <dc:type>primary_text</dc:type>
    <dcterms:accessRights>standard, as per PDSC Access form</dcterms:accessRights>
    <dc:description>SIDE A<p>1. Elicitation Session - Discussion and
      translation of Lise's and Marie-Claire's Songs and Stories from
      Tape 18 (Tamedal)<p><p>SIDE B<p>1. Elicitation Session: Discussion
      of and translation of Lise's and Marie-Clare's songs and stories
      from Tape 018 (Tamedal)<p>2. Kastom Story 1 - Bislama
      (Alec). Language as given: Tiraq</dc:description>
  </olac:olac>
    
Linguistic Annotation
---------------------

[Bird2001Annotation]_

multiple overlapping trees over shared data


Exercises
---------

#. |easy| Write a program to filter out just the date field (``dt``) without
   having to list the fields we wanted to retain.

#. |easy| Print an index of a lexicon.  For each lexical entry, construct a
   tuple of the form ``(gloss, lexeme)``, then sort and print them all.

#. |easy| What is the frequency of each consonant and vowel contained in
   lexeme fields?

#. |easy| How many entries were last modified in 2004?

#. |soso| Write a program that scans an HTML dictionary file to find entries
   having an illegal part-of-speech field, and reports the *headword* for
   each entry.

#. |soso| Write a program to find any parts of speech (``ps`` field) that
   occurred less than ten times.  Perhaps these are typing mistakes?

#. |soso| We saw a method for discovering cases of whole-word reduplication.
   Write a function to find words that may contain partial
   reduplication.  Use the ``re.search()`` method, and the following
   regular expression: ``(..+)\1``

#. |soso| We saw a method for adding a ``cv`` field.  There is an interesting
   issue with keeping this up-to-date when someone modifies the content
   of the ``lx`` field on which it is based.  Write a version of this
   program to add a ``cv`` field, replacing any existing ``cv`` field.

#. |soso| Write a program to add a new field ``syl`` which gives a count of
   the number of syllables in the word.

#. |soso| Write a function which displays the complete entry for a lexeme.
   When the lexeme is incorrectly spelled it should display the entry
   for the most similarly spelled lexeme.
 
#. |hard| Obtain a comparative wordlist in CSV format, and write a program
   that prints those cognates having an edit-distance of at least three
   from each other.

#. |hard| Build an index of those lexemes which appear in example sentences.
   Suppose the lexeme for a given entry is *w*.
   Then add a single cross-reference field ``xrf`` to this entry, referencing
   the headwords of other entries having example sentences containing
   $w$.  Do this for all entries and save the result as a toolbox-format file.
  


-----------------------
Converting Data Formats
-----------------------

* write our own parser and formatted print
* use existing libraries, e.g. csv


Formatting Entries
------------------

We can also print a formatted
version of a lexicon.  It allows us to request specific fields without
needing to be concerned with their relative ordering in the original file.

    >>> lexicon = toolbox.parse_corpus('rotokas.dic')
    >>> for entry in lexicon[70:80]:
    ...     lx = entry.findtext('lx')
    ...     ps = entry.findtext('ps')
    ...     ge = entry.findtext('ge')
    ...     print "%s (%s) '%s'" % (lx, ps, ge)
    kakae (???) 'small'
    kakae (CLASS) 'child'
    kakaevira (ADV) 'small-like'
    kakapikoa (???) 'small'
    kakapikoto (N) 'newborn baby'
    kakapu (V) 'place in sling for purpose of carrying'
    kakapua (N) 'sling for lifting'
    kakara (N) 'arm band'
    Kakarapaia (N) 'village name'
    kakarau (N) 'frog'

We can use the same idea to generate HTML tables instead of plain text.
This would be useful for publishing a Toolbox lexicon on the web.
It produces HTML elements ``<table>``, ``<tr>`` (table row), and
``<td>`` (table data).

    >>> html = "<table>\n"
    >>> for entry in lexicon[70:80]:
    ...     lx = entry.findtext('lx')
    ...     ps = entry.findtext('ps')
    ...     ge = entry.findtext('ge')
    ...     html += "  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\n" % (lx, ps, ge)
    >>> html += "</table>"
    >>> print html
    <table>
      <tr><td>kakae</td><td>???</td><td>small</td></tr>
      <tr><td>kakae</td><td>CLASS</td><td>child</td></tr>
      <tr><td>kakaevira</td><td>ADV</td><td>small-like</td></tr>
      <tr><td>kakapikoa</td><td>???</td><td>small</td></tr>
      <tr><td>kakapikoto</td><td>N</td><td>newborn baby</td></tr>
      <tr><td>kakapu</td><td>V</td><td>place in sling for purpose of carrying</td></tr>
      <tr><td>kakapua</td><td>N</td><td>sling for lifting</td></tr>
      <tr><td>kakara</td><td>N</td><td>arm band</td></tr>
      <tr><td>Kakarapaia</td><td>N</td><td>village name</td></tr>
      <tr><td>kakarau</td><td>N</td><td>frog</td></tr>
    </table>

XML output

    >>> import sys
    >>> from nltk.etree.ElementTree import ElementTree
    >>> tree = ElementTree(lexicon[3])
    >>> tree.write(sys.stdout)
    <record>
      <lx>kaa</lx>
      <ps>N</ps>
      <pt>MASC</pt>
      <cl>isi</cl>
      <ge>cooking banana</ge>
      <tkp>banana bilong kukim</tkp>
      <pt>itoo</pt>
      <sf>FLORA</sf>
      <dt>12/Aug/2005</dt>
      <ex>Taeavi iria kaa isi kovopaueva kaparapasia.</ex>
      <xp>Taeavi i bin planim gaden banana bilong kukim tasol long paia.</xp>
      <xe>Taeavi planted banana in order to cook it.</xe>
    </record>

-----------------------
Analyzing Language Data
-----------------------

I.e. linguistic exploration

Export to statistics package via CSV

In this section we consider a variety of analysis tasks.

**Reduplication:**
First, we will develop a program to find reduplicated words.  In order
to do this we need to store all verbs, along with their English glosses.
We need to keep the glosses so that they can be displayed alongside the
wordforms.  The following code defines a Python dictionary ``lexgloss``
which maps verbs to their English glosses:

    >>> lexgloss = {}
    >>> for entry in lexicon:
    ...     lx = entry.findtext('lx')
    ...     ps = entry.findtext('ps')
    ...     if lx and ps and ps[0] == 'V':
    ...         lexgloss[lx] = entry.findtext('ge')
    kasi (burn); kasikasi (angry)
    kee (shatter); keekee (chipped)
    kauo (jump); kauokauo (jump up and down)
    kea (confused); keakea (lie)
    kape (unable to meet); kapekape (embrace)
    kapo (fasten.cover.strip); kapokapo (fasten.cover.strips)
    kavo (collect); kavokavo (perform sorcery)
    karu (open); karukaru (open)
    kare (return); karekare (return)
    kari (rip); karikari (tear)
    kae (blow); kaekae (tempt)

Next, for each verb ``lex``, we will check if the lexicon contains the
reduplicated form ``lex+lex``.  If it does, we report both forms along with
their glosses.

    >>> for lex in lexgloss:
    ...     if lex+lex in lexgloss:
    ...         print "%s (%s); %s (%s)" %\
    ...             (lex, lexgloss[lex], lex+lex, lexgloss[lex+lex])
    kuvu (fill.up); kuvukuvu (fill up)
    kitu (store); kitukitu (scrub clothes)
    kiru (have sore near mouth); kirukiru (crisp)
    kopa (swallow); kopakopa (gulp.down)
    kasi (burn); kasikasi (angry)
    koi (high pitched sound); koikoi (groan with pain)
    kee (shatter); keekee (chipped)
    kauo (jump); kauokauo (jump up and down)
    kea (confused); keakea (lie)
    kovo (work); kovokovo (play)
    kove (fell); kovekove (drip repeatedly)
    kape (unable to meet); kapekape (embrace)
    kapo (fasten.cover.strip); kapokapo (fasten.cover.strips)
    koa (skin); koakoa (bark a tree)
    kipu (paint); kipukipu (rub.on)
    koe (spoon out a solid); koekoe (spoon out)
    kotu (bite); kotukotu (gnash teeth)
    kavo (collect); kavokavo (perform sorcery)
    kuri (scrape); kurikuri (scratch repeatedly)
    karu (open); karukaru (open)
    kare (return); karekare (return)
    kari (rip); karikari (tear)
    kiro (write); kirokiro (write)
    kae (blow); kaekae (tempt)
    koru (make return); korukoru (block)
    kosi (exit); kosikosi (exit)

**Complex Search Criteria:**
Phonological description typically identifies the segments, alternations,
syllable canon and so forth.  It is relatively
straightforward to count up the occurrences of all the different types
of CV syllables that occur in lexemes.

In the following example, we first import the regular expression and
probability modules.  Then we iterate over the lexemes to find all
sequences of a non-vowel ``[^aeiou]`` followed by a vowel ``[aeiou]``.

    >>> fd = nltk.FreqDist()
    >>> lexemes = [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]
    >>> for lex in lexemes:
    ...     for syl in tokenize.regexp(lex, pattern=r'[^aeiou][aeiou]'):
    ...         fd.inc(syl)

Now, rather than just printing the syllables and their frequency
counts, we can tabulate them to generate a useful display.

    >>> for vowel in 'aeiou':
    ...     for cons in 'ptkvsr':
    ...          print '%s%s:%4d ' % (cons, vowel, fd[cons+vowel]),
    ...     print
    pa:  83  ta:  47  ka: 428  va:  93  sa:   0  ra: 187
    pe:  31  te:   8  ke: 151  ve:  27  se:   0  re:  63
    pi: 105  ti:   0  ki:  94  vi: 105  si: 100  ri:  84
    po:  34  to: 148  ko: 430  vo:  48  so:   2  ro:  89
    pu:  51  tu:  37  ku: 175  vu:  49  su:   1  ru:  79
    
Consider the ``t`` and ``s`` columns, and observe that ``ti`` is not
attested, while ``si`` is frequent.  This suggests that a phonological
process of palatalisation is operating in the language.  We would then
want to consider the other syllables involving ``s``
(e.g. the single entry having *su*, namely *kasuari* 'cassowary' is a loanword).

**Prosodically-motivated search:**
A phonological description may include an examination of the segmental
and prosodic constraints on well-formed morphemes and lexemes.  For
example, we may want to find trisyllabic verbs ending in a long vowel.
Our program can make use of the fact that syllable onsets are
obligatory and simple (only consist of a single consonant).
First, we will encapsulate the syllabic counting part in a separate
function.  It gets the CV template of the word ``cv(word)`` and counts
the number of consonants it contains:

    >>> def num_cons(word):
    ...     template = cv(word)
    ...     return template.count('C')

We also encapsulate the vowel test in a function, as this improves the
readability of the final program.  This function returns the value
``True`` just in case ``char`` is a vowel.

    >>> def is_vowel(char):
    ...     return (char in 'aeiou')

Over time we may create a useful collection of such functions.  We can
save them in a file ``utilities.py``, and then at the start of each
program we can simply import all the functions in one go using ``from
utilities import *``.  We take the entry to be a verb if the first
letter of its part of speech is a ``V``.  Here, then, is the program
to display trisyllabic verbs ending in a long vowel:

    >>> for entry in lexicon:
    ...     lx = entry.findtext('lx')
    ...     if lx:
    ...         ps = entry.findtext('ps')
    ...         if num_cons(lx) == 3 and ps[0] == 'V'\
    ...           and is_vowel(lx[-1]) and is_vowel(lx[-2]):
    ...             ge = entry.findtext('ge')
    ...             print "%s (%s) '%s'" % (lx, ps, ge)
    kaetupie (V) 'tighten'
    kakupie (V) 'shout'
    kapatau (V) 'add to'
    kapuapie (V) 'wound'
    kapupie (V) 'close tight'
    kapuupie (V) 'close'
    karepie (V) 'return'
    karivai (V) 'have an appetite'
    kasipie (V) 'care for'
    kasirao (V) 'hot'
    kaukaupie (V) 'shine intensely'
    kavorou (V) 'covet'
    kavupie (V) 'leave.behind'
    kekepie (V) 'show'
    keruria (V) 'persistent'
    ketoopie (V) 'make sprout from seed'
    kipapie (V) 'wan samting tru'
    koatapie (V) 'put in'
    koetapie (V) 'investigate'
    koikoipie (V) 'make groan with pain'
    kokepie (V) 'make.rain'
    kokoruu (V) 'insect-infested'
    kokovae (V) 'sing'
    kokovua (V) 'shave the hair line'
    kopiipie (V) 'kill'
    korupie (V) 'take outside'
    kosipie (V) 'make exit'
    kovopie (V) 'make work'
    kukuvai (V) 'shelter head'
    kuvaupie (V) 'desert'

**Finding Minimal Sets:**
In order to establish a contrast segments (or lexical properties, for
that matter), we would like to find pairs of words which are identical
except for a single property.  For example, the words pairs *mace* vs
*maze* and *face* vs *faze* |mdash| and many others like them |mdash| demonstrate the
existence of a phonemic distinction between *s* and *z* in English.
NLTK provides flexible support for constructing minimal sets, using
the ``MinimalSet()`` class.  This class needs three pieces of information
for each item to be added:
``context``: the material that must be fixed across all members of a minimal set;
``target``: the material that changes across members of a minimal set;
``display``: the material that should be displayed for each item.


=========================  ==================  ===============  ===========
Examples of Minimal Set Parameters
---------------------------------------------------------------------------
Minimal Set                Context             Target           Display
=========================  ==================  ===============  ===========
*bib*, *bid*, *big*        first two letters   third letter     word
*deal (N)*, *deal (V)*     whole word          pos              word (pos)
=========================  ==================  ===============  ===========

We begin by creating a list of parameter values, generated from the
full lexical entries.  In our first example, we will print minimal sets
involving lexemes of length 4, with a target position of 1 (second
segment).  The ``context`` is taken to be the entire word, except for
the target segment.  Thus, if lex is ``kasi``, then context is
``lex[:1]+'_'+lex[2:]``, or ``k_si``.  Note that no parameters are
generated if the lexeme does not consist of exactly four segments.

    >>> pos = 1
    >>> ms = nltk.MinimalSet((lex[:pos] + '_' + lex[pos+1:], lex[pos], lex)
    ...                       for lex in lexemes if len(lex) == 4)

Now we print the table of minimal sets.  We specify that each
context was seen at least 3 times.

    >>> for context in ms.contexts(3):
    ...     print context + ':',
    ...     for target in ms.targets():
    ...         print "%-4s" % ms.display(context, target, "-"),
    ...     print
    k_si: kasi -    kesi kusi kosi
    k_va: kava -    -    kuva kova
    k_ru: karu kiru keru kuru koru
    k_pu: kapu kipu -    -    kopu
    k_ro: karo kiro -    -    koro
    k_ri: kari kiri keri kuri kori
    k_pa: kapa -    kepa -    kopa
    k_ra: kara kira kera -    kora
    k_ku: kaku -    -    kuku koku
    k_ki: kaki kiki -    -    koki

Observe in the above example that the context, target, and displayed
material were all based on the lexeme field.  However, the idea of
minimal sets is much more general.  For instance, suppose we wanted to
get a list of wordforms having more than one possible part-of-speech.
Then the target will be part-of-speech field, and the context will be
the lexeme field.  We will also display the English gloss field.

    >>> entries = [(e.findtext('lx'), e.findtext('ps'), e.findtext('ge'))
    ...               for e in lexicon
    ...               if e.findtext('lx') and e.findtext('ps') and e.findtext('ge')]
    >>> ms = nltk.MinimalSet((lx, ps[0], "%s (%s)" % (ps[0], ge))
    ...               for (lx, ps, ge) in entries)
    >>> for context in ms.contexts()[:10]:
    ...     print "%10s:" % context, "; ".join(ms.display_all(context))
      kokovara: N (unripe coconut); V (unripe)
         kapua: N (sore); V (have sores)
          koie: N (pig); V (get pig to eat)
          kovo: C (garden); N (garden); V (work)
        kavori: N (crayfish); V (collect crayfish or lobster)
        korita: N (cutlet?); V (dissect meat)
          keru: N (bone); V (harden like bone)
      kirokiro: N (bush used for sorcery); V (write)
        kaapie: N (hook); V (snag)
           kou: C (heap); V (lay egg)

.. Analysing Texts:
   Use the toolbox lexicon to tag a text with part-of-speech information,
   then do POS-sensitive concordance-style searches.

The following program uses ``MinimalSet`` to find pairs of entries in the
corpus which have different attachments based on the *verb* only.

.. doctest-ignore::
    >>> ms = nltk.MinimalSet()
    >>> for entry in ppattach.dictionary('training'):
    ...     target  = entry['attachment']
    ...     context = (entry['noun1'], entry['prep'], entry['noun2'])
    ...     display = (target, entry['verb'])
    ...     ms.add(context, target, display)
    >>> for context in ms.contexts():
    ...     print context, ms.display_all(context)

Here is one of the pairs found by the program.

.. ex::
   :: 

      received (NP offer) (PP from group)
      rejected (NP offer (PP from group))


This finding gives us clues to a structural difference: the verb
:lx:`receive` usually comes with two following arguments; we receive
something *from* someone.  In contrast, the verb :lx:`reject` only
needs a single following argument; we can reject something without
needing to say where it originated from.



---------------
Further Reading
---------------

[Robinson2007Toolbox]_

Bird, Steven (1999).  Multidimensional exploration of online linguistic field data
    *Proceedings of the 29th Meeting of the North-East Linguistic
    Society*, pp 33-50.

.. include:: footer.txt
