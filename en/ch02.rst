.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: add Mark Twain
.. TODO: discussion of resource rich/poor languages in section on corpora in other languages
         number of languages in the world, Ethnologue, etc
.. TODO: explain double vs single vs triple quotes for strings
.. TODO: negative indices of lists

.. _chap-corpora:

================================
2. Corpora: Large Bodies of Text
================================

[Introduction]

The goal of this chapter is to answer the following questions:

#. how can we write programs to access text from wider range
   of |NLTK| corpora?
#. how can we write programs to access text from local files and
   from the web, in order to get hold of an unlimited range of
   language material?
#. what new Python idioms are needed?

.. _sec-extracting-text-from-corpora:

-----------------------------------------------
Computing with Language: Accessing Text Corpora
-----------------------------------------------

A text corpus is a large body of text, containing a careful balance of material in
one or more genres.  We examined some small text collections in Chapter chap-introduction_,
such as the presidential inaugural addresses.  This particular corpus actually contains dozens of
individual texts |mdash| one per address |mdash| but we glued them end-to-end
and treated them like chapters of a book, i.e. as a single text.  In this
section we will examine a variety of text corpora and will see how to select
individual texts, and how to work with them.

The Gutenberg Corpus
--------------------

|NLTK| includes a small selection of texts from the `Project Gutenberg
<http://www.gutenberg.org/>`_ electronic text archive containing
some 25,000 free electronic books.  We begin
by getting the Python interpreter to load the |NLTK| package,
then ask to see ``nltk.corpus.gutenberg.files()``, the files in
|NLTK|\ 's corpus of Gutenberg texts:  

    >>> import nltk
    >>> nltk.corpus.gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's pick out the first of these texts |mdash| *Emma* by Jane Austen |mdash| and
give it a short name ``emma``, then find out how many words it contains: 

    >>> emma = nltk.corpus.gutenberg.words("austen-emma.txt")
    >>> len(emma)
    192432

.. note:: In |NLTK| 0.9.5 you cannot do concordancing (and other tasks from
   Section sect-computing-with-language-texts-and-words_) using a text
   defined this way.  Instead you have to do the following:

       >>> emma = nltk.Text(nltk.corpus.gutenberg.words("austen-emma.txt"))

The long name refers to the ``words()`` function of the ``gutenberg``
module in |NLTK|\ 's ``corpus`` package.
It gets cumbersome to type such long names all the time, so Python provides
another version of the import statement, as follows:

    >>> from nltk.corpus import gutenberg
    >>> gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's write a short program to display other information about each text:

    >>> for file in gutenberg.files():
    ...     num_chars = len(gutenberg.raw(file))
    ...     num_words = len(gutenberg.words(file))
    ...     num_sents = len(gutenberg.sents(file))
    ...     num_vocab = len(set(w.lower() for w in gutenberg.words(file)))
    ...     print num_chars/num_words, num_words/num_sents, num_words/num_vocab, file
    ... 
    4 21 24 austen-emma.txt
    4 23 16 austen-persuasion.txt
    4 24 20 austen-sense.txt
    4 33 73 bible-kjv.txt
    4 18 4 blake-poems.txt
    4 17 10 chesterton-ball.txt
    4 19 10 chesterton-brown.txt
    4 16 10 chesterton-thursday.txt
    4 24 13 melville-moby_dick.txt
    4 52 9 milton-paradise.txt
    4 12 7 shakespeare-caesar.txt
    4 13 6 shakespeare-hamlet.txt
    4 13 5 shakespeare-macbeth.txt
    4 35 10 whitman-leaves.txt

This program has displayed three statistics for each text:
average word length, average sentence length, and the number of times each vocabulary
item appears in the text on average (our lexical diversity score).
Observe that average word length appears to be a general property of English, since it is
always `4`:math:.  Average sentence length and lexical diversity
appear to be characteristics of particular authors.

This example also showed how we can access the "raw" text of the book,
not split up into words.  The ``raw()`` function gives us the contents of the file
without any linguistic processing.  So, for example, ``len(gutenberg.raw("blake-poems.txt")``
tells us how many *letters* occur in the text, including the spaces between words.
The ``sents()`` function divides the text up into its sentences, where each sentence is
a list of words:

    >>> macbeth_sentences = gutenberg.sents("shakespeare-macbeth.txt")
    >>> macbeth_sentences
    [['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',
    '1603', ']'], ['Actus', 'Primus', '.'], ...]
    >>> macbeth_sentences[1038]
    ['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',
    'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']

.. note:: Most |NLTK| corpus readers include a variety of access methods
   apart from ``words()``.  We access the raw file contents using ``raw()``,
   and get the content sentence by sentence using ``sents()``.  Richer
   linguistic content is available from some corpora, such as part-of-speech
   tags, dialogue tags, syntactic trees, and so forth; we will see these
   in later chapters.

Web Text
--------

Although Project Gutenberg contains thousands of books, it represents established
literature.  Its important to consider less formal language as well.  |NLTK|\ 's
small collection of web text includes content from a Firefox discussion forum,
conversations overheard in New York, the movie script of *Pirates of the Carribean*,
personal advertisements, and wine reviews:

    >>> for f in nltk.corpus.webtext.files():
    ...     print f, nltk.corpus.webtext.raw(f)[:70]
    ... 
    firefox.txt Cookie Manager: "Don't allow sites that set removed cookies to set fut
    overheard.txt White guy: So, do you have any plans for this evening? Asian girl: Yea
    pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Ros
    singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encounters.
    wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawberrie 

There is also a corpus of instant messaging chat sessions, originally collected
by the Naval Postgraduate School for research on automatic detection of internet predators.
The corpus contains over 10,000 posts, anonymized by replacing usernames with generic
names of the form "UserNNN", and manually edited to remove any other identifying information.
The corpus is organized into 15 files, where each file contains several hundred posts
collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a
generic adults chatroom).  The filename contains the date, chatroom,
and number of posts, e.g. ``10-19-20s_706posts.xml`` contains 706 posts gathered from
the 20s chat room on 10/19/2006.

    >>> chatroom = nltk.corpus.nps_chat.posts("10-19-20s_706posts.xml")
    >>> chatroom[123]
    ['i', 'do', "n't", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',
    'I', 'can', 'look', 'in', 'a', 'mirror', '.']

The Brown Corpus
----------------

The Brown Corpus was the first million-word electronic
corpus of English, created in 1961 at Brown University.
This corpus contains text from many sources, and the sources
have been categorized by genre, and given a label ``a`` through ``r``,
as set out in Table brown-categories_.

.. table:: brown-categories

   ===  =================  ===  ==================  ===  ================  ===  ================
   Sec  Genre              Sec  Genre               Sec  Genre             Sec  Genre
   ===  =================  ===  ==================  ===  ================  ===  ================
   a    Press: Reportage   b    Press: Editorial    c    Press: Reviews    d    Religion
   e    Skill and Hobbies  f    Popular Lore        g    Belles-Lettres    h    Government
   j    Learned            k    Fiction: General    k    Fiction: General  l    Fiction: Mystery
   m    Fiction: Science   n    Fiction: Adventure  p    Fiction: Romance  r    Humor
   ===  =================  ===  ==================  ===  ================  ===  ================

   Sections of the Brown Corpus

We can access the corpus as a list of words, or a list of sentences (where each sentence
is itself just a list of words).  We can optionally specify a section of the corpus to read:

    >>> nltk.corpus.brown.categories()
    ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'j', 'k', 'l', 'm', 'n', 'p', 'r']
    >>> nltk.corpus.brown.words(categories='a')
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> nltk.corpus.brown.sents(categories='a')
    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]

We can use the Brown Corpus to study systematic differences between
genres, a kind of linguistic inquiry known as `stylistics`:dt:.
For example, Table brown-modals_ was constructed by counting
the number of times various modal words appear in different sections of the corpus:

.. table:: brown-modals

   ==================  ===  =====  ===  =====  ====  ====
   Genre               can  could  may  might  must  will 
   ==================  ===  =====  ===  =====  ====  ====
   skill and hobbies   273  59     130  22     83    259 
   humor               17   33     8    8      9     13 
   fiction: science    16   49     4    12     8     16 
   press: reportage    94   86     66   36     50    387 
   fiction: romance    79   195    11   51     46    43 
   religion            84   59     79   12     54    64 
   ==================  ===  =====  ===  =====  ====  ====

   Use of Modals in Brown Corpus, by Genre

|nopar|
Observe that the most frequent modal in the reportage genre is
`will`:lx:, suggesting a focus on the future, while the most frequent
modal in the romance genre is `could`:lx:, suggesting a focus on possibilities.

Reuters Corpus
--------------

The Reuters Corpus contains 10,788 news documents totaling 1.3 million words.
The documents have been classified for relevance to 90 topics, and grouped
into two sets, called "training" and "test" (for training and testing algorithms
that automatically detect the topic of a document, as we will explore further
in Chapter chap-data-intensive_).

    >>> nltk.corpus.reuters.files()
    ('test/14826', 'test/14828', 'test/14829', 'test/14832', ...)
    >>> reuters.categories() 
    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',
    'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',
    'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]

Unlike the Brown Corpus, categories in the Reuters corpus overlap with each
other, simply because a news story often covers multiple topics.  We can
ask for the topics covered by one or more documents, or for the documents
included in one or more categories:

    >>> reuters.categories('training/9865')
    ['barley', 'corn', 'grain', 'wheat']
    >>> reuters.categories(['training/9865', 'training/9880'])
    ['barley', 'corn', 'grain', 'money-fx', 'wheat']
    >>> reuters.files('barley') 
    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]
    >>> reuters.files(['barley', 'corn']) 
    ['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',
    'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]

.. note:: Many other English text corpora are provided with |NLTK|.  For a
   list see Appendix app-modules_.  For more examples of access |NLTK| corpora,
   please consult the online guide at ``http://nltk.org/doc/guides/corpus.html``.

Inaugural Address Corpus
------------------------

This corpus has a time dimension...

    >>> import nltk
    >>> inaugural = nltk.corpus.inaugural
    >>> cfd = nltk.ConditionalFreqDist((target, file[:4])
    ...           for file in inaugural.files()
    ...           for w in inaugural.words(file)
    ...           for target in ["america", "citizen"]
    ...           if w.lower().startswith(target))
    >>> cfd.plot([file[:4] for file in inaugural.files()])



Corpora in Other Languages
--------------------------

NLTK comes with corpora for many languages, though in some cases
you will need to learn how to manipulate character encodings in Python
before using these corpora (see Appendix app-unicode_).

    >>> print nltk.corpus.nps_chat.words()
    ['now', 'im', 'left', 'with', 'this', 'gay', 'name', ...]
    >>> nltk.corpus.cess_esp.words()
    ['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...]
    >>> nltk.corpus.floresta.words()
    ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]
    >>> nltk.corpus.udhr.files()
    ('Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',
    'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',
    'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...)
    >>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
    ['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]
    >>> nltk.corpus.indian.words('hindi.pos')
    ['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3',
    '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4\x82\xe0\xa4\xa7', ...]

The last of these corpora, ``udhr`` contains the Universal Declaration of Human Rights
in over 300 languages.  (Note that the names of the files in this corpus include
information about character encoding, and for now we will stick with texts in ISO Latin-1, or ASCII)

Let's use a conditional frequency distribution to examine the differences in word lengths,
for a selection of languages included in this corpus.
The output is shown in Figure fig-word-len-dist_ (run the program yourself to see a color plot).

    >>> languages = ['Chickasaw', 'English', 'German_Deutsch',
    ...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
    >>> cfd = nltk.ConditionalFreqDist((lang, len(word)) for lang in languages
    ...          for word in nltk.corpus.udhr.words(lang + "-Latin1"))
    >>> cfd.plot(range(36))

.. _fig-word-len-dist:
.. figure:: ../images/word-len-dist.png
   :scale: 25

   Cumulative Word Length Distributions for Several Languages


You will probably have other text sources, stored in files on your computer or accessible
via the web.  We'll discuss how to work with these in Chapter chap-words_.

.. _sec-defining-functions:

-------------------------
More Python: Reusing Code
-------------------------

By this time you've probably retyped a lot of code.  If you mess up when retyping a complex example you have
to enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so
far.  In this section we see two important ways to reuse code: text editors and Python functions.

Creating Programs with a Text Editor
------------------------------------

The Python interative interpreter performs your instructions as soon as you type
them.  Often, it is better to compose a multi-line program using a text editor,
then ask Python to run the whole program at once.  Using |IDLE|, you can do
this by going to the ``File`` menu and opening a new window.  Try this now, and
enter the following one-line program:

::

     msg = 'Monty Python'

Save this program in a file called ``test.py``, then
go to the ``Run`` menu, and select the command ``Run Module``.
The result in the main |IDLE| window should look like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    >>>

Now, where is the output showing the value of ``msg``? The answer is
that the program in ``test.py`` will show a value only if you explicitly tell
it to, using the ``print`` command. So add another line to
``test.py`` so that it looks as follows:

::

     msg = 'Monty Python'
     print msg

Select ``Run Module`` again, and this time you should get output that
looks like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    Monty Python
    >>>

From now on, you have a choice of using the interactive interpreter or a
text editor to create your programs.  It is often convenient to test your ideas
using the interpreter, revising a line of code until it does what you expect,
and consulting the interactive help facility.  Once you're ready, you can paste
the code (minus any ``>>>`` prompts) into the text editor,
continue to expand it, and finally save the program
in a file so that you don't have to type it in again later.
Give the file a short but descriptive name, using all lowercase letters and separating
words with underscore, and using the ``.py`` filename extension, e.g. ``monty_python.py``.

.. note::
   Our inline code examples will continue to include the ``>>>`` and ``...`` prompts
   as if we are interacting directly with the interpreter.  As they get more complicated,
   you should instead type them into the editor, without the prompts, and run them
   from the editor as shown above.

Functions
---------

Suppose that you work on analyzing text involves different forms
of the same word, and that part of your program needs to work out
the plural form of a given singular noun.  Suppose it needs to do this
work in two places, once when it is processing some texts, and again
when it is processing user input.

Rather than repeating the same code several times over, it is more
efficient and reliable to localize this work inside a `function`:dt:.
A function is just a named block of code that performs some well-defined
task.  It usually has some inputs, also known as `parameters`:dt:,
and it may produce a result, also known as a `return value`:dt:.
We define a function using the keyword ``def`` followed by the
function name and any input parameters, followed by the body of the
function.  Here's the function we saw in section sect-computing-with-language-texts-and-words_:

    >>> def score(text):
    ...     return len(text) / len(set(text))

We use the keyword ``return`` to indicate the value that is
produced as output by the function.  In the above example,
all the work of the function is done in the ``return`` statement.
Here's an equivalent definition which does the same work
using multiple lines of code.  We'll change the parameter name
to remind you that this is an arbitrary choice:

    >>> def score(my_text_data):
    ...     word_count = len(my_text_data)
    ...     vocab_size = len(set(my_text_data))
    ...     richness_score = word_count / vocab_size
    ...     return richness_score

Notice that we've created some new variables inside the body of the function.
These are *local variables* and are not accessible outside the function.
Notice also that defining a function like this produces no output.
Functions do nothing until they are "called" (or "invoked").     

Let's return to our earlier scenario, and actually define a simple plural
function.  The function ``plural()`` in Figure plural_
takes a singular noun and generates a plural form (one which is not always
correct).

.. pylisting:: plural
   :caption: Example of a Python function

   def plural(word):
       if word.endswith('y'):
           return word[:-1] + 'ies'
       elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
           return word + 'es'
       elif word.endswith('an'):
           return word[:-2] + 'en'
       return word + 's'

   >>> plural('fairy')
   'fairies'
   >>> plural('woman')
   'women'

(There is much more to be said about functions, but
we will hold off until Section sec-functions_.)

Modules
-------

Over time you will find that you create a variety of useful little text processing functions,
and you end up copy-pasting them from old programs to new ones.  While file contains the
latest version of the function you want to use?
It makes life a lot easier if you can collect your work into a single place, and
access previously defined functions without any copying and pasting.

To do this, save your function(s) in a file called (say) ``textproc.py``.
Now, you can access your work simply by importing it from the file:

.. doctest-ignore::
    >>> from textproc import plural
    >>> plural('wish')
    wishes
    >>> plural('fan')
    fen

Our plural function has an error, and we'll need to fix it.  This time, we won't
produce another version, but instead we'll fix the existing one.  Thus, at every
stage, there is only one version of our plural function, and no confusion about
which one we should use.

A collection of variable and function definitions in a file is called a Python
`module`:dt:.  A collection of related modules is called a `package`:dt:.
|NLTK|\ 's code for processing the Brown Corpus is an example of a module,
and its collection of code for processing all the different corpora is
an example of a package.  |NLTK| itself is a set of packages, sometimes
called a `library`:dt:.

-----------------
Lexical Resources
-----------------

A lexicon is a collection of words and/or phrases along with associated information such
as part of speech and sense definitions.  The simplest kind of lexicon is nothing more
than a sorted list of words.  Sophisticated lexicons include complex structure within and
across the individual entries.  In this section we'll look at some lexical resources
included with |NLTK|.   

Wordlist Corpora
----------------

|NLTK| includes some corpora that are nothing more than wordlists.
The Words corpus is the ``/usr/dict/words`` file from Unix, used by
some spell checkers.  We can use it to find unusual or mis-spelt
words in a text corpus:

    >>> def unusual_words(text):
    ...     text_vocab = set(w.lower() for w in text if w.isalpha())
    ...     english_vocab = set(w.lower() for w in nltk.corpus.words.words())
    ...     unusual = text_vocab.difference(english_vocab)
    ...     return sorted(unusual)
    ...
    >>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
    ['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary', 'adieus',
    'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham', 'amiably', 'annamaria',
    'annuities', 'apologising', 'arbour', 'archness', 'ardour', 'artlessness', 'assiduities', ...]
    >>> unusual_words(nltk.corpus.nps_chat.words())
    ['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros', 'actualy', 'adduser',
    'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk', 'agaibn', 'agurlwithbigguns',
    'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', 'ahhah', 'ahhahahaha', 'ahhh', 'ahhhh', ...]

There is a corpus of `stopwords`:dt:, high-frequency glue words that we sometimes
want to filter out of a text before further processing.

    >>> nltk.corpus.stopwords.words('english')
    ['a', "a's", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually',
    'after', 'afterwards', 'again', 'against', "ain't", 'all', 'allow', 'allows', 'almost',
    'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', ...]

Let's define a function to compute what fraction of words in a text are *not* in the
stopwords list:

    >>> def content_fraction(text):
    ...     stopwords = nltk.corpus.stopwords.words('english')
    ...     content = [w for w in text if w.lower() not in stopwords]
    ...     return 1.0 * len(content) / len(text)
    ...    
    >>> content_fraction(nltk.corpus.reuters.words())
    0.65997695393285261

Thus, with the help of stopwords we filter out a third of the words of the text.

One more wordlist corpus is the Names corpus, containing 8,000 names categorized by gender.
The male and female names are stored in separate files.  Let's find names which appear
in both files, i.e. names that are ambiguous for gender:

    >>> names = nltk.corpus.names
    >>> names.files()
    ('female.txt', 'male.txt')
    >>> [w for w in names.words('male.txt') if w in names.words('female.txt')]
    ['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',
    'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',
    'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ..]

It is well known that names ending in the letter `a`:lx: are almost always female.
We can see this, and some other patterns, in the graph in Figure fig-cfd-gender_,
which is produced by the following code:

    >>> cfd = nltk.ConditionalFreqDist((file, name[-1])
    ...           for file in names.files()
    ...           for name in names.words(file))
    >>> cfd.plot()

.. _fig-cfd-gender:
.. figure:: ../images/cfd-gender.png
   :scale: 25

   Frequency of Final Letter of Female vs Male Names


Tabular Lexicons
----------------

As we have seen, the entries in a wordlist lack internal structure |mdash| they are just words.
A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word
plus some properties in each row.  |NLTK| includes the CMU Pronouncing Dictionary,
used by speech synthesizers. 

    >>> entries = nltk.corpus.cmudict.entries()
    >>> len(entries)
    127069
    >>> for entry in entries[40000:40010]:
    ...     print entry
    ... 
    ('fir', 1, ('F', 'ER1'))
    ('fire', 1, ('F', 'AY1', 'ER0'))
    ("fire's", 1, ('F', 'AY1', 'ER0', 'Z'))
    ('fire', 2, ('F', 'AY1', 'R'))
    ('firearm', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M'))
    ('firearm', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M'))
    ('firearms', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'))
    ('firearms', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'))
    ('fireball', 1, ('F', 'AY1', 'ER0', 'B', 'AO2', 'L'))
    ('fireball', 2, ('F', 'AY1', 'R', 'B', 'AO2', 'L'))

For each word, it provides a pronunciation number and a list of phonetic
codes |mdash| distinct labels for each contrastive sound |mdash|
known as `phones`:lx:.  Observe that `fire`:lx: has two pronunciations:
the one-syllable ``F AY1 R``, and the two-syllable ``F AY1 ER0``.
The symbols in the CMU Pronouncing Dictionary are from the *Arpabet*,
described in more detail at ``http://en.wikipedia.org/wiki/Arpabet``

Each entry consists of three pieces of information, and we can
process these individually by complicating the ``for`` statement
slightly.  Instead of writing ``for entry in entries:``, we replace
``entry`` with *three* variable names.  Each of the pieces of a single
entry are assigned to each of the three variables.  (Underscore
is a legal variable name, and we'll use this name when we don't plan to do
anything with the variable later.)

    >>> for w, _, pron in entries:
    ...     if len(pron) == 3:
    ...         ph1, ph2, ph3 = pron
    ...         if ph1 == 'P' and ph3 == 'T':
    ...             print w, ph2,
    ...
    pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1
    pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1
    pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1

The above program scans the lexicon for entries with a pronunciation consisting of
three phones (``len(pron) == 3``).  If the condition is true, we assign the contents
of ``pron`` to three new variables ``ph1``, ``ph2`` and ``ph3``.  Notice the unusual
form of the statement which does that work.

Here's another example of the same ``for`` statement, this time used inside a list
comprehension.  This program finds all words whose pronunciation ends with a syllable
sounding like `nicks`:lx:.  You could use this method to find rhyming words.

    >>> syllable = ('N', 'IH0', 'K', 'S')
    >>> [w for w, _, pron in entries if pron[-4:] == syllable]
    ["atlantic's", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',
    'chetniks', "clinic's", 'clinics', 'conics', 'cynics', 'diasonics', "dominic's",
    'ebonics', 'electronics', "electronics'", 'endotronics', "endotronics'", 'enix', ...]

Notice that the one pronunciation is spelt in several ways: `nics`:lx:, `niks`:lx:, `nix`:lx:,
even `ntic's`:lx: with a silent `t`:lx, for the word `atlantic's`:lx:.  Let's look for some other
mismatches between pronunciation and writing.  Can you summarize the purpose of
the following examples and explain how they work?

    >>> [w for w, _, pron in entries if pron[-1] == 'M' and w[-1] == 'n']
    ['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']
    >>> sorted(set(w[:2] for w, _, pron in entries if pron[0] == 'N' and w[0] != 'n'))
    ['gn', 'kn', 'mn', 'pn']

The phones contain digits, to represent 
primary stress (``1``), secondary stress (``2``) and no stress (``0``).
As our final example, we define a function to extract the stress digits
and then scan our lexicon to find words having a particular stress pattern.

    >>> def stress(pron):
    ...     return [int(char) for phoneme in pron for char in phoneme if char.isdigit()] 
    >>> [w for w, _, pron in entries if stress(pron) == [0, 1, 0, 2, 0]]
    ['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',
    'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',
    'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]
    >>> [w for w, _, pron in entries if stress(pron) == [0, 2, 0, 1, 0]]
    ['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',
    'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',
    'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]

Note that this example has a user-defined function inside the condition of
a list comprehension.

[Summary of tabular lexicons; forward reference to discussion about processing CSV files]

Shoebox and Toolbox Lexicons
----------------------------

[basic introduction from ch 13]


WordNet
-------

`WordNet`:idx: is a semantically-oriented dictionary of English,
similar to a traditional thesaurus but with a richer structure.
WordNet groups words into synonym sets, or `synsets`:dt:, each with
its own definition and with links to other synsets.
WordNet 3.0 data is distributed with NLTK, and includes 117,659 synsets.

Although WordNet was originally developed for research
in psycholinguistics, it is widely used in NLP and Information Retrieval.
WordNets are being developed for many other languages, as documented
at ``http://www.globalwordnet.org/``.

Senses and Synonyms
-------------------

Consider the following sentence:

.. _carex1:
.. ex::
   Benz is credited with the invention of the motorcar.

If we replace `motorcar`:lx: in carex1_ by `automobile`:lx:, the
meaning of the sentence stays pretty much the same:

.. _carex2:
.. ex::
   Benz is credited with the invention of the automobile.

Since everything else in the sentence has remained unchanged, we can
conclude that the words `motorcar`:lx: and `automobile`:lx: have the
same meaning, i.e. they are `synonyms`:dt:. 

In order to look up the senses of a word, we need to pick
a part of speech for the word.  WordNet contains four dictionaries: ``N``
(nouns), ``V`` (verbs), ``ADJ`` (adjectives), and ``ADV``
(adverbs). To simplify our discussion, we will focus on the ``N``
dictionary here.  Let's look up `motorcar`:lx: in the ``N`` dictionary.

    >>> from nltk import wordnet
    >>> car = wordnet.N['motorcar']
    >>> car
    motorcar (noun)

The variable ``car`` is now bound to a ``Word`` object.
Words will often have more than sense, where each
sense is represented by a synset. However,
`motorcar`:lx: only has one sense in WordNet, as we can discover
using ``len()``.  We can then find the synset (a set of
lemmas), the words it contains, and a gloss.

    >>> len(car)
    1
    >>> car[0]
    {noun: car, auto, automobile, machine, motorcar}
    >>> list(car[0])
    ['car', 'auto', 'automobile', 'machine', 'motorcar']
    >>> car[0].gloss
    'a motor vehicle with four wheels; usually propelled by an
    internal combustion engine; 
    "he needs a car to get to work"'

The ``wordnet`` module also defines ``Synset``\ s.
Let's look at a word which is `polysemous`:dt:\ ;
that is, which has multiple synsets:

    >>> poly = wordnet.N['pupil']
    >>> for synset in poly:
    ...     print synset
    {noun: student, pupil, educatee}
    {noun: pupil}
    {noun: schoolchild, school-age_child, pupil}
    >>> poly[1].gloss
    'the contractile aperture in the center of the iris of the eye;
    resembles a large black dot'

The WordNet Hierarchy
---------------------

WordNet synsets correspond to abstract concepts, which may or may not
have corresponding words in English.  These concepts are linked together in a hierarchy.
Some are very general, such as *Entity*, *State*, *Event* |mdash| these are called
`unique beginners`:dt:.  Others, such as *gas guzzler* and
*hatchback*, are much more specific. A small portion of a concept
hierarchy is illustrated in Figure wn-hierarchy_. The edges between nodes
indicate the hypernym/hyponym relation; the dotted line at the top is
intended to indicate that *artifact* is a non-immediate hypernym of *motorcar*. 

.. _wn-hierarchy:
.. figure:: ../images/wordnet-hierarchy.png
   :scale: 15

   Fragment of WordNet Concept Hierarchy

WordNet makes it easy to navigate between concepts.
For example, given a concept like *motorcar*,
we can look at the concepts that are more specific;
the (immediate) `hyponyms`:dt:. Here is one way to carry out this
navigation:

    >>> for concept in car[0][wordnet.HYPONYM][:10]:
    ...     print concept
    {noun: ambulance}
    {noun: beach_wagon, station_wagon, wagon, estate_car, beach_waggon, station_waggon, waggon}
    {noun: bus, jalopy, heap}
    {noun: cab, hack, taxi, taxicab}
    {noun: compact, compact_car}
    {noun: convertible}
    {noun: coupe}
    {noun: cruiser, police_cruiser, patrol_car, police_car, prowl_car, squad_car}
    {noun: electric, electric_automobile, electric_car}
    {noun: gas_guzzler}

|nopar|
We can also move up the hierarchy, by looking at broader concepts than
*motorcar*, e.g. the immediate `hypernym`:dt: of a concept:

    >>> car[0][wordnet.HYPERNYM]
    [{noun: motor_vehicle, automotive_vehicle}]

We can also look for the hypernyms of hypernyms.  In fact, from any
synset we can trace (multiple) paths back to a unique beginner.
Synsets have a method for doing this, called ``tree()``,
which produces a nested list structure.

    >>> pprint.pprint(wordnet.N['car'][0].tree(wordnet.HYPERNYM))
    [{noun: car, auto, automobile, machine, motorcar},
     [{noun: motor_vehicle, automotive_vehicle},
      [{noun: self-propelled_vehicle},
       [{noun: wheeled_vehicle},
        [{noun: vehicle},
         [{noun: conveyance, transport},
          [{noun: instrumentality, instrumentation},
           [{noun: artifact, artefact},
            [{noun: whole, unit},
             [{noun: object, physical_object},
              [{noun: physical_entity}, [{noun: entity}]]]]]]]],
        [{noun: container},
         [{noun: instrumentality, instrumentation},
          [{noun: artifact, artefact},
           [{noun: whole, unit},
            [{noun: object, physical_object},
             [{noun: physical_entity}, [{noun: entity}]]]]]]]]]]]

A related method ``closure()`` produces a flat version of this structure,
with repeats eliminated.
Both of these functions take an optional ``depth`` argument that permits
us to limit the number of steps to take.
(This is important when using unbounded relations like ``SIMILAR``.)
Table wordnet-rel_ lists the most important lexical relations supported
by WordNet; see ``dir(wordnet)`` for a full list.

.. table:: wordnet-rel

   ===========  ================  ==============================================
   Hypernym     more general      `animal`:lx: is a hypernym of `dog`:lx:
   Hyponym      more specific     `dog`:lx: is a hyponym of `animal`:lx:
   Meronym      part of           `door`:lx: is a meronym of `house`:lx:
   Holonym      has part          `house`:lx: is a holonym of `door`:lx:
   Synonym      similar meaning   `car`:lx: is a synonym of `automobile`:lx:
   Antonym      opposite meaning  `like` is an antonym of `dislike`:lx:
   Entailment   necessary action  `step` is an entailment of `walk`:lx:
   ===========  ================  ==============================================

   Major WordNet Lexical Relations

Recall that we can iterate over the words of a synset, with ``for word in synset``.
We can also test if a word is in a dictionary, e.g. ``if word in wordnet.V``.
As our last task, let's put these together to find "animal words" that are used as verbs.
Since there are a lot of these, we will cut this off at depth 4.
Can you think of the animal and verb sense of each word?

    >>> animals = wordnet.N['animal'][0].closure(wordnet.HYPONYM, depth=4)
    >>> [word for synset in animals for word in synset if word in wordnet.V]
    ['pet', 'stunt', 'prey', 'quarry', 'game', 'mate', 'head', 'dog',
     'stray', 'dam', 'sire', 'steer', 'orphan', 'spat', 'sponge',
     'worm', 'grub', 'pooch', 'toy', 'queen', 'baby', 'pup', 'whelp',
     'cub', 'kit', 'kitten', 'foal', 'lamb', 'fawn', 'bird', 'grouse',
     'hound', 'bulldog', 'stud', 'hog', 'baby', 'fish', 'cock', 'parrot',
     'frog', 'beetle', 'bug', 'bug', 'queen', 'leech', 'snail', 'slug',
     'clam', 'cockle', 'oyster', 'scallop', 'scollop', 'escallop', 'quail']

NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet.
It can be accessed with ``nltk.corpus.verbnet``.

.. _getting-organized:

------------------------------
More Python: Getting Organized
------------------------------

A text, as we have seen, is treated in Python as a list of words.
An important property of lists is that we can "look up" a particular
item by giving its index, e.g. ``text1[100]``.  Notice how we specify
a number, and get back a word.  We can think of a list as a simple
kind of table, as shown in Figure maps01_.

.. _maps01:
.. figure:: ../images/maps01.png
   :scale: 25

   List Look-up

Contrast this situation with frequency distributions (section computing-with-language-simple-statistics_),
where we specify a word in order to get back a number, e.g. ``fdist['monstrous']``, which
tells us the number of times a given word has occurred in a text.  Look-up using words is
familiar to anyone who has used a dictionary.  Some more examples are shown in
Figure maps02_.

.. _maps02:
.. figure:: ../images/maps02.png
   :scale: 22

   Dictionary Look-up

In the case of a phone book, we look up an entry using a `name`:em:,
and get back a number.  When we type a domain name in a web browser,
the computer looks this up to get back an IP address.  A word
frequency table allows us to look up a word and find its frequency in
a text collection.  In all these cases, we are mapping from names to
numbers, rather than the other way round.
In general, we would like to be able to map between
arbitrary types of information.  Table linguistic-objects_ lists a variety
of linguistic objects, along with what they map.

.. table:: linguistic-objects

    +--------------------+-------------------------------------------------+
    | Linguistic Object  |                      Maps                       |
    |                    +------------+------------------------------------+
    |                    |    from    | to                                 |
    +====================+============+====================================+
    |Document Index      |Word        |List of pages (where word is found) |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Thesaurus           |Word sense  |List of synonyms                    |
    +--------------------+------------+------------------------------------+
    |Dictionary          |Headword    |Entry (part of speech, sense        |
    |                    |            |definitions, etymology)             |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Comparative Wordlist|Gloss term  |Cognates (list of words, one per    |
    |                    |            |language)                           |
    +--------------------+------------+------------------------------------+
    |Morph Analyzer      |Surface form|Morphological analysis (list of     |
    |                    |            |component morphemes)                |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+

    Linguistic Objects as Mappings from Keys to Values

Most often, we are mapping from a "word" to some structured object.
For example, a document index maps from a word (which we can represent
as a string), to a list of pages (represented as a list of integers).
In this section, we will see how to represent such mappings in Python.

Accessing Data with Data
------------------------

Python provides a `dictionary`:dt: data type that can be used for
mapping between arbitrary types.

.. Note:: A Python dictionary is somewhat like a linguistic dictionary
   |mdash| they both give you a systematic means of looking things up,
   and so there is some potential for confusion. However, we hope that
   it will usually be clear from the context which kind of dictionary
   we are talking about.

Here we define ``pos`` to be an empty dictionary and then add three
entries to it, specifying the part-of-speech of some words.  We add
entries to a dictionary using the familiar square bracket notation:

    >>> pos = {}
    >>> pos['colorless'] = 'adj'
    >>> pos['furiously'] = 'adv'
    >>> pos['ideas'] = 'n'

So, for example, ``pos['colorless'] = 'adj'`` says that the look-up
value of ``'colorless'`` in ``pos`` is the string ``'adj'``.

.. Monkey-patching to get our dict examples to print consistently:

    >>> from nltk import SortedDict
    >>> pos = SortedDict(pos)

To look up a value in ``pos``, we again use indexing notation, except
now the thing inside the square brackets is the item whose value we
want to recover:

    >>> pos['ideas']
    'n'
    >>> pos['colorless']
    'adj'

The item used for look-up is called the `key`:dt:, and the
data that is returned is known as the `value`:dt:.  As with indexing
a list or string, we get an exception when we try to access the value
of a key that does not exist:

    >>> pos['missing']
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    KeyError: 'missing'

This raises an important question.  Unlike lists and strings, where we
can use ``len()`` to work out which integers will be legal indices, how
do we work out the legal keys for a dictionary?  Fortunately, we can
check whether a key exists in a dictionary using the ``in`` operator:

    >>> 'colorless' in pos
    True
    >>> 'missing' in pos
    False
    >>> 'missing' not in pos
    True

Notice that we can use ``not in`` to check if a key is `missing`:em:.  Be
careful with the ``in`` operator for dictionaries: it only applies to
the keys and not their values.  If we check for a value, e.g. ``'adj'
in pos``, the result is ``False``, since ``'adj'`` is not a key.
We can loop over all the entries in a dictionary using a ``for`` loop.

    >>> for word in pos:
    ...     print word, pos[word]
    ... 
    colorless adj
    furiously adv
    ideas n

We can see what the contents of the dictionary look like by inspecting
the variable ``pos``.  Note the presence of the colon character to separate
each key from its corresponding value:

    >>> pos
    {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}

Here, the contents of the dictionary are shown as `key-value
pairs`:dt:.  As you can see,  the order of the key-value pairs is different
from the order in which they were originally entered.  This is because
dictionaries are not sequences but mappings. The keys in a mapping
are not inherently ordered, and any ordering that we might want to impose on the keys
exists independently of the mapping.  As we shall see later, this
gives us a lot of flexibility.  

We can use the same key-value pair format to create a dictionary:

    >>> pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}

.. Monkey-patching to get our dict examples to print consistently:

    >>> pos = SortedDict(pos)

Using the dictionary methods ``keys()``, ``values()`` and ``items()``,
we can access the keys and values as separate lists,
and also the key-value pairs:

    >>> pos.keys()
    ['colorless', 'furiously', 'ideas']
    >>> pos.values()
    ['adj', 'adv', 'n']
    >>> pos.items()
    [('colorless', 'adj'), ('furiously', 'adv'), ('ideas', 'n')]
    >>> for (key, val) in pos.items():
    ...     print key, "==>", val
    ...
    colorless ==> adj
    furiously ==> adv
    ideas ==> n

Note that keys are forced to be unique.
Suppose we try to use a dictionary to store the fact that the
word `content`:lx: is both a noun and a verb:

    >>> pos['content'] = 'n'
    >>> pos['content'] = 'v'
    >>> pos
    {'content': 'v', 'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}

Initially, ``pos['content']`` is given the value ``'n'``, and this is
immediately overwritten with the new value ``'v'``.
In other words, there is only one entry for ``'content'``.
If we wanted to store multiple values in that entry, we could use a list,
e.g. ``pos['content'] = ['n', 'v']``.

Sorting Dictionaries by Value
-----------------------------

    >>> from operator import itemgetter
    >>> pos.items()
    >>> sorted(pos.items, key=itemgetter(1), reverse=True)


Dictionaries vs Frequency Distributions
---------------------------------------

|NLTK|\ 's frequency distributions (``nltk.FreqDist``) is just
a special kind of dictionary which has additional support for sorting
and plotting that are needed in language processing.

We can use dictionaries to count word occurrences, emulating the
method used for tallying words (Figure tally_).  
We begin by initializing
an empty dictionary, then process each word of the text.  If the word hasn't
been seen before, we add it to our list with a zero count.  If we've
seen it before we increment its count using the ``+=`` operator.
If we ask for the keys we get them in an arbitrary order.

    >>> counts = {}
    >>> for word in nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'):
    ...     if word not in counts:
    ...         counts[word] = 0
    ...     counts[word] += 1
    ...
    >>> counts['Scotland']
    12
    >>> counts['the']
    692
    >>> counts.keys()
    ['Lead', 'doubts', 'felt', 'hath', 'protest', 'sleep', 'thirst', 'Barke', 'hate',
    'goodnesse', 'forget', 'whose', 'Hose', 'solliciting', 'euery', 'Keepes', ...]

However, it is preferable to use |NLTK|\ 's support for frequency distributions,
since it is more compact and has more functionality.  If we ask to see the keys
we get them in order of decreasing frequency.

    >>> counts = nltk.FreqDist(nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'))
    >>> counts['Scotland']
    12
    >>> counts.keys()
    [',', '.', "'", 'the', ':', 'and', 'I', 'of', 'to', '?', 'd', 'a', 'you', 'in', 'my',
    'And', 'is', 'that', 'not', 'it', 'Macb', 'with', 's', 'his', 'be', 'The', 'haue', ...]
    
Generating Random Text with Style
---------------------------------

We have used frequency distributions to count the number of occurrences of
each word in a text.  Here we will generalize this idea to look at the
distribution of words in a given context.
A `conditional frequency distribution`:dt: is a collection
of frequency distributions, each one for a different condition.
Here the condition will be the preceding word.

In Figure random_, we've defined a function ``train_model()`` that
uses ``ConditionalFreqDist()`` to count words as they appear
relative to the context defined by the preceding word (stored in ``prev``).
It scans the corpus, incrementing the appropriate counter, and
updating the value of ``prev``.  The function ``generate_model()``
contains a simple loop to generate text: we set an initial
context, pick the most likely token in that context as our next
word (using ``max()``), and then use that word as our new context.
This simple approach to text generation tends to get stuck in loops;
another method would be to randomly choose the next word from among
the available words.

.. pylisting:: random
   :caption: Generating Random Text in the Style of Genesis

   def train_model(text):
       cfdist = nltk.ConditionalFreqDist()
       prev = None
       for word in text:
           cfdist[prev].inc(word)
           prev = word
       return cfdist

   def generate_model(cfdist, word, num=15):
       for i in range(num):
           print word,
           word = cfdist[word].max()

   >>> model = train_model(nltk.corpus.genesis.words('english-kjv.txt'))
   >>> model['living']
   <FreqDist with 16 samples>
   >>> list(model['living'])
   ['substance', ',', '.', 'thing', 'soul', 'creature']
   >>> generate_model(model, 'living')
   living creature that he said , and the land of the land of the land

.. _sec-accessing-text:

-------------------------
Accessing Text on the Web
-------------------------

Electronic Books
----------------

A small sample of texts from Project Gutenberg appears in the |NLTK| corpus collection.
However, you may be interested in analyzing other texts from Project Gutenberg.
You can browse the catalog of 25,000 free online books at
``http://www.gutenberg.org/catalog/``, and obtain a URL to an ASCII text file.
Although 90% of the texts in Project Gutenberg are in English, it
includes material in over 50 other languages, including Catalan, Chinese, Dutch,
Finnish, French, German, Italian, Portuguese and Spanish (with more than
100 texts each).

Text number 2554 is an English translation of *Crime and Punishment*,
and we can access it as follows:

    >>> from urllib import urlopen
    >>> url = "http://www.gutenberg.org/files/2554/2554.txt"
    >>> raw = urlopen(url).read()
    >>> type(raw)
    <type 'str'>
    >>> len(raw)
    1176831
    >>> raw[:75]
    'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n'

.. note:: The read() process will take a few seconds as it downloads this large book.
   If you're using an internet proxy which is not correctly detected by Python,
   you may need to specify the proxy manually as follows:
   
       >>> proxies = {'http': 'http://www.someproxy.com:3128'}
       >>> raw = urllib.urlopen(url, proxies=proxies).read()
   
The variable ``raw`` contains a string with 1,176,831 characters.  This is the raw
content of the book, including many details we are not interested in such as
whitespace, line breaks and blank lines.  Instead, we want to break it up into
words and punctuation, as we saw in Chapter chap-introduction_.  This step is
called `tokenization`:dt:, and it produces our familiar structure, a list of words
and punctuation.  From now on we will call these `tokens`:dt:.

    >>> tokens = nltk.wordpunct_tokenize(raw)
    >>> type(tokens)
    <type 'list'>
    >>> text[:10]
    ['The', 'Project', 'Gutenberg', 'EBook', 'of', 'Crime', 'and', 'Punishment', ',', 'by']
    >>> len(text)
    255809

If we now take the further step of creating an |NLTK| text from this list, we can do
all of the other linguistic processing we saw in Chapter chap-introduction_, along with
the regular list operations like slicing:

    >>> text = nltk.Text(tokens)
    >>> type(text)
    <type 'nltk.text.Text'>
    >>> text[1020:1060]
    ['CHAPTER', 'I', 'On', 'an', 'exceptionally', 'hot', 'evening', 'early', 'in',
    'July', 'a', 'young', 'man', 'came', 'out', 'of', 'the', 'garret', 'in',
    'which', 'he', 'lodged', 'in', 'S', '.', 'Place', 'and', 'walked', 'slowly',
    ',', 'as', 'though', 'in', 'hesitation', ',', 'towards', 'K', '.', 'bridge', '.']
    >>> text.collocations()
    Katerina Ivanovna; Pulcheria Alexandrovna; Avdotya Romanovna; Pyotr
    Petrovitch; Project Gutenberg; Marfa Petrovna; Rodion Romanovitch;
    Sofya Semyonovna; Nikodim Fomitch; did not; Hay Market; Andrey
    Semyonovitch; old woman; Literary Archive; Dmitri Prokofitch; great
    deal; United States; Praskovya Pavlovna; Porfiry Petrovitch; ear rings

Notice that "Project Gutenberg" appears as a collocation in this text.
Each text downloaded from Project Gutenberg contains a header with the
name of the text, the author, the names of people who scanned and
corrected the text, a license, and so on.  Sometimes this information
appears in a footer at the end of the file.  We cannot reliably detect
where the content begins and ends, and so have to resort to manual
inspection of the file, to discover unique strings that mark the beginning
and the end, before trimming ``raw`` to be just the content and nothing else:

    >>> raw.find("PART I")
    5303
    >>> raw.rfind("End of Project Gutenberg's Crime")
    1157681
    >>> raw = raw[5303:1157681]

The "find" and "reverse find" functions help us get the right index values.
Now the raw text begins with "PART I", and goes up to (but not including) the
phrase that marks the end of the content.

This was our first brush with reality: texts found on the web may contain
unwanted material, and there may not be an automatic way to remove it.
With a small amount of extra work we can extract the material we need.

Dealing with HTML
-----------------

A large amount of text on the web is in the form of |HTML| documents.
You can use a web browser to save a page as text to a local
file, then access this as described in Section sec-files-and-strings_.
However, if you're going to do this a lot, its easiest to get Python
to do the work directly.  The first step is the same as before,
using ``urlopen``.  For fun we'll pick a BBC News story
called *Blondes to die out in 200 years*, an entertaining
example of an urban legend reported as established scientific
fact (as noted on languagelog.org).

    >>> url = "http://news.bbc.co.uk/2/hi/health/2284783.stm"
    >>> html = urlopen(url).read()
    >>> html[:60]
    '<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN'

You can type ``print html`` to see the |HTML| content in all its glory,
including meta tags, an image map, JavaScript, forms, and tables.

.. note:: Remember that there's nothing special about the names of our variables,
   such as ``url`` and ``html``; we chose them to make the code easier to follow.
   If we called them ``var1`` and ``var2`` the result would be the same.

Getting text out of |HTML| is a sufficiently common task that |NLTK| provides
a helper function ``nltk.clean_html()``, which takes an |HTML| string and
returns text.

    >>> raw = nltk.clean_html(html)
    >>> tokens = nltk.wordpunct_tokenize(raw)
    >>> tokens
    ['BBC', 'NEWS', '|', 'Health', '|', 'Blondes', "'", 'to', 'die', 'out', ...]

This still contains lots of unwanted material concerning site navigation and related
stories.  With some trial and error you can find the start and end indexes of the
content and select the tokens of interest, and initialize a text as before.

    >>> tokens = tokens[96:399]
    >>> text = nltk.Text(tokens)
    >>> text.concordance('gene')
     they say too few people now carry the gene for blondes to last beyond the next tw
    t blonde hair is caused by a recessive gene . In order for a child to have blonde 
    to have blonde hair , it must have the gene on both sides of the family in the gra
    there is a disadvantage of having that gene or by chance . They don ' t disappear 
    ondes would disappear is if having the gene was a disadvantage and I do not think 


Processing Google Results
-------------------------

[how to extract google hits]

LanguageLog example for `absolutely`:lx:

.. table:: absolutely

   +-----------------+------------+-------------+-------------+--------------+
   | Google hits     |`adore`:lx: | `love`:lx:  |`like`:lx:   |`prefer`:lx:  |
   +-----------------+------------+-------------+-------------+--------------+
   | `absolutely`:lx:|     289,000|      905,000|       16,200|           644|
   +-----------------+------------+-------------+-------------+--------------+
   | `definitely`:lx:|       1,460|       51,000|      158,000|        62,600|
   +-----------------+------------+-------------+-------------+--------------+
   | ratio           |       198:1|         18:1|         1:10|          1:97|
   +-----------------+------------+-------------+-------------+--------------+

   `Absolutely`:lx: vs `Definitely`:lx: (Liberman 2005, LanguageLog.org)


Extracting Text from PDF, MSWord and other Binary Formats
---------------------------------------------------------

ASCII text and |HTML| text are human readable formats.  Text often comes in binary
formats |mdash| like PDF and MSWord |mdash| that can only be opened using specialized
software.  Third-party libraries such as ``pypdf`` and ``pywin32`` can be used to access
these formats [appendix section on this?].  For once-off conversion of a few documents,
it is simpler to open the document with a suitable application, then save it as text
to the local drive, and access it as described below.
If the document is already on the web, you can enter its URL in Google's search box.
This often includes a link to an |HTML| version of the document, which you can save
as text.  


The Processing Pipeline
-----------------------

Figure pipeline1_ summarizes what we have covered in this section, including the process
of building a vocabulary that we saw in Chapter chap-introduction_.  (One step, normalization,
will be discussed later in this chapter.)

[Discuss types of object at each stage of the pipeline; and ``type()``]

.. _pipeline1:
.. figure:: ../images/pipeline1.png
   :scale: 40

   The Processing Pipeline

Integers, strings and lists are all kinds of `data types`:dt: in
Python, and have types ``int``, ``str`` and ``list`` respectively. In
fact, every value in Python has a type. Python's ``type()`` function
will tell you what an object's type is:

    >>> oddments = ['cat', 'cat'.index('a'), 'cat'.split()]
    >>> for e in oddments:
    ...     type(e)
    ... 
    <type 'str'>
    <type 'int'>
    <type 'list'>

The type determines what operations you can perform on the data
value. So, for example, we have seen that we can index strings and
lists, but we can't index integers:

    >>> one = 'cat'
    >>> one[0]
    'c'
    >>> two = [1, 2, 3]
    >>> two[1]
    2
    >>> three = 1234
    >>> three[2]
    Traceback (most recent call last):
    File "<pyshell#95>", line 1, in -toplevel-
    three[2]
    TypeError: 'int' object is unsubscriptable

The fact that this is a problem with types is signalled by the class of
error, i.e., ``TypeError``; an object being "unsubscriptable" means we
can't index into it.

Similarly, we can concatenate strings with strings, and lists with
lists, but we cannot concatenate strings with lists:

    >>> query = 'Who knows?'
    >>> beatles = ['john', 'paul', 'george', 'ringo']
    >>> query + beatles
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: cannot concatenate 'str' and 'list' objects

You may also have noticed that our analogy between operations on
strings and numbers works for multiplication and addition, but not
subtraction or division:

    >>> 'very' * 3 
    'veryveryvery'
    >>> 'very' - 'y'
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: unsupported operand type(s) for -: 'str' and 'str'
    >>> 6 / 2
    3
    >>> 'very' / 2
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    TypeError: unsupported operand type(s) for /: 'str' and 'int'

These error messages are another example of Python telling us that we
have got our data types in a muddle. In the first case, we are told
that the operation of substraction (i.e., ``-``) cannot apply to
objects of type ``str``, while in the second, we are told that
division cannot take ``str`` and ``int`` as its two operands.

Type conversions: ``str()``, ``int()``, ``list()``.

.. _sec-files-and-strings:

-----------------
Files and Strings
-----------------

[Introduction]

Reading Local Files
-------------------

.. Monkey-patching to fake the file/web examples in this section:

    >>> from StringIO import StringIO
    >>> def fake_open(filename, mode=None):
    ...     return StringIO('Hello World!\nThis is a test file.\n')
    >>> def fake_urlopen(url):
    ...     return StringIO('<!doctype html public "-//W3C//DTD HTML 4.0 Transitional//EN"')
    >>> open = fake_open
    >>> import urllib
    >>> urllib.urlopen = fake_urlopen

It is easy to access local files in Python.  As an exercise, create a
file called ``corpus.txt`` using a text editor, and enter the
following text::

   Time flies like an arrow.
   Fruit flies like a banana.

|nopar| 
Be sure to save the file as plain text. You also need to make sure
that you have saved the file in the same directory or folder in which
you are running the Python interactive interpreter.

.. Note:: If you are using |IDLE|, you can easily create this file by
   selecting the *New Window* command in the *File* menu, typing
   the required text into this window, and then saving the file as
   ``corpus.txt`` in the first directory that |IDLE| offers in the
   pop-up dialogue box.


|nopar| 
The next step is to `open`:dt: a file using the built-in function ``open()``
which has two parameters, the name of the file, here ``corpus.txt``, and the
mode to open the file with (``'r'`` means to open the file for reading,
and ``'U'`` stands for "Universal", which lets us ignore the different
conventions used for marking newlines).

.. doctest-ignore::
    >>> f = open('corpus.txt', 'rU')

.. Note:: If the interpreter cannot find your file, it will give an
   error like this:

   .. doctest-ignore::
      >>> f = open('corpus.txt', 'rU')
      Traceback (most recent call last):
      File "<pyshell#7>", line 1, in -toplevel-
      f = open('corpus.txt', 'rU')
      IOError: [Errno 2] No such file or directory: 'corpus.txt'

   To check that the file that you are trying to open is really in the
   right directory, use |IDLE|\ 's *Open* command in the *File* menu;
   this will display a list of all the files in the directory where
   |IDLE| is running. An alternative is to examine the current
   directory from within Python:

   .. doctest-ignore::
      >>> import os
      >>> os.listdir('.')

|nopar| 
There are several methods for reading the file.
The following uses the method ``read()`` on the file object
``f``; this reads the entire contents of a file into a string.

.. doctest-ignore::
    >>> f.read() 
    'Hello World!\nThis is a test file.\n'

|nopar|
Recall that the ``'\n'`` characters are `newlines`:dt:\ ; this
is equivalent to pressing *Enter* on a keyboard and starting a new line. 
Note that we can open and read a file in one step:

    >>> text = open('corpus.txt', 'rU').read()

|nopar|
We can also read a file one line at a time using the ``for`` loop construct:

    >>> f = open('corpus.txt', 'rU')
    >>> for line in f:
    ...     print line[:-1]
    Hello world!
    This is a test file.

|nopar| 
Here we use the slice ``[:-1]`` to remove the newline character at the end of
the input line.

It is possible to use the methods described above
with ``nltk.data.find()`` method to access and read NLTK's corpus files
directly:

    >>> file = nltk.data.find('corpora/gutenberg/melville-moby_dick.txt')
    >>> raw = open(file, 'rU').read()


Strings
-------

When applied to strings, the ``+`` operation is called
`concatenation`:dt:. It produces a new string that is a copy of the
two original strings pasted together end-to-end.  Notice that
concatenation doesn't do anything clever like insert a space between
the words.  The Python interpreter has no way of knowing that you want
a space; it does `exactly`:em: what it is told.  Given the example
of ``+``, you might be able guess what multiplication will do:

    >>> 'very' + 'very' + 'very'
    'veryveryvery'
    >>> 'very' * 3
    'veryveryvery'

.. CHARACTER BIGRAMS...

Printing and Inspecting Strings
-------------------------------

So far, when we have wanted to look at the contents of a variable or
see the result of a calculation, we have just typed the variable name
into the interpreter.  We can also see the contents of ``msg`` using
``print msg``:

    >>> monty = 'Monty Python'
    >>> monty
    'Monty Python'
    >>> print monty
    Monty Python

In the first case above, we `inspect`:em: the variable by giving its
name.  The interpreter prints the Python representation of its value.
Since its a string, the result is quoted.  In the second case, we've
asked the interpreter to print the contents of the variable.  Since there
are no quotation characters inside the string itself, none are printed.
We can use ``print`` to display more than one item on a line, in various ways:

    >>> grail = 'Holy Grail'
    >>> print monty grail
    Monty PythonHoly Grail
    >>> print monty, grail
    Monty Python Holy Grail
    >>> print monty, "and the", grail
    Monty Python and the Holy Grail    
    

.. note:: If you have created some variable ``v`` and want to find out about
   it, then type ``help(v)`` to read the help entry for this kind of object.
   Type ``dir(v)`` to see a list of operations that are defined on the
   object.

Accessing Individual Characters
-------------------------------

As we saw in Section sec-a-closer-look-at-python-texts-as-lists-of-words_ for lists, strings are indexed, starting from zero:

    >>> msg = 'Hello World'
    >>> msg[0]
    'H'
    >>> msg[3]
    'l'
    >>> msg[5]
    ' '

.. Note:: Be careful to distinguish between the string ``' '``, which
   is a single whitespace character, and ``''``, which is the empty string.

The easiest way to grasp this counting from zero is to think of a string
index as specifying how many steps the computer has to take, starting from
the beginning of the string, to get to the desired character.  As for
lists, if we try to access an index that is outside of the string we get an
error:

    >>> msg[11]
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    IndexError: string index out of range

As with lists, we can use negative indexes for strings,
where ``-1`` is the index of the last character.

    >>> msg[-1]
    'd'
    >>> msg[-3]
    'r'
    >>> msg[-6]
    ' '

We can visualize negative indexes as shown in Figure indexing02_.
[Redo this diagram] 

.. _indexing02:
.. figure:: ../images/indexing02.png
   :scale: 40


   Negative Indexes

Using positive and negative indexes, we have two ways to refer to
any position in a string.  In this case, when the string had a length of 11,
indexes ``5`` and ``-6`` both refer to the same character (a space), and:
``5 = len(msg) - 6``.

Accessing Substrings
--------------------

[condense this section]

In |NLP| we usually want to access more than one character at a time. This is
also pretty simple; we just need to specify a start and end index.  For example,
the following code accesses the substring starting at index ``1``, up to (but not including)
index ``4``:

    >>> msg[1:4]
    'ell'

The notation ``:4`` is known as a `slice`:dt:.
Here we see the characters are ``'e'``, ``'l'`` and ``'l'`` which correspond
to ``msg[1]``, ``msg[2]`` and ``msg[3]``, but not ``msg[4]``. This is because
a slice `starts`:em: at the first index but finishes `one before`:em: the end index.
This is consistent with indexing: indexing also starts from
zero and goes up to `one before`:em: the length of the string. We can see this
by slicing with the value of ``len()``:

    >>> len(msg)
    11
    >>> msg[0:11]
    'Hello World'

We can also slice with negative indices |mdash| the same basic rule of starting
from the start index and stopping one before the end index applies;
here we stop before the space character:

    >>> msg[0:-6]
    'Hello'

Python provides two shortcuts for commonly used slice values. If the
start index is ``0`` then you can leave it out, and if the
end index is the length of the string then you can leave it out:

    >>> msg[:3]
    'Hel'
    >>> msg[6:]
    'World'

The first example above selects the first three characters from the string,
and the second example selects from the character with index 6, namely ``'W'``,
to the end of the string.

We can also find the position of a substring within a string, using ``find()``:

    >>> msg.find('World')
    6

Analyzing Strings
-----------------

* character frequency plot, e.g get text in some language using ``language_x = nltk.corpus.udhr.raw(x)``,
  then construct its frequency distribution ``fdist = FreqDist(language_x)``, then
  view the distribution with ``fdist.keys()`` and ``fdist.plot()``.

* functions involving strings, e.g. determining past tense

* built-ins, ``find()``, ``rfind()``, ``index()``, ``rindex()``

* revisit string tests like ``endswith()`` from chapter 1

Counting Letters
----------------

We can count individual letters as well.  For each word `w`:math:, we have
to loop over each character contained in the word.  We can do this as follows:

    >>> fdist = FreqDist(char for w in text1 for char in w)
    >>> fdist.samples()
    >>> ['e', 't', 'a', 'o', 'n', 's', 'i', 'h', 'r', 'l', 'd', 'u', 'm', 'c',
        'w', 'g', 'f', ',', 'y', 'p', 'b', 'v', 'k', '.', '-', ';', 'I', '"',
        "'", 'A', 'T', 'S', '!', 'H', 'B', 'W', 'E', 'q', 'N', 'C', 'P', 'x',
        '?', 'O', 'L', 'j', 'R', 'F', 'M', 'D', 'G', 'z', 'Y', 'Q', 'J', 'U',
        ')', '(', ':', 'K', 'V', '1', '0', '2', '8', '5', '7', '3', '*', '4',
        'Z', '6', '9', '_', 'X', '[', ']', '$', '&']

If we're just interested in letter frequencies, we should ignore the case
distinction by normalizing everything to lowercase, and filter out non-alphabetic characters.
Here's how we can do it:

    >>> fdist = FreqDist(char.lower() for w in text1 for char in w if w.isalpha())
    >>> fdist.keys()
    ['e', 't', 'a', 'o', 'n', 'i', 's', 'h', 'r', 'l', 'd', 'u', 'm', 'c', 'w',
    'f', 'g', 'p', 'b', 'y', 'v', 'k', 'q', 'j', 'x', 'z']

This gives us the letters of the alphabet, with the most frequently occurring letters
listed first (this is quite complicated and we'll explain it more carefully below).
You might like to visualize the distribution using ``fdist.plot()``.
The relative character frequencies of a text can be used in automatically identifying
the language of the text.

These methods for operating on every element of a list are surprisingly powerful.
Let's enrich things one more time:

    >>> [char.lower() for w in text1 for char in w if char.isalpha()]
    ['m', 'o', 'b', 'y', 'd', 'i', 'c', 'k', 'b', 'y', 'h', 'e', 'r', 'm', 'a', 'n', ...]
    >>>

For every word in the text, for every character in the word, if the character is
alphabetic, lowercase it.  There's three variables involved here, and we have to
use them correctly or it won't work.  Here's the general template:
``[f(x) for y in z for x in y if g(x)]``.

Lists vs Strings
----------------

Strings and lists are both kind of `sequence`:dt:. As such, they can
both be indexed and sliced:

    >>> query = 'Who knows?'
    >>> beatles = ['john', 'paul', 'george', 'ringo']
    >>> query[2]
    'o'
    >>> beatles[2]
    'george'
    >>> query[:2]
    'Wh'
    >>> beatles[:2]
    ['john', 'paul']

Similarly, strings can be concatenated and so can lists (though not
with each other!): 

    >>> newstring = query + " I don't"
    >>> newlist = beatles + ['brian', 'george']

When we open a file
for reading into a Python program, we get a string,
corresponding to the contents of the whole file. If we try to use a ``for`` loop to
process the elements of this string, all we can pick out are the
individual characters |mdash| we don't get to choose the
granularity. By contrast, the elements of a list can be as big or
small as we like: for example, they could be paragraphs, sentence,
phrases, words, characters. So lists have the advantage that we
can be flexible about the elements they contain, and
correspondingly flexible about any downstream processing.
So one of the first things we are likely to do in a piece of NLP
code is tokenize a string into a list of strings (Section sec-tokenization_).
Conversely, when we want to write our results to a file, or to a terminal,
we will usually format them as a string (Section sec-formatting_).

Lists and strings do not have exactly the same functionality.
Lists have the added power that you can change their elements. Let's
imagine that we want to change the 0th element of ``cgi`` to
``'colorful'``, we can do that by assigning the new value
to the index ``cgi[0]``:

    >>> cgi = ['colorless', 'green', 'ideas']
    >>> cgi[0] = 'colorful'
    >>> cgi
    ['colorful', 'green', 'ideas']

|nopar| On the other hand if we try to do that with a *string*
|mdash| changing the 0th character in ``msg`` to ``'J'`` |mdash| we get:

.. doctest-ignore::
    >>> msg[0] = 'J'
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    TypeError: object does not support item assignment

|nopar| This is because strings are `immutable`:dt: |mdash| you can't change a
string once you have created it.  However, lists are `mutable`:dt:,
and their contents can be modified at any time.  As a result, lists
support operations that modify the original value rather than producing a new value.
A method is a function that is associated with a particular object.
A method is called on the object by giving the object's name,
then a period, then the name of the method, and finally the parentheses
containing any arguments.  For example, in the following code we use
the ``sort()`` and ``reverse()`` methods:

    >>> chomsky.sort()
    >>> chomsky.reverse()
    >>> chomsky
    ['sleep', 'ideas', 'green', 'furiously', 'colorless']

As you will see, the prompt reappears immediately on the line after
``chomsky.sort()`` and ``chomsky.reverse()``. That is because these
methods do not produce a new list, but instead modify the original list
stored in the variable ``chomsky``.  

Lists also have an ``append()`` method for adding items to the
end of the list and an ``index()`` method for finding the index
of particular items in the list:

    >>> chomsky.append('said')
    >>> chomsky.append('Chomsky')
    >>> chomsky
    ['sleep', 'ideas', 'green', 'furiously', 'colorless', 'said', 'Chomsky']
    >>> chomsky.index('green')
    2

Finally, just as a reminder, you can create lists of any values you like.
As you can see in the following example for a lexical entry, the values
in a list do not even have to have the same type (though this is usually
not a good idea, as we will explain in Section sec-back-to-the-basics_).

    >>> bat = ['bat', [[1, 'n', 'flying mammal'], [2, 'n', 'striking instrument']]]
    >>>

---------------------
Low-Density Languages
---------------------

* ethnologue info


-------
Summary
-------

* A text corpus is a balanced collection of texts.  NLTK comes with many corpora,
  e.g. the Brown Corpus, ``nltk.corpus.brown``.
* A dictionary is used to map between arbitrary types of information,
  such as a string and a number: ``freq['cat'] = 12``.  We create
  dictionaries using the brace notation: ``pos = {}``,
  ``pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}``.

---------
Exercises
---------

#. |easy| Define a frequency distribution over the letters of a text, e.g. 
   ``fdist = nltk.FreqDist(gutenberg.raw("blake-poems.txt"))``.  Now plot
   this distribution, using ``fdist.plot()``, to see which letters are more
   frequent.  Now do this using texts from the Universal Declaration of
   Human Rights, to see how languages differ in their frequency of use of
   different letters.

#. |soso| **Exploring text genres:**
   Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

#. |soso| The CMU Pronunciation Dictionary contains multiple pronunciations
   for certain words.  How many distinct words does it contain?  What fraction
   of words in this dictionary have more than one possible pronunciation?

#. |easy| Familiarize yourself with the WordNet interface, by reading the
   documentation available via ``help(wordnet)``.  Try out the text-based
   browser, ``wordnet.browse()``.

#. |easy| Investigate the holonym / meronym relations for some nouns.  Note that there
   are three kinds (member, part, substance), so access is more specific,
   e.g., ``wordnet.MEMBER_MERONYM``, ``wordnet.SUBSTANCE_HOLONYM``.

.. Currently too hard since we can't iterate over synsets easily
   #. |soso| What percentage of noun synsets have no hyponyms?

#. |easy| The polysemy of a word is the number of senses it has.
   Using WordNet, we can determine that the noun *dog* has 7 senses
   with: ``len(nltk.wordnet.N['dog'])``.
   Compute the average polysemy of nouns, verbs, adjectives and
   adverbs according to WordNet.

#. |soso| What is the branching factor of the noun hypernym hierarchy?
   (For all noun synsets that have hyponyms, how many do they have on average?)

#. |soso| Define a function ``supergloss(s)`` that takes a synset ``s`` as its argument
   and returns a string consisting of the concatenation of the glosses of ``s``, all
   hypernyms of ``s``, and all hyponyms of ``s``.

#. |talk| Review the mappings in Table linguistic-objects_.  Discuss any other
   examples of mappings you can think of.  What type of information do they map
   from and to?

#. |easy| Using the Python interpreter in interactive mode, experiment with
   the examples in this section.  Create a dictionary ``d``, and add
   some entries.  What happens if you try to access a non-existent
   entry, e.g. ``d['xyz']``?

#. |easy| Try deleting an element from a dictionary, using the syntax
   ``del d['abc']``.  Check that the item was deleted.

#. |easy| Create a dictionary ``e``, to represent a single lexical entry
   for some word of your choice.
   Define keys like ``headword``, ``part-of-speech``, ``sense``, and
   ``example``, and assign them suitable values.

#. |soso| Write a program to find all words that occur at least three times in the Brown Corpus.

#. |soso| Write a program to generate a table of token/type ratios, as we saw in
   Table brown-types_.  Include the full set of Brown Corpus genres (``nltk.corpus.brown.categories()``).
   Which genre has the lowest diversity (greatest number of tokens per type)?
   Is this what you would have expected?

#. |soso| Modify the text generation program in Figure random_ further, to
   do the following tasks:

   a) Store the *n* most likely words in a list ``lwords`` then randomly
      choose a word from the list using ``random.choice()``.

   b) Select a particular genre, such as a section of the Brown Corpus,
      or a genesis translation, one of the Gutenberg texts, or one of the Web texts.  Train
      the model on this corpus and get it to generate random text.  You
      may have to experiment with different start words. How intelligible
      is the text?  Discuss the strengths and weaknesses of this method of
      generating random text.

   c) Now train your system using two distinct genres and experiment
      with generating text in the hybrid genre.  Discuss your observations.

#. |soso| Write a program to print the most frequent bigrams
   (pairs of adjacent words) of a text,
   omitting non-content words, in order of decreasing frequency.

#. |soso| Write a program to create a table of word frequencies by genre,
   like the one given above for modals.  Choose your own words and
   try to find words whose presence (or absence) is typical of a genre.
   Discuss your findings.

#. |hard| **Zipf's Law**:
   Let *f(w)* be the frequency of a word *w* in free text. Suppose that
   all the words of a text are ranked according to their frequency,
   with the most frequent word first. Zipf's law states that the
   frequency of a word type is inversely proportional to its rank
   (i.e. *f.r = k*, for some constant *k*). For example, the 50th most
   common word type should occur three times as frequently as the
   150th most common word type.

   a) Write a function to process a large text and plot word
      frequency against word rank using ``pylab.plot``. Do
      you confirm Zipf's law? (Hint: it helps to use a logarithmic scale).
      What is going on at the extreme ends of the plotted line?

   #) Generate random text, e.g. using ``random.choice("abcdefg ")``,
      taking care to include the space character.  You will need to
      ``import random`` first.  Use the string
      concatenation operator to accumulate characters into a (very)
      long string.  Then tokenize this string, and generate the Zipf
      plot as before, and compare the two plots.  What do you make of
      Zipf's Law in the light of this?

#. |hard| Modify the ``generate_model()`` function in Figure random_ to use Python's
   ``random.choose()`` method to randomly pick the next word from
   the available set of words.

#. |soso| Write a function ``tf()`` that takes a word and the name of a section
   of the Brown Corpus as arguments, and computes the text frequency of the word
   in that section of the corpus.

#. |easy| Try the examples in this section, then try the following.

   a) Create a variable called ``msg`` and put a message
      of your own in this variable.  Remember that strings need
      to be quoted, so you will need to type something like:
      ``msg = "I like NLP!"``
   b) Now print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` command.
   c) Try various arithmetic expressions using this string, e.g.
      ``msg + msg``, and ``5 * msg``.
   d) Define a new string ``hello``, and then try ``hello + msg``.
      Change the ``hello`` string so that it ends with a space
      character, and then try ``hello + msg`` again.

#. |easy| Consider the following two expressions which have the same
   result.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| Define a string ``s = 'colorless'``.  Write a Python statement
   that changes this to "colourless" using only the slice and
   concatenation operations.

#. |easy| Try the slice examples from this section using the interactive
   interpreter.  Then try some more of your own.  Guess what the result
   will be before executing the command.

#. |easy| We can use the slice notation to remove morphological endings on
   words.  For example, ``'dogs'[:-1]`` removes the last character of
   ``dogs``, leaving ``dog``.  Use slice notation to remove the
   affixes from these words (we've inserted a hyphen to
   indicate the affix boundary, but omit this from your strings):
   ``dish-es``, ``run-ning``, ``nation-ality``, ``un-do``,
   ``pre-heat``.

#. |easy| We saw how we can generate an ``IndexError`` by indexing beyond the end
   of a string.  Is it possible to construct an index that goes too far to
   the left, before the start of the string?

#. |easy| We can also specify a "step" size for the slice. The following
   returns every second character within the slice: ``msg[6:11:2]``.
   It also works in the reverse direction: ``msg[10:5:-2]``
   Try these for yourself, then experiment with different step values.

#. |easy| What happens if you ask the interpreter to evaluate ``msg[::-1]``?
   Explain why this is a reasonable result.

#. |easy| Define a conditional frequency distribution over the Names corpus
   that allows you to see which initial letters are more frequent for males
   vs females (cf. Figure fig-cfd-gender_).

#. |soso| Write a function that finds the 50 most frequently occurring words
   of a text that are not stopwords.


.. include:: footer.rst

