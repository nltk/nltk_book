.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: add Mark Twain
.. TODO: discussion of resource rich/poor languages in section on corpora in other languages
         number of languages in the world, Ethnologue, etc
.. TODO: explain double vs single vs triple quotes for strings
.. TODO: negative indices of lists
.. TODO: TextCollection
.. TODO: extracting dates from a tokenized text
.. TODO: finding a sequence of words matching some pattern (including doubled words, e.g. "the thing is is that")
.. TODO: non-mutability of dictionary keys

.. _chap-corpora:

=====================================
2. Text Corpora and Lexical Resources
=====================================

Practical work in Natural Language Processing usually involves
a variety established bodies of linguistic data, or `corpora`:dt:.
The goal of this chapter is to answer the following questions:

#. what are some useful text corpora and lexical resources, and how can we access them with Python
#. which Python constructs are most helpful for this work?
#. how do we re-use code effectively?

This chapter continues to present programming concepts by example, in the
context of a discussion about some linguistic processing task, before
exploring each Python construct systematically.  Don't worry if you see
an example that contains something unfamiliar; simply try it out and see
what it does, and |mdash| if you're game |mdash| modify it by substituting
some part of the code with a different text or word.  This way you will
associate a task with a programming idiom, and learn the hows and whys later.

.. _sec-extracting-text-from-corpora:

----------------------
Accessing Text Corpora
----------------------

A text corpus is a large body of text, containing a careful balance of material in
one or more genres.  We examined some small text collections in Chapter chap-introduction_,
such as the presidential inaugural addresses.  This particular corpus actually contains dozens of
individual texts |mdash| one per address |mdash| but we glued them end-to-end
and treated them like chapters of a book, i.e. as a single text.  In this
section we will examine a variety of text corpora and will see how to select
individual texts, and how to work with them.

The Gutenberg Corpus
--------------------

|NLTK| includes a small selection of texts from the `Project Gutenberg
<http://www.gutenberg.org/>`_ electronic text archive containing
some 25,000 free electronic books.  We begin
by getting the Python interpreter to load the |NLTK| package,
then ask to see ``nltk.corpus.gutenberg.files()``, the files in
|NLTK|\ 's corpus of Gutenberg texts:  

    >>> import nltk
    >>> nltk.corpus.gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's pick out the first of these texts |mdash| *Emma* by Jane Austen |mdash| and
give it a short name ``emma``, then find out how many words it contains: 

    >>> emma = nltk.corpus.gutenberg.words('austen-emma.txt')
    >>> len(emma)
    192432

.. note:: In |NLTK| 0.9.5 you cannot do concordancing (and other tasks from
   Section sect-computing-with-language-texts-and-words_) using a text
   defined this way.  Instead you have to do the following:

       >>> emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))

The long name refers to the ``words()`` function of the ``gutenberg``
module in |NLTK|\ 's ``corpus`` package.
It gets cumbersome to type such long names all the time, so Python provides
another version of the import statement, as follows:

    >>> from nltk.corpus import gutenberg
    >>> gutenberg.files()
    ('austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt',
    'blake-poems.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt',
    'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt')

Let's write a short program to display other information about each text:

    >>> for file in gutenberg.files():
    ...     num_chars = len(gutenberg.raw(file))
    ...     num_words = len(gutenberg.words(file))
    ...     num_sents = len(gutenberg.sents(file))
    ...     num_vocab = len(set(w.lower() for w in gutenberg.words(file)))
    ...     print num_chars/num_words, num_words/num_sents, num_words/num_vocab, file
    ... 
    4 21 24 austen-emma.txt
    4 23 16 austen-persuasion.txt
    4 24 20 austen-sense.txt
    4 33 73 bible-kjv.txt
    4 18 4 blake-poems.txt
    4 17 10 chesterton-ball.txt
    4 19 10 chesterton-brown.txt
    4 16 10 chesterton-thursday.txt
    4 24 13 melville-moby_dick.txt
    4 52 9 milton-paradise.txt
    4 12 7 shakespeare-caesar.txt
    4 13 6 shakespeare-hamlet.txt
    4 13 5 shakespeare-macbeth.txt
    4 35 10 whitman-leaves.txt

This program has displayed three statistics for each text:
average word length, average sentence length, and the number of times each vocabulary
item appears in the text on average (our lexical diversity score).
Observe that average word length appears to be a general property of English, since it is
always `4`:math:.  Average sentence length and lexical diversity
appear to be characteristics of particular authors.

This example also showed how we can access the "raw" text of the book,
not split up into words.  The ``raw()`` function gives us the contents of the file
without any linguistic processing.  So, for example, ``len(gutenberg.raw('blake-poems.txt')``
tells us how many *letters* occur in the text, including the spaces between words.
The ``sents()`` function divides the text up into its sentences, where each sentence is
a list of words:

    >>> macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')
    >>> macbeth_sentences
    [['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare',
    '1603', ']'], ['Actus', 'Primus', '.'], ...]
    >>> macbeth_sentences[1038]
    ['Double', ',', 'double', ',', 'toile', 'and', 'trouble', ';',
    'Fire', 'burne', ',', 'and', 'Cauldron', 'bubble']
    >>> longest_len = max(len(s) for s in macbeth_sentences)
    >>> [s for s in macbeth_sentences if len(s) == longest_len]
    [['Doubtfull', 'it', 'stood', ',', 'As', 'two', 'spent', 'Swimmers', ',', 'that',
    'doe', 'cling', 'together', ',', 'And', 'choake', 'their', 'Art', ':', 'The',
    'mercilesse', 'Macdonwald', ...], ...]

.. note:: Most |NLTK| corpus readers include a variety of access methods
   apart from ``words()``.  We access the raw file contents using ``raw()``,
   and get the content sentence by sentence using ``sents()``.  Richer
   linguistic content is available from some corpora, such as part-of-speech
   tags, dialogue tags, syntactic trees, and so forth; we will see these
   in later chapters.

Web and Chat Text
-----------------

Although Project Gutenberg contains thousands of books, it represents established
literature.  Its important to consider less formal language as well.  |NLTK|\ 's
small collection of web text includes content from a Firefox discussion forum,
conversations overheard in New York, the movie script of *Pirates of the Carribean*,
personal advertisements, and wine reviews:

    >>> from nltk.corpus import webtext
    >>> for f in webtext.files():
    ...     print f, webtext.raw(f)[:70]
    ... 
    firefox.txt Cookie Manager: "Don't allow sites that set removed cookies to set fut
    overheard.txt White guy: So, do you have any plans for this evening? Asian girl: Yea
    pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Ros
    singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encounters.
    wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawberrie 

There is also a corpus of instant messaging chat sessions, originally collected
by the Naval Postgraduate School for research on automatic detection of internet predators.
The corpus contains over 10,000 posts, anonymized by replacing usernames with generic
names of the form "UserNNN", and manually edited to remove any other identifying information.
The corpus is organized into 15 files, where each file contains several hundred posts
collected on a given date, for an age-specific chatroom (teens, 20s, 30s, 40s, plus a
generic adults chatroom).  The filename contains the date, chatroom,
and number of posts, e.g. ``10-19-20s_706posts.xml`` contains 706 posts gathered from
the 20s chat room on 10/19/2006.

    >>> from nltk.corpus import nps_chat 
    >>> chatroom = nps_chat.posts('10-19-20s_706posts.xml')
    >>> chatroom[123]
    ['i', 'do', "n't", 'want', 'hot', 'pics', 'of', 'a', 'female', ',',
    'I', 'can', 'look', 'in', 'a', 'mirror', '.']

The Brown Corpus
----------------

The Brown Corpus was the first million-word electronic
corpus of English, created in 1961 at Brown University.
This corpus contains text from 500 sources, and the sources
have been categorized by genre.  
Table brown-sources_ gives an example of each genre
(for a complete list, see ``http://icame.uib.no/brown/bcm-los.html``).

.. table:: brown-sources

  ===  ========  ===============  =========================================================================
  ID   File      Genre            Description
  ===  ========  ===============  =========================================================================
  A16  ``ca16``  news             Chicago Tribune: *Society Reportage*
  B02  ``cb02``  editorial        Christian Science Monitor: *Editorials*
  C17  ``cc17``  reviews          Time Magazine: *Reviews*
  D12  ``cd12``  religion         Underwood: *Probing the Ethics of Realtors*
  E36  ``ce36``  hobbies          Norling: *Renting a Car in Europe*
  F25  ``cf25``  lore             Boroff: *Jewish Teenage Culture*
  G22  ``cg22``  belles_lettres   Reiner: *Coping with Runaway Technology*
  H15  ``ch15``  government       US Office of Civil and Defence Mobilization: *The Family Fallout Shelter*
  J17  ``cj19``  learned          Mosteller: *Probability with Statistical Applications*
  K04  ``ck04``  fiction          W.E.B. Du Bois: *Worlds of Color*
  L13  ``cl13``  mystery          Hitchens: *Footsteps in the Night*
  M01  ``cm01``  science_fiction  Heinlein: *Stranger in a Strange Land*
  N14  ``cn15``  adventure        Field: *Rattlesnake Ridge*
  P12  ``cp12``  romance          Callaghan: *A Passion in Rome*
  R06  ``cr06``  humor            Thurber: *The Future, If Any, of Comedy*
  ===  ========  ===============  =========================================================================
  
  Example Document for Each Section of the Brown Corpus

We can access the corpus as a list of words, or a list of sentences (where each sentence
is itself just a list of words).  We can optionally specify particular categories or files to read:

    >>> from nltk.corpus import brown
    >>> brown.categories()
    ['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',
    'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance',
    'science_fiction']
    >>> brown.words(categories='news')
    ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
    >>> brown.words(files=['cg22'])
    ['Does', 'our', 'society', 'have', 'a', 'runaway', ',', ...]
    >>> brown.sents(categories=['news', 'editorial', 'reviews'])
    [['The', 'Fulton', 'County'...], ['The', 'jury', 'further'...], ...]

We can use the Brown Corpus to study systematic differences between
genres, a kind of linguistic inquiry known as `stylistics`:dt:.
Let's compare genres in their usage of modal verbs.  The first step
is to produce the counts for a particular genre:

    >>> news_text = brown.words(categories='news')
    >>> fdist = nltk.FreqDist(w.lower() for w in news_text)
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> for m in modals:
    ...     print m + ':' + fdist[m],
    ...
    can:94 could:87 may:93 might:38 must:53 will:389

.. note:: |TRY|
   Choose a different section of the Brown Corpus, and adapt the above
   method to count a selection of `wh`:lx: words, such as `what`:lx:,
   `when`:lx:, `where`:lx:, `who`:lx: and `why`:lx:.

Next, we need to do this for each genre of interest.  To save re-typing,
we can put the above code into a function, and use the function several
times over.  However, there is an even better way, using
|NLTK|\ 's support for conditional frequency distributions
(Section sec-conditional-frequency-distributions_), as follows:

    >>> cfd = nltk.ConditionalFreqDist((g,w)
                  for g in brown.categories()
                  for w in brown.words(categories=g))
    >>> genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']
    >>> modals = ['can', 'could', 'may', 'might', 'must', 'will']
    >>> cfd.tabulate(conditions=genres, samples=modals)
                     can could  may might must will
               news   93   86   66   38   50  389
           religion   82   59   78   12   54   71
            hobbies  268   58  131   22   83  264
    science_fiction   16   49    4   12    8   16
            romance   74  193   11   51   45   43
              humor   16   30    8    8    9   13

Observe that the most frequent modal in the news genre is
`will`:lx:, suggesting a focus on the future, while the most frequent
modal in the romance genre is `could`:lx:, suggesting a focus on possibilities.

Reuters Corpus
--------------

The Reuters Corpus contains 10,788 news documents totaling 1.3 million words.
The documents have been classified for relevance to 90 topics, and grouped
into two sets, called "training" and "test" (for training and testing algorithms
that automatically detect the topic of a document, as we will explore further
in Chapter chap-data-intensive_).

    >>> from nltk.corpus import reuters
    ('test/14826', 'test/14828', 'test/14829', 'test/14832', ...)
    >>> reuters.categories() 
    ['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa',
    'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn',
    'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', ...]

Unlike the Brown Corpus, categories in the Reuters corpus overlap with each
other, simply because a news story often covers multiple topics.  We can
ask for the topics covered by one or more documents, or for the documents
included in one or more categories:

    >>> reuters.categories('training/9865')
    ['barley', 'corn', 'grain', 'wheat']
    >>> reuters.categories(['training/9865', 'training/9880'])
    ['barley', 'corn', 'grain', 'money-fx', 'wheat']
    >>> reuters.files('barley') 
    ['test/15618', 'test/15649', 'test/15676', 'test/15728', 'test/15871', ...]
    >>> reuters.files(['barley', 'corn']) 
    ['test/14832', 'test/14858', 'test/15033', 'test/15043', 'test/15106',
    'test/15287', 'test/15341', 'test/15618', 'test/15618', 'test/15648', ...]

For convenience, the corpus methods accept a single name or a list of names.
Similarly, we can specify the words or sentences we want in terms of files or categories.

    >>> reuters.words('training/9865')
    ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]
    >>> reuters.words(['training/9865', 'training/9880'])
    ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]
    >>> reuters.words(categories='barley')
    ['FRENCH', 'FREE', 'MARKET', 'CEREAL', 'EXPORT', ...]
    >>> reuters.words(categories=['barley', 'corn'])
    ['THAI', 'TRADE', 'DEFICIT', 'WIDENS', 'IN', 'FIRST', ...]

.. note:: Many other English text corpora are provided with |NLTK|.  For a
   list see Appendix app-modules_.  For more examples of access |NLTK| corpora,
   please consult the online guide at ``http://nltk.org/doc/guides/corpus.html``.

Inaugural Address Corpus
------------------------

We saw this corpus in section sect-computing-with-language-texts-and-words_,
but treated it as a single text.  The graph in Figure fig-inaugural_,
used word offset as one of the axes, but this is difficult to interpret.
However, this corpus is actually a collection of 55 texts,
one for each presidential address.  An interesting property of
this collection is its time dimension:

    >>> from nltk.corpus import inaugural
    >>> inaugural.files()
    ('1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', ...)
    >>> [file[:4] for file in inaugural.files()]
    ['1789', '1793', '1797', '1801', '1805', '1809', '1813', '1817', '1821', ...]

Notice that the year of each text appears in its filename.  To get the year
out of the file name, we extracted the first four characters, using ``file[:4]``.

Let's look at how the words `America`:lx: and `citizen`:lx: are used over time.
The following code will count similar words, such as plurals of these words, or
the word `Citizens`:lx: as it would appear at the start of a sentence (how?).
The result is shown in Figure fig-inaugural2_.

    >>> cfd = nltk.ConditionalFreqDist((target, file[:4])
    ...           for file in inaugural.files()
    ...           for w in inaugural.words(file)
    ...           for target in ['america', 'citizen']
    ...           if w.lower().startswith(target))
    >>> cfd.plot()

.. _fig-inaugural2:
.. figure:: ../images/inaugural2.png
   :scale: 20

   Conditional Frequency Distribution for Two Words in the Inaugural Address Corpus


Corpora in Other Languages
--------------------------

NLTK comes with corpora for many languages, though in some cases
you will need to learn how to manipulate character encodings in Python
before using these corpora (see Appendix app-unicode_).

    >>> nltk.corpus.cess_esp.words()
    ['El', 'grupo', 'estatal', 'Electricit\xe9_de_France', ...]
    >>> nltk.corpus.floresta.words()
    ['Um', 'revivalismo', 'refrescante', 'O', '7_e_Meio', ...]
    >>> nltk.corpus.udhr.files()
    ('Abkhaz-Cyrillic+Abkh', 'Abkhaz-UTF8', 'Achehnese-Latin1', 'Achuar-Shiwiar-Latin1',
    'Adja-UTF8', 'Afaan_Oromo_Oromiffa-Latin1', 'Afrikaans-Latin1', 'Aguaruna-Latin1',
    'Akuapem_Twi-UTF8', 'Albanian_Shqip-Latin1', 'Amahuaca', 'Amahuaca-Latin1', ...)
    >>> nltk.corpus.udhr.words('Javanese-Latin1')[11:]
    ['Saben', 'umat', 'manungsa', 'lair', 'kanthi', 'hak', ...]
    >>> nltk.corpus.indian.words('hindi.pos')
    ['\xe0\xa4\xaa\xe0\xa5\x82\xe0\xa4\xb0\xe0\xa5\x8d\xe0\xa4\xa3',
    '\xe0\xa4\xaa\xe0\xa5\x8d\xe0\xa4\xb0\xe0\xa4\xa4\xe0\xa4\xbf\xe0\xa4\xac\xe0\xa4\x82\xe0\xa4\xa7', ...]

The last of these corpora, ``udhr`` contains the Universal Declaration of Human Rights
in over 300 languages.  (Note that the names of the files in this corpus include
information about character encoding, and for now we will stick with texts in ISO Latin-1, or ASCII)

Let's use a conditional frequency distribution to examine the differences in word lengths,
for a selection of languages included in this corpus.
The output is shown in Figure fig-word-len-dist_ (run the program yourself to see a color plot).

    >>> from nltk.corpus import udhr
    >>> languages = ['Chickasaw', 'English', 'German_Deutsch',
    ...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
    >>> cfd = nltk.ConditionalFreqDist((lang, len(word))
    ...          for lang in languages
    ...          for word in udhr.words(lang + '-Latin1'))
    >>> cfd.plot()

.. _fig-word-len-dist:
.. figure:: ../images/word-len-dist.png
   :scale: 25

   Cumulative Word Length Distributions for Several Languages

.. note:: |TRY|
   Pick a language of interest in ``udhr.files()``, and define a variable
   ``raw_text = udhr.raw('Language-Latin1')``.  Now plot a frequency
   distribution of the letters of the text using ``nltk.FreqDist(raw_text).plot()``.

For many other languages, substantial corpora are not yet available.  Often there is
no government or industrial support for developing language resources, and individual
efforts are piecemeal and hard to discover or re-use.  Some languages have no
established writing system, or are endangered.  A good place to check
is the search service of the *Open Language Archives Community*, at ``http://www.language-archives.org/``.
This service indexes the catalogs of dozens of language resource archives and publishers.

.. note::
   The most complete inventory of the world's languages is *Ethnologue*, ``http://www.ethnologue.com/``.

Text Corpus Structure
---------------------

The corpora we have seen exemplify a variety of common corpus structures, summarized in Figure fig-text-corpus-structure_.
The simplest kind lacks any structure: it is just a collection of texts.
Often, texts are grouped into categories that might correspond to genre, source, author, language, etc.
Sometimes these categories overlap, notably in the case of topical categories, since a text can be
relevant to more than one topic.  Occasionally, text collections have temporal structure,
news collections being the most common.


.. _fig-text-corpus-structure:
.. figure:: ../images/text-corpus-structure.png
   :scale: 150

   Common Structures for Text Corpora (one point per text)

|NLTK|\ 's corpus readers support efficient access to a variety of corpora, and can
easily be extended to work with new corpora [REF].  Table tab-corpus_ lists the
basic methods provided by the corpus readers. 

.. table:: tab-corpus

   ===============================  ==========================================================
   Example                          Description
   ===============================  ==========================================================
   ``files()``                      the files of the corpus
   ``categories()``                 the categories of the corpus
   ``abspath(file)``                the location of the given file on disk
   ``words()``                      the words of the whole corpus
   ``words(files=[f1,f2,f3])``      the words of the specified files                  
   ``words(categories=[c1,c2])``    the words of the specified categories                  
   ``sents()``                      the sentences of the specified categories
   ``sents(files=[f1,f2,f3])``      the sentences of the specified files                  
   ``sents(categories=[c1,c2])``    the sentences of the specified categories                  
   ===============================  ==========================================================

   Basic Methods Defined in |NLTK|\ 's Corpus Package
   
.. note::
   For more information about |NLTK|\ 's Corpus Package, type ``help(nltk.corpus.reader)``
   at the Python prompt, or see ``http://nltk.org/doc/guides/corpus.html``.
   You will probably have other text sources, stored in files on your computer or accessible
   via the web.  We'll discuss how to work with these in Chapter chap-words_.

.. _sec-conditional-frequency-distributions:

-----------------------------------
Conditional Frequency Distributions
-----------------------------------

When the texts of a corpus are divided into several categories,
by genre, topic, author, etc, we can maintain separate wordcounts for each category
to enable study of systematic differences between the categories.
In the previous section we did this using |NLTK|\ 's ``ConditionalFreqDist``
data type.  A `conditional frequency distribution`:dt: is a collection
of frequency distributions, each one for a different "condition".
The condition will often be the category of the text.  Figure tally2_
depicts a fragment of a conditional frequency distribution having just
two conditions, one for news text and one for romance text.

.. _tally2:
.. figure:: ../images/tally2.png
   :scale: 50

   Counting Words Appearing in a Text Collection (a conditional frequency distribution)


Counting Words by Genre
-----------------------

In section sec-extracting-text-from-corpora_ we saw a
conditional frequency distribution where the condition
was the section of the Brown Corpus, and for each
condition we counted words:

    >>> cfd = nltk.ConditionalFreqDist((g,w)
                  for g in brown.categories()
                  for w in brown.words(categories=g))

Let's break this down, and look at just two genres, news and romance.  For each
genre, we loop over every word in the genre, producing pairs consisting of
the genre and the word:

    >>> pairs = [(g,w) for g in ['news', 'romance'] for w in brown.words(categories=g)]
    >>> len(pairs)
    170576
    >>> pairs[:4]
    [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]
    >>> pairs[-4:]
    [('romance', 'afraid'), ('romance', 'not'), ('romance', "''"), ('romance', '.')]

We can now use this list of pairs to create a ``ConditionalFreqDist``, and
save it in a variable ``cfd``.  As usual, we can type the name of the
variable to inspect it, and verify it has two conditions:   


    >>> cfd = nltk.ConditionalFreqDist(pairs)
    >>> cfd
    <ConditionalFreqDist with 2 conditions>
    >>> cfd.conditions()
    ['news', 'romance']

Let's access the two conditions, and satisfy ourselves that each is just
a frequency distribution:

    >>> cfd['news']
    <FreqDist with 100554 samples>
    >>> cfd['romance']
    <FreqDist with 70022 samples>
    >>> list(cfd['romance'])
    [',', '.', 'the', 'and', 'to', 'a', 'of', '``', "''", 'was', 'I', 'in', 'he', 'had',
    '?', 'her', 'that', 'it', 'his', 'she', 'with', 'you', 'for', 'at', 'He', 'on', 'him',
    'said', '!', '--', 'be', 'as', ';', 'have', 'but', 'not', 'would', 'She', 'The', ...]
    >>> cfd['romance']['could']
    193

Apart from combining two or more frequency distributions, and being easy to initialize,
a ``ConditionalFreqDist`` provides some useful methods for tabulation and plotting.
We can optionally specify which conditions to display with a ``conditions=`` parameter.
When we omit it, we get all the conditions.

.. note:: |TRY|
   Find out which days of the week are most newsworthy, and which are most romantic.
   Define a variable called ``days`` containing a list of days of the week, i.e.
   ``['Monday', ...]``.  Now tabulate the counts for these words using
   ``cfd.tabulate(samples=days)``.  Now try the same thing using ``plot`` in place of ``tabulate``.

Other Conditions
----------------

The plot in Figure fig-word-len-dist_ is based on a conditional frequency distribution
where the condition is the name of the language
and the counts being plotted are derived from word lengths.
It exploits the fact that the filename for each language is the language name followed
by``'-Latin1'`` (the character encoding).
 
    >>> cfd = nltk.ConditionalFreqDist((lang, len(word))
    ...          for lang in languages
    ...          for word in udhr.words(lang + '-Latin1'))

The plot in Figure fig-inaugural2_ is based on a conditional frequency distribution
where the condition is either of two words `america`:lx: or `citizen`:lx:, and the
counts being plotted are the number of times the word occurs in a particular speech.
It expoits the fact that the filename for each speech, e.g. ``1865-Lincoln.txt``
contains the year as the first four characters.

    >>> cfd = nltk.ConditionalFreqDist((target, file[:4])
    ...           for file in inaugural.files()
    ...           for w in inaugural.words(file)
    ...           for target in ['america', 'citizen']
    ...           if w.lower().startswith(target))

This code will generate the tuple ``('america', '1865')`` for
every instance of a word whose lowercased form starts with "america"
|mdash| such as "Americans" |mdash| in the file ``1865-Lincoln.txt``.

Generating Random Text with Bigrams
-----------------------------------

We can use a conditional frequency distribution to create
a table of bigrams (word pairs).  The ``bigrams()``
function takes a list of words and builds a list of
consecutive word pairs:

    >>> sent = ['In', 'the', 'beginning', 'God', 'created', 'the', 'heaven',
    ...   'and', 'the', 'earth', '.']
    >>> nltk.bigrams(sent)
    [('In', 'the'), ('the', 'beginning'), ('beginning', 'God'), ('God', 'created'),
    ('created', 'the'), ('the', 'heaven'), ('heaven', 'and'), ('and', 'the'),
    ('the', 'earth'), ('earth', '.')]
    
In Figure random_, we
treat each word as a condition, and for each one we effectively
create a frequency distribution over the following words.

The function ``generate_model()``
contains a simple loop to generate text: we set an initial
context, pick the most likely token in that context as our next
word (using ``max()``), and then use that word as our new context.
This simple approach to text generation tends to get stuck in loops;
another method would be to randomly choose the next word from among
the available words.

.. pylisting:: random
   :caption: Generating Random Text in the Style of Genesis

   def generate_model(cfdist, word, num=15):
       for i in range(num):
           print word,
           word = cfdist[word].max()

   >>> bigrams = nltk.bigrams(nltk.corpus.genesis.words('english-kjv.txt'))
   >>> model = nltk.ConditionalFreqDist(bigrams)
   >>> model['living']
   <FreqDist with 16 samples>
   >>> list(model['living'])
   ['substance', ',', '.', 'thing', 'soul', 'creature']
   >>> generate_model(model, 'living')
   living creature that he said , and the land of the land of the land

Summary
-------

.. table:: conditionalfreqdist

   =======================================  ================================================================
   Example                                  Description
   =======================================  ================================================================
   ``cfdist = ConditionalFreqDist(pairs)``  create a conditional frequency distribution
   ``cfdist.conditions()``                  alphabetically sorted list of conditions
   ``cfdist[condition]``                    the frequency distribution for this condition
   ``cfdist[condition][sample]``            frequency for the given sample for this condition
   ``cfdist.tabulate()``                    tabulate the conditional frequency distribution
   ``cfdist.plot()``                        graphical plot of the conditional frequency distribution
   ``cfdist1 < cfdist2``                    samples in ``cfdist1`` occur less frequently than in ``cfdist2``
   =======================================  ================================================================

   Methods Defined for |NLTK|\ 's Conditional Frequency Distributions

.. _sec-defining-functions:

-------------------------
More Python: Reusing Code
-------------------------

By this time you've probably retyped a lot of code.  If you mess up when retyping a complex example you have
to enter it again.  Using the arrow keys to access and modify previous commands is helpful but only goes so
far.  In this section we see two important ways to reuse code: text editors and Python functions.

Creating Programs with a Text Editor
------------------------------------

The Python interative interpreter performs your instructions as soon as you type
them.  Often, it is better to compose a multi-line program using a text editor,
then ask Python to run the whole program at once.  Using |IDLE|, you can do
this by going to the ``File`` menu and opening a new window.  Try this now, and
enter the following one-line program:

::

     msg = 'Monty Python'

Save this program in a file called ``test.py``, then
go to the ``Run`` menu, and select the command ``Run Module``.
The result in the main |IDLE| window should look like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    >>>

Now, where is the output showing the value of ``msg``? The answer is
that the program in ``test.py`` will show a value only if you explicitly tell
it to, using the ``print`` statement. So add another line to
``test.py`` so that it looks as follows:

::

     msg = 'Monty Python'
     print msg

Select ``Run Module`` again, and this time you should get output that
looks like this:

.. doctest-ignore::
    >>> ================================ RESTART ================================
    >>>
    Monty Python
    >>>

From now on, you have a choice of using the interactive interpreter or a
text editor to create your programs.  It is often convenient to test your ideas
using the interpreter, revising a line of code until it does what you expect,
and consulting the interactive help facility.  Once you're ready, you can paste
the code (minus any ``>>>`` prompts) into the text editor,
continue to expand it, and finally save the program
in a file so that you don't have to type it in again later.
Give the file a short but descriptive name, using all lowercase letters and separating
words with underscore, and using the ``.py`` filename extension, e.g. ``monty_python.py``.

.. note::
   Our inline code examples will continue to include the ``>>>`` and ``...`` prompts
   as if we are interacting directly with the interpreter.  As they get more complicated,
   you should instead type them into the editor, without the prompts, and run them
   from the editor as shown above.

Functions
---------

Suppose that you work on analyzing text involves different forms
of the same word, and that part of your program needs to work out
the plural form of a given singular noun.  Suppose it needs to do this
work in two places, once when it is processing some texts, and again
when it is processing user input.

Rather than repeating the same code several times over, it is more
efficient and reliable to localize this work inside a `function`:dt:.
A function is just a named block of code that performs some well-defined
task.  It usually has some inputs, also known as `parameters`:dt:,
and it may produce a result, also known as a `return value`:dt:.
We define a function using the keyword ``def`` followed by the
function name and any input parameters, followed by the body of the
function.  Here's the function we saw in section sect-computing-with-language-texts-and-words_:

    >>> def score(text):
    ...     return len(text) / len(set(text))

We use the keyword ``return`` to indicate the value that is
produced as output by the function.  In the above example,
all the work of the function is done in the ``return`` statement.
Here's an equivalent definition which does the same work
using multiple lines of code.  We'll change the parameter name
to remind you that this is an arbitrary choice:

    >>> def score(my_text_data):
    ...     word_count = len(my_text_data)
    ...     vocab_size = len(set(my_text_data))
    ...     richness_score = word_count / vocab_size
    ...     return richness_score

Notice that we've created some new variables inside the body of the function.
These are *local variables* and are not accessible outside the function.
Notice also that defining a function like this produces no output.
Functions do nothing until they are "called" (or "invoked").     

Let's return to our earlier scenario, and actually define a simple plural
function.  The function ``plural()`` in Figure plural_
takes a singular noun and generates a plural form (one which is not always
correct).

.. pylisting:: plural
   :caption: Example of a Python function

   def plural(word):
       if word.endswith('y'):
           return word[:-1] + 'ies'
       elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:
           return word + 'es'
       elif word.endswith('an'):
           return word[:-2] + 'en'
       return word + 's'

   >>> plural('fairy')
   'fairies'
   >>> plural('woman')
   'women'

(There is much more to be said about functions, but
we will hold off until Section sec-functions_.)

Modules
-------

Over time you will find that you create a variety of useful little text processing functions,
and you end up copy-pasting them from old programs to new ones.  While file contains the
latest version of the function you want to use?
It makes life a lot easier if you can collect your work into a single place, and
access previously defined functions without any copying and pasting.

To do this, save your function(s) in a file called (say) ``textproc.py``.
Now, you can access your work simply by importing it from the file:

.. doctest-ignore::
    >>> from textproc import plural
    >>> plural('wish')
    wishes
    >>> plural('fan')
    fen

Our plural function has an error, and we'll need to fix it.  This time, we won't
produce another version, but instead we'll fix the existing one.  Thus, at every
stage, there is only one version of our plural function, and no confusion about
which one we should use.

A collection of variable and function definitions in a file is called a Python
`module`:dt:.  A collection of related modules is called a `package`:dt:.
|NLTK|\ 's code for processing the Brown Corpus is an example of a module,
and its collection of code for processing all the different corpora is
an example of a package.  |NLTK| itself is a set of packages, sometimes
called a `library`:dt:.

.. _sec-lexical-resources:

-----------------
Lexical Resources
-----------------

A lexicon, or lexical resource, is a collection of words and/or phrases along
with associated information such as part of speech and sense definitions.
Lexical resources are secondary to texts, and are usually created and enriched with the help
of texts.  For example, ``sorted(set(text))`` builds the vocabulary of a text,
and ``FreqDist(text)`` counts the frequency of each word in the text.  Both
of these are simple lexical resources.  Similarly, a concordance 
(Section sect-computing-with-language-texts-and-words_)
gives us information about word usage that might help in the preparation of
a dictionary.

The simplest kind of lexicon is nothing more than a sorted list of words.
Sophisticated lexicons include complex structure within and across
the individual entries.  In this section we'll look at some lexical resources
included with |NLTK|.

Wordlist Corpora
----------------

|NLTK| includes some corpora that are nothing more than wordlists.
The Words corpus is the ``/usr/dict/words`` file from Unix, used by
some spell checkers.  We can use it to find unusual or mis-spelt
words in a text corpus, as shown in Figure unusual_.

.. pylisting:: unusual
   :caption: Using a Lexical Resource to Filter a Text

    def unusual_words(text):
        text_vocab = set(w.lower() for w in text if w.isalpha())
        english_vocab = set(w.lower() for w in nltk.corpus.words.words())
        unusual = text_vocab.difference(english_vocab)
        return sorted(unusual)
    
    >>> unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))
    ['abbeyland', 'abhorrence', 'abominably', 'abridgement', 'accordant', 'accustomary',
    'adieus', 'affability', 'affectedly', 'aggrandizement', 'alighted', 'allenham',
    'amiably', 'annamaria', 'annuities', 'apologising', 'arbour', 'archness', ...]
    >>> unusual_words(nltk.corpus.nps_chat.words())
    ['aaaaaaaaaaaaaaaaa', 'aaahhhh', 'abou', 'abourted', 'abs', 'ack', 'acros',
    'actualy', 'adduser', 'addy', 'adoted', 'adreniline', 'ae', 'afe', 'affari', 'afk',
    'agaibn', 'agurlwithbigguns', 'ahah', 'ahahah', 'ahahh', 'ahahha', 'ahem', 'ahh', ...]

There is also a corpus of `stopwords`:dt:, high-frequency glue words that we sometimes
want to filter out of a text before further processing.

    >>> from nltk.corpus import stopwords
    >>> stopwords.words('english')
    ['a', "a's", 'able', 'about', 'above', 'according', 'accordingly', 'across',
    'actually', 'after', 'afterwards', 'again', 'against', "ain't", 'all', 'allow',
    'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', ...]

Let's define a function to compute what fraction of words in a text are *not* in the
stopwords list:

    >>> def content_fraction(text):
    ...     stopwords = nltk.corpus.stopwords.words('english')
    ...     content = [w for w in text if w.lower() not in stopwords]
    ...     return 1.0 * len(content) / len(text)
    ...    
    >>> content_fraction(nltk.corpus.reuters.words())
    0.65997695393285261

Thus, with the help of stopwords we filter out a third of the words of the text.
Notice that we've combined two different kinds of corpus here, using a lexical
resource to filter the content of a text corpus.

.. _fig-target:
.. figure:: ../images/target.png
   :scale: 25

   A Word Puzzle Known as "Target" 

A wordlist is useful for solving word puzzles, such as the one in Figure fig-target_.
Our program iterates through every word and, for each one, checks whether it meets
the conditions.  The obligatory letter and length constraint are easy to check (and we'll
only look for words with six or more letters here).
It is trickier to check that candidate solutions only use combinations of the
supplied letters, especially since some letters appear twice (here, the letter `v`:lx:).
We use the ``FreqDist`` comparison method to check that the frequency of each
*letter* in the candidate word is less than or equal to the frequency of the
corresponding letter in the puzzle.

    >>> puzzle_letters = nltk.FreqDist('egivrvonl')
    >>> obligatory = 'r'
    >>> wordlist = nltk.corpus.words.words()
    >>> [w for w in wordlist if len(w) >= 6
                             and obligatory in w
                             and nltk.FreqDist(w) <= puzzle_letters]
    ['glover', 'gorlin', 'govern', 'grovel', 'ignore', 'involver', 'lienor',
    'linger', 'longer', 'lovering', 'noiler', 'overling', 'region', 'renvoi',
    'revolving', 'ringle', 'roving', 'violer', 'virole']

.. note:: |TRY|
   Can you think of an English word that contains `gnt`:lx:?  Write Python code
   to find any such words in the wordlist.

One more wordlist corpus is the Names corpus, containing 8,000 first names categorized by gender.
The male and female names are stored in separate files.  Let's find names which appear
in both files, i.e. names that are ambiguous for gender:

    >>> names = nltk.corpus.names
    >>> names.files()
    ('female.txt', 'male.txt')
    >>> male_names = names.words('male.txt')
    >>> female_names = names.words('female.txt')
    >>> [w for w in male_names if w in female_names]
    ['Abbey', 'Abbie', 'Abby', 'Addie', 'Adrian', 'Adrien', 'Ajay', 'Alex', 'Alexis',
    'Alfie', 'Ali', 'Alix', 'Allie', 'Allyn', 'Andie', 'Andrea', 'Andy', 'Angel',
    'Angie', 'Ariel', 'Ashley', 'Aubrey', 'Augustine', 'Austin', 'Averil', ..]

It is well known that names ending in the letter `a`:lx: are almost always female.
We can see this and some other patterns in the graph in Figure fig-cfd-gender_,
produced by the following code:

    >>> cfd = nltk.ConditionalFreqDist((file, name[-1])
    ...           for file in names.files()
    ...           for name in names.words(file))
    >>> cfd.plot()

.. _fig-cfd-gender:
.. figure:: ../images/cfd-gender.png
   :scale: 25

   Frequency of Final Letter of Female vs Male Names


A Pronouncing Dictionary
------------------------

As we have seen, the entries in a wordlist lack internal structure |mdash| they are just words.
A slightly richer kind of lexical resource is a table (or spreadsheet), containing a word
plus some properties in each row.  |NLTK| includes the CMU Pronouncing Dictionary,
used by speech synthesizers. 

    >>> entries = nltk.corpus.cmudict.entries()
    >>> len(entries)
    127069
    >>> for entry in entries[40000:40010]:
    ...     print entry
    ... 
    ('fir', 1, ('F', 'ER1'))
    ('fire', 1, ('F', 'AY1', 'ER0'))
    ("fire's", 1, ('F', 'AY1', 'ER0', 'Z'))
    ('fire', 2, ('F', 'AY1', 'R'))
    ('firearm', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M'))
    ('firearm', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M'))
    ('firearms', 1, ('F', 'AY1', 'ER0', 'AA2', 'R', 'M', 'Z'))
    ('firearms', 2, ('F', 'AY1', 'R', 'AA2', 'R', 'M', 'Z'))
    ('fireball', 1, ('F', 'AY1', 'ER0', 'B', 'AO2', 'L'))
    ('fireball', 2, ('F', 'AY1', 'R', 'B', 'AO2', 'L'))

For each word, this lexicon provides a pronunciation number and a list of phonetic
codes |mdash| distinct labels for each contrastive sound |mdash|
known as `phones`:lx:.  Observe that `fire`:lx: has two pronunciations:
the one-syllable ``F AY1 R``, and the two-syllable ``F AY1 ER0``.
The symbols in the CMU Pronouncing Dictionary are from the *Arpabet*,
described in more detail at ``http://en.wikipedia.org/wiki/Arpabet``

Each entry consists of three pieces of information, and we can
process these individually, using a more complex version of the ``for`` statement.
Instead of writing ``for entry in entries:``, we replace
``entry`` with *three* variable names.  The three pieces of a single
entry are assigned to each of the three variables.  One of the
variable names is "_" and we'll use this name when we don't
plan to do anything with the variable later.

    >>> for w, _, pron in entries:
    ...     if len(pron) == 3:
    ...         ph1, ph2, ph3 = pron
    ...         if ph1 == 'P' and ph3 == 'T':
    ...             print w, ph2,
    ...
    pait EY1 pat AE1 pate EY1 patt AE1 peart ER1 peat IY1 peet IY1 peete IY1 pert ER1
    pet EH1 pete IY1 pett EH1 piet IY1 piette IY1 pit IH1 pitt IH1 pot AA1 pote OW1
    pott AA1 pout AW1 puett UW1 purt ER1 put UH1 putt AH1

The above program scans the lexicon for entries with a pronunciation consisting of
three phones (``len(pron) == 3``).  If the condition is true, we assign the contents
of ``pron`` to three new variables ``ph1``, ``ph2`` and ``ph3``.  Notice the unusual
form of the statement which does that work: ``ph1, ph2, ph3 = pron``.

Here's another example of the same ``for`` statement, this time used inside a list
comprehension.  This program finds all words whose pronunciation ends with a syllable
sounding like `nicks`:lx:.  You could use this method to find rhyming words.

    >>> syllable = ('N', 'IH0', 'K', 'S')
    >>> [w for w, _, pron in entries if pron[-4:] == syllable]
    ["atlantic's", 'audiotronics', 'avionics', 'beatniks', 'calisthenics', 'centronics',
    'chetniks', "clinic's", 'clinics', 'conics', 'cynics', 'diasonics', "dominic's",
    'ebonics', 'electronics', "electronics'", 'endotronics', "endotronics'", 'enix', ...]

Notice that the one pronunciation is spelt in several ways: `nics`:lx:, `niks`:lx:, `nix`:lx:,
even `ntic's`:lx: with a silent `t`:lx:, for the word `atlantic's`:lx:.  Let's look for some other
mismatches between pronunciation and writing.  Can you summarize the purpose of
the following examples and explain how they work?

    >>> [w for w, _, pron in entries if pron[-1] == 'M' and w[-1] == 'n']
    ['autumn', 'column', 'condemn', 'damn', 'goddamn', 'hymn', 'solemn']
    >>> sorted(set(w[:2] for w, _, pron in entries if pron[0] == 'N' and w[0] != 'n'))
    ['gn', 'kn', 'mn', 'pn']

The phones contain digits, to represent 
primary stress (``1``), secondary stress (``2``) and no stress (``0``).
As our final example, we define a function to extract the stress digits
and then scan our lexicon to find words having a particular stress pattern.

    >>> def stress(pron):
    ...     return [int(char) for phone in pron for char in phone if char.isdigit()] 
    >>> [w for w, _, pron in entries if stress(pron) == [0, 1, 0, 2, 0]]
    ['abbreviated', 'abbreviating', 'accelerated', 'accelerating', 'accelerator',
    'accentuated', 'accentuating', 'accommodated', 'accommodating', 'accommodative',
    'accumulated', 'accumulating', 'accumulative', 'accumulator', 'accumulators', ...]
    >>> [w for w, _, pron in entries if stress(pron) == [0, 2, 0, 1, 0]]
    ['abbreviation', 'abbreviations', 'abomination', 'abortifacient', 'abortifacients',
    'academicians', 'accommodation', 'accommodations', 'accreditation', 'accreditations',
    'accumulation', 'accumulations', 'acetylcholine', 'acetylcholine', 'adjudication', ...]

Note that this example has a user-defined function inside the condition of
a list comprehension.

We can use any lexical resource to process a text, e.g. to filter out words having
some lexical property (like nouns), or mapping every word of the text.
For example, the following text-to-speech function looks up each word
of the text in the pronunciation dictionary.

    >>> prondict = nltk.corpus.cmudict.transcriptions()
    >>> text = ['natural', 'language', 'processing']
    >>> [ph for w in text for ph in prondict[w][0]]
    ['N', 'AE1', 'CH', 'ER0', 'AH0', 'L', 'L', 'AE1', 'NG', 'G', 'W', 'AH0', 'JH',
    'P', 'R', 'AA1', 'S', 'EH0', 'S', 'IH0', 'NG']
    

[Summary of tabular lexicons; forward reference to discussion about processing CSV files]

Comparative Wordlists
---------------------

Another example of a tabular lexicon is the `comparative wordlist`:dt:, a spreadsheet
with one row for each word, and a column for each of several different languages.
|NLTK| includes so-called `Swadesh wordlists`:dt:, lists of about 200 common words
in several languages.  The languages are identified using an ISO 639 two-letter code.

    >>> from nltk.corpus import swadesh
    >>> swadesh.files()
    ('be', 'bg', 'bs', 'ca', 'cs', 'cu', 'de', 'en', 'es', 'fr', 'hr', 'it', 'la', 'mk', 'nl',
    'pl', 'pt', 'ro', 'ru', 'sk', 'sl', 'sr', 'sw', 'uk')
    >>> swadesh.words('en')
    ['I', 'you (singular), thou', 'he', 'we', 'you (plural)', 'they', 'this', 'that', 'here',
    'there', 'who', 'what', 'where', 'when', 'how', 'not', 'all', 'many', 'some', 'few', 'other',
    'one', 'two', 'three', 'four', 'five', 'big', 'long', 'wide', 'thick', 'heavy', 'small', ...] 

We can also access cognate words from multiple languages using the ``entries()`` method,
specifying a list of languages.  With one further step we can convert this into a simple dictionary.

    >>> swadesh.entries(['en', 'fr'])
    [('I', 'je'), ('you (singular), thou', 'tu, vous'), ('he', 'il'), ('we', 'nous'),
    ('you (plural)', 'vous'), ('they', 'ils, elles'), ('this', 'ceci'), ('that', 'cela'), ...]
    >>> en2fr = dict(swadesh.entries(['en', 'fr']))
    >>> en2fr['dog']
    'chien'
    >>> en2fr['throw']
    'jeter'

(We will explore this example in the context of our systematic presentation of Python
dictionaries in Section sec-dictionaries_.)
We can compare words in various Germanic and Romance languages:

    >>> languages = ['en', 'de', 'nl', 'ca', 'es', 'fr', 'pt', 'it', 'la']
    >>> for i in [139, 140, 141, 142]:
    ...     print swadesh.entries(languages)[i]
    ... 
    ('say', 'sagen', 'zeggen', 'dir', 'decir', 'dire', 'dizer', 'dire', 'dicere')
    ('sing', 'singen', 'zingen', 'cantar', 'cantar', 'chanter', 'cantar', 'cantare', 'canere')
    ('play', 'spielen', 'spelen', 'jugar', 'jugar', 'jouer', 'jogar, brincar', 'giocare', 'ludere')
    ('float', 'schweben', 'zweven', 'flotar', 'flotar', 'flotter', 'flutuar, boiar', 'galleggiare', 'fluctuare')



Shoebox and Toolbox Lexicons
----------------------------

Perhaps the single most popular tool used by linguists for managing data
is *Toolbox*, previously known as *Shoebox*
(freely downloadable from ``http://www.sil.org/computing/toolbox/``).
A Toolbox file consists of a collection of entries,
where each entry is made up of one or more fields.
Most fields are optional or repeatable, which means that this kind of
lexical resource cannot be treated as a table or spreadsheet.

Here is an example of an entry in the dictionary of the Rotokas language, for
the word `kaa`:lx: meaning "to gag":

    >>> from nltk.corpus import toolbox
    >>> toolbox.entries('rotokas.dic')
    [('kaa', [('ps', 'V'), ('pt', 'A'), ('ge', 'gag'), ('tkp', 'nek i pas'), ('dcsv', 'true'),
    ('vx', '1'), ('sc', '???'), ('dt', '29/Oct/2005'),
    ('ex', 'Apoka ira kaaroi aioa-ia reoreopaoro.'),
    ('xp', 'Kaikai i pas long nek bilong Apoka bikos em i kaikai na toktok.'),
    ('xe', 'Apoka is gagging from food while talking.')]), ...]

Each field has a label such as ``ps`` (part of speech) and contents, such as ``V`` (verb).
The ``ge`` field contains the English gloss, and the last three fields contain
an example sentence in Rotokas and its translations into Tok Pisin and English.

The loose structure of Toolbox files makes it hard for us to do much more with them
at this stage.  XML provides a powerful way to process this kind of corpus and
we will return to this topic in Chapter chap-data_.


.. note::
   The Rotokas language is spoken on the island of Bougainville, Papua New Guinea.
   This lexicon was contributed to |NLTK| by Stuart Robinson.   
   Rotokas is notable for having the smallest alphabet of any written language
   (12 letters), ``http://en.wikipedia.org/wiki/Rotokas_language``
    


.. _sec-dictionaries:

------------------------
The Dictionary Data Type
------------------------

A text, as we have seen, is treated in Python as a list of words.
An important property of lists is that we can "look up" a particular
item by giving its index, e.g. ``text1[100]``.  Notice how we specify
a number, and get back a word.  We can think of a list as a simple
kind of table, as shown in Figure maps01_.

.. _maps01:
.. figure:: ../images/maps01.png
   :scale: 30

   List Look-up

Contrast this situation with frequency distributions (section computing-with-language-simple-statistics_),
where we specify a word, and get back a number, e.g. ``fdist['monstrous']``, which
tells us the number of times a given word has occurred in a text.  Look-up using words is
familiar to anyone who has used a dictionary.  Some more examples are shown in
Figure maps02_.

.. _maps02:
.. figure:: ../images/maps02.png
   :scale: 22

   Dictionary Look-up

In the case of a phonebook, we look up an entry using a `name`:em:,
and get back a number.  When we type a domain name in a web browser,
the computer looks this up to get back an IP address.  A word
frequency table allows us to look up a word and find its frequency in
a text collection.  In all these cases, we are mapping from names to
numbers, rather than the other way round.
In general, we would like to be able to map between
arbitrary types of information.  Table linguistic-objects_ lists a variety
of linguistic objects, along with what they map.

.. table:: linguistic-objects

    +--------------------+-------------------------------------------------+
    | Linguistic Object  |                      Maps                       |
    |                    +------------+------------------------------------+
    |                    |    from    | to                                 |
    +====================+============+====================================+
    |Document Index      |Word        |List of pages (where word is found) |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Thesaurus           |Word sense  |List of synonyms                    |
    +--------------------+------------+------------------------------------+
    |Dictionary          |Headword    |Entry (part of speech, sense        |
    |                    |            |definitions, etymology)             |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+
    |Comparative Wordlist|Gloss term  |Cognates (list of words, one per    |
    |                    |            |language)                           |
    +--------------------+------------+------------------------------------+
    |Morph Analyzer      |Surface form|Morphological analysis (list of     |
    |                    |            |component morphemes)                |
    |                    |            |                                    |
    +--------------------+------------+------------------------------------+

    Linguistic Objects as Mappings from Keys to Values

Most often, we are mapping from a "word" to some structured object.
For example, a document index maps from a word (which we can represent
as a string), to a list of pages (represented as a list of integers).
In this section, we will see how to represent such mappings in Python.

Dictionaries in Python
----------------------

Python provides a `dictionary`:dt: data type that can be used for
mapping between arbitrary types.  It is like a conventional dictionary,
in that it gives you an efficient way to look things up.  However,
as we will see, it has a much wider range of uses.  Let's revisit
the CMU Pronouncing Dictionary, this time using Python's dictionary
interface:

    >>> prondict = nltk.corpus.cmudict.transcriptions()
    >>> prondict['fire']
    [('F', 'AY1', 'ER0'), ('F', 'AY1', 'R')]
    >>> prondict['blog']
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    KeyError: 'blog'
    >>> prondict['blog'] = [('B', 'L', 'AA1', 'G')]
    >>> prondict['blog']
    [('B', 'L', 'AA1', 'G')]

We look up a dictionary by specifying its name, followed by a `key`:dt:
(such as the word `fire`:lx:) inside square brackets: ``prondict['fire']``.
If we try to look up a non-existent key, we get a ``KeyError``,
as we did when indexing a list with an integer that was too large.
The word `blog`:lx: is missing from the pronouncing dictionary,
so we tweak our version by assigning a value for this key
(this has no effect on the |NLTK| corpus; next time we access it,
`blog`:lx: will still be absent).

Now let's experiment with dictionaries on a smaller scale, building up
one of our own.
Here we define ``pos`` to be an empty dictionary and then add four
entries to it, specifying the part-of-speech of some words.  We add
entries to a dictionary using the familiar square bracket notation:

    >>> pos = {}
    >>> pos['colorless'] = 'adjective'
    >>> pos['ideas'] = 'noun'
    >>> pos['sleep'] = 'verb'
    >>> pos['furiously'] = 'adverb'

So, for example, ``pos['colorless'] = 'adjective'`` says that
the part-of-speech of `colorless`:lx: is "adjective", or more
specifically, that the look-up value of the key ``'colorless'`` in ``pos``
is the value ``'adjective'``.  Now we can look up our dictionary
using these keys:

    >>> pos['ideas']
    'noun'
    >>> pos['colorless']
    'adjective'
    >>> pos['missing']
    Traceback (most recent call last):
      File "<stdin>", line 1, in ?
    KeyError: 'missing'

A Closer Look at Python Dictionaries
------------------------------------

We have just seen how to access a dictionary entry, and what happens
when an entry does not exist.  This raises an important question.
Unlike lists and strings, where we
can use ``len()`` to work out which integers will be legal indices, how
do we work out the legal keys for a dictionary?  We simply convert the
dictionary to a list, or use the dictionary in a context where a list
is expected:

    >>> list(pos)
    ['ideas', 'furiously', 'colorless', 'sleep']
    >>> sorted(pos)
    ['colorless', 'furiously', 'ideas', 'sleep']
    >>> [w for w in pos if w.endswith('s')]
    ['colorless', 'ideas']

.. note::
   When you type `list(pos)` you might see a different order
   to the one shown above.  This is because
   dictionaries are not sequences but mappings (cf Figure maps02_),
   and the keys are not inherently ordered.  If we want
   to see them in order, we simply need to sort them.

The last example above used a ``for`` loop to iterate over all keys
in the dictionary.  We can also use the ``for`` loop in another way,
just we did for printing lists:

    >>> for word in sorted(pos):
    ...     print word ":", pos[word]
    ... 
    colorless: adjective
    furiously: adverb
    sleep: verb
    ideas: noun

We can see what the contents of the dictionary look like by inspecting
the variable ``pos``.

    >>> pos
    {'furiously': 'adverb', 'ideas': 'noun', 'colorless': 'adjective', 'sleep': 'verb'}

Here, the contents of the dictionary are shown as `key-value
pairs`:dt:, separated by a colon.  As you can see,  the order
of the key-value pairs is different
from the order in which they were originally entered.

We can use the same key-value pair format to create a dictionary:

    >>> pos = {'colorless': 'adjective', 'ideas': 'noun', 'sleep': 'verb', 'furiously': 'adverb'}

Using the dictionary methods ``keys()``, ``values()`` and ``items()``,
we can access the keys and values as separate lists,
and also the key-value pairs:

    >>> pos.keys()
    ['colorless', 'furiously', 'sleep', 'ideas']
    >>> pos.values()
    ['adjective', 'adverb', 'verb', 'noun']
    >>> pos.items()
    [('colorless', 'adjective'), ('furiously', 'adverb'), ('sleep', 'verb'), ('ideas', 'noun')]
    >>> for key, val in sorted(pos.items()):
    ...     print key ":", val
    ...
    colorless: adjective
    furiously: adverb
    ideas: noun
    sleep: verb

Note that keys are forced to be unique.
Suppose we try to use a dictionary to store the fact that the
word `sleep`:lx: can be used as a verb and as a noun:

    >>> pos['sleep'] = 'verb'
    >>> pos['sleep'] = 'noun'
    >>> pos['sleep']
    'noun'

Initially, ``pos['sleep']`` is given the value ``'verb'``, and this is
immediately overwritten with the new value ``'noun'``.
In other words, there is only one entry for ``'sleep'``.
If we wanted to store multiple values in that entry, we could use a list,
e.g. ``pos['sleep'] = ['noun', 'verb']``.  In fact, this is what we
saw at the start of this section for the CMU Pronouncing Dictionary,
so that we could have multiple pronunciations for a single word.

Converting Lists to Dictionaries
--------------------------------

We've just seen how ``list(pos)`` created a list of keys from a dictionary
consisting of key-value mappings.  Sometimes we need to go the other way,
and convert a stream of input tokens into a dictionary, in order to be able
to conveniently and quickly look them up later.  We do this by creating
a list of pairs, and converting it to a dictionary using ``dict()``: 

    >>> from nltk.corpus import swadesh
    >>> word_pairs = swadesh.entries(['fr', 'en'])
    >>> word_pairs
    [('je', 'I'), ('tu, vous', 'you (singular), thou'), ('il', 'he'), ('nous', 'we'), ...]
    >>> translate = dict(word_pairs)
    >>> translate['chien']
    'dog'
    >>> translate['jeter']
    'throw'

We can make our simple translator more useful by adding other source languages.
Let's get the German-English and Spanish-English pairs, convert each to a
dictionary, then *update* our original ``translate`` dictionary with these
additional mappings:

    >>> translate.update(dict(swadesh.entries(['de', 'en'])))
    >>> translate.update(dict(swadesh.entries(['es', 'en'])))
    >>> translate['Hund']
    'dog'
    >>> translate['perro']
    'dog'
    

Incrementally Updating a Dictionary
----------------------------------- 

We can use dictionaries to count word occurrences, emulating the
method used for tallying words (Figure tally_).
We begin by initializing
an empty dictionary, then process each word of the text.  If the word hasn't
been seen before, we add it to our list with a zero count.  If we've
seen it before we increment its count using the ``+=`` operator.

.. pylisting:: dictionary
   :caption: Incrementally Updating a Dictionary, and Sorting by Value

    >>> counts = {}
    >>> for word in nltk.corpus.gutenberg.words('shakespeare-macbeth.txt'):
    ...     if word not in counts:
    ...         counts[word] = 0
    ...     counts[word] += 1
    ...
    >>> counts['Scotland']
    12
    >>> counts['the']
    692
    >>> list(counts)
    ['Lead', 'doubts', 'felt', 'hath', 'protest', 'sleep', 'thirst', 'Barke', 'hate',
    'goodnesse', 'forget', 'whose', 'Hose', 'solliciting', 'euery', 'Keepes', ...]
    
    >>> from operator import itemgetter
    >>> sorted(counts.items(), key=itemgetter(1), reverse=True)
    [(',', 1962), ('.', 1235), ("'", 637), ('the', 531), (':', 477), ('and', 376),
    ('I', 333), ('of', 315), ('to', 311), ('?', 241), ('d', 224), ('a', 214),
    ('you', 184), ('in', 173), ('my', 170), ('And', 170), ('is', 166), ...]
    >>> [w for w,c in sorted(counts.items(), key=itemgetter(1), reverse=True)]
    [',', '.', "'", 'the', ':', 'and', 'I', 'of', 'to', '?', 'd', 'a', 'you', 'in',
    'my', 'And', 'is', 'that', 'not', 'it', 'Macb', 'with', 's', 'his', 'be', 'The',
    'haue', 'me', 'your', 'our', '-', 'him', 'for', 'That', 'Enter', 'this', 'he', ...]

The listing in Figure dictionary_ illustrates an important idiom for sorting a dictionary in terms of its
values, to show the words in decreasing order of frequency.  We use the ``sorted()``
function, with ``key`` and ``reverse`` parameters.  The sort key is the second element of
each pair, which we would normally access using an index ``[1]``.  We can't use this expression
on its own (i.e. ``key=[1]``) since ``[1]`` looks like a list containing the integer 1.
Instead we use ``itemgetter(1)``, a function which does the same thing:

    >>> pair = ('?', 241)
    >>> pair[1]
    241
    >>> itemgetter(1)(pair)
    241

The listing in Figure dictionary_ illustrates an important programming idiom
for working with dictionaries, and we'll give a schematic version here:

|    ``my_dictionary = {}``
|    ``for`` *item* ``in`` *sequence*\ ``:``
|        ``if`` *item_key* ``not in my_dictionary:``
|            ``my_dictionary[``\ *item_key*\ ``] =`` *empty_value*
|        *update* ``my_dictionary[``\ *item_key*\ ``]`` *with information about item*

Here's another example, where we index the words according to their last two letters:

    >>> last_letters = {}
    >>> for word in words:
    ...     key = word[-2:]
    ...     if key not in last_letters:
    ...         last_letters[key] = []
    ...     last_letters[key].append(word)
    ...
    >>> last_letters['ly']
    ['abjectly', 'ably', 'abnormally', 'abortively', 'abruptly', 'absently', 'absolutely',
    'abstractly', 'absurdly', 'abundantly', 'abysmally', 'academically', 'acceptably', ...]
    >>> last_letters['zy']
    ['breezy', 'buzzy', 'cozy', 'crazy', 'dizzy', 'frenzy', 'fuzzy', 'hazy', 'jazzy', ...]

The following example uses the same pattern to create an anagram dictionary.
(You might experiment with the third line to get an idea of why this program works.)

    >>> anagrams = {}
    >>> for word in words:
    ...     key = ''.join(sorted(word))
    ...     if key not in anagrams:
    ...         anagrams[key] = []
    ...     anagrams[key].append(word)
    ...
    >>> anagrams['aegilnrt']
    ['alerting', 'altering', 'integral', 'relating', 'triangle']

Frequency Distributions
-----------------------

|NLTK|\ 's frequency distribution support (``nltk.FreqDist``) is just
a special kind of dictionary which has extra functions for sorting
and plotting that are needed in language processing.  Instead of the
lengthy code in Figure dictionary_, we can write the following:

    >>> text = nltk.corpus.gutenberg.words('shakespeare-macbeth.txt')
    >>> counts = nltk.FreqDist(text)
    >>> counts['Scotland']
    12
    >>> list(counts)
    [',', '.', "'", 'the', ':', 'and', 'I', 'of', 'to', '?', 'd', 'a', 'you', 'in', 'my',
    'And', 'is', 'that', 'not', 'it', 'Macb', 'with', 's', 'his', 'be', 'The', 'haue', ...]

Notice that we can initialize a ``FreqDist`` with a list, without bothering
to expicitly process the items of the list one at a time in our program.
Unlike a regular dictionary, when we convert a ``FreqDist`` into a list, the
keys appear in order of decreasing frequency.

Inverting a Dictionary
----------------------

Dictionaries support efficient lookup, so long as you want to get the value for
any key.  If ``d`` is a dictionary and ``k`` is a key, we type ``d[k]`` and
immediately obtain the value.  Finding a key given a value is slower and more
cumbersome:

    >>> [key for key,value in counts.items() if value == 16]
    ['call', 'sleepe', 'take', 'where', 'Your', 'Father', 'looke', 'owne']

If we expect to do this kind of "reverse lookup" often, it helps to construct
a dictionary that maps values to keys.  In the case that no two keys have
the same value, this is an easy thing to do.  We just get all the key-value
pairs in the dictionary, and create a new dictionary of value-key pairs:

    >>> pos2 = dict((value, key) for (key, value) in pos.items())
    >>> pos2['noun']
    'ideas'

Let's make our part of speech dictionary a bit more realistic and add some more words,
to create the situation where multiple keys have the same value.  The above method
will not work in this case (why not?).  Instead, we need to use our method of incrementally
updating a dictionary, as follows:

    >>> pos.update({'cats': 'noun', 'scratch': 'verb', 'peacefully': 'adverb', 'old': 'adjective'})
    >>> pos2 = {}
    >>> for key,value in pos.items():
    ...     if value not in pos2:
    ...         pos2[value] = []
    ...     pos2[value].append(key)
    ... 
    >>> pos2['adverb']
    ['peacefully', 'furiously']

Now, we have inverted the ``pos`` dictionary, and can look up any part of speech and find
all words having that part of speech.

Summary
-------

Thanks to their versatility, Python dictionaries are extremely useful in most areas of |NLP|.
We already made heavy use of dictionaries in Chapter chap-introduction_, since
|NLTK|\ 's ``FreqDist`` objects are just a special case of dictionaries for counting things.
Table tab-dict_ lists the most important dictionary methods you should know. 

.. table:: tab-dict

   ==================================  ==========================================================
   Example                             Description
   ==================================  ==========================================================
   ``d = {}``                          create an empty dictionary and assign it to ``d``
   ``d[key] = value``                  assign a value to a given dictionary key
   ``list(d)``, ``d.keys()``           the list of keys of the dictionary
   ``key in d``                        test whether a particular key is in the dictionary
   ``d.values()``                      the list of values in the dictionary
   ``dict([(k1,v1), (k2,v2), ...])``   create a dictionary from a list of key-value pairs
   ==================================  ==========================================================

   Summary of Python's Dictionary Methods
   

---------------
WordNet (NOTES)
---------------

.. note::
   NLTK's WordNet interface is being changed; this section discusses the existing
   interface, and will be rewritten in the next version.

.. note::
   (Instructions on using WordNet graphical browser; URL for online version.)

`WordNet`:idx: is a semantically-oriented dictionary of English,
similar to a traditional thesaurus but with a richer structure.
WordNet groups words into synonym sets, or `synsets`:dt:, each with
its own definition and with links to other synsets.
WordNet 3.0 data is distributed with NLTK, and includes 117,659 synsets.

Although WordNet was originally developed for research
in psycholinguistics, it is widely used in NLP and Information Retrieval.
WordNets are being developed for many other languages, as documented
at ``http://www.globalwordnet.org/``.

Senses and Synonyms
-------------------

Consider the sentence in carex1_.
If we replace the word `motorcar`:lx: in carex1_ by `automobile`:lx:,
to get carex2_, the meaning of the sentence stays pretty much the same:

.. ex::
   .. _carex1:
   .. ex::
      Benz is credited with the invention of the motorcar.

   .. _carex2:
   .. ex::
      Benz is credited with the invention of the automobile.

Since everything else in the sentence has remained unchanged, we can
conclude that the words `motorcar`:lx: and `automobile`:lx: have the
same meaning, i.e. they are `synonyms`:dt:. 

In order to look up the senses of a word, we need to pick
a part of speech for the word.  WordNet contains four dictionaries: ``N``
(nouns), ``V`` (verbs), ``ADJ`` (adjectives), and ``ADV``
(adverbs). To simplify our discussion, we will focus on the ``N``
dictionary here.  Let's look up `motorcar`:lx: in the ``N`` dictionary.

    >>> from nltk import wordnet
    >>> car = wordnet.N['motorcar']
    >>> car
    motorcar (noun)

The variable ``car`` is now bound to a ``Word`` object.
Words will often have more than sense, where each
sense is represented by a synset. However,
`motorcar`:lx: only has one sense in WordNet, as we can discover
using ``len()``.  We can then find the synset (a set of
lemmas), the words it contains, and a gloss.

    >>> len(car)
    1
    >>> car[0]
    {noun: car, auto, automobile, machine, motorcar}
    >>> list(car[0])
    ['car', 'auto', 'automobile', 'machine', 'motorcar']
    >>> car[0].gloss
    'a motor vehicle with four wheels; usually propelled by an
    internal combustion engine; 
    "he needs a car to get to work"'

The ``wordnet`` module also defines ``Synset``\ s.
Let's look at a word which is `polysemous`:dt:\ ;
that is, which has multiple synsets:

    >>> poly = wordnet.N['pupil']
    >>> for synset in poly:
    ...     print synset
    {noun: student, pupil, educatee}
    {noun: pupil}
    {noun: schoolchild, school-age_child, pupil}
    >>> poly[1].gloss
    'the contractile aperture in the center of the iris of the eye;
    resembles a large black dot'

The WordNet Hierarchy
---------------------

WordNet synsets correspond to abstract concepts, and they don't always
have corresponding words in English.  These concepts are linked together in a hierarchy.
Some concepts are very general, such as *Entity*, *State*, *Event* |mdash| these are called
`unique beginners`:dt:.  Others, such as *gas guzzler* and
*hatchback*, are much more specific. A small portion of a concept
hierarchy is illustrated in Figure wn-hierarchy_. The edges between nodes
indicate the hypernym/hyponym relation...

.. _wn-hierarchy:
.. figure:: ../images/wordnet-hierarchy.png
   :scale: 15

   Fragment of WordNet Concept Hierarchy

WordNet makes it easy to navigate between concepts.
For example, given a concept like *motorcar*,
we can look at the concepts that are more specific;
the (immediate) `hyponyms`:dt:. Here is one way to carry out this
navigation:

    >>> for concept in car[0][wordnet.HYPONYM][:10]:
    ...     print concept
    {noun: ambulance}
    {noun: beach_wagon, station_wagon, wagon, estate_car, beach_waggon, station_waggon, waggon}
    {noun: bus, jalopy, heap}
    {noun: cab, hack, taxi, taxicab}
    {noun: compact, compact_car}
    {noun: convertible}
    {noun: coupe}
    {noun: cruiser, police_cruiser, patrol_car, police_car, prowl_car, squad_car}
    {noun: electric, electric_automobile, electric_car}
    {noun: gas_guzzler}

|nopar|
We can also move up the hierarchy, by looking at broader concepts than
*motorcar*, e.g. the immediate `hypernym`:dt: of a concept:

    >>> car[0][wordnet.HYPERNYM]
    [{noun: motor_vehicle, automotive_vehicle}]

We can also look for the hypernyms of hypernyms.  In fact, from any
synset we can trace (multiple) paths back to a unique beginner.
Synsets have a method for doing this, called ``tree()``,
which produces a nested list structure.

    >>> pprint.pprint(wordnet.N['car'][0].tree(wordnet.HYPERNYM))
    [{noun: car, auto, automobile, machine, motorcar},
     [{noun: motor_vehicle, automotive_vehicle},
      [{noun: self-propelled_vehicle},
       [{noun: wheeled_vehicle},
        [{noun: vehicle},
         [{noun: conveyance, transport},
          [{noun: instrumentality, instrumentation},
           [{noun: artifact, artefact},
            [{noun: whole, unit},
             [{noun: object, physical_object},
              [{noun: physical_entity}, [{noun: entity}]]]]]]]],
        [{noun: container},
         [{noun: instrumentality, instrumentation},
          [{noun: artifact, artefact},
           [{noun: whole, unit},
            [{noun: object, physical_object},
             [{noun: physical_entity}, [{noun: entity}]]]]]]]]]]]

A related method ``closure()`` produces a flat version of this structure,
with repeats eliminated.
Both of these functions take an optional ``depth`` argument that permits
us to limit the number of steps to take.
(This is important when using unbounded relations like ``SIMILAR``.)
Table wordnet-rel_ lists the most important lexical relations supported
by WordNet; see ``dir(wordnet)`` for a full list.

.. table:: wordnet-rel

   ===========  ================  ==============================================
   Hypernym     more general      `animal`:lx: is a hypernym of `dog`:lx:
   Hyponym      more specific     `dog`:lx: is a hyponym of `animal`:lx:
   Meronym      part of           `door`:lx: is a meronym of `house`:lx:
   Holonym      has part          `house`:lx: is a holonym of `door`:lx:
   Synonym      similar meaning   `car`:lx: is a synonym of `automobile`:lx:
   Antonym      opposite meaning  `like` is an antonym of `dislike`:lx:
   Entailment   necessary action  `step` is an entailment of `walk`:lx:
   ===========  ================  ==============================================

   Major WordNet Lexical Relations

Recall that we can iterate over the words of a synset, with ``for word in synset``.
We can also test if a word is in a dictionary, e.g. ``if word in wordnet.V``.
As our last task, let's put these together to find "animal words" that are used as verbs.
Since there are a lot of these, we will cut this off at depth 4.
Can you think of the animal and verb sense of each word?

    >>> animals = wordnet.N['animal'][0].closure(wordnet.HYPONYM, depth=4)
    >>> sorted(set(w for synset in animals for w in synset if w in wordnet.V))
    ['baby', 'beetle', 'bird', 'bug', 'bulldog', 'clam', 'cock', 'cockle',
    'cub', 'dam', 'dog', 'escallop', 'fawn', 'fish', 'foal', 'frog',
    'game', 'grouse', 'grub', 'head', 'hog', 'hound', 'kit', 'kitten', 'lamb',
    'leech', 'mate', 'orphan', 'oyster', 'parrot', 'pet', 'pooch', 'prey', 'pup',
    'quail', 'quarry', 'queen', 'scallop', 'scollop', 'sire', 'slug', 'snail',
    'spat', 'sponge', 'steer', 'stray', 'stud', 'stunt', 'toy', 'whelp', 'worm']


NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet.
It can be accessed with ``nltk.corpus.verbnet``.

* find all pairs of nouns which occur in the same sentence

-------
Summary
-------

* A text corpus is a balanced collection of texts.  NLTK comes with many corpora,
  e.g. the Brown Corpus, ``nltk.corpus.brown``.
* Some text corpora are categorized, e.g. by genre or topic; sometimes the
  categories of a corpus overlap each other.
* A dictionary is used to map between arbitrary types of information,
  such as a string and a number: ``freq['cat'] = 12``.  We create
  dictionaries using the brace notation: ``pos = {}``,
  ``pos = {'furiously': 'adv', 'ideas': 'n', 'colorless': 'adj'}``.
  
---------------
Further Reading
---------------

Learn more about functions in Python by reading Chapter 4 of
[Lutz2003LP]_.

Archives of the CORPORA mailing list.

[Woods86]_

LDC, ELRA

The online documentation at |NLTK-API| contains extensive reference material
for all |NLTK| modules.

---------
Exercises
---------

#. |soso| Investigate the table of modal distributions and look for other patterns.
   Try to explain them in terms of your own impressionistic understanding
   of the different genres.  Can you find other closed classes of words that
   exhibit significant differences across different genres?

#. |soso| The CMU Pronunciation Dictionary contains multiple pronunciations
   for certain words.  How many distinct words does it contain?  What fraction
   of words in this dictionary have more than one possible pronunciation?

#. |easy| Familiarize yourself with the WordNet interface, by reading the
   documentation available via ``help(wordnet)``.  Try out the text-based
   browser, ``wordnet.browse()``.

#. |easy| Investigate the holonym / meronym relations for some nouns.  Note that there
   are three kinds (member, part, substance), so access is more specific,
   e.g., ``wordnet.MEMBER_MERONYM``, ``wordnet.SUBSTANCE_HOLONYM``.

#. |easy| The polysemy of a word is the number of senses it has.
   Using WordNet, we can determine that the noun *dog* has 7 senses
   with: ``len(nltk.wordnet.N['dog'])``.
   Compute the average polysemy of nouns, verbs, adjectives and
   adverbs according to WordNet.

#. |soso| What is the branching factor of the noun hypernym hierarchy?
   (For all noun synsets that have hyponyms, how many do they have on average?)

#. |soso| Define a function ``supergloss(s)`` that takes a synset ``s`` as its argument
   and returns a string consisting of the concatenation of the glosses of ``s``, all
   hypernyms of ``s``, and all hyponyms of ``s``.

#. |talk| Review the mappings in Table linguistic-objects_.  Discuss any other
   examples of mappings you can think of.  What type of information do they map
   from and to?

#. |easy| Using the Python interpreter in interactive mode, experiment with
   the examples in this section.  Create a dictionary ``d``, and add
   some entries.  What happens if you try to access a non-existent
   entry, e.g. ``d['xyz']``?

#. |easy| Try deleting an element from a dictionary, using the syntax
   ``del d['abc']``.  Check that the item was deleted.

#. |easy| Create a dictionary ``e``, to represent a single lexical entry
   for some word of your choice.
   Define keys like ``headword``, ``part-of-speech``, ``sense``, and
   ``example``, and assign them suitable values.

#. |soso| Write a program to find all words that occur at least three times in the Brown Corpus.

#. |soso| Write a program to generate a table of token/type ratios, as we saw in
   Table brown-types_.  Include the full set of Brown Corpus genres (``nltk.corpus.brown.categories()``).
   Which genre has the lowest diversity (greatest number of tokens per type)?
   Is this what you would have expected?

#. |soso| Modify the text generation program in Figure random_ further, to
   do the following tasks:

   a) Store the *n* most likely words in a list ``lwords`` then randomly
      choose a word from the list using ``random.choice()``.

   b) Select a particular genre, such as a section of the Brown Corpus,
      or a genesis translation, one of the Gutenberg texts, or one of the Web texts.  Train
      the model on this corpus and get it to generate random text.  You
      may have to experiment with different start words. How intelligible
      is the text?  Discuss the strengths and weaknesses of this method of
      generating random text.

   c) Now train your system using two distinct genres and experiment
      with generating text in the hybrid genre.  Discuss your observations.

#. |soso| Write a program to print the most frequent bigrams
   (pairs of adjacent words) of a text,
   omitting non-content words, in order of decreasing frequency.

#. |soso| Write a program to create a table of word frequencies by genre,
   like the one given above for modals.  Choose your own words and
   try to find words whose presence (or absence) is typical of a genre.
   Discuss your findings.

#. |hard| **Zipf's Law**:
   Let *f(w)* be the frequency of a word *w* in free text. Suppose that
   all the words of a text are ranked according to their frequency,
   with the most frequent word first. Zipf's law states that the
   frequency of a word type is inversely proportional to its rank
   (i.e. *f.r = k*, for some constant *k*). For example, the 50th most
   common word type should occur three times as frequently as the
   150th most common word type.

   a) Write a function to process a large text and plot word
      frequency against word rank using ``pylab.plot``. Do
      you confirm Zipf's law? (Hint: it helps to use a logarithmic scale).
      What is going on at the extreme ends of the plotted line?

   #) Generate random text, e.g. using ``random.choice("abcdefg ")``,
      taking care to include the space character.  You will need to
      ``import random`` first.  Use the string
      concatenation operator to accumulate characters into a (very)
      long string.  Then tokenize this string, and generate the Zipf
      plot as before, and compare the two plots.  What do you make of
      Zipf's Law in the light of this?

#. |hard| Modify the ``generate_model()`` function in Figure random_ to use Python's
   ``random.choose()`` method to randomly pick the next word from
   the available set of words.

#. |soso| Write a function ``tf()`` that takes a word and the name of a section
   of the Brown Corpus as arguments, and computes the text frequency of the word
   in that section of the corpus.

#. |easy| Try the examples in this section, then try the following.

   a) Create a variable called ``msg`` and put a message
      of your own in this variable.  Remember that strings need
      to be quoted, so you will need to type something like:
      ``msg = "I like NLP!"``
   b) Now print the contents of this variable in two ways, first
      by simply typing the variable name and pressing enter, then
      by using the ``print`` statement.
   c) Try various arithmetic expressions using this string, e.g.
      ``msg + msg``, and ``5 * msg``.
   d) Define a new string ``hello``, and then try ``hello + msg``.
      Change the ``hello`` string so that it ends with a space
      character, and then try ``hello + msg`` again.

#. |easy| Consider the following two expressions which have the same
   result.  Which one will typically be more relevant in |NLP|?  Why?

   a) ``"Monty Python"[6:12]``
   b) ``["Monty", "Python"][1]``

#. |easy| Define a string ``s = 'colorless'``.  Write a Python statement
   that changes this to "colourless" using only the slice and
   concatenation operations.

#. |easy| Try the slice examples from this section using the interactive
   interpreter.  Then try some more of your own.  Guess what the result
   will be before executing the command.

#. |easy| We can use the slice notation to remove morphological endings on
   words.  For example, ``'dogs'[:-1]`` removes the last character of
   ``dogs``, leaving ``dog``.  Use slice notation to remove the
   affixes from these words (we've inserted a hyphen to
   indicate the affix boundary, but omit this from your strings):
   ``dish-es``, ``run-ning``, ``nation-ality``, ``un-do``,
   ``pre-heat``.

#. |easy| We saw how we can generate an ``IndexError`` by indexing beyond the end
   of a string.  Is it possible to construct an index that goes too far to
   the left, before the start of the string?

#. |easy| We can also specify a "step" size for the slice. The following
   returns every second character within the slice: ``msg[6:11:2]``.
   It also works in the reverse direction: ``msg[10:5:-2]``
   Try these for yourself, then experiment with different step values.

#. |easy| What happens if you ask the interpreter to evaluate ``msg[::-1]``?
   Explain why this is a reasonable result.

#. |easy| Define a conditional frequency distribution over the Names corpus
   that allows you to see which initial letters are more frequent for males
   vs females (cf. Figure fig-cfd-gender_).

#. |soso| Write a function that finds the 50 most frequently occurring words
   of a text that are not stopwords.

#. |easy| Use the corpus module to read ``austen-persuasion.txt``.
   How many word tokens does this book have?  How many word types?

#. |easy| Use the Brown corpus reader ``nltk.corpus.brown.words()`` or the Web text corpus
   reader ``nltk.corpus.webtext.words()`` to access some sample text in two different genres.

#. |easy| Read in the texts of the *State of the Union* addresses, using the
   ``state_union`` corpus reader.  Count occurrences of ``men``, ``women``,
   and ``people`` in each document.  What has happened to the usage of these
   words over time?

#. |hard| Define a function ``find_language()`` that takes a string
   as its argument, and returns a list of languages that have that
   string as a word.  Use the ``udhr`` corpus and limit your searches
   to files in the Latin-1 encoding.

#. |soso| Write a program to guess the number of syllables contained in a text,
   making use of the CMU Pronouncing Dictionary.

#. |soso| Define a function ``hedge(text)`` which processes a
   text and produces a new version with the word
   ``'like'`` between every third word.

#. |soso| Write a program to score the similarity of two nouns as the depth
   of their first common hypernym.

#. |hard| Use one of the predefined similarity measures to score
   the similarity of each of the following pairs of words.
   Rank the pairs in order of decreasing similarity.
   How close is your ranking to the order given here?
   (Note that this order was established experimentally
   by [MillerCharles1998]_.)

::
      car-automobile, gem-jewel, journey-voyage, boy-lad,
      coast-shore, asylum-madhouse, magician-wizard, midday-noon,
      furnace-stove, food-fruit, bird-cock, bird-crane, tool-implement,
      brother-monk, lad-brother, crane-implement, journey-car,
      monk-oracle, cemetery-woodland, food-rooster, coast-hill,
      forest-graveyard, shore-woodland, monk-slave, coast-forest,
      lad-wizard, chord-smile, glass-magician, rooster-voyage, noon-string.      

.. Currently too hard since we can't iterate over synsets easily
   #. |soso| What percentage of noun synsets have no hyponyms?

.. include:: footer.rst

