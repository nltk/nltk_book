.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: update images in parser-problem table (NP -> NP PP replaced
.. by Nom -> Nom PP

.. standard global imports

    >>> import nltk, re, pprint

.. _chap-advanced-parsing:

==========================================
9. Chart Parsing and Probabilistic Parsing
==========================================

.. _ch09-introduction:

------------
Introduction
------------

Chapter chap-parse_ started with an introduction to constituent
structure in English, showing how words in a sentence group together
in predictable ways. We showed how to describe this structure using
syntactic tree diagrams, and observed that it is sometimes desirable
to assign more than one such tree to a given string. In this case, we
said that the string was structurally ambiguous; and example was `old
men and women`:lx:. 

Treebanks are language resources in which the syntactic structure of a
corpus of sentences has been annotated, usually by hand. However, we
would also like to be able to produce trees algorithmically. A
context-free phrase structure grammar (|CFG|) is a formal model for
describing whether a given string can be assigned a particular
constituent structure. Given a set of syntactic categories, the |CFG|
uses a set of productions to say how a phrase of some category *A* can
be analyzed into a sequence of smaller parts |alpha|\ :sub:`1`
... |alpha|\ :sub:`n`.  But a grammar is a static description of a set
of strings; it does not tell us what sequence of steps we need to take
to build a constituent structure for a string. For this, we need to
use a parsing algorithm.  We presented two such algorithms: Top-Down Recursive
Descent Bottom-Up Shift-Reduce (sec-parsing_).
As we pointed out, both parsing approaches suffer
from important shortcomings. The Recursive Descent parser cannot
handle left-recursive productions (e.g., productions such as `np`:gc:
|rarr| `np pp`:gc:), and blindly expands categories top-down without
checking whether they are compatible with the input string. The
Shift-Reduce parser is not guaranteed to find a valid parse for the
input even if one exists, and builds substructure without checking
whether it is globally consistent with the grammar. As we will
describe further below, the Recursive Descent parser is also
inefficient in its search for parses.

So, parsing builds trees over sentences, according to a phrase
structure grammar.  Now, all the examples we gave in Chapter
chap-parse_ only involved toy grammars containing a handful of
productions. What happens if we try to scale up this approach to deal
with realistic corpora of language? Unfortunately, as the coverage of
the grammar increases and the length of the input sentences grows, the
number of parse trees grows rapidly.  In fact, it grows at an
astronomical rate.

Let's explore this issue with the help of a simple example.
The word
`fish`:lx: is both a noun and a verb.  We can make up the sentence
`fish fish fish`:lx:, meaning *fish like to fish for other fish*.
(Try this with `police`:lx: if you prefer something more sensible.)
Here is a toy grammar for the "fish" sentences.

    >>> grammar = nltk.parse_cfg("""
    ... S -> NP V NP
    ... NP -> NP Sbar
    ... Sbar -> NP V 
    ... NP -> 'fish'
    ... V -> 'fish'
    ... """)

.. note:: Remember that our program samples assume you
   begin your interactive session or your program with: ``import nltk, re, pprint``

Now we can try parsing a longer sentence, `fish fish fish fish
fish`:lx:, which amongst other things, means 'fish that other fish
fish are in the habit of fishing fish themselves'. We use the |NLTK|
chart parser, which is presented later on in this chapter.  This
sentence has two readings.

    >>> tokens = ["fish"] * 5
    >>> cp = nltk.ChartParser(grammar, nltk.parse.TD_STRATEGY)
    >>> for tree in cp.nbest_parse(tokens):
    ...     print tree
    (S (NP (NP fish) (Sbar (NP fish) (V fish))) (V fish) (NP fish))
    (S (NP fish) (V fish) (NP (NP fish) (Sbar (NP fish) (V fish))))

As the length of this sentence goes up (3, 5, 7, ...) we get the
following numbers of parse trees:
1; 2; 5; 14; 42; 132; 429; 1,430; 4,862; 16,796; 58,786; 208,012; ...
(These are the `Catalan numbers`:idx:, which we saw in an exercise
in Section sec-algorithm-design-strategies_).
The last of these is for a sentence of length 23, the average length
of sentences in the  WSJ section of Penn Treebank.  For a sentence
of length 50 there would be over 10\ :superscript:`12` parses, and this
is only half the length of the Piglet sentence
(Section sec-more-observations-about-grammar_),
which young children process effortlessly.
No practical |NLP| system could construct millions of trees for a
sentence and choose the appropriate one in the context.
It's clear that humans don't do this either!

Note that the problem is not with our choice of example. 
[Church1982CSA]_ point out that the syntactic ambiguity of `pp`:gc:
attachment in sentences like pp_ also grows in proportion to the Catalan
numbers.

.. _pp:
.. ex:: Put the block in the box on the table.

So much for structural ambiguity; what about lexical ambiguity?
As soon as we try to construct a broad-coverage grammar, we
are forced to make lexical entries highly ambiguous for their part of
speech.  In a toy grammar, `a`:lx: is only a determiner, `dog`:lx: is
only a noun, and `runs`:lx: is only a verb.  However, in a
broad-coverage grammar, `a` is also a noun (e.g. `part a`:lx:),
`dog`:lx: is also a verb (meaning to follow closely), and `runs`:lx:
is also a noun (e.g. `ski runs`:lx:).  In fact, all words can be
referred to by name: e.g. `the verb 'ate' is spelled with three
letters`:lx:; in speech we do not need to supply quotation marks.
Furthermore, it is possible to *verb* most nouns.  Thus a parser for a
broad-coverage grammar will be overwhelmed with ambiguity.  Even
complete gibberish will often have a reading, e.g. `the a are of
I`:lx:.  As [Abney1996SML]_ has pointed out, this is not word salad but a
grammatical noun phrase, in which `are`:lx: is a noun meaning a
hundredth of a hectare (or 100 sq m), and `a`:lx: and `I`:lx: are
nouns designating coordinates, as shown in Figure are_.

.. _are:
.. figure:: ../images/are.png
   :scale: 20

   The a are of I

|nopar|
Even though this phrase is unlikely, it is still grammatical and a
a broad-coverage parser should be able to construct a parse tree
for it.  Similarly, sentences that seem to be
unambiguous, such as `John saw Mary`:lx:, turn out to have other
readings we would not have anticipated (as Abney explains).  This
ambiguity is unavoidable, and leads to horrendous inefficiency in
parsing seemingly innocuous sentences. 

Let's look more closely at this issue of efficiency.
The top-down recursive-descent parser presented
in Chapter chap-parse_ can be very inefficient, since it often builds
and discards the same sub-structure many times over.  We see this
in Figure parser-problem_, where a phrase `the block`:lx: is identified
as a noun phrase several times, and where this information is discarded
each time we backtrack.

.. Note::
   You should try the recursive-descent parser demo if you haven't
   already: ``nltk.draw.srparser.demo()``

.. table:: parser-problem

   +-----------------------------------+-----------------------------------+
   | a. Initial stage                  | b. Backtracking                   |
   |                                   |                                   |
   | |findtheblock1|                   | |findtheblock2|                   |
   +-----------------------------------+-----------------------------------+
   | c. Failing to match `on`:lx:      | d. Completed parse                |
   |                                   |                                   |
   | |findtheblock3|                   | |findtheblock4|                   |
   +-----------------------------------+-----------------------------------+

   Backtracking and Repeated Parsing of Subtrees

.. |findtheblock1| image:: ../images/findtheblock1.png
   :scale: 85
.. |findtheblock2| image:: ../images/findtheblock2.png
   :scale: 85
.. |findtheblock3| image:: ../images/findtheblock3.png
   :scale: 85
.. |findtheblock4| image:: ../images/findtheblock4.png
   :scale: 85


In this chapter, we will present two independent methods for dealing with ambiguity.
The first is `chart parsing`:em:, which uses the algorithmic technique
of dynamic programming to derive the parses of an ambiguous
sentence more `efficiently`:em:.  The second is
`probabilistic parsing`:em:, which allows us to `rank`:em:
the parses of an ambiguous sentence on the basis of evidence from corpora.

---------------------
Probabilistic Parsing
---------------------

.. TODO: mention interest in having weights is because they can be learned.
   Without this it is mysterious why we would want to bother.
   Technical aspects follow, but this is important motivation (Steven)

.. TODO: expand opening discussion to remind reader why we're doing this
   (we had moved stuff to front of chapter) (Steven)

As we pointed out in the introduction to this
chapter, dealing with ambiguity is a key challenge to broad coverage
parsers. We have shown how chart parsing can help improve the
efficiency of computing multiple parses of the same sentences. But
the sheer number of parses can be just overwhelming. We will show how
probabilistic parsing helps to manage a large space of parses.  
However, before we deal with these parsing issues,
we must first back up and introduce weighted grammars.

Weighted Grammars
-----------------

We begin by considering the verb `give`:lx:.  This verb requires both
a direct object (the thing being given) and an indirect object (the
recipient).  These complements can be given in either order, as
illustrated in example dative_.  In the "prepositional dative" form,
the indirect object appears last, and inside a prepositional phrase,
while in the "double object" form, the indirect object comes first:

.. _dative:
.. ex::
   .. ex::
      Kim gave a bone to the dog
   .. ex::
      Kim gave the dog a bone

Using the Penn Treebank sample, we can examine all instances of
prepositional dative and double object constructions involving
`give`:lx:, as shown in Figure give_.

.. pylisting:: give
   :caption: Usage of Give and Gave in the Penn Treebank sample

   def give(t):
       return t.node == 'VP' and len(t) > 2 and t[1].node == 'NP'\
              and (t[2].node == 'PP-DTV' or t[2].node == 'NP')\
              and ('give' in t[0].leaves() or 'gave' in t[0].leaves())
   def sent(t):
       return ' '.join(token for token in t.leaves() if token[0] not in '*-0')
   def print_node(t, width):
           output = "%s %s: %s / %s: %s" %\
               (sent(t[0]), t[1].node, sent(t[1]), t[2].node, sent(t[2]))
           if len(output) > width:
               output = output[:width] + "..."
           print output

   >>> for tree in nltk.corpus.treebank.parsed_sents():
   ...     for t in tree.subtrees(give):
   ...         print_node(t, 72)
   gave NP: the chefs / NP: a standing ovation
   give NP: advertisers / NP: discounts for maintaining or increasing ad sp...
   give NP: it / PP-DTV: to the politicians
   gave NP: them / NP: similar help
   give NP: them / NP: 
   give NP: only French history questions / PP-DTV: to students in a Europe...
   give NP: federal judges / NP: a raise
   give NP: consumers / NP: the straight scoop on the U.S. waste crisis
   gave NP: Mitsui / NP: access to a high-tech medical product
   give NP: Mitsubishi / NP: a window on the U.S. glass industry
   give NP: much thought / PP-DTV: to the rates she was receiving , nor to ...
   give NP: your Foster Savings Institution / NP: the gift of hope and free...
   give NP: market operators / NP: the authority to suspend trading in futu...
   gave NP: quick approval / PP-DTV: to $ 3.18 billion in supplemental appr...
   give NP: the Transportation Department / NP: up to 50 days to review any...
   give NP: the president / NP: such power
   give NP: me / NP: the heebie-jeebies
   give NP: holders / NP: the right , but not the obligation , to buy a cal...
   gave NP: Mr. Thomas / NP: only a `` qualified '' rating , rather than ``...
   give NP: the president / NP: line-item veto power

We can observe a strong tendency for the shortest complement to appear
first.  However, this does not account for a form like
``give NP: federal judges / NP: a raise``, where animacy may be
playing a role.  In fact there turn out to be a large number of contributing
factors, as surveyed by [Bresnan2006GG]_.

How can such tendencies be expressed in a conventional context free
grammar?  It turns out that they cannot.  However, we can address the
problem by adding weights, or probabilities, to the productions of a grammar.

A `probabilistic context free grammar`:dt: (or *PCFG*) is a context free
grammar that associates a probability with each of its productions.
It generates the same set of parses for a text that the corresponding
context free grammar does, and assigns a probability to each parse.
The probability of a parse generated by a PCFG is simply the product
of the probabilities of the productions used to generate it.

The simplest way to define a PCFG is to load it from a specially
formatted string consisting of a sequence of weighted productions,
where weights appear in brackets, as shown in Figure pcfg1_.

.. pylisting:: pcfg1
   :caption: Defining a Probabilistic Context Free Grammar (PCFG)

   grammar = nltk.parse_pcfg("""
       S    -> NP VP              [1.0]
       VP   -> TV NP              [0.4]
       VP   -> IV                 [0.3]
       VP   -> DatV NP NP         [0.3]
       TV   -> 'saw'              [1.0]
       IV   -> 'ate'              [1.0]
       DatV -> 'gave'             [1.0]
       NP   -> 'telescopes'       [0.8]
       NP   -> 'Jack'             [0.2]
       """)
   >>> print grammar
   Grammar with 9 productions (start state = S)
       S -> NP VP [1.0]
       VP -> TV NP [0.4]
       VP -> IV [0.3]
       VP -> DatV NP NP [0.3]
       TV -> 'saw' [1.0]
       IV -> 'ate' [1.0]
       DatV -> 'gave' [1.0]
       NP -> 'telescopes' [0.8]
       NP -> 'Jack' [0.2]

It is sometimes convenient to combine multiple productions into a single line,
e.g. ``VP -> TV NP [0.4] | IV [0.3] | DatV NP NP [0.3]``.
In order to ensure that the trees generated by the grammar form a
probability distribution, PCFG grammars impose the constraint
that all productions with a given left-hand side must have
probabilities that sum to one.
The grammar in Figure pcfg1_ obeys this constraint: for ``S``,
there is only one production, with a probability of 1.0; for ``VP``,
0.4+0.3+0.3=1.0; and for ``NP``, 0.8+0.2=1.0.
The parse tree returned by ``parse()`` includes probabilities:

    >>> viterbi_parser = nltk.ViterbiParser(grammar)
    >>> print viterbi_parser.parse(['Jack', 'saw', 'telescopes'])
    (S (NP Jack) (VP (TV saw) (NP telescopes))) (p=0.064)

The next two sections introduce two probabilistic parsing algorithms
for PCFGs.  The first is an A* parser that uses Viterbi-style dynamic
programming to find the single most likely parse for a given text.
Whenever it finds multiple possible parses for a subtree, it discards
all but the most likely parse.  The second is a bottom-up chart parser
that maintains a queue of edges, and adds them to the chart one at a
time.  The ordering of this queue is based on the probabilities
associated with the edges, allowing the parser to expand more likely
edges before less likely ones.  Different queue orderings are used to
implement a variety of different search strategies.  These algorithms
are implemented in the ``nltk.parse.viterbi`` and
``nltk.parse.pchart`` modules.

A* Parser
---------

An `A* Parser`:dt: is a bottom-up PCFG parser that uses
dynamic programming to find the single most likely parse for a text [Klein2003Astar]_.
It parses texts by iteratively filling in a 
`most likely constituents table`:dt:.  This table records the most likely tree for each
span and node value.  For example, after parsing the sentence "I saw the man with
the telescope" with the grammar ``cfg.toy_pcfg1``, the most likely constituents table
contains the following entries (amongst others):

.. table:: mlct

   ===== ==== ==================================================================  ============
   Span  Node Tree                                                                Prob
   ===== ==== ==================================================================  ============
   [0:1] NP   (NP I)                                                              0.15
   [6:7] NP   (NN telescope)                                                      0.5
   [5:7] NP   (NP the telescope)                                                  0.2
   [4:7] PP   (PP with (NP the telescope))                                        0.122
   [0:4] S    (S (NP I) (VP saw (NP the man)))                                    0.01365
   [0:7] S    (S (NP I) (VP saw (NP (NP the man) (PP with (NP the telescope)))))  0.0004163250
   ===== ==== ==================================================================  ============

   Fragment of Most Likely Constituents Table

Once the table has been completed, the parser
returns the entry for the most likely constituent that spans the
entire text, and whose node value is the start symbol.  For this
example, it would return the entry with a span of [0:6] and a node
value of "S".

Note that we only record the *most likely* constituent for any given
span and node value.  For example, in the table above, there are
actually two possible constituents that cover the span [1:6] and have
"VP" node values.

1. "saw the man, who has the telescope":

  (VP saw
     (NP (NP John)
          (PP with (NP the telescope))))

2. "used the telescope to see the man":

  (VP saw
     (NP John)
     (PP with (NP the telescope)))

|nopar|
Since the grammar we are using to parse the text indicates that the
first of these tree structures has a higher probability, the parser
discards the second one.

**Filling in the Most Likely Constituents Table:**
Because the grammar used by ``ViterbiParse`` is a PCFG, the
probability of each constituent can be calculated from the
probabilities of its children.  Since a constituent's children can
never cover a larger span than the constituent itself, each entry of
the most likely constituents table depends only on entries for
constituents with *shorter* spans (or equal spans, in the case of
unary and epsilon productions).

``ViterbiParse`` takes advantage of this fact, and fills in the most
likely constituent table incrementally.  It starts by filling in the
entries for all constituents that span a single element of text.
After it has filled in all the table entries for constituents that
span one element of text, it fills in the entries for constituents
that span two elements of text.  It continues filling in the entries
for constituents spanning larger and larger portions of the text,
until the entire table has been filled.

To find the most likely constituent with a given span and node value,
``ViterbiParse`` considers all productions that could produce that
node value.  For each production, it checks the most likely
constituents table for sequences of children that collectively cover
the span and that have the node values specified by the production's
right hand side.  If the tree formed by applying the production to the
children has a higher probability than the current table entry, then
it updates the most likely constituents table with the new tree.

**Handling Unary Productions and Epsilon Productions:**
A minor difficulty is introduced by unary productions and epsilon
productions: an entry of the most likely constituents table might
depend on another entry with the same span.  For example, if the
grammar contains the production ``V`` |rarr| ``VP``, then the table
entries for ``VP`` depend on the entries for ``V`` with the same span.
This can be a problem if the constituents are checked in the wrong
order.  For example, if the parser tries to find the most likely
constituent for a ``VP`` spanning [1:3] before it finds the most
likely constituents for ``V`` spanning [1:3], then it can't apply the
``V`` |rarr| ``VP`` production.

To solve this problem, ``ViterbiParse`` repeatedly checks each span
until it finds no new table entries.  Note that cyclic grammar
productions (e.g. ``V`` |rarr| ``V``) will *not* cause this procedure
to enter an infinite loop.  Since all production probabilities are
less than or equal to 1, any constituent generated by a cycle in the
grammar will have a probability that is less than or equal to the
original constituent; so ``ViterbiParse`` will discard it.

In NLTK, we create Viterbi parsers using ``ViterbiParse()``.
Note that since ``ViterbiParse`` only finds the single most likely
parse, that ``nbest_parse()`` will never return more than one parse.

.. pylisting:: viterbi-parse

   grammar = nltk.parse_pcfg('''
     NP  -> NNS [0.5] | JJ NNS [0.3] | NP CC NP [0.2]
     NNS -> "men" [0.1] | "women" [0.2] | "children" [0.3] | NNS CC NNS [0.4]
     JJ  -> "old" [0.4] | "young" [0.6]
     CC  -> "and" [0.9] | "or" [0.1]
     ''')
   viterbi_parser = nltk.ViterbiParser(grammar)

   >>> sent = 'old men and women'.split()
   >>> print viterbi_parser.parse(sent)
   (NP (JJ old) (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)    

The ``trace`` method can be used to set the level of tracing output
that is generated when parsing a text.  Trace output displays the
constituents that are considered, and indicates which ones are added
to the most likely constituent table.  It also indicates the
likelihood for each constituent.

    >>> viterbi_parser.trace(3)
    >>> print viterbi_parser.parse(sent)
    Inserting tokens into the most likely constituents table...
       Insert: |=...| old
       Insert: |.=..| men
       Insert: |..=.| and
       Insert: |...=| women
    Finding the most likely constituents spanning 1 text elements...
       Insert: |=...| JJ -> 'old' [0.4]                 0.4000000000 
       Insert: |.=..| NNS -> 'men' [0.1]                0.1000000000 
       Insert: |.=..| NP -> NNS [0.5]                   0.0500000000 
       Insert: |..=.| CC -> 'and' [0.9]                 0.9000000000 
       Insert: |...=| NNS -> 'women' [0.2]              0.2000000000 
       Insert: |...=| NP -> NNS [0.5]                   0.1000000000 
    Finding the most likely constituents spanning 2 text elements...
       Insert: |==..| NP -> JJ NNS [0.3]                0.0120000000 
    Finding the most likely constituents spanning 3 text elements...
       Insert: |.===| NP -> NP CC NP [0.2]              0.0009000000 
       Insert: |.===| NNS -> NNS CC NNS [0.4]           0.0072000000 
       Insert: |.===| NP -> NNS [0.5]                   0.0036000000 
      Discard: |.===| NP -> NP CC NP [0.2]              0.0009000000 
      Discard: |.===| NP -> NP CC NP [0.2]              0.0009000000 
    Finding the most likely constituents spanning 4 text elements...
       Insert: |====| NP -> JJ NNS [0.3]                0.0008640000 
      Discard: |====| NP -> NP CC NP [0.2]              0.0002160000 
      Discard: |====| NP -> NP CC NP [0.2]              0.0002160000 
    (NP (JJ old) (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)


A Bottom-Up PCFG Chart Parser
-----------------------------

The `A* parser`:idx: described in the previous section finds
the single most likely parse for a given text.  However, when parsers
are used in the context of a larger NLP system, it is often necessary
to produce several alternative parses.  In the context of an overall system,
a parse that is assigned low probability by the parser might still have the
best overall probability.

For example, a probabilistic parser might decide that the most likely
parse for "I saw John with the cookie" is is the structure with the
interpretation "I used my cookie to see John"; but that parse would be
assigned a low probability by a semantic system.  Combining the
probability estimates from the parser and the semantic system, the
parse with the interpretation "I saw John, who had my cookie" would be
given a higher overall probability.

This section describes a probabilistic bottom-up chart parser.
It maintains an `edge queue`:dt:, and adds these edges to the chart one at a time.
The ordering of this queue is based on the probabilities associated with the edges,
and this allows the parser to insert the most probable edges first.
Each time an edge is added to the chart, it may become possible
to insert new edges, so these are added to the queue.
The bottom-up chart parser continues adding the edges in the
queue to the chart until enough complete parses have been found, or
until the edge queue is empty.

Like an edge in a regular chart, a probabilistic edge
consists of a dotted production, a span, and a (partial) parse tree.
However, unlike ordinary charts, this time the tree is weighted
with a probability.  Its probability
is the product of the probability of the production that
generated it and the probabilities of its children.  For example, the
probability of the edge ``[Edge: S`` |rarr| ``NP`` |dot| ``VP, 0:2]``
is the probability of the PCFG production ``S`` |rarr| ``NP VP``
multiplied by the probability of its `np`:gc: child.
(Note that an edge's tree only includes children for elements to the left
of the edge's dot.  Thus, the edge's probability does *not* include
probabilities for the constituents to the right of the edge's dot.)

Bottom-Up PCFG Strategies
-------------------------

The `edge queue`:idx: is a sorted list of edges that can be added to the
chart.  It is initialized with a single edge for each token in the
text, with the form ``[Edge: token |rarr| |dot|]``.
As each edge from the queue is added to the chart, it may
become possible to add further edges, according to two rules:
(i) the Bottom-Up Initialization Rule can be used to add a
self-loop edge whenever an edge whose dot is in position 0 is added to the chart; or
(ii) the Fundamental Rule can be used to combine a new edge
with edges already present in the chart.  These additional edges
are queued for addition to the chart.

By changing the sort order used by the queue, we can control the
strategy that the parser uses to explore the search space.  Since
there are a wide variety of reasonable search strategies,
``BottomUpChartParser()`` does not define any sort order.
Instead, different strategies are implemented in subclasses of ``BottomUpChartParser()``.

.. We should either explain "inside probabilities" or rename this parser (to
        ``LowestCostFirstParser``?). 

**Lowest Cost First:**
The simplest way to order the edge queue is to sort edges by the
probabilities of their associated trees (``nltk.InsideChartParser()``).
This ordering concentrates the efforts of the parser on those edges
that are more likely to be correct analyses of their underlying tokens.

The probability of an edge's tree provides an upper bound on the
probability of any parse produced using that edge.  The probabilistic
"cost" of using an edge to form a parse is one minus its tree's
probability.  Thus, inserting the edges with the most likely trees
first results in a `lowest-cost-first search strategy`:dt:.
Lowest-cost-first search is optimal: the first
solution it finds is guaranteed to be the best solution.

However, lowest-cost-first search can be rather inefficient.  Recall that a
tree's probability is the product of the probabilities of all the
productions used to generate it.  Consequently, smaller trees tend to have higher
probabilities than larger ones.  Thus, lowest-cost-first search tends to work
with edges having small trees before considering edges with larger trees.
Yet any complete parse of the text will necessarily have a
large tree, and so this strategy will tend to produce complete parses only
once most other edges are processed.

Let's consider this problem from another angle.
The basic shortcoming with lowest-cost-first search is that it ignores the
probability that an edge's tree will be part of a complete parse.  The parser will
try parses that are locally coherent even if they are unlikely to
form part of a complete parse.  Unfortunately, it can be quite
difficult to calculate the probability that a tree is part of a
complete parse.  However, we can use a variety of techniques to
approximate that probability.

**Best-First Search:**
This method sorts the edge queue in descending order of the edges'
span, no the assumption that edges having a larger span are more likely
to form part of a complete parse.
Thus, ``LongestParse`` employs a `best-first search strategy`:dt:,
where it inserts the edges that are closest to producing
complete parses before trying any other edges.  Best-first search is
*not* an optimal search strategy: the first solution it finds is not
guaranteed to be the best solution.  However, it will usually find a
complete parse much more quickly than lowest-cost-first search.

**Beam Search:**
When large grammars are used to parse a text, the edge queue can grow
quite long.  The edges at the end of a large well-sorted queue are
unlikely to be used.  Therefore, it is reasonable to remove (or
*prune*) these edges from the queue.  This strategy is known as
`beam search`:dt:; it only keeps the best partial results.
The bottom-up chart parsers take an optional parameter ``beam_size``;
whenever the edge queue grows longer than this, it is pruned.
This parameter is best used in conjunction with ``InsideChartParser()``.
Beam search reduces the space requirements for lowest-cost-first
search, by discarding edges that are not likely to be used.  But beam
search also loses many of lowest-cost-first search's more useful
properties.  Beam search is not optimal: it is not guaranteed to find
the best parse first.  In fact, since it might prune a necessary edge,
beam search is not even `complete`:idx:\ : it is not guaranteed to return a
parse if one exists.

In NLTK we can construct these parsers using
``InsideChartParser``, ``LongestChartParser``, ``RandomChartParser``.

.. pylisting:: bottom-up-chart-parsers

   inside_parser = nltk.InsideChartParser(grammar)
   longest_parser = nltk.LongestChartParser(grammar)
   beam_parser = nltk.InsideChartParser(grammar, beam_size=20)

   >>> print inside_parser.parse(sent)
   (NP (JJ old) (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)
   >>> for tree in inside_parser.nbest_parse(sent):
   ...     print tree
   (NP
     (JJ old)
     (NNS (NNS men) (CC and) (NNS women))) (p=0.000864)
   (NP
     (NP (JJ old) (NNS men))
     (CC and)
     (NP (NNS women))) (p=0.000216)

The ``trace`` method can be used to set the level of tracing output
that is generated when parsing a text.  Trace output displays edges as
they are added to the chart, and shows the probability for each edges'
tree.

    >>> inside_parser.trace(3)
    >>> trees = inside_parser.nbest_parse(sent)
      |. . . [-]| [3:4] 'women'                          [1.0]
      |. . [-] .| [2:3] 'and'                            [1.0]
      |. [-] . .| [1:2] 'men'                            [1.0]
      |[-] . . .| [0:1] 'old'                            [1.0]
      |. . [-] .| [2:3] CC -> 'and' *                    [0.9]
      |. . > . .| [2:2] CC -> * 'and'                    [0.9]
      |[-] . . .| [0:1] JJ -> 'old' *                    [0.4]
      |> . . . .| [0:0] JJ -> * 'old'                    [0.4]
      |> . . . .| [0:0] NP -> * JJ NNS                   [0.3]
      |. . . [-]| [3:4] NNS -> 'women' *                 [0.2]
      |. . . > .| [3:3] NP -> * NNS                      [0.5]
      |. . . > .| [3:3] NNS -> * NNS CC NNS              [0.4]
      |. . . > .| [3:3] NNS -> * 'women'                 [0.2]
      |[-> . . .| [0:1] NP -> JJ * NNS                   [0.12]
      |. . . [-]| [3:4] NP -> NNS *                      [0.1]
      |. . . > .| [3:3] NP -> * NP CC NP                 [0.2]
      |. [-] . .| [1:2] NNS -> 'men' *                   [0.1]
      |. > . . .| [1:1] NP -> * NNS                      [0.5]
      |. > . . .| [1:1] NNS -> * NNS CC NNS              [0.4]
      |. > . . .| [1:1] NNS -> * 'men'                   [0.1]
      |. . . [->| [3:4] NNS -> NNS * CC NNS              [0.08]
      |. [-] . .| [1:2] NP -> NNS *                      [0.05]
      |. > . . .| [1:1] NP -> * NP CC NP                 [0.2]
      |. [-> . .| [1:2] NNS -> NNS * CC NNS              [0.04]
      |. [---> .| [1:3] NNS -> NNS CC * NNS              [0.036]
      |. . . [->| [3:4] NP -> NP * CC NP                 [0.02]
      |[---] . .| [0:2] NP -> JJ NNS *                   [0.012]
      |> . . . .| [0:0] NP -> * NP CC NP                 [0.2]
      |. [-> . .| [1:2] NP -> NP * CC NP                 [0.01]
      |. [---> .| [1:3] NP -> NP CC * NP                 [0.009]
      |. [-----]| [1:4] NNS -> NNS CC NNS *              [0.0072]
      |. [-----]| [1:4] NP -> NNS *                      [0.0036]
      |. [----->| [1:4] NNS -> NNS * CC NNS              [0.00288]
      |[---> . .| [0:2] NP -> NP * CC NP                 [0.0024]
      |[-----> .| [0:3] NP -> NP CC * NP                 [0.00216]
      |. [-----]| [1:4] NP -> NP CC NP *                 [0.0009]
      |[=======]| [0:4] NP -> JJ NNS *                   [0.000864]
      |. [----->| [1:4] NP -> NP * CC NP                 [0.00072]
      |[=======]| [0:4] NP -> NP CC NP *                 [0.000216]
      |. [----->| [1:4] NP -> NP * CC NP                 [0.00018]
      |[------->| [0:4] NP -> NP * CC NP                 [0.0001728]
      |[------->| [0:4] NP -> NP * CC NP                 [4.32e-05]

.. _sec-grammar-induction:

-----------------
Grammar Induction
-----------------

As we have seen, PCFG productions are just like CFG productions,
adorned with probabilities.  So far, we have simply specified these
probabilities in the grammar.  However, it is more usual to *estimate*
these probabilities from training data, namely a collection of parse
trees or *treebank*.

The simplest method uses *Maximum Likelihood Estimation*, so called
because probabilities are chosen in order to maximize the likelihood
of the training data.  The probability of a production
``VP`` |rarr| ``V NP PP`` is *p(V,NP,PP | VP)*.  We calculate this as
follows::

                        count(VP → V NP PP)
      P(V,NP,PP | VP) = -------------------
                        count(VP → ...)

..  I couldn't make the |rarr| macro work in the formula above, the
    literal Unicode arrow charicter works properly.

Here is a simple program that induces a grammar from the first
three parse trees in the Penn Treebank corpus:

    >>> from itertools import islice
    >>> productions = []
    >>> S = nltk.Nonterminal('S')
    >>> for tree in nltk.corpus.treebank.parsed_sents('wsj_0002.mrg'):
    ...      productions += tree.productions()
    >>> grammar = nltk.induce_pcfg(S, productions)
    >>> for production in grammar.productions()[:10]:
    ...      print production
    CC -> 'and' [1.0]
    NNP -> 'Agnew' [0.166666666667]
    JJ -> 'industrial' [0.2]
    NP -> CD NNS [0.142857142857]
    , -> ',' [1.0]
    S -> NP-SBJ NP-PRD [0.5]
    VP -> VBN S [0.5]
    NNP -> 'Rudolph' [0.166666666667]
    NP -> NP PP [0.142857142857]
    NNP -> 'PLC' [0.166666666667]

Normal Forms
------------

Grammar induction usually involves normalizing the grammar
in various ways.  NLTK trees
support binarization (Chomsky Normal Form), parent annotation,
Markov order-N smoothing, and unary collapsing:

    >>> treebank_string = """(S (NP-SBJ (NP (QP (IN at) (JJS least) (CD nine) (NNS tenths)) )
    ...     (PP (IN of) (NP (DT the) (NNS students) ))) (VP (VBD passed)))"""
    >>> t = nltk.bracket_parse(treebank_string)
    >>> print t
    (S
      (NP-SBJ
        (NP (QP (IN at) (JJS least) (CD nine) (NNS tenths)))
        (PP (IN of) (NP (DT the) (NNS students))))
      (VP (VBD passed)))
    >>> t.collapse_unary(collapsePOS=True)
    >>> print t
    (S
      (NP-SBJ
        (NP+QP (IN at) (JJS least) (CD nine) (NNS tenths))
        (PP (IN of) (NP (DT the) (NNS students))))
      (VP+VBD passed))
    >>> t.chomsky_normal_form()
    >>> print t
    (S
      (NP-SBJ
        (NP+QP
          (IN at)
          (NP+QP|<JJS-CD-NNS>
            (JJS least)
            (NP+QP|<CD-NNS> (CD nine) (NNS tenths))))        
        (PP (IN of) (NP (DT the) (NNS students))))
      (VP+VBD passed))

These trees are shown in treetransforms_.

.. _treetransforms:
.. ex:: 
   .. ex::
      .. tree:: (S (NP-SBJ (NP (QP (IN at) (JJS least) (CD nine) (NNS tenths))) (PP (IN of) (NP (DT the) (NNS students)))) (VP (VBD passed)))
   .. ex::
      .. tree:: (S (NP-SBJ (NP+QP (IN at) (JJS least) (CD nine) (NNS tenths)) (PP (IN of) (NP (DT the) (NNS students)))) (VP+VBD passed))
   .. ex::
      .. tree:: (S (NP-SBJ (NP+QP (IN at) (NP+QP|\<JJS-CD-NNS\> (JJS least) (NP+QP|\<CD-NNS\> (CD nine) (NNS tenths)))) (PP (IN of) (NP (DT the) (NNS students)))) (VP+VBD passed))


------------------
Dependency Grammar
------------------

Rather than starting from the grouping of words into constituents,
dependency grammar takes as basic the notion that
one word can be dependent on another (namely, its head). The root of a
sentence is usually taken to be the main verb, and every other word is
either dependent on the root, or connects to it through a path of
dependencies. Figure depgraph0_ illustrates a dependency graph, where
the head of the arrow points to the head of a dependency. 

.. _depgraph0:
.. ex::
    .. image:: ../images/depgraph0.png
       :scale: 30

As you will see, the arcs in Figure depgraph0_ are labeled with the
particular dependency relation that holds between a dependent and its
head. For example, `Esso`:lx: bears the subject relation to `said`:lx:
(which is the head of the whole sentence), and `Tuesday`:lx: bears a
verbal modifier (`vmod`:gc:) relation to `started`:lx:.

An alternative way of representing the dependency relationships is illustrated
in the tree depgraph1_,
where dependents are shown as daughters of their heads.

.. _depgraph1:
.. ex::
    .. tree:: (said Esso (started (field the Whiting) production Tuesday))

One format for encoding dependency information places each word on a
line, followed by its part-of-speech tag, the index of its head, and
the label of the dependency relation (cf. [Nivre2006MP]_). The index
of a word is implicitly given by the ordering of the lines (with 1 as the
first index). This is illustrated in the following code snippet:

    >>> from nltk.parse import DependencyGraph
    >>> dg = DependencyGraph().read("""Esso    NNP 2   SUB
    ... said    VBD 0   ROOT
    ... the     DT  5   NMOD
    ... Whiting NNP 5   NMOD
    ... field   NN  6   SUB
    ... started VBD 2   VMOD
    ... production  NN  6   OBJ
    ... Tuesday NNP 6   VMOD""")

As you will see, this format also adopts the convention that the head
of the sentence is dependent on an empty node, indexed as 0. We can
use the ``deptree()`` method of a ``DependencyGraph()`` object to build an |NLTK|
tree like that illustrated earlier in depgraph1_.

    >>> tree = dg.deptree()
    >>> tree.draw()                                 # doctest: +SKIP


----------
Conclusion
----------

[to be written]

---------------
Further Reading
---------------

Section 13.4 of [JurafskyMartin2008]_ covers chart parsing, and Chapter 14
contains a more formal presentation of statistical parsing.

* [Manning2003Probabilistic]_

* [Klein2003Astar]_

---------
Exercises
---------

#. |easy| Consider the sequence of words:
   `Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo`:lx:.
   This is a grammatically correct sentence, as explained at
   ``http://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo.``
   Consider the tree diagram presented on this Wikipedia page, and write down a suitable
   grammar.  Normalize case to lowercase, to simulate the problem that a listener has when hearing
   this sentence.  Can you find other parses for this sentence?  
   How does the number of parse trees grow as the sentence gets longer?
   (More examples of these sentences can be found at ``http://en.wikipedia.org/wiki/List_of_homophonous_phrases``).

#. |soso| Consider the algorithm in Figure wfst_.  Can you explain why
   parsing context-free grammar is proportional to `n`:sup:`3`\ ?

#. |soso| Modify the functions ``init_wfst()`` and ``complete_wfst()`` so
   that the contents of each cell in the |WFST| is a set of
   non-terminal symbols rather than a single non-terminal.

#. |hard| Modify the functions ``init_wfst()`` and ``complete_wfst()`` so
   that when a non-terminal symbol is added to a cell in the |WFST|, it includes
   a record of the cells from which it was derived. Implement a
   function that will convert a |WFST| in this form to a parse tree.

#. |easy| Use the graphical chart-parser interface to experiment with
   different rule invocation strategies. Come up with your own strategy
   that you can execute manually using the graphical interface. Describe
   the steps, and report any efficiency improvements it has (e.g. in terms
   of the size of the resulting chart). Do these improvements depend on
   the structure of the grammar? What do you think of the prospects for
   significant performance boosts from cleverer rule invocation
   strategies?

#. |easy| We have seen that a chart parser adds but never removes edges
   from a chart.  Why?

#. |soso| Write a program to compare the efficiency of a top-down chart parser
   compared with a recursive descent parser (Section sec-parsing_).
   Use the same grammar and input sentences for both.  Compare their performance
   using the ``timeit`` module (Section XREF).


.. include:: footer.rst
