.. -*- mode: rst -*-
.. include:: ../definitions.txt
.. sectnum::

===========
5. Chunking
===========

------------
Introduction
------------

Chunking is an efficient and robust approach to parsing natural
language, and a popular alternative to the parsing that we will
see in later chapters.  Chunks are non-overlapping spans of text,
usually consisting of a head word (such as a noun) and the adjacent
modifiers and function words (such as adjectives and determiners).

There are two chief motivations for chunking: to locate information,
or to ignore information.  In the former case, we may want to extract
all noun phrases so they can be indexed.  A text retrieval system
could the use such an index to support efficient retrieval for queries
involving terminological expressions.

The reverse side of the coin is to *ignore* information.  Suppose that
we want to study syntactic patterns, finding particular verbs in a
corpus and displaying their arguments.  For instance, here are some uses of
the verb ``gave`` in the Wall Street Journal (in the Penn Treebank corpus sample).
After doing NP-chunking, the internal details of each noun phrase have been
suppessed, allowing us to see some higher-level patterns::

  gave NP
  gave up NP in NP
  gave NP up
  gave NP NP
  gave NP to NP
    
|nopar|
In this way we can acquire information about the complementation
patterns of a verb like ``gave``, for use in the development of a
grammar.

Chunking in NLTK begins with tagged text, represented as a flat tree:

    >>> from nltk_lite import chunk
    >>> input = chunk.tagstr2tree("the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN")
    >>> input.draw()

.. tree:: (S (DT the) (JJ little) (NN cat) (VBD sat) (IN on) (DT the) (NN mat))

Next, we write regular expressions over tag sequences.  The
following example identifies any noun phrases that are comprised of an
optional determiner, followed by any number of adjectives, then a
noun.

    >>> cp = chunk.Regexp('S', "NP: {<DT>?<JJ>*<NN>}")

|nopar| We create a chunker ``cp`` which can then be used
repeatedly to parse tagged input.  The result of chunking is also
a tree, but with some extra structure:

    >>> cp.parse(input).draw()

.. tree:: (S (NP (DT the) (JJ little) (NN cat)) (VBD sat) (IN on) (NP (DT the) (NN mat)))

In this chapter we explore chunking in depth, beginning with the
definition and representation of chunks.  We will see regular
expression and n-gram approaches to chunking, and will develop
and evaluate chunkers using the CoNLL-2000 chunking corpus.

--------------------------------
Defining and Representing Chunks
--------------------------------

An Analogy
----------

Two of the most common operations in language processing are
*segmentation* and *labeling*.  In tokenization we *segment*
a sequence of characters into tokens, while in tagging we *label* each of
these tokens.  Moreover, these two operations go hand in hand.  We
segment a stream of characters into linguistically meaningful pieces
(e.g.  as words) only so that we can classify those pieces (e.g. with
their part-of-speech categories) and then identify higher-level
structures.  The result of such classification is represented by
adding a label to the item in question.

In this chapter we do this segmentation and labeling at a higher
level, as illustrated in Figure chunk-segmentation_.  The solid boxes show
word-level segmentation and labeling, while the dashed boxes show a
higher-level segmentation and labeling.  These larger pieces are
called *chunks*, and the process of identifying them is called
`chunking`:dt:.

.. _chunk-segmentation:
.. figure:: ../images/chunk-segmentation.png

   Segmentation and Labeling at both the Token and Chunk Levels

Chunking is like tokenization and tagging in other respects.  First,
chunking can skip over material in the input.  Observe that only some
of the tagged tokens have been chunked, while others are left out.
Compare this with the way that tokenization has omitted spaces and
punctuation characters.  Second, chunking typically uses
regular expressions to identify material of interest.  For example, the chunk in
the above diagram could have been found by the expression
``<DT>?<JJ>*<NN>`` which matches an optional determiner, followed by
zero or more adjectives, followed by a noun.  Compare this with the
way that tokenization and tagging both make use of regular
expressions.
    
Chunking vs Parsing
-------------------

Chunking is akin to parsing in the sense that it can be used to build
hierarchical structure over text.  There are several important
differences, however.  First, as noted above, chunking is not
exhaustive, and typically omits items in the surface string.  Second,
where parsing constructs deeply nested structures, chunking creates
structures of fixed depth, (typically depth 2).  These
chunks often correspond to the lowest level of grouping identified in the
full parse tree, as illustrated in the parsing and chunking examples
in parsing-chunking_ below:

.. _parsing-chunking:
.. ex::
  .. ex::
    .. tree:: (NP <NP G.K. Chesterton> ,
                  (NP <NP author> of
                      (NP <NP The Man> who was <NP Thursday> ) ) )
  .. ex::
    .. tree:: (? <NP G.K. Chesterton> ,
                 <NP author> of
                 <NP The Man> who was
                 <NP Thursday> )

Another significant motivation for chunking is its robustness and
efficiency relative to parsing.  Parsing uses recursive phrase
structure grammars and arbitrary-depth trees.  Parsing has problems
with robustness, given the difficulty in getting broad coverage and in
resolving ambiguity.  Parsing is also relatively inefficient: the time
taken to parse a sentence grows with the cube of the length of the
sentence, while the time taken to chunk a sentence only grows
linearly.

Representing Chunks: Tags vs Trees
----------------------------------

As befits its intermediate status between tagging and parsing,
chunk structures can be represented using either tags or trees.  The most
widespread file representation uses so-called `IOB tags`:dt:.  In this
scheme, each token is tagged with one of three special chunk tags,
``INSIDE``, ``OUTSIDE``, or ``BEGIN``.  A token is tagged as ``BEGIN``
if it is at the beginning of a chunk, and contained within that chunk.
Subsequent tokens within the chunk are tagged ``INSIDE``.  All other
tokens are tagged ``OUTSIDE``.  An example of this scheme is shown
in Figure chunk-tagrep_.

.. _chunk-tagrep:
.. figure:: ../images/chunk-tagrep.png

   Tag Representation of Chunk Structures

IOB tags have become the standard way to represent chunk structures in
files, and we will also be using this format.  Here is an example of
the file representation of the information in Figure chunk-tagrep_::

  He PRP B-NP
  saw VBD O
  the DT B-NP
  big JJ I-NP
  dog NN I-NP

|nopar| In this representation, there is one token per line, each with
its part-of-speech tag and its chunk tag.  Chunk tags use ``B``,
``I``, and ``O`` for 'begin', 'inside', and 'outside' respectively.
Chunk tags also specify the chunk type (here ``NP``).  Of course, it
is not necessary to specify a chunk type for tokens that appear
outside a chunk.  We will see later that this format permits us to
represent more than one chunk type, so long as the chunks do not
overlap.  This file format was developed as part of the chunking evaluation
task run by the *Conference on Natural Language Learning* in 2000, and
has come to be called the `IOB Format`:dt:.

Chunk structures can also be represented using trees.  These have the
benefit that each chunk is a constituent that can be manipulated
directly.  An example is shown in Figure chunk-treerep_:
    
.. _chunk-treerep:
.. figure:: ../images/chunk-treerep.png

   Tree Representation of Chunk Structures

|nopar|
NLTK uses trees for its internal representation of chunks, and
provides methods for reading and writing such trees to the IOB
format.

--------
Chunking
--------

A `chunker`:dt: finds contiguous, non-overlapping spans of
related tokens and groups them together into *chunks*.  Chunkers
often operate on tagged texts, and use the tags to make chunking
decisions.  In this section we will see how to write a special
type of regular expression over part-of-speech tags, and then how
to combine these into a chunk grammar.  Then we will use the
grammar to chunk some tagged text.

Tag Patterns
------------

A `tag pattern`:dt: is a sequence of part-of-speech tags delimited
using angle brackets, e.g. ``<DT><JJ><NN>``.  Tag patterns are
identical to the regular expression patterns we have already seen,
except for two differences which make them easier to use for chunk
parsing.  First, the angle brackets group their contents into atomic
units, so "``<NN>+``" matches one or more repetitions of the tag
string "``<NN>``"; and "``<NN|JJ>``" matches the tag strings
"``<NN>``" or "``<JJ>``."  Second, the period wildcard operator is
constrained not to cross tag delimiters, so that "``<N.*>``" matches
any single tag starting with "``N``."  Now, consider the following
noun phrases from the Wall Street Journal::

  another/DT sharp/JJ dive/NN
  trade/NN figures/NNS
  any/DT new/JJ policy/NN measures/NNS
  earlier/JJR stages/NNS
  Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP

.. todo: how are they to view this CoNLL data?

We can match these using a slight refinement of the first tag pattern
above: ``<DT>?<JJ.*>*<NN.*>``.  This can be used to chunk any sequence
of tokens beginning with an optional determiner ``DT``, followed by
zero or more adjectives of any type ``JJ.*`` (such as relative
adjectives like ``earlier/JJR``), followed by a single noun of any
type ``NN.*``.  It is easy to find many more difficult examples::

  his/PRP$ Mansion/NNP House/NNP speech/NN
  the/DT price/NN cutting/VBG
  3/CD %/NN to/TO 4/CD %/NN
  more/JJR than/IN 10/CD %/NN
  the/DT fastest/JJS developing/VBG trends/NNS
  's/POS skill/NN

Chunking with Regular Expressions
---------------------------------

The chunker begins with a flat structure in which no tokens are
chunked.  Patterns are applied in turn, successively updating the
chunk structure.  Once all of the patterns have been applied, the
resulting chunk structure is returned.  Here is an simple chunk
grammar consisting of two patterns.  The first pattern
matches an optional determiner, zero or more adjectives, then
a noun.  We also define some input to be chunked.

    >>> grammar = r"""
    ... NP:
    ...   {<DT>?<JJ>*<NN>}    # chunk determiners, adjectives and nouns
    ...   {<NNP>+}            # chunk sequences of proper nouns
    ... """
    >>> input = chunk.tagstr2tree("the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN")

|nopar|
Next we can set up a chunker and run it on the input:

    >>> cp = chunk.Regexp('S', grammar)
    >>> print cp.parse(input)
    (S:
      (NP: ('the', 'DT') ('little', 'JJ') ('cat', 'NN'))
      ('sat', 'VBD')
      ('on', 'IN')
      (NP: ('the', 'DT') ('mat', 'NN')))

If a tag pattern matches at multiple overlapping locations, the first
match takes precedence.  For example, if we apply a rule that matches
two consecutive nouns to a text containing three consecutive nouns,
then the first two nouns will be chunked:

    >>> input = chunk.tagstr2tree("money/NN market/NN fund/NN")
    >>> grammar = "NP: {<NN><NN>}  # Chunk two consecutive nouns"
    >>> cp = chunk.Regexp('S', grammar)
    >>> print cp.parse(input)
    (S: (NP: ('money', 'NN') ('market', 'NN')) ('fund', 'NN'))

Developing Chunkers
-------------------

Creating a good chunker usually requires several iterations of
development and testing, during which existing rules are refined and
new rules are added.  In order to diagnose any problems, it often
helps to trace the execution of a chunker, using the ``trace``
argument of the chunker.  The tracing output shows the rules that are
applied, and uses braces to show the chunks that are created at each
stage of processing.
In the following example, two chunk patterns are applied to the input
sentence.  The first rule finds all sequences of three tokens whose
tags are ``DT``, ``JJ``, and ``NN``, and the second rule finds any
sequence of tokens whose tags are either ``DT`` or ``NN``.
      
.. Note: Each rule must have a "description" associated with it, which
   provides a short explanation of the purpose or the effect of the rule.
   This description is accessed via the ``descr()`` method.
    
    >>> input = chunk.tagstr2tree("the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN")
    >>> grammar = r"""
    ... NP:
    ...   {<DT><JJ><NN>}      # Chunk det+adj+noun
    ...   {<DT|NN>+}          # Chunk sequences of NN and DT
    ... """
    >>> cp = chunk.Regexp('S', grammar)
    >>> print cp.parse(input, trace=1)
    # Input:
     <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
    # Chunk det+adj+noun:
    {<DT>  <JJ>  <NN>} <VBD>  <IN>  <DT>  <NN> 
    # Chunk sequences of NN and DT:
    {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
    (S:
      (NP: ('the', 'DT') ('little', 'JJ') ('cat', 'NN'))
      ('sat', 'VBD')
      ('on', 'IN')
      (NP: ('the', 'DT') ('mat', 'NN')))

Observe that when we chunk material that is already partially chunked,
the chunker will only create chunks that do not partially overlap
existing chunks.  Thus, if we apply these two rules in reverse order,
we will get a different result:

    >>> grammar = r"""
    ... NP:
    ...   {<DT|NN>+}          # Chunk sequences of NN and DT
    ...   {<DT><JJ><NN>}      # Chunk det+adj+noun
    ... """
    >>> cp = chunk.Regexp('S', grammar)
    >>> print cp.parse(input, trace=1)
    # Input:
     <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
    # Chunk sequences of NN and DT:
    {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
    # Chunk det+adj+noun:
    {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
    (S:
      (NP: ('the', 'DT'))
      ('little', 'JJ')
      (NP: ('cat', 'NN'))
      ('sat', 'VBD')
      ('on', 'IN')
      (NP: ('the', 'DT') ('mat', 'NN')))

Here, rule 2 ("chunk det+adj+noun") did not find any chunks, since all
chunks that matched its tag pattern overlapped with existing chunks.

Exercises
---------

#. **Chunking Demonstration:**
   Run the chunking demonstration::

      from nltk_lite.parse import chunk
      chunk.demo()  # the chunker

#. Write a tag pattern to match noun phrases containing plural head nouns,
   e.g. "many/JJ researchers/NNS", "two/CD weeks/NNS", "both/DT new/JJ positions/NNS".
   Try to do this by generalizing the tag pattern that handled singular
   noun phrases.

#. Write tag pattern to cover noun phrases that contain gerunds,
   e.g. "the/DT receiving/VBG end/NN", "assistant/NN managing/VBG editor/NN".
   Add these patterns to the grammar, one per line.  Test your work using
   some tagged sentences of your own devising.

#. Write one or more tag patterns to handle coordinated noun phrases,
   e.g. "July/NNP and/CC August/NNP",
   "all/DT your/PRP$ managers/NNS and/CC supervisors/NNS",
   "company/NN courts/NNS and/CC adjudicators/NNS".

#. Sometimes a word is incorrectly tagged, e.g. the head noun in
   "12/CD or/CC so/RB cases/VBZ".  Instead of manually correcting
   these errors, good chunkers are able to work with the erroneous
   output of taggers.  Look for other examples of correctly chunked
   noun phrases with incorrect tags.

----------
Scaling Up
----------

Reading the CoNLL Chunking Format
---------------------------------

Using the ``nltk_lite.corpora`` module we can load Wall Street Journal
text that has been tagged, then chunked using the CoNLL IOB
(inside/outside/begin) notation.  The chunk categories provided in
this corpus are NP, VP and PP.  As we have seen, each sentence is
represented using multiple lines, as shown below::
    
  he PRP B-NP
  accepted VBD B-VP
  the DT B-NP
  position NN I-NP
  ...

|nopar| A conversion function ``chunk.conllstr2tree()`` builds a tree
representation from one of these multi-line strings.  Moreover, it
permits us to choose any subset of the three chunk types to use.  The
example below produces only ``NP`` chunks:
    
    >>> text = '''
    ... he PRP B-NP
    ... accepted VBD B-VP
    ... the DT B-NP
    ... position NN I-NP
    ... of IN B-PP
    ... vice NN B-NP
    ... chairman NN I-NP
    ... of IN B-PP
    ... Carlyle NNP B-NP
    ... Group NNP I-NP
    ... , , O
    ... a DT B-NP
    ... merchant NN I-NP
    ... banking NN I-NP
    ... concern NN I-NP
    ... . . O
    ... '''
    >>> chunk.conllstr2tree(text).draw()

.. tree:: (S (NP (PRP he))
             (VBD accepted)
             (NP (DT the) (NN position))
             (IN of)
             (NP (NN vice) (NN chairman))
             (IN of)
             (NP (NNP Carlyle) (NNP Group))
             (, ,)
             (NP (DT a) (NN merchant) (NN banking) (NN concern))
             (. .))
  
Accessing the CoNLL 2000 Corpus
-------------------------------

The CoNLL 2000 corpus contains 270k words of Wall Street Journal text,
with part-of-speech tags and chunk tags in the format we have already seen.
We can access this data using an NLTK corpus reader called ``conll2000``.
Here is an example:

    >>> from nltk_lite.corpora import conll2000, extract
    >>> print extract(2000, conll2000.chunked())
    (S:
      (NP: ('Health-care', 'JJ') ('companies', 'NNS'))
      (VP: ('should', 'MD') ('get', 'VB'))
      ('healthier', 'JJR')
      (PP: ('in', 'IN'))
      (NP: ('the', 'DT') ('third', 'JJ') ('quarter', 'NN'))
      ('.', '.'))

This just showed three chunk types, for NP, VP and PP.
We can also select which chunk types to read:

    >>> from nltk_lite.corpora import conll2000, extract
    >>> print extract(2000, conll2000.chunked(chunk_types=('NP',)))
    (S:
      (NP: ('Health-care', 'JJ') ('companies', 'NNS'))
      ('should', 'MD')
      ('get', 'VB')
      ('healthier', 'JJR')
      ('in', 'IN')
      (NP: ('the', 'DT') ('third', 'JJ') ('quarter', 'NN'))
      ('.', '.'))


Simple Evaluation
-----------------

Armed with a corpus, it is possible to do some simple evaluation...

Baseline, with empty grammar.  Nothing is chunked.

    >>> cp = chunk.Regexp('S', "")
    >>> print chunk.accuracy(cp, conll2000.chunked(chunk_types=('NP',)))
    0.440845995079

    >>> grammar = r"""NP: {<DT|JJ|NN.*>+}"""
    >>> cp = chunk.Regexp('S', grammar)
    >>> print chunk.accuracy(cp, conll2000.chunked(chunk_types=('NP',)))
    0.80523504324

Baseline Chunker
----------------

    >>> def chunked_tags(train):
    ...     """Generate a list of tags that tend to appear inside chunks"""
    ...     from nltk_lite.probability import ConditionalFreqDist
    ...     cfdist = ConditionalFreqDist()
    ...     for t in train:
    ...         for word, tag, chtag in chunk.tree2conlltags(t):
    ...             if chtag == "O":
    ...                 cfdist[tag].inc(False)
    ...             else:
    ...                 cfdist[tag].inc(True)
    ...     return [tag for tag in cfdist.conditions() if cfdist[tag].max() == True]




    >>> def baseline_chunker(train):
    ...     import re
    ...     chunk_tags = [re.sub(r'(\W)', r'\\\1', tag) for tag in chunked_tags(train)]
    ...     grammar = 'NP: {<' + '|'.join(chunk_tags) + '>+}'
    ...     return chunk.Regexp('S', grammar)

    >>> cp = baseline_chunker(conll2000.chunked(files='train', chunk_types=('NP',)))
    >>> print chunk.accuracy(cp, conll2000.chunked(files='test', chunk_types=('NP',)))
    0.914262194736


Splitting and Merging
---------------------



Chinking
--------
      
Sometimes it is easier to define what we *don't* want to include in a
chunk than it is to define what we *do* want to include.  In these
cases, it may be easier to build a chunker using a method called
`chinking`:dt:.

The word `chink`:dt: initially meant a sequence of stopwords,
according to a 1975 paper by Ross and Tukey (cited by Abney in the
recommended reading for this chapter).
Following Abney, we define a *chink* is a sequence
of tokens that is not included in a chunk.
In the following example, ``sat/VBD on/IN`` is a chink::

  [ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]

Chinking is the process of removing a sequence of tokens from a
chunk.  If the sequence of tokens spans an entire chunk, then the
whole chunk is removed; if the sequence of tokens appears in the
middle of the chunk, these tokens are removed, leaving two chunks
where there was only one before.  If the sequence is at the beginning
or end of the chunk, these tokens are removed, and a smaller chunk
remains.  These three possibilities are illustrated in the following
table:

+--------------------------------------------------------------------------+
| Chinking                                                                 |
+=============+=====================+===================+==================+
|             | Entire chunk        | Middle of a chunk | End of a chunk   |
+-------------+---------------------+-------------------+------------------+
| *Input*     | [a/DT  big/JJ       | [a/DT  big/JJ     | [a/DT  big/JJ    | 
|             | cat/NN]             | cat/NN]           | cat/NN]          |
+-------------+---------------------+-------------------+------------------+
| *Operation* | Chink "DT JJ NN"    | Chink "JJ"        | Chink "DT"       |
+-------------+---------------------+-------------------+------------------+
| *Pattern*   | "}DT JJ NN{"        | "}JJ{"            | "}DT{"           |
+-------------+---------------------+-------------------+------------------+
| *Output*    | a/DT  big/JJ        | [a/DT] big/JJ     | [a/DT  big/JJ]   |
|             | cat/NN              | [cat/NN]          | cat/NN           |
+-------------+---------------------+-------------------+------------------+

In the following grammar, we put the entire sentence into a single
chunk, then excise the chink:

    >>> grammar = r"""
    ... NP:
    ...   {<.*>+}          # Chunk everything
    ...   }<VBD|IN>+{      # Chink sequences of VBD and IN
    ... """
    >>> cp = chunk.Regexp('S', grammar)
    >>> print cp.parse(input)
    (S:
      (NP: ('the', 'DT') ('little', 'JJ') ('cat', 'NN'))
      ('sat', 'VBD')
      ('on', 'IN')
      (NP: ('the', 'DT') ('mat', 'NN')))
    >>> print chunk.accuracy(cp, conll2000.chunked(files='test', chunk_types=('NP',)))
    0.581041433607

A chunk grammar can use any number of chunking and chinking patterns
in any order.

Multiple Chunk Types
--------------------

    >>> grammar = r"""
    ... NP: {<DT>?<JJ>*<NN>}    # chunk determiners, adjectives and nouns
    ... VP: {<TO>?<VB.*>}       # VP = verb words
    ... """
    >>> cp = chunk.Regexp('S', grammar)



Exercises
---------

#. **IOB Tagging:**
   A common file representation of chunks uses the tags ``BEGIN``,
   ``INSIDE`` and ``OUTSIDE``.  Why are three tags necessary?  What
   problem would be caused if we used ``INSIDE`` and ``OUTSIDE`` tags
   exclusively?
        
#. **CoNLL 2000 Corpus:**
   In this section we saw how chunked data could be read from the Treebank
   corpus.  Write a similar program to access the first sentence of the
   CoNLL 2000 corpus.  You will need to import the ``conll2000`` module
   from the ``nltk_lite.corpora`` package.

#. **Format Conversion:**
   We have seen two file formats for chunk data, and NLTK-Lite provides corpus
   readers for both.

   a) Write functions ``chunk2brackets()`` and ``chunk2iob()`` which take a single
      chunk structure as their sole argument, and return the required multi-line string
      representation.
   b) Write command-line conversion utilities ``bracket2iob.py`` and ``iob2bracket.py``
      that take a file in Treebank or CoNLL format (resp) and convert it to the other
      format.  (Obtain some raw Treebank or CoNLL data from the NLTK Corpora, save it
      to a file, and then use ``open(filename).readlines()`` to access it from Python.)

#. **Simple Chunker:**
   Pick one of the three chunk types in the CoNLL corpus.
   Inspect the CoNLL corpus and try to observe any patterns in the POS tag sequences
   that make up this kind of chunk.  Develop a simple chunker using
   the regular expression chunker ``chunk.Regexp``.
   Discuss any tag sequences that are difficult to chunk reliably.

#. **Automatic Analysis:**
   Pick one of the three chunk types in the CoNLL corpus.  Write functions
   to do the following tasks for your chosen type:

   a) List all the tag sequences that occur with each instance of this chunk type.
   b) Count the frequency of each tag sequence, and produce a ranked list in
      order of decreasing frequency; each line should consist of an integer (the frequency)
      and the tag sequence.
   c) Inspect the high-frequency tag sequences.  Use these as the basis for
      developing a better chunker.

#. **Chinking:** An early definition of *chunk* was the material that occurs between chinks.
   Develop a chunker which starts by putting the whole sentence in a single
   chunk, and then does the rest of its work solely by chinking.
   Determine which tags (or tag sequences) are most likely to make up chinks
   with the help of your own utility program.  Compare the performance and
   simplicity of this approach relative to a chunker based entirely on
   chunk rules.

#. **Inherent ambiguity:** We saw in the tagging chapter that it is possible to
   establish an upper limit to tagging performance by looking for ambiguous n-grams,
   n-grams that are tagged in more than one possible way in the training data.
   Apply the same method to determine an upper limit on the performance of an n-gram
   chunker.

#. **Complex Chunker:**
   Develop a chunker for one of the chunk types in the CoNLL corpus using a
   regular-expression based chunk grammar ``RegexpChunk``.  Use any
   combination of rules for chunking, chinking, merging or splitting.

#. **Baseline NP Chunker:**
   The baseline chunker presented in the evaluation section tends to
   create larger chunks than it should.  For example, the
   phrase:
   ``[every/DT time/NN] [she/PRP] sees/VBZ [a/DT newspaper/NN]``
   contains two consecutive chunks, and our baseline chunker will
   incorrectly combine the first two: ``[every/DT time/NN she/PRP]``.
   Write a program that finds which of these chunk-internal tags
   typically occur at the start of a chunk, then
   devise a ``SplitRule`` that will split up these chunks.
   Combine this rule with the existing baseline chunker and
   re-evaluate it, to see if you have discovered an improved baseline.

#. **Predicate structure:**
   Develop an NP chunker which converts POS-tagged text into a list of
   tuples, where each tuple consists of a verb followed by a sequence of
   noun phrases and prepositions,
   e.g. ``the little cat sat on the mat`` becomes ``('sat', 'on', 'NP')``...
        
---------------
N-Gram Chunking
---------------

Our approach to chunking has been to try to detect structure based on
the part-of-speech tags.  We have seen that the IOB format represents
this extra structure using another kind of tag.  The question arises
then, as to whether we could use the same n-gram tagging methods we
saw in the last chapter, applied to a different vocabulary.

The first step is to get the word,tag,chunk triples from the CoNLL
corpus and map these to ``tag,chunk`` pairs:

    >>> from nltk_lite import tag
    >>> chunk_data = [[(t,c) for w,t,c in chunk.tree2conlltags(chtree)]
    ...              for chtree in conll2000.chunked()]

Now we can train and score a `unigram chunker`:dt: on this data, just as if
it was a tagger:

    >>> unigram_chunker = tag.Unigram()
    >>> unigram_chunker.train(chunk_data)
    >>> print tag.accuracy(unigram_chunker, chunk_data)
    0.781378851068

This chunker does reasonably well.  Let's look at the errors it makes.
Consider the opening phrase of the first sentence of the chunking
data, here shown with part of speech tags:

  Confidence/NN in/IN the/DT pound/NN is/VBZ widely/RB expected/VBN
  to/TO take/VB another/DT sharp/JJ dive/NN

We can try the unigram chunker out on this first sentence by creating
some "tokens" using ``[t for t,c in chunk_data[0]]``, then running
our chunker over them using ``list(unigram_chunker.tag(tokens))``.
The unigram chunker only looks at the tags, and tries to add chunk
tags.  Here is what it comes up with:

  NN/I-NP IN/B-PP DT/B-NP NN/I-NP VBZ/B-VP RB/O VBN/I-VP TO/B-PP
  VB/I-VP DT/B-NP JJ/I-NP NN/I-NP

Notice that it tags the first noun ``Confidence/NN`` incorrectly as ``I-NP`` and not
``B-NP``, because nouns usually do not occur at the start of noun
phrases in the training data.  It correctly tags the second ``pound/NN`` as
``I-NP`` (this noun occurs after a determiner).  It incorrectly tags
``widely/RB`` as outside ``O``, and it incorrectly tags the
infinitival ``to/TO`` as ``B-PP``, as if it was a preposition starting a
prepositional phrase.

[Why these problems might go away if we look at the previous chunk tag?]

Let's run a bigram chunker:

    >>> bigram_chunker = tag.Bigram(backoff=unigram_chunker)
    >>> bigram_chunker.train(chunk_data)
    >>> print tag.accuracy(bigram_chunker, chunk_data)
    0.89312652614

We can run the bigram chunker over the same sentence as before
using ``list(bigram_chunker.tag(tokens))``.
Here is what it comes up with:

  NN/B-NP IN/B-PP DT/B-NP NN/I-NP VBZ/B-VP RB/I-VP VBN/I-VP TO/I-VP
  VB/I-VP DT/B-NP JJ/I-NP NN/I-NP

This is 100% correct.

Exercises
---------

#. **Bigram chunker:** The bigram chunker scores about 90% accuracy.
   Study its errors and try to work out why it doesn't get 100% accuracy.

#. (Advanced) **Modularity:**
   Consider the way an n-gram tagger uses recent tags to inform its tagging choice.
   Now observe how a chunker may re-use this sequence information.  For example,
   both tasks will make use of the information that nouns tend to follow adjectives
   (in English).  It would appear that the same information is being maintained in
   two places.  Is this likely to become a problem as the size of the rule sets grows?
   If so, speculate about any ways that this problem might be addressed.

-----------------
Cascaded Chunking
-----------------

[DRAFT, TO BE EXPANDED]

So far, our chunk structures have been relatively flat: trees
consisting of tagged tokens, optionally grouped under a chunk node
such as ``NP``.  It is possible to build chunk structures of arbitrary
depth, simply by creating a multi-stage chunk grammar.

First we define several chunkers, e.g. for noun phrases, prepositional
phrases, verb phrases, and sentences.

    >>> grammar = """
    ... NP: {<DT|JJ|NN.*>+}       # Chunk sequences of DT, JJ, NN
    ... PP: {<IN><NP>}            # Chunk prepositions followed by NP
    ... VP: {<VB.*><NP|PP|S>+$}   # Chunk verbs and arguments/adjuncts
    ... S:  {<NP><VP>$}           # Chunk NP, VP
    ... VP: {<VB.*><NP|PP|S>+$}   # Chunk verbs and arguments/adjuncts
    ... S:  {<NP><VP>$}           # Chunk NP, VP
    ... """
    >>> cp = chunk.Regexp('S', grammar)

Next, we create some tagged data and chunk it:

    >>> input = chunk.tagstr2tree("""John/NNP thinks/VBZ Mary/NN saw/VBD the/DT cat/NN
    ...           sit/VB on/IN the/DT mat/NN""")
    >>> print cp.parse(input)
    (S:
      (NP: ('John', 'NNP'))
      ('thinks', 'VBZ')
      (S:
        (NP: ('Mary', 'NN'))
        (VP:
          ('saw', 'VBD')
          (S:
            (NP: ('the', 'DT') ('cat', 'NN'))
            (VP:
              ('sit', 'VB')
              (PP: ('on', 'IN') (NP: ('the', 'DT') ('mat', 'NN'))))))))

.. Note:: At present there is no systematic way of evaluating these
   cascading chunkers in NLTK.

----------
Conclusion
----------

[More discussion of statistical vs rule-based approach.]

In this chapter we have explored a robust method for identifying
structure in text using chunkers.  There are a surprising number
of different ways to chunk a sentence.  The chunk rules can add, shift
and remove chunk delimiters in many ways, and the chunk rules can be
combined in many ways.  One can use a small number of very complex
rules, or a long sequence of much simpler rules.  One can hand-craft a
collection of rules, or train up a brute-force method using existing
chunked text.

We have seen that the same light-weight methods that were successful in
tagging can be applied in the recognition of simple linguistic structure.
The resulting structured information is useful in information extraction
tasks and in the description of the syntactic environments of words.
The latter will be invaluable as we move to full parsing.
      
Like tagging, chunking is an example of lightweight methodology in
natural language processing: how far can we get with identifying
linguistic structures (such as phrases, verb arguments, etc) with
recourse only to local, surface context?  Also like tagging, chunking
cannot be done perfectly.  For example, as pointed out by Abney
(1996), we cannot correctly analyze the structure of the sentence *I
turned off the spectroroute* without knowing the meaning of
*spectroroute*; is it a kind of road or a type of device?  Without
knowing this, we cannot tell whether *off* is part of a prepositional
phrase indicating direction, or whether *off* is part of the verb-particle
construction *turn off*.  This structural ambiguity is shown in spectroroute_.

.. _spectroroute:
.. ex::
  .. ex:: Prepositional phrase: [ I ] [ turned ] [ off the spectroroute ]
  .. ex:: Verb-particle construction: [ I ] [ turned off ] [ the spectroroute ]

A recurring theme of this chapter has been *diagnosis*.  The simplest
kind is manual, when we inspect the output of a chunker and observe
some undesirable behavior that we would like to fix.  We have also
seen three objective approaches to diagnosis.  The first approach is
to write utility programs to analyze the training data, such as
counting the number of times a given part-of-speech tag occurs inside
and outside an NP chunk.  The second approach is to perform error
analysis on the missed and incorrect chunks produced by the chunker.
Sometimes those errors can be fixed.  In other cases we may
observe shortcomings in the methodology itself, cases where we cannot
hope to get the correct answer because the system simply does not have
access to the necessary information.  The third approach is to
evaluate the system against some gold standard data to obtain an
overall performance score; we can use this diagnostically by
parameterising the system, specifying which chunk rules are used on a
given run, and tabulating performance for different parameter
combinations.  Careful use of these diagnostic methods permits us to
*tune* the performance of our system.  We will see this theme emerge
again later in chapters dealing with other topics in natural language
processing.

---------------
Further Reading
---------------

Abney, Steven (1996). Tagging and Partial Parsing.
In: Ken Church, Steve Young, and Gerrit Bloothooft (eds.),
*Corpus-Based Methods in Language and Speech.*
Kluwer Academic Publishers, Dordrecht.
``http://www.vinartus.net/spa/95a.pdf``

Abney's Cass system:
   ``http://www.vinartus.net/spa/97a.pdf``


.. include:: footer.txt
