\documentclass[11pt]{article}
\usepackage{acl02sub,url,alltt,epsfig}
\title{NLTK: The Natural Language Toolkit}
\author{
Edward Loper and Steven Bird\\
Department of Computer and Information Science and Linguistic Data Consortium\\
University of Pennsylvania, Philadelphia, PA 19104-6389, USA
}
\summary{%
NLTK, the Natural Language Toolkit, is a suite of program modules,
tutorials and problem sets, providing ready-to-use CL courseware.
NLTK covers symbolic and statistical
NLP, and is interfaced to annotated corpora.
Students augment and replace existing components, learning
structured programming by example, and manipulating sophisticated
models from the outset.}
\paperid{Pxxxx}
\keywords{teaching, courseware, Python, corpora}
\contact{Edward Loper}
\conference{this paper has not been submitted to any other conferences}
\date{\today}

% Note: the maximum paper length is 6 pages.

% Outline:
%   - abstract
%   - introduction
%   - choice of programming language
%   - design criteria
%   - python implementation (??)
%   - modules
%   - uses
%     - assignments/projects
%     - class demonstrations
%   - evaluation
%   - other approaches
%   - conclusion

% To do:
%   - Fill in OVERVIEW in the intro
%   - Write up evaluation
%   - Write up other approaches
%   - Write up conclusion
%   - Get rid of Python implementation section, or at least shorten it?
%   - General clean-up
%   - ispell

% We need more content on:
%   - documentation
%   - corpora?

\newenvironment{sv}{\scriptsize\begin{alltt}}{\end{alltt}\normalsize}

\begin{document}
\makeidpage
\maketitle

\begin{abstract}
Students in computational linguistics courses must often learn a new
programming language.  In some cases, such as when courses are offered
in linguistics departments, the students may be learning to program
for the very time.  In order to do interesting projects, it is usually
necessary for students to do many low-level ``housekeeping'' tasks.
At the same time, teachers of computational linguistics courses
sometimes feel that they spend too much time teaching students to
program, and not enough time teaching the subject itself.  They may
even avoid programming assignments altogether.  However, we believe
that it is crucial for a first computational linguistics course to
include a strong practical component, in which students develop real
programs to solve real problems with real data.

Python is a new object-oriented scripting language which runs on all
platforms.  Python has been praised as ``executable pseudocode,'' since
programs are so easy to write.  Recently, we have been developing
NLTK, an open-source Natural Language Toolkit written in Python.
In this presentation, we will motivate, describe and demonstrate NLTK.

NLTK, the Natural Language Toolkit, is a suite of program modules,
tutorials and problem sets.  NLTK covers symbolic and statistical
natural language processing, and is interfaced to annotated corpora.
Students augment and replace existing NLTK components, learning
structured programming by example, and manipulating sophisticated
models from the outset.  Along with extensive documentation and
problem sets, NLTK provides self-contained, ready-to-use CL
courseware.
\end{abstract}

\section{Introduction}

Teachers of introductory courses on computational linguistics are
often faced with the challenge of setting up a practical programming
component for student assignments and projects.  This is difficult not
just because of the variety of data structures, but also because of
the diverse range of topics which may need to be included in the
syllabus.  A widespread practice is to teach multiple programming
languages, where each language provides native data structures and functions
that are a good fit for the task at hand (e.g. Prolog for parsing,
Perl for corpus processing, a finite-state toolkit for morphological
analysis).  By relying on the built-in features of various languages,
the teacher avoids having to develop a lot of software infrastructure.
An unfortunate consequence is that a significant part of the course
must be devoted to teaching these languages.  Further, many
interesting projects require the languages to be bridged, e.g. a
project that involved syntactic parsing of corpus data from a
morphologically rich language would involve all three languages
mentioned above (Perl for file I/O, format conversions and overall
program control, with calls out to a finite state toolkit for
morphological analysis and to a Prolog engine for parsing).  It is
clear that these considerable overheads and shortcomings warrant a
fresh approach.

Apart from the practical component, computational linguistics courses
may also depend on software for in-class demonstrations.  This context
calls for highly interactive graphical user interfaces making it
possible to view program state (e.g. the chart of a chart parser),
observe program execution step-by-step (e.g. execution of a
finite-state machine), and even make minor modifications to programs
in response to ``what if'' questions from the class.  Because of these
difficulties it is common to avoid live demonstrations, and keep
classes for theoretical presentations only.  Apart from being dull,
this approach leaves students to solve important practical problems on
their own, or to deal with them less efficiently in office hours.

In this paper we describe a new approach to the above challenges,
a streamlined and flexible way of organizing the practical component
of an introductory computational linguistics course.  We describe
NLTK, the Natural Language Toolkit, which we have developed in
conjunction with a course we have taught at the University of Pennsylvania.

OVERVIEW OF THE PAPER

All materials discussed here are available under an open
source license from \url{nltk.sf.net}.
NLTK runs on most platforms, including Windows, OS X, Linux, and
UNIX.

\section{Choice of programming language}

The most basic step in setting up a practical component is choosing a
suitable programming language.  Here we list the desiderata that influence
our choice.

First, the language must have a shallow learning curve, so that novice
programmers get immediate rewards for their efforts.
Second, the language must support rapid prototyping and a short develop/test cycle;
an obligatory compilation step is a serious detraction.
Third, the code should be self-documenting,
with a transparent syntax and semantics.
Fourth, it should be easy to write structured programs, ideally object-oriented
but without the burden associated with languages like C++.
Finally, the language must have an easy-to-use graphics library to support
the development of simple graphical user interfaces.

In surveying the available languages, we believe that Python offers an especially good
fit to the above requirements.  Python is an object-oriented scripting language
developed by Guido van Rossum [REF] and available on all platforms
\url{www.python.org}.
Python offers a shallow learning curve by virtue of the
fact that it was designed to be easily learnt by children [REF].
As an interpreted language, Python is suitable for rapid prototyping.
Python code is exceptionally readable, and it has been praised as
``executable pseudocode.''
Python is an object-oriented language, but not punitively so, and it
is easy to encapsulate data and methods inside Python classes.
Finally, Python has an interface to the ``tk'' graphics toolkit, and
writing graphical interfaces is straightforward.

\section{Design criteria}

Several criteria were considered in the design and implementation of
the toolkit.  The design criteria are divided into primary and
secondary criteria, and within those sections are listed in the order
of their importance.  It was also important to decide what goals the
toolkit would \emph{not} attempt to accomplish; we therefore include
an explicit set of non-requirements, which the toolkit is not expected
to satisfy.

\subsection{Primary Design Criteria}

\noindent\textit{\textbf{Ease of Use.}} The primary purpose of the toolkit is
to allow students to concentrate on building NLP systems.  The more
time students must spend learning to use the toolkit, the less useful
it is.

\noindent\textit{\textbf{Consistency.}} The toolkit should use consistent data
structures and interfaces.

\noindent\textit{\textbf{Extensibility.}} The toolkit should easily
accommodate new components, whether those components replicate or
extend the toolkit's existing functionality.  Thus, the toolkit's
design should be modular, with simple and well-defined interfaces
between modules.  The toolkit should also be structured in such a way
that it is obvious where new extensions would fit into the toolkit's
infrastructure.

\noindent\textit{\textbf{Documentation.}} The toolkit, its data structures,
and its implementation all need to be documented.  Three documentation
types are necessary: technical reports, to explain the toolkit's
design and implementation; tutorials, to teach students how to use the
toolkit to perform specific tasks; and reference documentation, to
describe every module, interface, class and method.  All nomenclature
must be carefully chosen and consistently used.

\subsection{Secondary Design Criteria}

\noindent\textit{\textbf{Simplicity.}} The toolkit should structure the
complexities of building NLP systems, not hide them.  Therefore, each
class defined by the toolkit should be simple enough that a student
could implement it by the time they finish the course.

\noindent\textit{\textbf{Modularity.}} The interaction between different
components of the toolkit should be kept to a minimum.  In particular,
it should be possible to complete individual projects using small
parts of the toolkit, without worrying about how they interact with
the rest of the toolkit.  This will allow students to learn to use the
toolkit incrementally throughout the course.  Modularity also makes it
easier to change or extend the toolkit.

\subsection{Non-Requirements}

\noindent\textit{\textbf{Comprehensiveness.}} The toolkit is not intended to
provide a comprehensive set of tools.  Indeed, there should be a wide
variety of ways in which students can extend the toolkit.

\noindent\textit{\textbf{Efficiency.}} The toolkit does not need to be highly
optimized for runtime performance.  However, it should be efficient
enough that students can use their NLP systems to perform real tasks.

\noindent\textit{\textbf{Cleverness.}} Clear designs and implementations are
far preferable to ingenious yet indecipherable ones.

%\section{Python Implementation}
%
%The structure of the Python language directly affects the design and
%implementation of the toolkit.  This section discusses how the design
%of the toolkit is affected by the Python language.
%
%% [EL] This is the only section that I think we might want to keep
%% from python impl.  But I'm not sure we even need to keep this one.
%\noindent\textbf{Interfaces.}
%Python does not directly implement interfaces.  The toolkit
%therefore implements interfaces as simple classes, all of whose methods 
%raise \texttt{AssertionError} exceptions.  A description of the
%functionality provided by the interface is contained in the
%class's documentation string.  Each method's documentation string
%specifies the behavior of that method.
%In order to implement an interface, a class should include that
%interface as one of its bases, and should override every
%(non-optional) method defined by the interface.
%Classes that define interfaces are named with a trailing ``I,'' such
%as \texttt{TokenizerI} or \texttt{EventI}.
%
%% [EL] Mention that Python doesn't provide native type checking?  Or
%% ditch this section? :)
%\noindent\textbf{Typing.}
%Type checking is an important safety device, allowing programmers to
%quickly locate errors in their code.  This is especially important for
%novice programmers, who have less experience with debugging.  Every
%function and method defined by the toolkit will check the types of its
%its arguments.  Given the impact on performance, the level of type checking
%will be adjustable.
%
%\noindent\textbf{Advanced Language Features.}
%The use of advanced language features and programming styles could
%potentially make the toolkit much more difficult to use or to
%understand.  The fewer language features that a student must learn in
%order to use the toolkit, the faster they can start developing NLP
%systems.  The language features used by
%the toolkit are summarized by figure \ref{fig:feature}.
%
%\begin{figure}
%\begin{centering}
%\begin{tabular}{|l|l|}
%\hline
%\noindent\textbf{Language Feature} & \textbf{Use} \\
%\hline
%Classes & used \\
%Exceptions & error conditions only \\
%Operator Overloading & used \\
%Python 2.0 Features & used \\
%Python 2.1 Features & not used \\
%Function Arguments & minimal use \\
%Lambda Functions & minimal use \\
%Mapping and Filtering & not used \\
%List Comprehensions & used \\
%Tkinter & used \\
%\hline
%\end{tabular}\\
%\end{centering}
%\caption{Advanced language features used by NLTK.}
%\label{fig:feature}
%\end{figure}

\section{Modules}
% [EL]: I think that this section works better as a textual
% description, rather than a long list.  Also, we're going to be short
% on space.. :)
% What order should these be in?  Put more impressive stuff at the
% beginning and end?

The toolkit is implemented as a collection of independant
\emph{modules}, each of which defines a specific data structure or
task.  

A set of core modules defines basic data types and processing tasks
that are used througout the toolkit.  The \texttt{token} module
defines basic classes for processing individual elements of text, such
as words or sentences.  The \texttt{tree} module provides data
structures for representing tree strucutres over text, such as syntax
trees and morphological trees.  The \texttt{probability} module
defines classes that encode frequency distributions and probability
distributions, including a variety of smoothing techniques.

The remaining modules define data structures and interfaces for
performing specific NLP tasks.  This list of modules will grow over
time, as we add new tasks and algorithms to the toolkit.

\subsection*{Parsing Modules}

The \texttt{parser} module defines a high-level interface for
producing trees that represent the structure of texts.  The
\texttt{chunkparser} module defines a subinterface for parsers that
identify non-overlapping linguistic groups (such as noun phrases) in
unrestricted text.  

Three modules provide implementations for these abstract interfaces.
The \texttt{srparser} module implements a simple shift-reduce parser.
The \texttt{chartparser} module provides a flexible parser that uses a
\emph{chart} to record hypotheses about syntactic constituents.  And
the \texttt{rechunkparser} module defines a transformational
regular-expression based implementation of the chunk parser interface.

\subsection*{Tagging Modules}

The \texttt{tagger} module defines a standard interface for augmenting
each token of a text with supplementary information, such as its part
of speech or its WordNet synset tag; and provides several
different implementations for that interface.

\subsection*{Finite State Automata}

% Steven: do you want to add anything here?  Say anything about HMMS? 
% Will you have implemented more interfaces by the time of ACL?
The \texttt{fsa} module defines a data type for encoding finite state
automata; and an interface for creating automata from
regular expressions.

\subsection*{Visualization}

A set of visualization modules define graphical interfaces for viewing
and editing data structures; and graphical tools for experimenting
with tasks.  The \texttt{draw.tree} module provides a
simple graphical interface for displaying tree structures; and the
\texttt{draw.tree\_edit} module provides an interface for building and
modifying tree structures.  The \texttt{draw.plot\_graph} module can be
used to graph mathematical functions.  The \texttt{draw.fsa} module
provides a graphical tool for displaying and simulating finite state
automata.  The \texttt{draw.chart} module provides an interactive
graphical tool for experimenting with chart parsers.

\subsection*{Text Classification}

The \texttt{classifier} module defines a standard interface for
classifying texts into categories.  This interface is currently
implemented by two modules: the \texttt{classifier.naivebayes} module
defines a text classifier based on the Naive Bayes assumption; and the
\texttt{classifier.maxent} module defines the maximum entropy model
for text classification, and includes two algorithms for training the
model: Generalized Iterative Scaling and Improved Iterative Scaling.

The \texttt{classifier.feature} module provides a standard encoding
for the information that is used to make decisions for a particular
classification task.  This standard encoding allows students to
experiment with the differences between different text classification
algorithms, using identical feature sets.

The \texttt{classifier.featureselection} module defines a standard
interface for choosing which features are relevant for a particular
classification task.  Good feature selection can signifigantly improve
classification performance.

%% Old content for this section:
%NLTK currently includes the following modules.
%
%\subsection*{Basics}
%
%\begin{description}
%\item[token:] Basic classes for processing individual
%      elements of text, such as words or sentences. 
%\item[tree:] Classes for representing tree structures
%      over text (such as syntax trees and morphological trees).
%\item[probability:] Classes that encode frequency
%      distributions and probability distributions.
%\end{description}
%
%\subsection*{Tagging}
%
%\begin{description}
%\item[tagger:] A standard interface to tag each token of
%      a text with supplementary information, such as its part of
%      speech; and several implementations of that interface.
%\end{description}
%
%\subsection*{Parsing}
%
%\begin{description}
%\item[parser:] A standard interface to produce trees
%      representing the structure of texts.
%\item[chartparser:] A flexible parser implementation
%      that uses a \emph{chart} to record hypotheses about
%      syntactic constituents.
%\item[srparser\_template:] A partial implementation of a
%      shift-reduce parser; completing the implementation was a
%      student exercise.
%\item[chunkparser:] A standard interface for robust
%      parsers used to identify non-overlapping linguistic groups
%      (such as noun phrases) in unrestricted text.
%\item[rechunkparser:] A regular-expression based
%      implementation of the chunk parser interface.
%\end{description}
%
%\subsection*{Text Classification}
%
%\begin{description}
%\item[classifier:] A standard interface for classifying
%      texts into categories.
%\item[classifier.feature:] A standard way of encoding
%      the information used to make classification decisions.
%\item[classifier.naivebayes:] A text classifier
%      implementation based on the Naive Bayes assumption.
%\item[classifier.maxent:] An implementation of the
%      maximum entropy model for text classification; and
%      implementations of the GIS and IIS algorithms for training
%      the classifier.
%\item[classifier.featureselection:] A standard interface
%      for choosing which features are relevant for making
%      classification decisions.
%\end{description}
%
%\subsection*{Visualization}
%
%\begin{description}
%\item[draw.tree:] A graphical representation for tree
%      structures, such as syntax trees and morphological trees. 
%\item[draw.tree\_edit:] A graphical interface used to
%      build and modify tree structures.
%\item[draw.plot\_graph:] A graphical tool to graph
%      arbitrary functions.
%\item[draw.chart:] An interactive graphical tool used to
%      experiment with chart parsers.
%\end{description}

\section{Uses of NLTK}

\subsection{Assigments and projects}

NLTK can be used to create student assignments of varying difficulty
and scope.  
% Use a module
In the simplest assignments, students experiment with an existing
module.  The wide variety of existing modules provide many opportunies
for creating these simple assignments.
% Edit/extend a module.
Once students become more familiar with the toolkit, they can be asked
to make minor changes or extensions to an existing module.
% Develop a new module.
A more challenging task is to develop a new module.  Here, NLTK
provides some useful starting points: predefined interfaces and data
structures, and existing modules that implement the same interface.
% Advanced projects..
Advanced projects typically involve the development of entirely new
functionality for a previously unsupported NLP task, or the
development of a complete system out of existing and new modules.

\subsubsection*{Example: Chunk Parsing}

% Akward
Here we illustrate the use of NLTK in an assignment of moderate
difficulty, for the case of chunk parsing.  This is a kind of parsing
where we only identify the main constituents of a phrase, a technique
with diverse applications.  The NLTK \texttt{chunk} module offers
a variety of rule formats.  For example,
\texttt{ChunkRule('<NN.*>')} builds chunks over consecutive noun tags;
\texttt{ChinkRule('<VB.>')} excises verbs from existing chunks;
\texttt{SplitRule('<NN>', '<DT>')} splits into two pieces any existing
chunk that contains a noun followed by a determiner; and
\texttt{MergeRule('<JJ>', '<JJ>')} combines two adjacent chunks
where the first chunk ends and the second chunk starts with \texttt{JJ}.
The goal of the assignment is to come up with a cascade of these rules,
and to create a set of chunks that are as similar as possible to the test set.

The chunking tutorial provides all the necessary code except for the
chunk rules.  The provided code is responsible for loading the
chunked, tagged text using an existing tokenizer, creating an
unchunked version of the text, applying the chunk rules to the
unchunked text, and scoring the result.  Students focus on the
NLP task only -- providing a ruleset with the best coverage.

In the remainder of this section we reproduce some of the cascades
created by the students.  The first example illustrates a combination
of several rule types:

\begin{sv}
cascade = [
  ChunkRule('<DT><NN.*><VB.><NN.*>'),
  ChunkRule('<DT><VB.><NN.*>'),
  ChunkRule('<.*>'),
  UnChunkRule('<IN|VB.*|CC|MD|RB.*>'),
  UnChunkRule("<,|{\textbackslash}.|``|''>"),
  MergeRule('<NN.*|DT|JJ.*|CD>', '<NN.*|DT|JJ.*|CD>'),
  SplitRule('<NN.*>', '<DT|JJ>')
]
\end{sv}

This next example illustrates a brute-force approach.  All tags most
commonly found inside of chunks are chunked, while other tags are excluded.

\begin{sv}
cascade = [
  ChunkRule('<{\textbackslash}\$|CD|DT|EX|PDT
             |PRP.*|WP.*|{\textbackslash}\#|FW
             |JJ.*|NN.*|POS|RBS|WDT>*')
]
\end{sv}

The third example involves putting everything in a single chunk,
then excising elements that do not belong.

\begin{sv}
cascade = [
  ChunkRule('<.*>+')
  ChinkRule('<VB.*|IN|CC|R.*|MD|WRB|TO|.|,>+')
]
\end{sv}

%% [EL] I'm not sure this is necessary:
%The main program which surrounds the student's work is moderately
%simple as well.  The line replaced by students is underlined.
%% [EL] I find the undeline hard to read; change to italics or bold or
%% vbar in the margin?  Or maybe a \Rightarrow of some sort in the margin?
%\begin{sv}
%from nltk.token import *
%from nltk.chunkparser import *
%from nltk.rechunkparser import *
%import os
%
%path = "/cis530/data/wsj/tagged/"
%
%sent_tok = RETokenizer('(=======+)?{\textbackslash}{\textbackslash}n{\textbackslash}{\textbackslash}n+', 0)
%sentences = []
%files = os.listdir(path)
%for file in files:
%    print 'Loading \%s...' \% file
%    text = open(path+file).read()
%    sentences += sent_tok.tokenize(text, source=file, unit='s')
%
%chunkscore = ChunkScore()
%
%\underline{cascade = [ ??? ]}
%chunkparser = REChunkParser(cascade)
%
%ctt = ChunkedTaggedTokenizer()
%for s in sentences:
%    correct = ctt.tokenize(s.type(), source=s.loc())
%    unchunked = unchunk(correct)
%    guess = chunkparser.parse(unchunked)
%    chunkscore.score(correct, guess)
% 
%print chunkscore
%\end{sv}

The score for each project is the precision, recall and F-measure.
Figure~\ref{fig:contest} shows the results; students with the best two
scores were presented with prizes.

\begin{figure}
\centerline{\epsfig{figure=contest.ps,width=\linewidth}}
\caption{Precision/Recall Graph for Chunking Competition}\label{fig:contest}
\vspace*{2ex}\hrule
\end{figure}

\subsection{Class demonstrations}

NLTK provides graphical tools that can be used to give in-class
demonstrations, which help to explain basic NLP concepts and
algorithms.  These interactive tools can be used to display relevant
data structures, and to show step-by-step executions of algorithms.
Both data structures and control flow can be easily modified during
the demonstration, in response questions from the class.

Since these graphical tools are included with the toolkit, they can
also be used by students.  This allows students to experiment with the
algorithms that they have seen presented in class.

\subsubsection*{Example: The Chart Parsing Tool}

The chart parsing tool can be used to explain the basic concepts
behind chart parsing, and to show how the algorithm works.  Chart
parsing is a flexible parsing algorithm that uses a data structure
called a \emph{chart} to record hypotheses about syntactic
constituents.  Each hypothesis is represented by a single \emph{edge}
on the chart.  A set of \emph{rules} determine when new edges can be
added to the chart.  This set of rules controls the overall behavior
of the parser (e.g., whether it parses top-down or bottom-up).

The chart parsing tool demonstrates the process of parsing a single
sentence, with a given grammar and lexicon.  Its display is divided
into three sections: the bottom section displays the chart; the middle
section displays the sentence; and the top section displays the
partial syntax tree corresponding to the selected edge.  Buttons along
the bottom of the window are used to control the execution of the
algorithm.  The main display window for the chart parsing tool is
shown in Figure~\ref{fig:chartparse}.   

This tool can be used to explain several different aspects of chart
parsing.  First, it can be used to explain the basic chart data
structure, and to show how edges can represent hypotheses about
syntactic constituents.  It can then be used to demonstrate and
explain the individual rules that the chart parsers uses to create new
edges.  Finally, it can be used to show how these individual rules
combine to find a complete parse for the sentence.

% Is ``user'' a good word here?  ``lecturer''?
To reduce the overhead of setting up demonstrations during lecture,
the user can define a list of preset charts.  The tool can then be
reset to any one of these charts at any time.

The chart parsing tool allows for flexible control of the parsing
algorithm.  At each step of the algorithm, the user can select which
rule or strategy they wish to apply.  This allows the user to
experiment with mixing different strategies (e.g., top-down and
bottom-up).  The user can exercise fine-grained control over the
algorithm by selecting which edge they wish to apply a rule to.  This
flexibility allows lecturers to use the tool to respond to a wide
variety of questions; and allows students to experiment with different
variations on the chart parsing algorithm.

\begin{figure}
%\centerline{\epsfig{figure=chart.ps,width=\linewidth}}
\caption{Chart Parsing Tool}\label{fig:chartparse}
\vspace*{2ex}\hrule
\end{figure}

\section{Evaluation}

About CIS-530: mixture of CS and linguistics students.
Approx 40 hours of classes.  All class materials (slides,
assignments) are posted on the course website
\url{http://www.cis.upenn.edu/~cis530/}.

The Language: Python was new to 95\% of students.  Some were initially
bothered about its block structuring, which is sensitive to white
space.  All quickly learnt the language, and reported that any initial
reservations were unfounded.  Many students said that they enjoyed
writing Python code.

CD-ROM: put NLTK and data on CD-ROM and gave it out at the start of class.

The toolkit: students liked being able to do interesting projects from
the outset, liked all the documentation, liked being able to run
everything on their machine at home, ...

Problems: available corpus data, ...

Other strengths / weaknesses

\section{Other approaches}

Java: \cite{Hammond02}
-- no obligation to cite this as it is unpublished
(and I haven't seen a copy).

\section{Conclusion}

Overview of the paper.

NOTES: Many CL textbooks provide good exercises at the end of each
chapter.  This isn't enough for many situations.  E.g. students spend
lots of time writing interface code (e.g. to process a corpus).
E.g. teachers spend lots of time creating do-able assignments...

NLTK is an open source project, and we welcome any contributions.  We
deliberately structured NLTK to facilitate parallel development.
Readers who are interested in contributing to NLTK, or who have any
suggestions for improvements, are encouraged to contact the authors.

\bibliographystyle{acl}
\bibliography{submission}

\end{document}
