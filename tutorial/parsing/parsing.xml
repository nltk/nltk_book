<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" [
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Parsing</title>
  </articleinfo>

  <section> <title> Introduction </title>

<para>
In the context of computational modelling, a language is often viewed
as a set of well-formed sentences.  Sequences of words which are not
grammatical are excluded from this set.  Now, since there is no
upper-bound on the length of a sentence, the number of possible
sentences is unbounded.
For example, it is possible to add an unlimited amount of
material to a sentence by using <literal>and</literal> or by
chaining relative clauses, e.g.
<literal>the dog that chased the cat that caught the mouse...</literal>.
There are an unlimited number of such sentences.
</para>

<para>[
EDWARD: talk about parsing as finding the structure of a sentence
rather than finding the sentences that are valid, as the former is
more compelling.
Don't use CF-PSG. (check more cases)
]</para>

<para>
Given that the resources of a computer, however large, are still
finite, it is necessary to devise a finite description of this
infinite set.  Such descriptions are called grammars.  We have already
encountered this possibility in the context of regular expressions.
For example, the expression <literal>a+</literal> describes the
infinite set
<literal>{a, aa, aaa, aaaa, ...}</literal>.  Apart from their
compactness, grammars usually capture important properties of the
language being studied, and can be used to systematically map between
sequences of words and abstract representations of their meaning.
Thus, even if we were to
impose an upper bound on sentence length to ensure the language was
finite, we would still want to come up with a compact
representation in the form of a grammar.
</para>

<para>
A well-formed sentence of a language is more than an arbitrary
sequence of words from the language.  Certain kinds of words usually
go together.  For instance,
determiners like <literal>the</literal> are typically
followed by adjectives or nouns, but not by verbs.  Groups of words form
intermediate structures called phrases or constituents.  These
constituents can be identified using standard syntactic tests, such as
substitution, movement and coordination.  For example, if a sequence
of words can be replaced with a pronoun, then that sequence is likely
to be a constituent.  The following example illustrates this test:
</para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para><emphasis>Ordinary daily multivitamin and mineral
    supplements</emphasis> could help adults with diabetes fight
    off some minor infections</para></listitem>
    <listitem><para><emphasis>They</emphasis> could help adults with
    diabetes fight off some minor infections</para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

<note><para>
Readers are referred to any introductory text on syntax for fuller
treatment of constituency, e.g. McCawley (1998) The Syntactic
Phenomena of English.
</para></note>

<para>
The structure of a sentence may be represented using a
phrase structure tree, in which the terminal symbols are the words of the sentence,
the pre-terminal symbols are parts of speech, and the remaining non-terminals
are syntactic constituents.  An example of such a tree is shown in
<xref linkend="parse_tree"/>.
</para>

<figure id="parse_tree"><title>Phrase Structure Tree</title>
<informaltable frame="all">
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="parse_tree.png" scale="50"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>
A <emphasis>grammar</emphasis> is a formal system which specifies
which sequences of words are well-formed in the language, and which
provides one or more phrase structures for the sequence.  We will
focus our attention on a particular kind of grammar called a
<emphasis>context-free grammar</emphasis> (CFG),
which is a collection of production rules of the form <literal>S -> NP
VP</literal>.  (To be well-formed, each non-terminal node and its children
must correspond to such a rule.)
</para>

<para>
A <emphasis>parser</emphasis> is a computational
system which processes input sentences according to the rules of the
grammar, and produces one or more constituent structures which conform
to the grammar.  We take a grammar to be a declarative specification
of well-formedness, and a parser to be a procedural interpretation of
the grammar.  [EDWARD: EXPAND THIS DISCUSSION]
In this chapter we will present context-free
grammars, and describe some simple parsers that work with them.
</para>

<para>
Parsing is important in linguistics and natural language processing
for a variety of reasons.  A parser permits a grammar to be evaluated
against a potentially large collection of test sentences, helping the
linguist to identify shortcomings in their analysis.  A parser can be
used as a model of psycholinguistic processing, and used to explain
the processing difficulties that humans have with certain syntactic
constructions (e.g. the so-called "garden path" sentences).  A parser
can serve as the first stage of processing natural language input for
a question-answering system.
</para>

    <warning><para> Some of the material in this tutorial is out of
    date.  We plan to re-write the parsing tutorials in the near
    future.  Until then, see the <ulink
    url="http://nltk.sourceforge.net/ref/nltk.parser.html">reference
    documentation for <literal>nltk.parser</literal></ulink> for more
    up-to-date information. </para></warning>

  </section> <!-- Introduction -->

    <section> <title> Linguistic Overview </title>

<!-- 
X.2 linguistic overview (for non-linguist readers)
    - how have linguists addressed the problem?
    - what are the shortcomings of the non-computational approach?

<para>
Parsing has a long and interesting history in computational
linguistics.
[NOTES: the perspective of early generative grammar;
early NLP approaches including ATNs and DCGs;
grammar formalisms and development environments.]
</para>
-->

    <para>
      Context-free grammars: the rule formalism
      and its interpretation.
    </para>

    <para>
      Discussion about how CFGs are created by hand.
    </para>

    <para>A simple grammar:  [EDWARD: use real arrows not dash greater-than, passim]
    </para>

<programlisting>
S -> NP VP
NP -> Det N
NP -> Det N PP
VP -> V NP PP
VP -> V NP
VP -> V
PP -> P NP

NP -> 'I'
Det -> 'the'
Det -> 'a'
N -> 'man'
V -> 'saw'
P -> 'in'
N -> 'park'
P -> 'with'
N -> 'dog'
N -> 'telescope'
</programlisting>

<para>
Shortcomings of this approach: unreliable, doesn't scale,
complex interactions amongst rules makes manual debugging
almost impossible.  Consequently linguists are unable to
work with large-coverage grammars.
</para>

    </section> <!-- Linguistic Overview -->

    <section> <title> Computational Approaches to Parsing </title>

    <para>[EDWARD: give more general introduction to TD and BU]</para>
<!--
X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)
-->

      <section> <title> Recursive Descent Parsing </title>

<para>
The simplest kind of parser interprets the grammar as a
collection of goals and subgoals.  The top-level goal is to
find an <literal>S</literal>.  The <literal>S -> NP
VP</literal> rule permits the parser to replace this goal
with two subgoals: find an <literal>NP</literal>, then find
a <literal>VP</literal>.  Each of these subgoals can be
replaced in turn by sub-sub-goals, using rules that have
<literal>NP</literal> and <literal>VP</literal> on their
left-hand side.  Eventually, this expansion process leads to
subgoals such as: find the word
<literal>telescope</literal>.  Such subgoals can be directly
compared against the input string, and succeed if the next
word is matched.
[EDWARD: "interprets the grammar as a specification of how to divide a goal into subgoals"
- goal driven parser]
</para>

<para>
The recursive descent parser builds a parse tree during the
above process.  With the initial goal (find an
<literal>S</literal>), the <literal>S</literal> root node is
created.  As the above process recursively expands its goals
using the rules of the grammar, the parse tree is extended downwards
(hence the name <emphasis>recursive descent</emphasis>).  We can see this
in action using the parser demonstration <literal>nltk.draw.rdparser</literal>.
To run this demonstration, use the following commands:
</para>

<programlisting>
&prompt;<command> from nltk.draw.rdparser import demo</command>
&prompt;<command> demo()</command>
</programlisting>

<para>
Six stages of the execution of this parser are shown in
<xref linkend="rdparser"/>.
</para>

<figure id="rdparser"><title>Six Stages of a Recursive Descent Parser: initial, after two rules,
after matching "the", failing to match "man", completed parse, backtracking <emphasis>BROKEN!</emphasis>
</title>
<informaltable frame="all">
<tgroup cols="3"><tbody><row><entry>
<graphic fileref="rdparser1.png" scale="24"/>
</entry><entry>
<graphic fileref="rdparser2.png" scale="24"/>
</entry><entry>
<graphic fileref="rdparser3.png" scale="24"/>
</entry></row><row><entry>
<graphic fileref="rdparser4.png" scale="24"/>
</entry><entry>
<graphic fileref="rdparser5.png" scale="24"/>
</entry><entry>
<graphic fileref="rdparser6.png" scale="24"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>Discussion: choosing which of several possible rules to apply;
backtracking (NB the rdparser code has a bug that prevents it
from backtracking properly.)  Discuss termination.</para>

<para>Problems with recursive descent parsing:
considers structures and words that are not attested;
backtracking may discard parsed constituents that need to be rebuilt;
for example, backtracking over <literal>VP -> V NP</literal>
will discard the structures created for the
<literal>V</literal> and <literal>NP</literal> non-terminals.  If the parser then proceeds
with <literal>VP -> V NP PP</literal>, then the structures for the
<literal>V</literal> and <literal>NP</literal> must be created again.
</para>

<para>
Recursive descent parsing is a kind of <emphasis>top-down
parsing</emphasis>.  These use the grammar to
<emphasis>predict</emphasis> what the input will be, before
inspecting any input.  However, since the input is available
to the parser all along, it would be more sensible to
consider the input sentence from the very beginning.  Such
an approach is called <emphasis>bottom-up
parsing</emphasis>, and is the topic of the next section.
</para>

</section>
<section> <title> Shift-Reduce Parsing </title>

<para>
The simplest kind of bottom-up parsing is known as shift-reduce
parsing.  The parser repeadly pushes the next input word onto a stack;
this is the <emphasis>shift</emphasis> operation.  If the top
<replaceable>n</replaceable> items on the stack match the
<replaceable>n</replaceable> items on the right-hand side of some
grammar rule, then they are all popped off the stack, and the item on
the left-hand side of the rule is pushed on the stack.  This
replacement of the top <replaceable>n</replaceable> items with a
single item is the <emphasis>reduce</emphasis> operation.  The parser finishes
when all the input is consumed and there is only one item remaining on the stack,
a parse tree with an <literal>S</literal> node as its root.
</para>

<para>
[add examples and motivate more - what are we doing with bottom up - find little pieces and expand...]
</para>

<note><para>
Note that the reduce operation may only be applied to the top of the stack.
Reducing items lower in the stack must be done before later items are pushed onto
the stack.
</para></note>

<para>
The shift-reduce parser builds a parse tree during the above process.
If the top of stack holds the word <literal>dog</literal> and if the
grammar has a rule <literal>N -> dog</literal> then the reduce
operation causes the word to be replaced with the parse tree for this
rule.  For convenience we will represent this tree as
<literal>N(dog)</literal>.  At a later stage, if the top of the stack
holds two items <literal>Det(the) N(dog)</literal> and if the grammar
has a rule <literal>NP -> Det N</literal> then the reduce operation
causes these two items to be replaced with <literal>NP(Det(the),
N(dog))</literal>.  This process continues until a parse tree for the
entire sentence has been constructed.  We can see this in action using
the parser demonstration <literal>nltk.draw.srparser</literal>.  To
run this demonstration, use the following commands:
</para>

<programlisting>
&prompt;<command> from nltk.draw.srparser import demo</command>
&prompt;<command> demo()</command>
</programlisting>

<para>
Six stages of the execution of this parser are shown in
<xref linkend="srparser"/>.
[EDWARD: use letter identifiers for subfigures.  Use two-across not three-across]
</para>

<figure id="srparser"><title>Six Stages of a Shift-Reduce Parser: initial, after one shift,
after shift reduce shift reduce, after recognizing the second NP, complex NP,
final step</title>
<informaltable frame="all">
<tgroup cols="3"><tbody><row><entry>
<graphic fileref="srparser1.png" scale="16"/>
</entry><entry>
<graphic fileref="srparser2.png" scale="16"/>
</entry><entry>
<graphic fileref="srparser3.png" scale="16"/>
</entry></row><row><entry>
<graphic fileref="srparser4.png" scale="16"/>
</entry><entry>
<graphic fileref="srparser5.png" scale="16"/>
</entry><entry>
<graphic fileref="srparser6.png" scale="16"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>
A shift-reduce parser may fail to parse the sentence, even though the
sentence is well-formed according to the grammar.  In such cases,
there are no remaining input words to shift, and there is no way to
reduce the remaining items on the stack, as exemplified in the left
example in <xref linkend="sr-conflict"/>.  The parser entered this
blind alley at an earlier stage shown in the middle example in
<xref linkend="sr-conflict"/>, when it reduced instead of shifted.
This situation is called a <emphasis>shift-reduce conflict</emphasis>.
At another possible stage of processing shown in the right example
in <xref linkend="sr-conflict"/>, the parser must choose between two
possible reductions, both matching the top items on the stack:
<literal>V -> V NP PP</literal> or <literal>NP -> NP PP</literal>.
This situation is called a <emphasis>reduce-reduce conflict</emphasis>.
</para>

<para>EDWARD: suggest diagram showing search tree with success and failure.</para>

<figure id="sr-conflict"><title>Conflict in Shift-Reduce Parsing</title>
<informaltable frame="all">
<tgroup cols="3"><tbody><row><entry>
<graphic fileref="srparser7.png" scale="16"/>
</entry><entry>
<graphic fileref="srparser8.png" scale="16"/>
</entry><entry>
<graphic fileref="srparser9.png" scale="16"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

<para>
Shift-reduce parsers may implement policies for resolving such
conflicts.  For example, they may address shift-reduce conflicts by
shifting only when no reduces are possible, and they may address reduce-reduce conflicts
by favouring the reduce operation that removes the most items from the stack.
No such policies are failsafe however.
</para>

<para>
The advantages of shift-reduce parsers over recursive descent parsers is that they
only build structure that corresponds to the words in the input.  Furthermore, they
only build each substructure once, e.g. <literal>NP(Det(the), N(man))</literal> is
only built and pushed onto the stack a single time, regardless of whether it will later
be used by the <literal>V -> V NP PP</literal> reduction or the <literal>NP -> NP PP</literal>
reduction.
</para>

</section>
</section>

    <section> <title> Advanced Topics in Parsing (optional) </title>

    <para>Parsing as a search problem.  Finding the right parse.</para>

<para>late closure etc</para>


<!--
X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions
-->

    </section>

    <section> <title> Parsing in NLTK </title>

    <para></para>

<!--
X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->

  <section> <title> Grammars and Lexicons </title>

    <para>A <glossterm>grammar</glossterm> is a formal specification
    for the structure of well-formed
    sentences in some language.  At present, only context-free grammars (CFGs) can be
    represented in NLTK.  A CFG consists of a set of context-free rules.  Context-free
    rules have the form <literal>X -&gt; Y</literal> where <literal>X</literal> is
    a non-terminal, and <literal>Y</literal> is a list of terminals and non-terminals.
    <literal>X</literal> and <literal>Y</literal> are known as the left-hand side and
    right-hand side respectively.
    </para>

    <para>
    In the simplest case, non-terminals and terminals are just Python
    strings.  However, it is possible to use any immutable Python object as a
    non-terminal or terminal.
    </para>

    <para>A grammar can be represented as a tuple of rules, while a lexicon
    can be represented as a tuple of lexical rules.  The only class we need
    to define then is <literal>Rule</literal>.

    </para>

    </section>

    <section> <title> Rules </title>

      <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.html"
      ><literal>nltk.rule</literal></ulink> module defines the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html"
      ><literal>Rule</literal></ulink> class, which is used to represent
      context free rules.  A <literal>Rule</literal> consists of a
      left-hand side and a right-hand side.</para>

      <itemizedlist>
        <listitem><para>The left-hand side is a single non-terminal, which
        may be any Python object.  In the simplest case it is just a string
        (e.g. "NP" or "VP").
        </para></listitem>

        <listitem><para>The right-hand side is a tuple of non-terminals and
        terminals, which may be any Python object.  In the simplest case
        these are strings (e.g. "Det", "the").
        </para></listitem>
      </itemizedlist>

      <para><literal>Rule</literal>s are created with the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#__init__"
      ><literal>Rule constructor</literal></ulink>, which takes a left-hand side
      and a right-hand side:</para>

<programlisting>
<emphasis># A typical grammar rule S -&gt; NP VP:</emphasis>
&prompt;<command> rule1 = Rule('S', ('NP', 'VP'))</command>
S -> NP VP

<emphasis># A typical lexical rule Det -&gt; 'the':</emphasis>
&prompt;<command> rule2 = Rule('Det', ('the',))</command>
Det -> the
</programlisting>

      <para>A <literal>Rule</literal>'s left-hand side and right-hand
      side are accessed with
      the <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#lhs"
      ><literal>lhs</literal></ulink>
      and <ulink url="http://nltk.sourceforge.net/ref/nltk.rule.Rule.html#rhs"
      ><literal>rhs</literal></ulink> methods:
      </para>

<programlisting>
&prompt;<command> rule1.lhs()</command>
'S'

&prompt;<command> rule2.rhs()</command>
('the',)
</programlisting>

      <para> A <literal>Rule</literal>'s right-hand side can also be accessed with
      standard sequence operators: </para>

<programlisting>
&prompt;<command> rule1[0]</command>
'NP'

&prompt;<command> rule2[1]</command>
IndexError: tuple index out of range

&prompt;<command> len(rule1)</command>
2

&prompt;<command> 'the' in rule2</command>
1

&prompt;<command> for cat in rule1: </command>
&prompt2;<command>     print cat</command>
NP
VP
</programlisting>

    </section> <!-- Rules -->

    <section> <title> Building Grammars and Lexicons from Rules </title>

    <para>Grammars and Lexicons can easily be built up from
    <literal>Rule</literal>s as shown in the following examples:</para>

<programlisting>
<emphasis># A simple grammar:</emphasis>
<command>
grammar = (
    Rule('S',('NP','VP')),
    Rule('NP',('Det','N')),
    Rule('NP',('Det','N', 'PP')),
    Rule('VP',('V','NP')),
    Rule('VP',('V','PP')),
    Rule('VP',('V','NP', 'PP')),
    Rule('VP',('V','NP', 'PP', 'PP')),
    Rule('PP',('P','NP'))
)
</command>

<emphasis># A simple lexicon:</emphasis>
<command>
lexicon = (
    Rule('NP',('I',)),
    Rule('Det',('the',)),
    Rule('Det',('a',)),
    Rule('N',('man',)),
    Rule('V',('saw',)),
    Rule('P',('in',)),
    Rule('P',('with',)),
    Rule('N',('park',)),
    Rule('N',('telescope',))
)
</command>
</programlisting>

  </section> <!-- Building Grammars and Lexicons from Rules -->

  <section> <title> Encoding Syntax Trees </title>

    <note>
      <para> The <literal>nltk.tree</literal> module currently has
      only minimal support for representing movement, traces, and
      co-indexing.  We plan to extend the class to support these
      features more fully in the future. </para>
    </note>

    <section> <title> Trees </title>

      <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.tree.html"
      ><literal>nltk.tree</literal></ulink> module defines the
      <ulink url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html"
      ><literal>Tree</literal></ulink> class, which is used to
      represent syntax trees.  A <literal>Tree</literal> consists of a
      <glossterm>node value</glossterm>, and one or more
      <glossterm>children</glossterm>. </para>

      <itemizedlist>
        <listitem> <para> The node value is a string containing the
        tree's constituent type (e.g., "NP" or "VP").
        </para></listitem>

        <listitem> <para> The children encode the hierarchical
        contents of the tree.  Each child is either a
        <glossterm>leaf</glossterm> or a
        <glossterm>subtree</glossterm>.</para></listitem>
      </itemizedlist>
    
      <note>
        <para> Although the <literal>Tree</literal> class is usually
        used for encoding syntax trees, it can be used to encode
        <emphasis>any</emphasis> homogenous hierarchical structure
        that spans a text (such as morphological structure or
        discourse structure).  In the general case, leaves and node
        values do not have to be strings. </para>
      </note>

      <para> A <literal>Tree</literal> with node value
      <replaceable>n</replaceable> and children
      <replaceable>c<subscript>1</subscript></replaceable>,
      <replaceable>c<subscript>2</subscript></replaceable>, ...
      <replaceable>c<subscript>n</subscript></replaceable> is written
      <literal>(<replaceable>n</replaceable>:
      <replaceable>c<subscript>1</subscript></replaceable>,
      <replaceable>c<subscript>2</subscript></replaceable>, ...
      <replaceable>c<subscript>n</subscript></replaceable>)</literal>.
      <literal>Tree</literal>s are created with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html#__init__"
      ><literal>Tree constructor</literal></ulink>, which takes a node
      value and zero or more children: </para>

<programlisting>
    <emphasis># A tree with one child, a leaf:</emphasis>
    &prompt;<command> tree1 = Tree('NP', 'John')</command>
    ('NP': 'John')

    <emphasis># A tree with two children, both of which are leaves:</emphasis>
    &prompt;<command> tree2 = Tree('NP', 'the', 'man')</command>
    ('NP': 'the' 'man')

    <emphasis># A tree with two children, one leaf and one subtree:</emphasis>
    &prompt;<command> tree3 = Tree('VP', 'saw', tree2)</command>
    ('VP': 'saw' ('NP': 'the' 'man'))
</programlisting>

      <para> A <literal>Tree</literal>'s node value is accessed with
      the <ulink url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html#node"
      ><literal>node</literal></ulink> method: </para>

<programlisting>
    &prompt;<command> tree1.node()</command>
    'NP'
</programlisting>

      <para> A <literal>Tree</literal>'s children are accessed with
      standard sequence operators: </para>

<programlisting>
    &prompt;<command> tree3[0]</command>
    'saw'
    &prompt;<command> tree3[1]</command>
    ('NP': 'the' 'man')
    &prompt;<command> len(tree3)</command>
    2
    &prompt;<command> 'saw' in tree3</command>
    1
    &prompt;<command> for child in tree3: </command>
    &prompt2;<command>     print child</command>
    saw 
    ('NP': 'the' 'man')
    &prompt;<command> [child.upper() for child in tree2] </command>
    ['THE', 'MAN']
    &prompt;<command> tree3[:] </command>
    ('saw', ('NP': 'the' 'man'))
</programlisting>

      <para> The printed representation for complex
      <literal>Tree</literal>s can be difficult to read.  In these
      cases, the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html#draw"
      ><literal>draw</literal></ulink> method can be very useful.  This
      method opens a new window, containing a graphical representation
      of the tree.  </para>

<programlisting>
    &prompt;<command> tree3.draw()</command>
</programlisting>

      <para> The tree display window allows you to zoom in and out; to
      collapse and expand subtrees; and to print the graphical
      representation to a postscript file. </para>

      <para> The <literal>Tree</literal> class implements a number of
      other useful methods.  See the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.Tree.html"
      ><literal>Tree</literal> reference documentation</ulink> for more
      information about these methods. </para>

<programlisting>
    &prompt;<command> tree3.leaves()</command>
    ('saw', 'the', 'man')
    &prompt;<command> tree3.height()</command>
    3
    &prompt;<command> tree3.nodes()</command>
    ('VP': ('NP':))
</programlisting>

    </section> <!-- Trees -->

    <section> <title> Tree Tokens </title>

      <para> NLTK makes a distinction between <glossterm>tree
      type</glossterm>s and <glossterm>tree token</glossterm>s, that
      is analogous to the distinction between word types and word
      tokens.  In particular, a tree token is an individual occurance
      of a tree type in a text; and a tree type is an abstract syntax
      tree, without context. </para>

      <para> Tree tokens are represented with the <ulink
      url="http://nltk.sourceforge.net/ref/nltk.tree.TreeToken.html"
      ><literal>TreeToken</literal></ulink> class.
      <literal>TreeToken</literal>s behave very much like
      <literal>Tree</literal>s, except that the leaves of a
      <literal>TreeToken</literal> are word tokens, not word types.  
      </para>

    </section> <!-- TreeTokens -->

  </section> <!-- Encoding Syntax Trees -->

  <section> <title> The Parser Interface </title>

    <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.html"
    ><literal>parser</literal></ulink> module defines the
    <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.ParserI.html"
    ><literal>ParserI</literal></ulink> class, which is the standard interface
    which all parsers should support.  This class should only be used in the
    definition of other parser classes, which should inherit from the
    <literal>ParserI</literal> class.</para>

    <para>The <literal>ParserI</literal> class defines
    <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.ParserI.html#parse"
    ><literal>parse</literal></ulink>, which takes a list of tokens as its argument
    and returns a list of <literal>TreeToken</literal>s.
    The <literal>parse</literal> function has an optional second argument
    which specifies the maximum number of parses to return.  No program should
    call the <literal>ParserI.parse</literal> method.  Derived classes must
    implement their own <literal>parse</literal> method.</para>

    <para>A parser returns an empty list of trees in the situation where it was
    unable to assign a tree to the list of tokens.  This happens in situations
    where the list of tokens forms an ungrammatical sentence, or when the grammar
    is deficient, or when the search strategy of the parser does not explore the
    section of the search space where the parse tree(s) are found.</para>

    <para>A parser returns
    more than one tree in the case of syntactic ambiguity.  The sentence being
    parsed may be genuinly ambiguous, or the grammar may be deficient.  Statistical
    parsers usually return the most likely parse (based on training data), or the
    set of <replaceable>n</replaceable> most likely parses.  The same interface
    applies to these cases.  (Where the probability of the parse tree needs to
    be returned, we assume this is stored in the root node of the tree.)  When
    asked to return <replaceable>n</replaceable> parses, we assume such a parser
    will rank parses in order of decreasing likelihood, and return the
    <replaceable>n</replaceable>-best parses.</para>

    <para>Parsers may be used anywhere where there is need for processing
    sequences with a grammar.  In the most common case, the sequence consists
    of a string of words forming a sentence.  However, other cases are possible,
    e.g. the string of characters forming a syllable, the string of morphemes
    forming a word, the string of sentences forming a text.  We could also have
    grammars over subsequences.  For example, the collection of XML elements in
    a document forms a subsequence of the document, and we could specify a grammar
    over those elements alone, ignoring document content.</para>

    <para>The <literal>ParserI</literal> class also defines a
    <ulink url="http://nltk.sourceforge.net/ref/nltk.parser.ParserI.html#parse"
    ><literal>parse_types</literal></ulink> method, which takes a list of token types
    (e.g. strings) and returns a list of <literal>Tree</literal>s.  This is
    useful in the case where the sentence to be parsed did not come from a
    tokenized text.  This method works by converting the list of types into
    a list of tokens, then calling <literal>parse</literal> as before.</para>

  </section> <!-- The Parser Interface -->

  <section> <title> Simple Parsers </title>

    <para>Simple top-down and bottom-up parsers can easily be defined
    using classes which inherit from <literal>ParserI</literal>.
    Here is some pseudocode to use as the basis of two simple modules.
    Writing these modules is left as an exercise for the reader.
    More work is required in order to return trees.</para>

<programlisting>
# elementary top-down parser
def tdparse(goal, sent): 
    if goal == sent == empty:
        pass # we're finished
    else:
        if goal[0] == sent[0]:
	    tdparse(goal[1:], sent[1:])
        else:
            for rule in grammar:
                if rule.lhs() == goal[0]:
                    make a local copy of the goal list
                    goal[:1] = rule.rhs()
                    tdparse(goal, sent)

# left-corner parser
def lcparse(goal, sent):
    # this is like tdparse, except the step which iterates over the rules
    # of the grammar only considers rules whose "left corner" matches the next
    # word of the input stream.  The left corner of a lexical rule is the
    # content of the rule.  The left corner of a non-lexical rule R is the left
    # corner of R[0], the first element on the RHS of the rule.

# elementary bottom-up parser
def buparse(sent):
    if sent == [S]:
        pass # we're finished
    else:
        for rule in grammar:
	    does rule.rhs() match any sublist of sent?
            if so, replace the substring with the LHS of the rule
            buparse(sent)
</programlisting>

    <para>The <ulink url="http://nltk.sourceforge.net/ref/nltk.srparser_template.html"
    ><literal>srparser_template</literal></ulink> module defines the
    <ulink url="http://nltk.sourceforge.net/ref/nltk.srparser_template.SRParser.html"
    ><literal>SRParser</literal></ulink> class, which is a template for a
    shift-reduce parser.</para>

  </section> <!-- The Parser Interface -->

  </section> <!-- Parsing in NLTK -->

</article>
