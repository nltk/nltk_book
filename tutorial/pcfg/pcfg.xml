<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [

<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">
<!ENTITY tutdoc "http://nltk.sourceforge.net/tutorial">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!-- Equation symbols -->
<!ENTITY i1 "i<subscript>1</subscript>">
<!ENTITY i2 "i<subscript>2</subscript>">
<!ENTITY inm1 "i<subscript>n-1</subscript>">

<!-- Dot (for dotted rules).  I'd like a bigger dot, but I don't see one. -->
<!ENTITY dot "&middot;">
]>

<!-- ==================================
This tutorial assumes that they've already read the parsing tutorial;
but that tutorial still needs a lot of work.  So there are some
holes, for now.
=================================== -->
<!-- ==================================
TO DO:
  - Learning grammars
  - Lexicalized grammars
  - InsideOutsidePCFGParser
  - Other minor updates
=================================== -->

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Probabilistic Parsing</title>
  </articleinfo>

  <section> <title> Introduction </title>

    <!-- ==== Remove this after we write the new tutorials ==== -->
    <note> 
      <para> We are currently re-writing the basic parsing tutorials
      to provide more background information.  As a result, this
      tutorial contains several references to "previously discussed
      material" that has not yet been written.  Until we finish
      re-writing the basic tutorials, we refer you to the reference
      documentation for the <ulink
      url="&refdoc;/nltk.parser.html"><literal>nltk.parser</literal>
      module</ulink> and the <ulink
      url="&refdoc;/nltk.cfg.html"><literal>nltk.cfg</literal>
      module</ulink> for more up-to-date information. </para>

      <para> Before reading this tutorial, you should be familiar with
      symbolic parsing and context free grammars.  This material will
      be covered in the re-written basic parsing tutorials.  But for
      now, we refer you to other published discussions of parsing,
      such as Jurafsky and Martin, Chapter 10.  </para>
    </note>

    <para> Parsing allows us to find tree structures representing the
    internal organization of a text.  These trees are useful for a
    wide variety of tasks, including semantic interpretation,
    information retrieval, and machine translation.  Unfortunately,
    most texts have a large number of possible structures.  This
    produces two problems for the symbolic approaches discussed in the
    <ulink url="&tutdoc;/parsing/t1.html">parsing tutorial</ulink>:
    </para>

    <itemizedlist>
      <listitem>
        <para>
        <glossterm>Ambiguity</glossterm>:<indexterm><primary
        >ambiguity</primary></indexterm> There is no way to decide
        which of the tree structures are likely to correctly represent
        the text's internal organization. </para>
      </listitem>

      <listitem> 
        <para>
        <glossterm>Efficiency</glossterm>:<indexterm><primary
        >efficiency</primary></indexterm> Parsing a text requires
        searching a very large space of possible tree structures.
        With no information about which subtrees are more likely to be
        included in a complete parse, it can take a long time to find
        even a single parse. </para>
      </listitem>
    </itemizedlist>

    <para> Probabilistic techniques provide tools to address both of
    these problems.  We can assign probabilities to parses, and use
    them to decide which structures are more likely to represent a
    text's internal organization; and we can use probabilities to
    guide our search of the space of possible tree structures. </para>

  </section> <!-- Introduction -->

  <section> <title> Probabilistic Parsers </title>

    <para> NLTK uses <literal>ProbabilisticParserI</literal> to define
    a general interface for probabilistic parsers.  This interface uses
    the <literal>ProbabilisticTreeToken</literal> class to associate
    probabilities with the parses for a text. </para>

    <section> <title> ProbabilisticTreeTokens </title>

      <para><literal>ProbabilisticTreeToken</literal> is a subclass of
      <literal>TreeToken</literal> that associates a probability with
      a tree token.  New <literal>ProbabilisticTreeToken</literal>s are
      created using the
      <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken.html#__init__">
      <literal>ProbabilisticTreeToken</literal> constructor</ulink>,
      which takes a probability, a node value, and zero or more
      children: </para>

<programlisting>
<emphasis># Define some text tokens:</emphasis>
&prompt; <command>words = WSTokenizer().tokenize('He ate my cookie')</command>
['He'@[0w], 'ate'@[1w], 'my'@[2w], 'cookie'@[3w]]

<emphasis># Define some probabilistic tree tokens: </emphasis>
&prompt; <command>tree1 = ProbabilisticTreeToken(0.3, 'NP', words[0])</command>
('NP': 'He')@[0w] (p=0.3)
&prompt; <command>tree2 = ProbabilisticTreeToken(0.02, 'NP', words[2], words[3])</command>
('NP': 'my' 'cookie')@[2w:4w] (p=0.02)
&prompt; <command>tree3 = ProbabilisticTreeToken(0.045, 'VP', words[1], tree2)</command>
('VP': 'ate' ('NP': 'my' 'cookie'))@[1w:4w] (p=0.045)
</programlisting>

      <para> The probability associated with a
      <literal>ProbabilisticTreeToken</literal> is returned by the
      <ulink url="&refdoc;/nltk.tree.ProbabilisticTreeToken.html#p">
      <literal>p</literal></ulink> method:</para>

<programlisting>
&prompt; <command>print tree1.p(), tree2.p(), tree3.p() </command>
0.3 0.02 0.045
</programlisting>

      <para> The <literal>ProbabilisticTreeToken</literal>'s nodes and
      children can be accessed using the same methods that are used
      with <literal>TreeToken</literal>s: </para>

<!-- ==== UPDATE THIS LATER ==== -->
<programlisting>
&prompt; <command>tree1.node()</command>
'NP'
&prompt; <command>tree1.children()</command>
('He'@[0w],)
&prompt; <command>tree3.leaves()</command>
('ate'@[1w], 'my'@[2w], 'cookie'@[3w])
</programlisting>

      <para> See the <ulink url="&tutdoc;/parsing/t1.html">parsing
      tutorial</ulink> or the <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken.html">
      <literal>ProbabilisticTreeToken</literal></ulink> reference
      documentation for more information on the methods
      defined by <literal>TreeToken</literal>. </para>

    </section> <!-- ProbabilisticTreeToken -->

    <section> <title> The Probabilistic Parser Interface </title>

      <para> <ulink
      url="&refdoc;/nltk.parser.ProbabilisticParserI.html">
      <literal>ProbabilisticParserI</literal></ulink> defines a
      standard interface for probabilistic parsers.  It extends the
      <ulink url="&refdoc;/nltk.parser.ParserI.html">
      <literal>ParserI</literal></ulink> interface in two ways.
      First, the parse trees returned by <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken.html#parse">
      <literal>parse</literal></ulink> and <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken.html#parse_n">
      <literal>parse_n</literal></ulink> are represented with
      <literal>ProbabilisticTreeToken</literal>s, instead of
      <literal>TreeToken</literal>s.  The
      <literal>ProbabilisticTreeToken</literal>s' <literal>p</literal>
      method can be used to access the probabilities associated with
      each parse. </para>

<programlisting>
&prompt; <command>parser = ViterbiPCFGParser(grammar) </command>
&prompt; <command>parse = parser.parse(words)</command>
('S': ('NP': 'He') 
      ('VP': ('V': 'saw') 
             ('NP': ('Det': my) 
                    ('N': 'cookie'))))@[0w:4w] (p=0.000282)
&prompt; <command>parse.p()</command>
p=0.000282
</programlisting>

      <para> Second, <literal>ProbabilisticParser</literal>s are
      required to implement the <ulink
      url="&refdoc;/nltk.tree.ProbabilisticTreeToken.html#parse_dist">
      <literal>parse_dist</literal></ulink> method, which returns a
      <literal>ProbDist</literal> with <literal>TreeToken</literal>
      samples. </para>

      <!-- ==== ADD EXAMPLE ==== -->
      <note><para> We are currently cleaning up the
      <literal>nltk.probability</literal> module; after we have
      finished, I will add an example use of
      <literal>parse_dist</literal> here. </para> </note>
      
    </section> <!-- ProbabilisticParserI -->
  </section> <!-- Probabilistic Parsers -->

  <section> <title> Probabilistic Context Free Grammars </title>
 
    <indexterm><primary>PCFG</primary></indexterm>
    <indexterm><primary>probabilistic context free grammar</primary>
    </indexterm>
    <para> A <glossterm>probabilistic context free grammar</glossterm>
    (or <glossterm>PCFG</glossterm>) is a context free grammar that
    associates a probability with each of its rules.  It generates the
    same set of parses for a text that the corresponding context free
    grammar does, and assigns a probability to each parse.  The
    probability of a parse generated by a PCFG is simply the product
    of the probabilities of the rules used to generate it.  </para>

    <para> Probabilistic context free grammars are implemented by the
    <ulink
    url="&refdoc;/nltk.cfg.PCFG.html"><literal>PCFG</literal></ulink>
    class, which is defined in the <literal>nltk.cfg</literal> module.
    Like <literal>CFG</literal>s, each <literal>PCFG</literal>
    consists of a start state and a set of rules.  But the rules are
    represented by <ulink url="&refdoc;/nltk.cfg.PCFG_Rule.html">
    <literal>PCFG_Rule</literal></ulink>, a subclass of
    <literal>CFG_Rule</literal> that associates a probability with a
    context free grammar rule.</para>

    <section> <title> PCFG Rules </title>

      <indexterm><primary>left-hand side</primary></indexterm>
      <indexterm><primary>right-hand side</primary></indexterm> <para>
      Like <literal>CFG_Rule</literal>s, each
      <literal>PCFG_Rule</literal> specifies that a nonterminal (the
      <glossterm>left-hand side</glossterm>) can be expanded to a
      sequence of terminals and nonterminals (the
      <glossterm>right-hand side</glossterm>).  In addition, each
      <literal>PCFG_Rule</literal> has a probability associated with
      it.  <literal>PCFG_Rule</literal>s are created using the <ulink
      url="&refdoc;/nltk.cfg.PCFG_Rule.html#__init__">
      <literal>PCFG_Rule</literal> constructor</ulink>, which takes a
      probability, a nonterminal left-hand side, and zero or more
      terminals and nonterminals for the right-hand side. </para>

<programlisting>
<emphasis># Define some nonterminals</emphasis>
&prompt; <command>(NP, V, VP) = [Nonterminal(s) for s in 'S NP VP'.split()]</command>

<emphasis># Create some PCFG rules</emphasis>
&prompt; <command>rule1 = PCFG_Rule(0.23, VP, V, NP)</command>
VP -&gt; NP S (p=0.23)
&prompt; <command>rule2 = PCFG_Rule(0.12, V, 'saw')</command>
NP -&gt; 'saw' (p=0.12)
&prompt; <command>rule3 = PCFG_Rule(0.04, NP, 'cookie')</command>
S -&gt; 'cookie' (p=0.04)
</programlisting>

      <para> The probability associated with a
      <literal>PCFG_Rule</literal> is returned by the
      <ulink url="&refdoc;/nltk.cfg.PCFG_Rule.html#p">
      <literal>p</literal></ulink> method:</para>

<programlisting>
&prompt; <command>print rule1.p(), rule2.p(), rule3.p() </command>
0.23 0.12 0.04
</programlisting>

      <para> As with <literal>CFG_Rule</literal>s, the left-hand side
      of a <literal>PCFG_Rule</literal> is returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFG_Rule.html#lhs">
      <literal>lhs</literal></ulink> method; and the right-hand side
      is returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFG_Rule.html#rhs">
      <literal>rhs</literal></ulink> method: </para>

<programlisting>
&prompt; <command>rule1.lhs() </command>
&lt;VP&gt;
&prompt; <command>rule1.rhs() </command>
(&lt;V&gt;, &lt;NP&gt;)
</programlisting>

    </section> <!-- PCFG_Rule -->

    <section> <title> PCFGs </title>

      <para> <literal>PCFG</literal>s are created using the <ulink
      url="&refdoc;/nltk.cfg.PCFG.html#__init__">
      <literal>PCFG</literal> constructor</ulink>, which takes a start
      symbol and a list of rules:</para>

<programlisting>
&prompt; <command>rules = [PCFG_Rule(1.0, S, VP, NP),
             PCFG_Rule(0.4, VP, 'saw', NP),
             PCFG_Rule(0.3, VP, 'ate'),
             PCFG_Rule(0.3, VP, 'gave', NP, NP),
             PCFG_Rule(0.8, NP, 'the', 'cookie'),
             PCFG_Rule(0.2, NP, 'Jack')]</command>
&prompt; <command>grammar = PCFG(S, rules) </command>
CFG with 6 rules (start state = S)
    S -&gt; VP NP (p=1)
    VP -&gt; 'saw' NP (p=0.4)
    VP -&gt; 'ate' (p=0.3)
    VP -&gt; 'gave' NP NP (p=0.3)
    NP -&gt; 'the' 'cookie' (p=0.8)
    NP -&gt; 'Jack' (p=0.2)
</programlisting>

      <para> In order to ensure that the trees generated by the
      grammar form a proper probability distribution,
      <literal>PCFG</literal> imposes the constraint that all rules
      with a given left-hand side must have probabilities that sum to
      one: </para>

        <itemizedlist>
          <listitem>
            <para>
              for all <replaceable>lhs</replaceable>:
              &#8721;<subscript><replaceable>rhs</replaceable></subscript>
              P(<replaceable>lhs</replaceable>&#8594;<replaceable
                 >rhs</replaceable>) = 1</para>
          </listitem>
        </itemizedlist>

      <para> The example grammar given
      above obeys this constraint: for <literal>S</literal>, there is
      only one rule, with a probability of 1.0; for
      <literal>VP</literal>, 0.4+0.3+0.3=1.0; and for
      <literal>NP</literal>, 0.8+0.2=1.0. </para>

      <para> As with <literal>CFG</literal>s, the start state
      <literal>PCFG</literal> is returned by the <ulink
      url="&refdoc;/nltk.cfg.PCFG.html#start">
      <literal>start</literal></ulink> method; and the are returned by
      the <ulink url="&refdoc;/nltk.cfg.PCFG.html#rules">
      <literal>rules</literal></ulink> method:</para>

<programlisting>
&prompt; <command>grammar.start()</command>
&lt;S&gt;
&prompt; <command>grammar.rules()</command>
[[Rule: S -&gt; VP NP (p=1)], 
 [Rule: VP -&gt; 'saw' NP (p=0.4)],
 [Rule: VP -&gt; 'ate' (p=0.4)],
 [Rule: VP -&gt; VP PP (p=0.2)],
 [Rule: NP -&gt; 'the' 'boy' (p=0.8)],
 [Rule: NP -&gt; 'Jack' (p=0.2)],
 [Rule: PP -&gt; 'under' NP (p=1.0)]]
</programlisting>

    </section> <!-- PCFG -->

  </section> <!-- PCFGs -->

  <section> <title> Learning Grammars </title>

    <!-- ==== TO DO: write this section ==== -->
    <note> <para> Work on defining a processing interface for learning
    grammars from treebanks, and implementations for that interface,
    are currently underway.  This section will be written once that
    work has been completed. </para> </note>

  </section> <!-- PCFGs -->

  <section> <title> Lexicalized PCFGs </title>

    <!-- ==== TO DO: write this section ==== -->
    <note> <para> In this section, I will discuss lexicalized PCFGs.
    Producing lexicalized PCFGs requires new classes to derive
    grammars from treebanks, but doesn't require any new parsing
    techniques. </para>
    </note>

  </section> <!-- Lexicalized PCFGs -->

  <section> <title> Probabilistic Parser Implementations </title>

    <para> The next two sections introduce two probabilistic parsing
    algorithms for PCFGs.  The first is a Viterbi-style algorithm that
    uses dynamic programming to find the single most likely parse for
    a given text.  Whenever it finds multiple possible parses for a
    subtree, it discards all but the most likely parse.  The second is
    a bottom-up chart parser that maintains a queue of edges, and adds
    them to the chart one at a time.  The ordering of this queue is
    based on the probabilities associated with the edges, allowing the
    parser to expand more likely edges before less likely ones.
    Different queue orderings are used to implement a variety of
    different search strategies.  Both of these algorithms are
    implemented in the <ulink url="&refdoc;/nltk.pcfgparser.html">
    <literal>nltk.pcfgparser</literal></ulink> module. </para>

    <!-- ==== CHANGE NAMES? ==== -->
    <note> <para> I am considering changing the names for the classes
    that implement these parsers.  Any suggestions are welcome.</para>
    </note>

  </section> <!-- Implementations -->

  <section> <title> A Viterbi-Style PCFG Parser </title>

    <indexterm><primary>most likely constituents table</primary>
    </indexterm>
    <para> <ulink
    url="&refdoc;/nltk.pcfgparser.ViterbiPCFGParser.html">
    <literal>ViterbiPCFGParser</literal></ulink> is a bottom-up PCFG
    parser that uses dynamic programming to find the single most
    likely parse for a text.  It parses texts by iteratively filling
    in a <glossterm>most likely constituents table</glossterm>.  This
    table records the most likely tree structure for each span and
    node value.  In particular, it has an entry for every start index,
    end index, and node value, recording the most likely subtree that
    spans from the start index to the end index, and has the given
    node value.  For example, after parsing the sentence "I saw John
    with my cookie," the most likely constituents table might be: </para>

    <informaltable>
      <tgroup cols="3">
        <!-- Give about 70% of the table to the tree entry -->
        <colspec colnum="1" colwidth="8*" align="center"/>
        <colspec colnum="2" colwidth="7*" align="center"/>
        <colspec colnum="3" colwidth="70*" align="left"/>
        <colspec colnum="3" colwidth="10*" align="left"/>
        <thead>
          <row>
            <entry align="center" valign="top">Span</entry>
            <entry align="center" valign="top">Node Value</entry>
            <entry align="center" valign="top">Tree</entry>
            <entry align="center" valign="top">Prob</entry>
          </row>
        </thead>
        <tbody>
          <row>
            <entry>[0:1]</entry><entry>NP</entry>
            <entry>(NP: <emphasis>I</emphasis>)</entry>
            <entry>0.3</entry>
          </row>
          <row>
            <entry>[2:3]</entry><entry>NP</entry>
            <entry>(NP: <emphasis>John</emphasis>)</entry>
            <entry>0.3</entry>
          </row>
          <row>
            <entry>[4:6]</entry><entry>NP</entry>
            <entry>(NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>)</entry>
            <entry>0.2</entry>
          </row>
          <row>
            <entry>[3:6]</entry><entry>PP</entry>
            <entry>(PP: <emphasis>with</emphasis> 
                     (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))</entry>
            <entry>0.1</entry>
          </row>
          <row>
            <entry>[2:6]</entry><entry>NP</entry>
            <entry>(NP: (NP: <emphasis>John</emphasis>) 
                     (PP: <emphasis>with</emphasis> 
                       (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>)))</entry>
            <entry>0.01</entry>
          </row>
          <row>
            <entry>[1:3]</entry><entry>VP</entry>
            <entry>(VP: <emphasis>saw</emphasis> (NP: <emphasis>John</emphasis>)))</entry>
            <entry>0.03</entry>
          </row>
          <row>
            <entry>[1:6]</entry><entry>VP</entry>
            <entry>(VP: <emphasis>saw</emphasis> 
                     (NP: (NP: <emphasis>John</emphasis>) 
                        (PP: <emphasis>with</emphasis> 
                           (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))))</entry>
            <entry>0.001</entry>
          </row>
          <row>
            <entry>[0:6]</entry><entry>S</entry>
            <entry>(S: (NP: <emphasis>I</emphasis>) 
                      (VP: <emphasis>saw</emphasis> 
                       (NP: (NP: <emphasis>John</emphasis>) 
                          (PP: <emphasis>with</emphasis> 
                             (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>)))))</entry>
            <entry>0.0001</entry>
          </row>
        </tbody>
      </tgroup>
    </informaltable>

    <para> Once the table has been completely filled in, the parser
    simply returns the entry for the most likely constituent that
    spans the entire text, and whose node value is the start symbol.
    For this example, it would return the entry with a span of [0:6]
    and a node value of "S". </para>

    <para> Note that we only record the <emphasis>most
    likely</emphasis> constituent for any given span and node value.
    For example, in the table above, there are actually two possible
    constituents that cover the span [1:6] and have "VP" node values:
    </para>

    <itemizedlist>
      <listitem>
        <para>(VP: <emphasis>saw</emphasis> 
                 (NP: (NP: <emphasis>John</emphasis>) 
                    (PP: <emphasis>with</emphasis> 
                       (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))))</para>
        <para>"saw John, who has my cookie"</para>
      </listitem>
      <listitem>
        <para>(VP: (VP: <emphasis>saw</emphasis> 
                      (NP: (NP: <emphasis>John</emphasis>))
                   (PP: <emphasis>with</emphasis> 
                      (NP: <emphasis>my</emphasis> <emphasis>cookie</emphasis>))))</para>
        <para>"used my cookie to see John"</para>
      </listitem>
    </itemizedlist>

    <para> Since the grammar we are using to parse the text indicates
    that the first of these tree structures has a higher probability,
    the parser discards the second one. </para>

    <section> <title> Filling in the Most Likely Constituents Table </title>

      <para> Because the grammar used by
      <literal>ViterbiPCFGParser</literal> is a PCFG, the probability
      of each constituent can be calculated from the probabilities of
      its children.  Since a constituent's children can never cover a
      larger span than the constituent itself, each entry of the most
      likely constituents table depends only on entries for
      constituents with <emphasis>shorter</emphasis> spans (or equal
      spans, in the case of unary and null productions). </para>

      <para> <literal>ViterbiPCFGParser</literal> takes advantage of
      this fact, and fills in the most likely constituent table
      incrementally.  It starts by filling in the entries for all
      constituents that span a single element of text.  After it has
      filled in all the table entries for constituents that span one
      element of text, it fills in the entries for constituents that
      span two elements of text.  It continues filling in the entries
      for constituents spanning larger and larger portions of the
      text, until the entire table has been filled.  </para>

      <para> To find the most likely constituent with a given span and
      node value, <literal>ViterbiPCFGParser</literal> considers all
      rules that could produce that node value.  For each rule, it
      checks the most likely constituents table for sequences of
      children that collectively cover the span and that have the node
      values specified by the rule's right hand side.  If the tree
      formed by applying the rule to the children has a higher
      probability than the current table entry, then it updates the
      most likely constituents table with the new tree. </para>

      <section> <title> Handling Unary Rules and Null Rules </title>

        <para> A minor difficulty is introduced by unary rules and null
        rules: an entry of the most likely constituents table might
        depend on another entry with the same span.  For example, if
        the grammar contains the rule "<literal>V&rarr;VP</literal>,"
        then the table entries for <literal>VP</literal> depend on the
        entries for <literal>V</literal> with the same span.  This can
        be a problem if the constituents are checked in the wrong
        order.  For example, if the parser try to find the most likely
        constituent for a <literal>VP</literal> spanning [1:3] before
        it finds the most likely constituents for <literal>V</literal>
        spanning [1:3], then it can't apply the
        "<literal>V&rarr;VP</literal>" rule. </para>
      
        <para> To solve this problem,
        <literal>ViterbiPCFGParser</literal> repeatedly checks each
        span until it finds no new table entries.  Note that cyclic
        grammar rules will <emphasis>not</emphasis> cause this
        procedure to enter an infinite loop.  Since all rule
        probabilities are less than or equal to 1, any constituent
        generated by a cycle in the grammar will have a probability
        that is less than or equal to the original constituent; so
        <literal>ViterbiPCFGParser</literal> will discard it. </para>

      </section> <!-- Unary & Null Rules -->

    </section> <!-- MLC table -->

  </section> <!-- ViterbiPCFGParser -->
  <section> <title> Using <literal>ViterbiPCFGParser</literal> </title>

    <para> <literal>ViterbiPCFGParser</literal>s are created using the
    <ulink
    url="&refdoc;/nltk.pcfgparser.ViterbiPCFGParser.html#__init__">
    <literal>ViterbiPCFGParser</literal> constructor</ulink>, which
    takes a <literal>PCFG</literal>: </para>

<programlisting>
&prompt; <command>viterbi_parser = ViterbiPCFGParser(grammar)</command>
&lt;ViterbiPCFGParser for &lt;CFG with 16 rules&gt;&gt;
</programlisting>

    <para> <literal>ViterbiPCFGParser</literal> implements all of the
    methods defined by the <literal>ProbabilisticParserI</literal>
    interface.  Note, however, that since
    <literal>ViterbiPCFGParser</literal> only finds the single most
    likely parse, that <literal>parse_n</literal> and
    <literal>parse_dist</literal> will never return more than one
    parse. </para>

<!-- ==== ADD A PARSE_DIST EXAMPLE ==== -->
<programlisting>
&prompt; <command>text1 = WSTokenizer().tokenize('the dog ate my cookie')</command>
&prompt; <command>text2 = WSTokenizer().tokenize('I saw John with my cookie')</command>

&prompt; <command>viterbi_parser.parse(text1)</command>
('S':
  ('NP': ('Det': 'the') ('N': 'dog'))
  ('VP': ('V': 'ate') ('NP': ('Det': 'my') 
                             ('N': 'cookie'))))@[0w:5w] (p=0.00175)
&prompt; <command>for parse in viterbi_parser.parse_n(text2):</command>
&prompt2; <command>    print parse</command>
('S':
  ('NP': 'I')
  ('VP':
    ('V': 'saw')
    ('NP':
      ('NP': 'John')
      ('PP':
        ('P': 'with')
        ('NP': ('Det': 'my') ('N': 'cookie'))))))@[0w:6w] (p=5.2040625e-05)
</programlisting>

    <para> The <ulink
    url="&refdoc;/nltk.pcfgparser.ViterbiPCFGParser.html#trace">
    <literal>trace</literal></ulink> method can be used to set the
    level of tracing output that is generated when parsing a text.
    Trace output displays the constituents that are considered, and
    indicates which ones are added to the most likely constituent
    table.  It also indicates the likelihood for each
    constituent. </para>

<programlisting>
&prompt; <command>viterbi_parser.trace(3)</command>
&prompt; <command>viterbi_parser.parse(text2)</command>

<emphasis>Inserting tokens into the most likely constituents table...</emphasis>
   Insert: |[=] . . . . .| I
   Insert: |. [=] . . . .| saw
   Insert: |. . [=] . . .| John
   Insert: |. . . [=] . .| with
   Insert: |. . . . [=] .| my
   Insert: |. . . . . [=]| cookie
<emphasis>Finding the most likely constituents spanning 1 text elements...</emphasis>
   Insert: |[=] . . . . .| NP -&gt; 'I' (p=0.15)       0.1500000
   Insert: |. [=] . . . .| V -&gt; 'saw' (p=0.65)      0.6500000
   Insert: |. [=] . . . .| VP -&gt; V (p=0.1)          0.0650000
   Insert: |. . [=] . . .| NP -&gt; 'John' (p=0.1)     0.1000000
   Insert: |. . . [=] . .| P -&gt; 'with' (p=0.61)     0.6100000
   Insert: |. . . . [=] .| Det -&gt; 'my' (p=0.2)      0.2000000
   Insert: |. . . . . [=]| N -&gt; 'cookie' (p=0.5)    0.5000000
<emphasis>Finding the most likely constituents spanning 2 text elements...</emphasis>
   Insert: |[=|=] . . . .| S -&gt; NP VP (p=1.0)       0.0097500
   Insert: |. [=|=] . . .| VP -&gt; V NP (p=0.5)       0.0325000
   Insert: |. . . . [=|=]| NP -&gt; Det N (p=0.5)      0.0500000
<emphasis>Finding the most likely constituents spanning 3 text elements...</emphasis>
   Insert: |[=|===] . . .| S -&gt; NP VP (p=1.0)       0.0048750
   Insert: |. . . [=|===]| PP -&gt; P NP (p=1.0)       0.0305000
<emphasis>Finding the most likely constituents spanning 4 text elements...</emphasis>
   Insert: |. . [=|=====]| NP -&gt; NP PP (p=0.25)     0.0007625
<emphasis>Finding the most likely constituents spanning 5 text elements...</emphasis>
   Insert: |. [===|=====]| VP -&gt; VP PP (p=0.4)      0.0003965
  Discard: |. [=|=======]| VP -&gt; V NP (p=0.5)       0.0002478
  Discard: |. [=|=======]| VP -&gt; V NP (p=0.5)       0.0002478
<emphasis>Finding the most likely constituents spanning 6 text elements...</emphasis>
   Insert: |[=|=========]| S -&gt; NP VP (p=1.0)       0.0000594

('S':
  ('NP': 'I')
  ('VP':
    ('VP': ('V': 'saw') ('NP': 'John'))
    ('PP':
      ('P': 'with')
      ('NP': ('Det': 'my') ('N': 'cookie')))))@[0w:6w] (p=5.9475e-05)
</programlisting>

    <para> The level of tracing output can also be set with an
    optional argument to <ulink
    url="&refdoc;/nltk.pcfgparser.ViterbiPCFGParser.html#__init__">
    <literal>ViterbiPCFGParser</literal>'s constructor</ulink>.  By
    default, no tracing output is generated.  Tracing output can be
    turned off by calling <literal>trace</literal> with a value of
    <literal>0</literal>. </para>

  </section> <!-- Using ViterbiPCFGParser -->

  <section> <title> A Bottom-Up PCFG Chart Parser </title>

    <section> <title> Introduction </title>

      <para> The Viterbi-style algorithm described in the previous
      section finds the single most likely parse for a given text.
      But for many applications, it is useful to produce several
      alternative parses.  This is often the case when probabilistic
      parsers are combined with other probabilistic systems.  In
      particular, the most probable parse may be assigned a low
      probability by other systems; and a parse that is given a low
      probability by the parser might have a better overall
      probability. </para>

      <para> For example, a probabilistic parser might decide that the
      most likely parse for "I saw John with the cookie" is is the
      structure with the interpretation "I used my cookie to see
      John"; but that parse would be assigned a low probability by a
      semantic system.  Combining the probability estimates from the
      parser and the semantic system, the parse with the
      interpretation "I saw John, who had my cookie" would be given a
      higher overall probability. </para>

      <para> This section describes
      <literal>BottomUpPCFGChartParser</literal>, a parser for
      <literal>PCFG</literal>s that can find multiple parses for a
      text.  It assumes that you have already read the chart parsing
      tutorial, and are familiar with the data structures and rules
      used for chart parsing. </para>

      <note> <para> The chart parsing tutorial is under construction
      Until it is completed, we refer you to the <ulink
      url="&tutdoc;/parsing/t1.html">parsing tutorial</ulink> and the reference
      documentation for the
      <ulink url="&refdoc;/nltk.chart.html">
      <literal>nltk.chart</literal></ulink> module for background
      information. </para> </note>

    </section> <!-- Intro -->
    
    <section> <title> The Basic Algorithm </title>

      <para> <ulink
      url="&refdoc;/nltk.pcfgparser.BottomUpPCFGChartParser.html">
      <literal>BottomUpPCFGChartParser</literal></ulink> is a
      bottom-up parser for <literal>PCFG</literal>s that uses a <ulink
      url="&refdoc;/nltk.chart.Chart.html">
      <literal>Chart</literal></ulink> to record partial results.  It
      maintains a queue of <ulink url="&refdoc;/nltk.chart.Edge.html">
      <literal>Edge</literal></ulink>s, and adds them to the chart one
      at a time.  The ordering of this queue is based on the
      probabilities associated with the edges, allowing the parser to
      insert more likely edges before exploring less likely ones.  For
      each edge that the parser adds to the chart, it may become
      possible to insert new edges into the chart; these are added to
      the queue.  <literal>BottomUpPCFGChartParser</literal> continues
      adding the edges in the queue to the chart until enough complete
      parses have been found, or until the edge queue is
      empty. </para>

    </section> <!-- The basic algorithm -->

    <section> <title> Probabilistic Edges </title>

      <indexterm><primary>probabilistic edge</primary></indexterm>
      
      <para> An <literal>Edge</literal> associates a dotted rule and a
      location with a (partial) parse tree.  A <glossterm>probabilistic
      edge</glossterm> can be formed by using a
      <literal>ProbabilisticTreeToken</literal>s to encode an edge's
      parse tree.  The <literal>ProbabilisticTreeToken</literal>'s
      probability is the product of the probability of the rule that
      generated it and the probabilities of its children.  For
      example, the probability associated with an edge
      [Edge:&nbsp;S&rarr;NP&dot;VP]@[0:2] is the probability of its
      NP child times the probability of the PCFG rule
      [Rule:&nbsp;S&rarr;NP&nbsp;VP].  Note that an edge's tree token
      only includes children for elements to the left of the edge's
      dot.  Thus, the edge's probability does <emphasis>not</emphasis>
      include any probabilities for the elements to the right of the
      edge's dot. </para>

    </section> <!-- Probabilistic Edges -->

    <section> <title> The Edge Queue </title>
      <indexterm><primary>edge queue</primary></indexterm>

      <!-- Bigger dot than dot?? -->
      <indexterm><primary>token edge</primary></indexterm>
      <para> The edge queue is a sorted list of
      <literal>Edge</literal>s that can be added to the chart.  It is
      initialized with a single edge for each token in the text.
      These <glossterm>token edges</glossterm> have the form
      [Edge:&nbsp;<replaceable>type</replaceable>&rarr;&dot;]@<replaceable>loc</replaceable>,
      where <replaceable>type</replaceable> is the token's type and
      <replaceable>loc</replaceable> is its location. </para>

      <para> As each edge from the queue is added to the chart, it may
      become possible to insert new edges into the chart; these new
      edges are added to the queue.  There are two ways that it can
      become possible to insert new edges into the chart: </para>

      <indexterm><primary>bottom-up initialization rule</primary></indexterm>
      <indexterm><primary>fundamental rule</primary></indexterm>
      <itemizedlist>
        <listitem> <para> The <glossterm>bottom-up initialization
        rule</glossterm> can be used to add a self-loop edge whenever
        an edge whose dot is in position 0 is added to the chart. </para>
        </listitem>
        <listitem> <para> The <glossterm>fundamental rule</glossterm>
        can be used to combine a new edge with edges already
        present in the chart. </para>
        </listitem>
      </itemizedlist>

      <para> The edge queue is implemented using a
      <literal>list</literal>.  For efficiency reasons,
      <literal>BottomUpPCFGChartParser</literal> uses
      <literal>pop</literal> to remove edges from the queue.  Thus,
      the front of the queue is the <emphasis>end</emphasis> of the
      list.  This needs to be kept in mind when implementing sorting
      orders for the queue: edges that should be tried first should be
      placed at the end of the list. </para>

    </section> <!-- The edge queue -->

    <section> <title> Sorting The Edge Queue </title>

      <para> By changing the sorting order used by the queue, we can
      control the strategy that the parser uses to search for parses
      of a text.  Since there are a wide variety of reasonable search
      strategies, <literal>BottomUpPCFGChartParser</literal> does not
      define the sorting order for the queue.  Instead,
      <literal>BottomUpPCFGParser</literal> is defined as an abstract
      class; and subclasses are used to implement a variety of
      different queue orderings.  Each subclass is required to define
      the <ulink
      url="&refdoc;/nltk.pcfgparser.BottomUpPCFGChartParser.html#sort_queue">
      <literal>sort_queue</literal></ulink> method, which sorts a
      given queue.  The remainder of this section describes four
      different subclasses of <literal>BottomUpPCFGParser</literal>
      that are defined in the <literal>nltk.pcfgparser</literal>
      module.</para>

      <section> <title> InsidePCFGParser </title>

        <note> <para> I should either explain "inside probabilities"
        or rename this parser (to
        <literal>LowestCostFirstPCFGParser</literal>?). </para>
        </note>

        <para> The simplest way to order the queue is to sort the
        edges by the probabilities of their tree tokens.  This
        ordering concentrates the efforts of the parser on edges that
        are more likely to be correct descriptions of the texts that
        they span.  This approach is implemented by the <ulink
        url="&refdoc;/nltk.pcfgparser.InsidePCFGParser.html">
        <literal>InsidePCFGParser</literal></ulink> class. </para>

        <indexterm><primary>lowest-cost-first search strategy</primary>
        </indexterm>
        <indexterm><primary>optimal search strategy</primary>
        </indexterm>
        <para> The probability of an edge's tree token provides an
        upper bound on the probability of any parse produced using
        that edge.  The probabilistic "cost" of using an edge to form a
        parse is one minus its tree token's probability.  Thus,
        inserting the edges with the most likely tree tokens first
        results in a <glossterm>lowest-cost-first</glossterm> search
        strategy.  Lowest-cost-first search is an
        <glossterm>optimal</glossterm> search strategy: the first
        solution it finds is guaranteed to be the best
        solution. </para>

        <para> However, lowest-cost-first search can be rather
        inefficient.  Since a tree's probability is the product of the
        probabilities of all the rules used to generate it, smaller
        trees tend to have higher probabilities than larger ones.
        Thus, lowest-cost-first search tends to insert edges with
        small tree tokens before moving on to edges with larger ones.
        But any complete parse of the text will necessarily have a
        large tree token; so complete parses will tend to be inserted
        after nearly all other edges. </para>

        <para> The basic problem with lowest-cost-first search is that
        it ignores the probability that an edge's tree is part of a
        complete parse.  It will try parses that are locally coherent,
        even if they are unlikely to form part of a complete parse.
        Unfortunately, it can be quite difficult to calculate the
        probability that a tree is part of a complete parse.  However,
        we can use a variety of techniques to approximate that
        probability. </para>

        <para> Since <literal>InsidePCFGParser</literal> is a subclass
        of <literal>BottomUpPCFGParser</literal>, it only needs to
        define a <literal>sort_queue</literal> method.  Thus, the
        implementation of <literal>InsidePCFGParser</literal> class is
        quite simple: </para>

<programlisting>
<command>class InsidePCFGParser(BottomUpPCFGChartParser):</command>
<command>    def sort_queue(self, queue, chart):</command>
<emphasis>        # Sort the edges by the probabilities of their tree tokens.</emphasis>
<command>        queue.sort(lambda e1,e2:cmp(e1.tree().p(), e2.tree().p()))</command>
</programlisting>

        <note> <para> We haven't used <literal>lambda</literal> before
        now; should we have a note here that explains it?
        Alternatively, should we define an external function (e.g.,
        <literal>cmp_tree_prob</literal>), and use that instead?
        Also, it wouldn't be a bad idea to talk about the sort method
        somewhere, e.g., to mention that it does in-place sorting and
        returns None. </para> </note>

      </section> <!-- InsidePCFGParser -->

      <section> <title> LongestPCFGParser </title>

        <indexterm><primary>best-first search strategy</primary>
        </indexterm>
        <para> <ulink
        url="&refdoc;/nltk.pcfgparser.LongestPCFGParser.html">
        <literal>LongestPCFGParser</literal></ulink> sorts its queue
        in descending order of the edges' lengths.  These lengths
        (properly normalized) provide a crude approximations to the
        probabilities that trees are part of complete parses.  Thus,
        the <literal>LongestPCFGParser</literal> employs a
        <glossterm>best-first</glossterm> search strategy, where it
        inserts the edges that are closest to producing complete
        parses before trying any other edges.  Best-first search is
        <emphasis>not</emphasis> an optimal search strategy: the first
        solution it finds is not guaranteed to be the best solution.
        However, it will usually find a complete parse much more
        quickly than lowest-cost-first search. </para>

        <para> Since <literal>LongestPCFGParser</literal> is a
        subclass of <literal>BottomUpPCFGParser</literal>, its
        implementation simply defines a <literal>sort_queue</literal>
        method: </para>

<programlisting>
<command>class LongestPCFGParser(BottomUpPCFGChartParser):</command>
<command>    def sort_queue(self, queue, chart):</command>
<emphasis>        # Sort the edges by the lengths of their tree tokens.</emphasis>
<command>        queue.sort(lambda e1,e2: cmp(len(e1.loc()), len(e2.loc())))</command>
</programlisting>
        
      </section> <!-- LongestPCFGParser -->
      
      <section> <title> BeamPCFGParser </title>

        <indexterm><primary>prune</primary></indexterm> 
        <para> When large grammars are used to parse a text, the edge
        queue can grow quite long.  The edges at the end of a large
        well-sorted queue are unlikely to be used.  Therefore, it is
        reasonable to remove (or <glossterm>prune</glossterm>) these
        edges from the queue. </para>

        <indexterm><primary>beam search</primary></indexterm> 
        <indexterm><primary>beam</primary></indexterm>
        <indexterm><primary>beam size</primary></indexterm> 
        <para> <ulink
        url="&refdoc;/nltk.pcfgparser.BeamPCFGParser.html">
        <literal>BeamPCFGParser</literal></ulink> provides a simple
        implementation of a pruning PCFG parser.  It uses the same
        sorting order as <literal>InsidePCFGParser</literal>.  But
        whenever the edge queue grows beyond a pre-defined maximum
        length, <literal>BeamPCFGParser</literal> truncates it.  The
        resulting search strategy, lowest-cost-first search with
        pruning, is a type of beam search.  (A <glossterm>beam
        search</glossterm> is a search strategy that only keeps the
        best partial results.)  The queue's predefined maximum length
        is called the <glossterm>beam size</glossterm> (or simply the
        <glossterm>beam</glossterm>).  A
        <literal>BeamPCFGParser</literal>'s beam size is set by the
        first argument to its constructor. </para>

        <indexterm><primary>complete</primary></indexterm> 
        <para> Beam search reduces the space requirements for
        lowest-cost-first search, by discarding edges that are not
        likely to be used.  But beam search also loses many of
        lowest-cost-first search's more useful properties.  Beam
        search is not optimal: it is not guaranteed to find the best
        parse first.  In fact, since it might prune a necessary edge,
        beam search is not even <glossterm>complete</glossterm>: it is
        not guaranteed to return a parse if one exists. </para>

        <para> The implementation for
        <literal>BeamPCFGParser</literal> defines two methods.  First,
        it overrides the constructor, since it needs to record the
        beam size.  And second, it defines the
        <literal>sort_queue</literal> method, which sorts the queue
        and discards any excess edges. </para>
        
<programlisting>
<command>class BeamPCFGParser(BottomUpPCFGChartParser):</command>
<command>    def __init__(self, beam_size, grammar, trace=0):</command>
<command>        BottomUpPCFGChartParser.__init__(self, grammar, trace)</command>
<command>        self._beam_size = beam_size</command>

<command>    def sort_queue(self, queue, chart):</command>
<emphasis>        # Sort the queue.</emphasis>
<command>        queue.sort(lambda e1,e2:cmp(e1.tree().p(), e2.tree().p()))</command>
<emphasis>        # Truncate the queue, if necessary.</emphasis>
<command>        if len(queue) > self._beam_size:</command>
<command>            queue[:] = queue[len(queue)-self._beam_size:]</command>
</programlisting>

        <para> Note that when truncating the queue,
        <literal>sort_queue</literal> uses the expression
        "<literal>queue[:]</literal>" to change the
        <emphasis>contents</emphasis> of the <literal>queue</literal>
        variable.  In particular, compare it to the following code,
        which reassigns the local variable <literal>queue</literal>,
        but does not modify the contents of the given list: </para>

<programlisting>
<emphasis>        # WRONG: This does not change the contents of the edge queue. </emphasis>
<command>        if len(queue) > self._beam_size:</command>
<command>            queue = queue[len(queue)-self._beam_size:]</command>
</programlisting>

        <note> <para> This might be too confusing.  Perhaps I should
        change the definition of <literal>sort_queue</literal> to
        return the "new" list, where the new list can simply be the
        old list.  Of course, you still might want to use
        <literal>queue[:]</literal> in this context, for efficiency
        reasons; and if we made that change, then the students would
        need to remember not to write code like the following:</para>

<programlisting>
<emphasis>        # WRONG: The sort method returns None.</emphasis>
<command>        return queue.sort(lambda e1,e2:cmp(e1.tree().p(), e2.tree().p()))</command>
</programlisting>
        </note>

      </section> <!-- BeamPCFGParser -->
      
      <section> <title> InsideOutsidePCFGParser </title>

        <!-- ==== TO DO: write this? ==== -->
        <note> <para> I plan to add an inside/outside parser; when I
        do, I'll add a description of it to this section. </para>
        </note>

      </section> <!-- InsideOutsidePCFGParser -->

    </section> <!-- Sorting the edge queue -->

  </section> <!-- BottomUpPCFGChartParser -->

  <section> <title> Using <literal>BottomUpPCFGChartParser</literal> </title>

    <para> New <literal>BottomUpPCFGChartParser</literal>s are created
    using the <literal>BottomUpPCFGChartParser</literal> subclasses's
    constructors.  These include:</para>

    <itemizedlist>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.pcfgparser.InsidePCFGParser.html#__init__">
      <literal>InsidePCFGParser</literal> constructor</ulink></para>
      </listitem>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.pcfgparser.LongestPCFGParser.html#__init__">
      <literal>LongestPCFGParser</literal> constructor</ulink></para>
      </listitem>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.pcfgparser.BeamPCFGParser.html#__init__">
      <literal>BeamPCFGParser</literal> constructor</ulink></para>
      </listitem>
      <listitem> <para> The <ulink
      url="&refdoc;/nltk.pcfgparser.RandomPCFGParser.html#__init__">
      <literal>RandomPCFGParser</literal> constructor</ulink></para>
      </listitem>
    </itemizedlist>

    <para> See the reference documentation for the <ulink
    url="&refdoc;/nltk.pcfgparser.BottomUpPCFGChartParser.html">
    <literal>BottomUpPCFGChartParser</literal></ulink> module for a
    complete list of subclasses.  Unless a subclass overrides the
    constructor, it takes a single <literal>PCFG</literal>: </para>

<programlisting>
&prompt; <command>inside_parser = InsidePCFGParser(grammar)</command>
&lt;InsidePCFGParser for &lt;CFG with 16 rules&gt;&gt;
&prompt; <command>longest_parser = LongestPCFGParser(grammar)</command>
&lt;LongestPCFGParser for &lt;CFG with 16 rules&gt;&gt;
&prompt; <command>beam_parser = BeamPCFGParser(20, grammar)</command>
&lt;BeamPCFGParser for &lt;CFG with 16 rules&gt;&gt;
</programlisting>

    <warning><para> <literal>BottomUpPCFGChartParser</literal> is an
    abstract class; you should not directly instantiate it.  If you
    try to use it to parse a text, it will raise an exception, since
    <literal>sort_queue</literal> is will be undefined. </para>
    </warning>

    <para> <literal>BottomUpPCFGChartParser</literal>s implement all
    of the methods defined by the
    <literal>ProbabilisticParserI</literal> interface. </para>

<!-- ==== ADD A PARSE_DIST EXAMPLE ==== -->
<programlisting>
&prompt; <command>text1 = WSTokenizer().tokenize('the dog ate my cookie')</command>
&prompt; <command>text2 = WSTokenizer().tokenize('I saw John with my cookie')</command>

&prompt; <command>inside_parser.parse(text1)</command>
('S':
  ('NP': ('Det': 'the') ('N': 'dog'))
  ('VP': ('V': 'ate') ('NP': ('Det': 'my') 
                             ('N': 'cookie'))))@[0w:5w] (p=0.00175)
&prompt; <command>for parse in inside_parser.parse_n(text2):</command>
&prompt2; <command>    print parse</command>
('S':
  ('NP': 'I')
  ('VP':
    ('V': 'saw')
    ('NP':
      ('NP': 'John')
      ('PP':
        ('P': 'with')
        ('NP': ('Det': 'my') ('N': 'cookie'))))))@[0w:6w] (p=5.2040625e-05)
('S':
  ('NP': 'I')
  ('VP':
    ('VP': ('V': 'saw') ('NP': 'John'))
    ('PP':
      ('P': 'with')
      ('NP': ('Det': 'my') ('N': 'cookie')))))@[0w:6w] (p=2.081625e-05)
</programlisting>

    <para> The <ulink
    url="&refdoc;/nltk.pcfgparser.BottomUpPCFGChartParser.html#trace">
    <literal>trace</literal></ulink> method can be used to set the
    level of tracing output that is generated when parsing a text.
    Trace output displays edges as they are added to the chart, and
    shows the probability for each edges' tree token. </para>

<programlisting>
&prompt; <command>inside_parser.trace(3)</command>
&prompt; <command>inside_parser.parse(text2)</command>

<emphasis>Initializing the queue with token edges...</emphasis>
<emphasis>Processing the edge queue...</emphasis>
  |[=] . . . . .| 'I' -&gt; *          1.0000000
  |. [=] . . . .| 'saw' -&gt; *        1.0000000
  |. . [=] . . .| 'John' -&gt; *       1.0000000
  |. . . [=] . .| 'with' -&gt; *       1.0000000
  |. . . . [=] .| 'my' -&gt; *         1.0000000
  |. . . . . [=]| 'cookie' -&gt; *     1.0000000
  |. > . . . . .| V -&gt; * 'saw'      0.6500000
  |. > . . . . .| VP -&gt; * V NP      0.7000000
  |. [=] . . . .| V -&gt; 'saw' *      0.6500000
  |. . . > . . .| P -&gt; * 'with'     0.6100000
         .               .               .
         .               .               .
         .               .               .
  |. . [=======>| NP -&gt; NP * PP     0.0001906
  |. [=========]| VP -&gt; VP PP *     0.0001387
  |[===========]| S -&gt; NP VP *      0.0000520
  |. [=========>| VP -&gt; VP * PP     0.0000346
  |[===========]| S -&gt; NP VP *      0.0000208
<emphasis>Found 2 parses with 54 edges</emphasis>
</programlisting>

  </section> <!-- Using BottomUpPCFGParser -->

  &index;
</article>
<!--  LocalWords:  authorinitials edl articleinfo para ulink url refdoc cfg Det
 -->
<!--  LocalWords:  Jurafsky tutdoc glossterm indexterm subtrees TreeToken dist
 -->
<!--  LocalWords:  ProbabilisticParserI ProbabilisticTreeToken WSTokenizer PCFG
 -->
<!--  LocalWords:  ProbabilisticTreeTokens ParserI ViterbiPCFGParser ProbDist
 -->
<!--  LocalWords:  ProbabilisticParser nonterminal nonterminals lhs rhs PCFGs
 -->
<!--  LocalWords:  treebanks underway Lexicalized lexicalized Viterbi subtree
 -->
<!--  LocalWords:  pcfgparser iteratively Prob constituent's unary viterbi loc
 -->
<!--  LocalWords:  BottomUpPCFGChartParser BottomUpPCFGParser subclasses cmp
 -->
<!--  LocalWords:  InsidePCFGParser LowestCostFirstPCFGParser prob len pre
 -->
<!--  LocalWords:  LongestPCFGParser BeamPCFGParser search's subclasses's
 -->
<!--  LocalWords:  InsideOutsidePCFGParser RandomPCFGParser
 -->
