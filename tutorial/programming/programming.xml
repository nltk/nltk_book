<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Programming Fundamentals and Python</title>
    &versiondate; &copyright;
  </articleinfo>

  <section id="python">
    <title> Programming Fundamentals and Python </title>

    <para> Introduction to processing lists and strings. </para>

    <para> First, list initialization, length, indexing, slicing: </para>

<programlisting><![CDATA[
>>> a = ['colourless', 'green', 'ideas']
>>> print a
['colourless', 'green', 'ideas']
>>> a
['colourless', 'green', 'ideas']
>>> len(a)
3
>>> a[1]
'green'
>>> a[-1]
'ideas'
>>> a[1:]
['green', 'ideas']
]]></programlisting>

    <para> Note that we had to explicitly print the result of the
      assignment above, using <literal>print a</literal>.  We achieved
      the same result by giving the variable name, which Python evaluates
      and prints.  (For conciseness we henceforth omit these print statements.)
      Below we see use of list concatenation, sorting and reversal.  The final
      command concatenates two list elements.
    </para>

<programlisting><![CDATA[
>>> b = a + ['sleep', 'furiously']
['colourless', 'green', 'ideas', 'sleep', 'furiously']
>>> b.sort()
['colourless', 'furiously', 'green', 'ideas', 'sleep']
>>> b.reverse()
['sleep', 'ideas', 'green', 'furiously', 'colourless']
>>> b[2] + b[1]
'greenideas'
]]></programlisting>

    <para> Simple for loop: </para>

<programlisting><![CDATA[
>>> for w in b:
...    print w[0]
...
's'
'i'
'g'
'f'
'c'
]]></programlisting>

    <para> Miscellaneous further interesting examples:</para>

<programlisting><![CDATA[
>>> b[2][1]
'r'
>>> b.index('green')
2
>>> b[5]
IndexError: list index out of range
>>> b[0] * 3
'sleepsleepsleep'
>>> c = ' '.join(b)
'sleep ideas green furiously colourless'
>>> c.split('r')
['sleep ideas g', 'een fu', 'iously colou', 'less']
>>> map(lambda x: len(x), b)
[5, 5, 5, 9, 10]
>>> [(x, len(x)) for x in b]
[('sleep', 5), ('ideas', 5), ('green', 5), ('furiously', 9), ('colourless', 10)]
]]></programlisting>

    <para> Next we'll take a look at Python ``dictionaries'' (or associative arrays). </para>

<programlisting><![CDATA[
>>> d = {}
>>> d['colourless'] = 'adj'
>>> d['furiously'] = 'adv'
>>> d['ideas'] = 'n'
>>> d.keys()
['colourless', 'furiously', 'ideas']
>>> d.values()
['adv', 'adj', 'n']
>>> d
{'furiously': 'adv', 'colourless': 'adj', 'ideas': 'n'}
>>> d.has_key('ideas')
1
>>> d.get('sleep')
None
>>> for w in d.keys():
...    print "%s [%s]," % (w, d[w]),
furiously [adv], colourless [adj], ideas [n],
]]></programlisting>

    <para> We can use dictionaries to count word occurrences.  For example,
      the following code reads <emphasis>Macbeth</emphasis> and counts the
      frequency of each word. </para>

<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg
# initialize a dictionary 
>>> words = {}
# tokenize Macbeth 
>>> book = gutenberg.read('shakespeare-macbeth.txt')
# process the individual tokens 
>>> for token in book['WORDS']:
    # get the token's text and normalize to lowercase 
...    word = token['TEXT'].lower()
    # get the current frequency count (or zero if it's undefined) and increment 
...    words[word] = words.get(word, 0) + 1
>>> print words['scotland']
18
# create a list of word frequencies
>>> frequencies = [(freq, word) for (word, freq) in words.items()]
# sort by frequency and reverse
>>> frequencies.sort(); frequencies.reverse()
>>> print frequencies[:10]
[(2262, 'the'), (1788, 'and'), (1356, 'to'), (1275, 'of'), (978, 'i'), (852, 'a'),
(714, 'that'), (687, 'you'), (657, 'in'), (615, 'my')]
]]></programlisting>

    <para> Finally, we look at Python's regular expression module, for
    substituting and searching within strings.  We use a utility
    function <literal>re_show</literal> to show how regular
    expressions match against substrings.
    </para>

<programlisting><![CDATA[
>>> import re
>>> from nltk.util import re_show
>>> string = "colourless green ideas sleep furiously"
>>> re_show('l', string)
co{l}our{l}ess green ideas s{l}eep furious{l}y
>>> re.sub('l', 's', string)
'cosoursess green ideas sseep furioussy'
>>> re_show('green', string)
colourless {green} ideas sleep furiously
>>> re.sub('green', 'red', string)
'colourless red ideas sleep furiously'
>>> re_show('[^aeiou][aeiou]', string)
{co}{lo}ur{le}ss g{re}en{ i}{de}as s{le}ep {fu}{ri}ously
>>> re.findall('[^aeiou][aeiou]', string)
['co', 'lo', 'le', 're', ' i', 'de', 'le', 'fu', 'ri']
>>> re.findall('([^aeiou])([aeiou])', string)
[('c', 'o'), ('l', 'o'), ('l', 'e'), ('r', 'e'), (' ', 'i'), ('d', 'e'),
('l', 'e'), ('f', 'u'), ('r', 'i')]
>>> re_show('(green|sleep)', string)
colourless {green} ideas {sleep} furiously
>>> re.findall('(green|sleep)', string)
['green', 'sleep']
]]></programlisting>

  </section>


  <section id="accessing">
    <title> Accessing NLTK </title>

    <para> NLTK consists of a set of Python
    <glossterm>modules</glossterm>, each of which defines classes and
    functions related to a single data structure or task.  Before you
    can use a module, you must <glossterm>import</glossterm> its
    contents.  The simplest way to import the contents of a module is
    to use the
    "<literal>from&nbsp;<replaceable>module</replaceable>&nbsp;import&nbsp;*</literal>"
    command.  For example, to import the contents of the
    <literal>nltk.token</literal> module, which is discussed in this
    tutorial, type: </para>

<programlisting><![CDATA[
>>> from nltk.token import *
]]></programlisting>
    
    <para> A disadvantage of the
    "<literal>from&nbsp;<replaceable>module</replaceable>&nbsp;import&nbsp;*</literal>" command is
    that it does not specify what objects are imported; and it is
    possible that some of the import objects will unintentionally
    cause conflicts.  To avoid this disadvantage, you can explicitly
    list the objects you wish to import.  For example, to import the
    <literal>Token</literal> and <literal>Location</literal> classes
    from the <literal>nltk.token</literal> module, type: </para>

<programlisting><![CDATA[
>>> from nltk.token import Token, CharSpanLocation
]]></programlisting>

    <para> Another option is to import the module itself, rather than
    its contents.  For example, to import the
    <literal>nltk.token</literal> module, type: </para>

<programlisting><![CDATA[
>>> import nltk.token
]]></programlisting>

    <para> Once a module is imported, its contents can then be accessed
    using fully qualified dotted names: </para>

<programlisting><![CDATA[
>>> nltk.token.Token(TEXT='dog')
'dog'@[?]
>>> nltk.token.CharSpanLocation(3,5,source='source.txt')
@[3:5]
]]></programlisting>

    <para> For more information about importing, see any Python
    textbook. </para>

    <para> NLTK is distributed with several corpora, listed in
      <xref linkend="corpora"/>.  Many of these corpora are
      supported by the NLTK <literal>corpus</literal> module.
      The following code listing shows how some of these corpora
      can be accessed.
    </para>

<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg 
>>> gutenberg.items() 
['austen-emma.txt', 'bible-kjv.txt', 'chesterton-brown.txt',
'austen-persuasion.txt', 'shakespeare-macbeth.txt', 'shakespeare-caesar.txt',
'shakespeare-hamlet.txt', 'chesterton-ball.txt', 'milton-paradise.txt',
'chesterton-thursday.txt', 'austen-sense.txt', 'blake-songs.txt',
'blake-poems.txt', 'whitman-leaves.txt']
>>> print gutenberg.read('milton-paradise.txt') 
<[<**This>@[2:8c], <is>@[9:11c], <the>@[12:15c], <Project>@[16:23c], <Gutenberg>@[24:33c],
<Etext>@[34:39c], <of>@[40:42c], <Paradise>@[43:51c], <Lost(Raben)**> ... ]>
>>> from nltk.corpus import brown
>>> brown.items()
['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09',
'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', ... ]
>>> brown.read('ca01')
<[<The/at>, <Fulton/np-tl>, <County/nn-tl>, <Grand/jj-tl>,
<Jury/nn-tl>, <said/vbd>, <Friday/nr>, <an/at>, <investigation/nn>,
<of/in>, <Atlanta's/np$>, <recent/jj>, <primary/nn>, <election/nn>,
<produced/vbd>, <``/``>, <no/at>, <evidence/nn>, <''/''>, <that/cs>,
<any/dti>, <irregularities/nns>, <took/vbd>, <place/nn>, <./.>, ...]>
>>> from nltk.corpus import treebank
>>> treebank.items()
['wsj_0001.prd', 'wsj_0002.prd', 'wsj_0003.prd', ...]
>>> treebank.read('parsed/wsj_0001.prd')
[ (S: (NP-SBJ: (NP: <Pierre> <Vinken>) ...)
        (VP: will ...)),
  (S: (NP-SBJ: <Mr.> <Vinken>)
        (VP: <is> ...)) ]
]]></programlisting>

  </section> <!-- Accessing NLTK -->

  <section id="tour">
    <title> A Tour of NLTK Modules </title>

    <para>[To be written]</para>
  </section> <!-- tour -->

<section> <title>NLTK Interfaces</title>

<para> <literal>TokenizerI</literal> is the first "interface"
      class we've encountered; at this point, we'll take a short
      digression to explain how interfaces are implemented in
      NLTK. </para>

<para> An <glossterm>interface</glossterm> gives a partial
      specification of the behavior of a class, including
      specifications for methods that the class should implement.  For
      example, a "comparable" interface might specify that a class
      must implement a comparison method.  Interfaces do not give a
      complete specification of a class; they only specify a minimum
      set of methods and behaviors which should be implemented by the
      class.  For example, the <literal>TokenizerI</literal> interface
      specifies that a tokenizer class must implement a
      <literal>tokenize</literal> method, which takes a
      <literal>string</literal>, and returns a list of
      <literal>Token</literal>s; but it does not specify what other
      methods the class should implement (if any).  </para>

<para> The notion of "interfaces" can be very useful in ensuring
      that different classes work together correctly.  Although the
      concept of "interfaces" is supported in many languages, such as
      Java, there is no native support for interfaces in
      Python. </para>

<para> NLTK therefore implements interfaces using classes, all
      of whose methods raise the
      <literal>NotImplementedError</literal> exception.  To
      distinguish interfaces from other classes, they are always named
      with a trailing "<literal>I</literal>".  If a class implements
      an interface, then it should be a subclass of the interface.
      For example, the <literal>WhitespaceTokenizer</literal> class implements
      the <literal>TokenizerI</literal> interface, and so it is a
      subclass of <literal>TokenizerI</literal>.  </para>

</section> <!-- NLTK Interfaces -->

<section>
  <title> Further Reading </title>

  <para>Development of NLTK:</para>

  <para>
  Edward Loper and Steven Bird (2002).
  NLTK: The Natural Language Toolkit,
  <emphasis>Proceedings of the ACL Workshop on Effective Tools and
    Methodologies for Teaching Natural Language Processing and Computational
    Linguistics</emphasis>,
  Somerset, NJ: Association for Computational Linguistics,
  pp. 62-69, http://arXiv.org/abs/cs/0205028
  </para>

  <para>
  BirdLoper04
  </para>

  <para>
  Edward Loper (2004).
  NLTK: Building a Pedagogical Toolkit in Python,
  <emphasis>PyCon DC 2004</emphasis>
  Python Software Foundation,
  http://www.python.org/pycon/dc2004/papers/
  </para>

  <para>Python:</para>

  <para>
  Guido Van Rossum (2003).
  <emphasis>An Introduction to Python</emphasis>,
  Network Theory Ltd;
  Guido Van Rossum (2003).
  <emphasis>The Python Language Reference</emphasis>,
  Network Theory Ltd,
  </para>

</section>

  <section id="exercises">
    <title> Exercises </title>

    <para>Using the Python interpreter in interactive mode, experiment
with words, texts, tokens, locations and tokenizers, and satisfy
yourself that you understand all the examples in the tutorial.
Now complete the following questions.</para>

<orderedlist>
  <listitem>
    <para>
    Describe the class of strings matched by the following regular
    expressions:
      <orderedlist>
        <listitem><para><literal>[a-zA-Z]+</literal></para></listitem>
        <listitem><para><literal>[A-Z][a-z]*</literal></para></listitem>
        <listitem><para><literal>\d+(\.\d+)?</literal></para></listitem>
        <listitem><para><literal>([bcdfghjklmnpqrstvwxyz][aeiou][bcdfghjklmnpqrstvwxyz])*</literal></para></listitem>
        <listitem><para><literal>\w+|[^\w\s]+</literal></para></listitem>
      </orderedlist>
    </para>
  </listitem>
  <listitem>
    <para>
    Write regular expressions to match the following classes of strings:
      <orderedlist>
        <listitem><para>A single determiner (assume that ``a,'' ``an,'' and ``the''
    are the only determiners).</para></listitem>
        <listitem><para>An arithmetic expression using integers, addition, and
    multiplication, such as <literal>2*3+8</literal>.</para></listitem>
      </orderedlist>
    </para>
  </listitem>
  <listitem>
    <para>
      Create and print a token with the text <emphasis>parrot</emphasis>
      and occupying the location from character 32-37 inclusive.
    </para>
  </listitem>
  <listitem>
    <para>
      Use the corpus module to tokenize <literal>austin-persuasion.txt</literal>.
      How many words does this book have?
    </para>
  </listitem>
</orderedlist>


  </section> <!-- Exercises -->


&index;
</article>

