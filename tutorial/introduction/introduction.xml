<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Introduction to Natural Language Processing</title>
    &versiondate; &copyright;
  </articleinfo>

<blockquote><attribution>John Ralston Saul</attribution>
<para>
The single and shortest definition of civilization may be the word
<emphasis>language</emphasis>...
Civilization, if it means something concrete, is the conscious but
unprogrammed mechanism by which humans communicate.  And through
communication they live with each other, think, create, and act.
</para>
</blockquote>

  <section id="goals"><title>The Language Challenge</title>

    <para>
      Language is the chief manifestation of human intelligence.
      Through language we express basic needs and lofty aspirations,
      technical know-how and flights of fantasy.  Ideas are
      shared over great separations of distance and time.
      The following samples from English illustrate the richness
      of language:
    </para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para>
      Overhead the day drives level and grey, hiding the sun by a
      flight of grey spears.
      (William Faulkner, <emphasis>As I Lay Dying</emphasis>, 1935)
    </para></listitem>
    <listitem><para>
      When using the toaster please ensure that the exhaust fan is
      turned on. (sign in dormitory kitchen)
    </para></listitem>
    <listitem><para>
      Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
      activities with Ki values of 45.1-271.6 &mu;M (Medline)
    </para></listitem>
    <listitem><para>
      Iraqi Head Seeks Arms (spoof headline,
        <literal>http://www.snopes.com/humor/nonsense/head97.htm</literal>)
    </para></listitem>
    <listitem><para>
      The earnest prayer of a righteous man has great power and wonderful
      results. (James 5:16b)
    </para></listitem>
    <listitem><para>
      Twas brillig, and the slithy toves did gyre and gimble in the wabe
      (Lewis Carroll, <emphasis>Jabberwocky</emphasis>, 1872)
    </para></listitem>
    <listitem><para>
       There are two ways to do this, AFAIK :smile:  (internet
         discussion archive)
    </para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

    <para>
      Thanks to this richness, the study of language is part of many
      disciplines outside of linguistics, including translation,
      literary criticism, philosophy, anthropology and psychology.
      Many less obvious disciplines investigate language use, such as
      law, hermeneutics, forensics, telephony, pedagogy, archaeology,
      cryptanalysis and speech pathology.  Each applies distinct
      methodologies to gather observations, develop theories and test
      hypotheses.  Yet all serve to deepen our understanding of
      language and of the intellect which is manifested in language.
    </para>

    <para>
      The importance of language to science and the arts is matched in
      significance by the cultural treasure that is inherent in
      language.  Each of the worlds ~7,000 human languages
      is rich in unique respects, in its oral
      histories and creation legends, down to its grammatical
      constructions and its very words and their nuances of meaning.
      Threatened remnant
      cultures have words to distinguish plant subspecies according to
      therapeutic uses which are unknown to science.  Languages evolve
      over time as they come into contact with each other and they
      provide a unique window onto human pre-history.  
      Technological change gives rise to new words like
      <emphasis>weblog</emphasis>
      and new morphemes like <emphasis>e-</emphasis> and <emphasis>cyber-</emphasis>.
      In many parts of the world, small linguistic variations from one
      town to the next add up to a completely different language in
      the space of a half-hour drive.  For its breathtaking complexity
      and diversity, human language is as a colourful tapestry
      stretching through time and space.
    </para>

    <para>
      Each new wave of computing technology has faced new challenges
      for language analysis.  Early machine languages gave way to
      high-level programming languages which are automatically parsed
      and interpreted.  Databases are interrogated using linguistic
      expressions like <literal>SELECT age FROM employee</literal>.
      Recently, computing devices have become ubiquitous and are often
      equipped with multimodal interfaces supporting text, speech,
      dialogue and pen gestures.  One way or another, building new
      systems for natural linguistic interaction will require
      sophisticated language analysis.
    </para>

    <para>
      Today, the greatest challenge for language analysis is presented by the
      explosion of text and multimedia content on the world-wide web.
      For many people, a large and growing fraction of work and
      leisure time is spent navigating and accessing this universe of
      information.  <emphasis>What tourist sites can I visit between
      Philadelphia and Pittsburgh on a limited budget?  What do expert
      critics say about Canon digital cameras?  What predictions about
      the steel market were made by credible commentators in the past
      week?</emphasis>  Answering such questions requires a combination of
      language processing tasks including information extraction,
      inference, and summarisation.  The scale of many such tasks
      calls for high-performance computing.
    </para>

    <para>
      As we have seen, <glossterm>natural language
      processing</glossterm>, or NLP, is important for scientific,
      economic, social, and cultural reasons.  NLP is experiencing
      rapid growth as its theories and methods are deployed in a
      variety of new language technologies.  For this reason it is
      important for a wide range of people to have a working knowledge
      of NLP.  Within academia, this includes people in areas from
      humanities computing and corpus linguistics through to computer
      science and artificial intelligence.  Within industry, this
      includes people in human-computer interaction, business
      information analysis, and web software development.  We hope
      that you, a member of this diverse audience reading these
      materials, will come to appreciate the workings of this rapidly
      growing field of NLP and will apply its techniques in the solution of
      real-world problems.  The following chapters present a
      carefully-balanced selection of theoretical foundations and
      practical application, and equips readers to work with large
      datasets, to create robust models of linguistic phenomena, and
      to deploy them in working language technologies.  By integrating
      all of this with the Natural Language Toolkit (NLTK), we hope
      this book opens up the exciting endeavour of practical natural
      language processing to a broader audience than ever before.
    </para>

    <note><para>
      An important aspect of learning NLP using these materials is
      to experience both the challenge and &mdash; we hope &mdash; the satisfaction
      of creating software to process natural language.  The accompanying
      software, NLTK, is available for free and runs on most operating systems
      including Linux/Unix, Mac OSX and Microsoft Windows.  You can
      download NTLK from <literal>nltk.sourceforge.net</literal>, along
      with extensive documentation.  We encourage you to install NLTK
      on your machine before reading beyond the end of this chapter.
    </para></note>

  </section> <!-- The Language Challenge -->

  <section><title>A Brief History of Natural Language Processing</title>

    <para>A long-standing challenge within computer science has been
    to build intelligent machines.  The chief measure of machine
    intelligence has been a linguistic one, namely the
    <glossterm>Turing Test</glossterm>.  Can a dialogue system,
    responding to a user's typed input with its own textual output,
    perform so naturally that users cannot distinguish it from
    a human interlocutor using the same interface?
    Today, there is substantial
    ongoing research and development in such areas as machine
    translation and spoken dialogue, and significant commercial
    systems are in widespread use.  The following dialogue
    illustrates a typical application:
    </para>

<programlisting>
S: How may I help you?

U: When is Saving Private Ryan playing?

S: For what theater?

U: The Paramount theater.

S: Saving Private Ryan is not playing at the Paramount theater, but
   it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. 
</programlisting>

    <para>
   Today's commercial dialogue systems are strictly limited to
   narrowly-defined domains. We could not ask the above system to
   provide driving instructions or details of nearby restaurants
   unless the requisite information had already been stored and
   suitable question and answer sentences had been incorporated into
   the language processing system.  Observe that the above system
   appears understand the user's goals: the user asks when a movie is
   showing and the system correctly determines from this that the user
   wants to see the movie. This inference seems so obvious to humans
   that we usually do not even notice it has been made, yet a natural
   language system needs to be endowed with this capability in order
   to interact naturally. Without it, when asked <literal>Do you know
   when Saving Private Ryan is playing</literal>, a system might
   simply &mdash; and unhelpfully &mdash; respond with a cold
   <literal>Yes</literal>. While it appears that this dialogue system
   can perform simple inferences, such sophistication is only found in
   cutting edge research prototypes.  Instead, the developers of
   commercial dialogue systems use contextual assumptions and simple business
   logic to ensure that the different ways in which a user might
   express requests or provide information are handled in a way that
   makes sense for the particular application.  Thus, whether the user
   says <literal>When is ... </literal>, or <literal>I want to know
   when ... </literal>, or <literal>Can you tell me when
   ... </literal>, simple rules will always result in users being
   presented with screening times.  This is sufficient for the
   system to provide a useful service.
    </para>

    <para>
   Despite some recent advances, it is generally true that those
   natural language systems which have been fully deployed still
   cannot perform common-sense reasoning or draw on world knowledge.
   We can wait for these difficult artificial intelligence problems to
   be solved, but in the meantime it is necessary to live with some
   severe limitations on the reasoning and knowledge capabilities of
   natural language systems. Accordingly, right from the beginning, an
   important goal of NLP research has been to make progress on the
   holy grail of natural linguistic interaction
   <emphasis>without</emphasis> recourse to this unrestricted
   knowledge and reasoning capability.  This is an old challenge, and
   so it is instructive to review the history of the field.
    </para>

    <para>
    The very notion that natural language could be treated in a
    computational manner grew out of a research program, dating back to
    the early 1900s, to reconstruct mathematical reasoning using logic,
    most clearly manifested in the work by Frege, Russell,
    Wittgenstein, Tarski, Lambek and Carnap.  This work led to the
    notion of language as a formal system amenable to automatic
    processing.  Three later developments laid the foundation for
    natural language processing.  The first was <emphasis><glossterm>formal
    language theory</glossterm></emphasis>.  This defined a language as a set of
    strings accepted by a class of automata, such as context-free
    languages and pushdown automata, and provided the underpinnings
    for computational syntax.
    </para>

    <para>
    The second development was <emphasis><glossterm>symbolic
    logic</glossterm></emphasis>. This provided a formal method for capturing
    selected aspects of natural language that are relevant
    for expressing logical proofs. A formal calculus in symbolic
    logic provides the syntax of a language, together with rules of
    inference and, possibly, rules of interpretation in a set-theoretic
    model; examples are propositional logic and first-order
    logic.  Given such a calculus, with a well-defined syntax and
    semantics, it becomes possible to associate meanings with
    expressions of natural language by translating them into
    expressions of the formal calculus. For example, if we translate
    <emphasis>John saw Mary</emphasis> into a formula
    <literal>saw(j,m)</literal>, we (implicitly or explicitly) intepret
    the English verb <emphasis>saw</emphasis> as a binary relation, and
    <emphasis>John</emphasis> and <emphasis>Mary</emphasis> as denoting
    individuals.  More general statements like <emphasis>All birds
    fly</emphasis> require quantifiers, in this case &forall; meaning
    <emphasis>for all</emphasis>: <literal>&forall;x: bird(x) &rarr;
    fly(x)</literal>.  This use of logic provided the technical
    machinery to perform inferences that are an important part of
    language understanding. The third development was the <emphasis>principle of
    compositionality</emphasis>. This was the notion that the meaning
    of a complex expression is comprised of the meaning of its parts
    and their mode of combination. This principle provided a useful
    correspondence between syntax and semantics, namely that the meaning of a
    complex expression could be computed recursively.  Given the
    representation of <emphasis>It is not true that
    &blank;<subscript>p</subscript></emphasis> as
    <literal>not(p)</literal> and <emphasis>John saw Mary</emphasis> as
    <literal>saw(j,m)</literal>, we can compute the interpretation of
    <emphasis>It is not true that John saw Mary</emphasis> recursively
    using the above information to get
    <literal>not(saw(j,m))</literal>. Today, this approach is most
    clearly manifested in a family of grammar formalisms known as
    unification-based grammar, and NLP applications implemented in the
    Prolog programming language.  We can call this approach
    <emphasis>high-church NLP</emphasis>, to highlight its
    preoccupation with order and its ritualized
    methods for ensuring correctness.
    </para>

    <para>
    A separate strand of development in the 1960s and 1970s eschewed
    the declarative/procedural distinction and the principle of
    compositionality.  They only seemed to get in the way of building
    practical systems.  For instance, early question answering systems
    employed fixed pattern-matching templates such as:
    <literal>How many &blank;<subscript>i</subscript>
    does &blank;<subscript>j</subscript> have?</literal>,
    where slot <literal>i</literal> is a feature or service,
    and slot <literal>j</literal> is a person or place.
    Each template came with a predefined semantic function,
    such as <literal>count(i,j)</literal>.  A user's question which
    matched the template would be mapped to the corresponding semantic function
    and then ``executed'' to obtain an answer, <literal>k = count(i,j)</literal>.
    This answer would be substituted into a new template:
    <literal>&blank;<subscript>j</subscript> has
    &blank;<subscript>k</subscript> &blank;<subscript>i</subscript></literal>.
    For example, the question
    <emphasis>How many airports<subscript>i</subscript>
    does London<subscript>j</subscript> have?</emphasis>
    can be mapped onto a template (as shown by the
    subscripts) and translated to an executable program.  The result can be
    substituted into a new template and returned to the user:
    <emphasis>London has five airports</emphasis>.  Finally, the
    subscripts are removed and the natural language answer is returned
    to the user.
    </para>

    <para>
    This approach to NLP is known as <emphasis><glossterm>semantic
    grammar</glossterm></emphasis>.  Such grammars are formalized like
    phrase-structure grammars, but their constituents are no longer
    grammatical categories like noun phrase, but semantic categories
    like <emphasis>Airport</emphasis> and <emphasis>City</emphasis>.
    These grammars work very well in limited domains, and are still
    widely used in spoken language systems.  However, they suffer from
    brittleness, duplication of grammatical structure in different
    semantic categories, and lack of portability.  We call this
    approach <emphasis>low-church NLP</emphasis>, to highlight its
    readiness to adopt new methods regardless of their prestige, and a
    concomitant disregard for tradition.
    </para>

    <para>
    The high-church vs low-church distinction introduced in the
    preceding paragraphs is alive today, and shows up in the
    distinction between symbolic vs statistical methods, deep vs
    shallow processing, binary vs gradient classifications, and
    scientific vs engineering goals.  However, these contrasts are
    highly nuanced, and the debate is no longer as polarised as it
    once was.  In fact, most of the discussions &mdash; and most of
    the advances even &mdash; involve a <emphasis>balancing act</emphasis>
    of the two extremes.  As a result, we are enjoying a new ``ecumenical'' era
    in NLP.  As a simple illustration, consider the way in which
    statistics from large corpora may serve as
    evidence for binary choices in a symbolic grammar.  For instance,
    dictionaries describe the words
    <emphasis>absolutely</emphasis> and
    <emphasis>definitely</emphasis> as nearly synonymous, yet their
    patterns of usage are quite distinct when combined with a
    following verb, as shown in <xref linkend="absolutely"/>.
    </para>

<figure id="absolutely">
  <title>Absolutely vs Definitely (Liberman 2005, LanguageLog.org)</title>
  <informaltable frame="all">
    <tgroup cols="5">
      <tbody>
        <row>
          <entry>Google hits</entry>
          <entry>adore</entry>
          <entry>love</entry>
          <entry>like</entry>
          <entry>prefer</entry>
        </row>
        <row>
          <entry>absolutely</entry>
          <entry>289,000</entry>
          <entry>905,000</entry>
          <entry>16,200</entry>
          <entry>644</entry>
        </row>
        <row>
          <entry>definitely</entry>
          <entry>1,460</entry>
          <entry>51,000</entry>
          <entry>158,000</entry>
          <entry>62,600</entry>
        </row>
        <row>
          <entry>ratio</entry>
          <entry>198/1</entry>
          <entry>18/1</entry>
          <entry>1/10</entry>
          <entry>1/97</entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>

<!--
      http://itre.cis.upenn.edu/~myl/languagelog/archives/002022.html
-->

    <para>
      Observe that <emphasis>absolutely adore</emphasis> is about 200
      times as popular as <emphasis>definitely adore</emphasis>, while
      <emphasis>absolutely prefer</emphasis> is about 100 times rarer
      then <emphasis>definitely prefer</emphasis>.  This information
      is used by statistical language models, but it also counts as
      evidence for a symbolic account of word combination in which
      <emphasis>absolutely</emphasis> can only modify extreme actions
      or attributes.  This information could be represented as a
      binary-valued feature of certain lexical items.  Thus, we see
      statistical information informing symbolic models.  Now that
      this information is codified, it is available to be exploited as
      a contextual feature for a statistical language modelling,
      alongside many other rich sources of symbolic information, like
      hand-constructed parse trees and semantic representations.  Now
      the circle is closed, and we see symbolic information informing
      statistical models.
    </para>

    <para>
      This new rapprochement between high-church and low-church NLP
      is giving rise to many exciting new developments.  We will touch
      on some of these in the ensuing pages.  We too will perform this balancing
      act, employing approaches to NLP that integrate these
      historically-opposed philosophies and methodologies.
    </para>

  </section> <!-- A Brief History of Natural Language Processing -->

  <section><title>An NLP Application: Information Extraction</title>

    <para>
      For many NLP applications, the priority is to extract meaning
      from written text. This must be done
      <emphasis>robustly</emphasis>; processing should not fail when
      unexpected or ill-formed input is received.  Today, robust
      semantic interpretation is easiest when shallow processing
      methods are used.  As we saw in the previous section, these
      methods severely limit (or even omit) syntactic analysis and
      restrict the complexity of the target semantic
      representations. In general, we understand the term
      <emphasis><glossterm>shallow semantics</glossterm></emphasis>
      to refer to the use of shallow analysis
      techniques to arrive at a representation of meaning (and not
      the `depth' of the meaning representations).
    </para>

    <para>
      One well-known instance of shallow semantics is
      <glossterm>Information Extraction</glossterm> (IE). In contrast
      to full text understanding, IE systems only try to recognise a
      limited number of pre-specified semantic topics and use a
      constrained semantic representation.  The initial phase of
      information extraction involves the detection of
      <glossterm>named entities</glossterm>, expressions that denote
      locations, people, companies, times, and monetary amounts.  (In
      fact, these named entities are nothing other than the low-level
      categories of the semantic grammars we saw in the previous
      section, but under a new guise.)  To illustrate, the following
      text contains several expressions which have been tagged as
      entities of types <literal>time</literal>,
      <literal>company</literal> and <literal>location</literal>:
    </para>

<programlisting><![CDATA[
The incident occurred <time>around 5.30pm</time> when a man walked into
<company>William Hill bookkeepers</company> on <location>Ware Road</location>,
and threatened a member of staff with a small black handgun. 
]]></programlisting>


    <para>
      The final output of an information extraction system, usually
      referred to as a <glossterm>template</glossterm>, consists of
      fixed number of slots that must be filled. More semantically, we
      can think of the template as denoting an event in which the
      participating entities play specific roles.  Here is an example
      corresponding to the above text:
    </para>

<programlisting><![CDATA[
Crime event
     Time: around 5.30pm
     Location: William Hill bookkeepers on Ware Road
     Suspect: man
     Weapon: small black handgun
]]></programlisting>

    <para>
      Early work in IE demonstrated that hand-built rules could
      achieve good accuracy in the tasks defined by the DARPA Message
      Understanding Conferences (MUC).  In general, highest accuracy
      has been achieved in identifying named entities, such as
      <literal>William Hill bookkeepers</literal>.  However, it is harder to
      identify which slots the entities should fill; and harder still
      to accurately identify which event the entities are
      participating in.  In recent years attention has shifted to
      making IE systems more portable to new domains by using some
      form of machine learning.  These systems work moderately well on
      marked-up text (e.g. HTML-formatted text), but their performance
      on free text still needs improvement before they can be adopted
      more widely.
    </para>

    <para>
      IE systems commonly use external knowledge sources in addition
      to the input text.  For example, named entity recognizers often
      use name or location gazetteers, extensive catalogues of people
      and places such as the Getty Thesaurus of Geographic Names.  In
      the bioinformatics domain it is common to draw on external
      knowledge bases for standard names of genes and proteins.  A
      general purpose lexical ontology called WordNet is often used in
      IE systems.  Sometimes formal domain ontologies are used as a
      knowledge source for IE, permitting systems to perform simple
      taxonomic inferences and resolve ambiguities in the
      classification of slot-fillers.
    </para>

    <para>
      For example, an IE system for extracting basketball statistics
      from news reports on the web might have to interpret the
      sentence <emphasis>Quincy played three games last
	weekend</emphasis>, where the ambiguous subject
      <emphasis>Quincy</emphasis> might be either a player or a team.
      An ontology in which <emphasis>Quincy</emphasis>
      is identified as the name of a college basketball team would
      permit the ambiguity to be resolved.  In other cases the
      template disambiguates the slot filler.  Thus, in the basketball domain,
      <emphasis>Washington</emphasis> might refer to one of many
      possible players, teams or locations.  If we know that the
      verb <emphasis>won</emphasis> requires an entity whose role in
      the taxonomy is a team, then we can resolve the ambiguity in
      the sentence <emphasis>Washington won</emphasis>.
    </para>

  </section> <!-- Language Technologies -->

<!--

  <section><title>A Sample of NLP Tasks</title>

    <para>
      
    </para>

    <para>
    a selection of: machine translation, dialog systems, document summarization,
    information extraction, text retrieval, question answering
    </para>

  </section>
-->

  <section><title>The Architecture of linguistic and NLP systems</title>

    <para>
      Within the approach to linguistic theory known as <glossterm>
	generative grammar</glossterm>, it is claimed that humans have
      distinct kinds of linguistic knowledge, organised into different
      modules: for example, knowledge of phonology and morphology,
      knowledge of syntax, knowledge of semantics. In a formal theory of
      linguistic competence, each of these different kinds of implicit
      knowledge is made explicit as different module of the theory.
      By `module' here we mean at least a set of primitive elements,
      together with a way of combining the elements into derived
      elements. For example, a phonological module might provide a
      set of phonemes, together with an operation for concatenating
      phonemes into phonological strings. Similarly, a syntactic
      module might provide labeled nodes as primitives, together wih
      a mechanism for assembling nodes into arbitrarily complex trees. A
      set of linguistic primitives, together with some operators for
      defining complex elements, is often called <glossterm>a level of
      representation</glossterm>. 
    </para>

    <para>
      As well as defining modules, a generative grammar will prescribe
      how the modules interact. For example, well-formed phonological
      strings will provide the phonological content of words, and words
      will provide the terminal elements of syntax trees. Similarly, a
      linguistic theory needs to specify some manner of mapping
      well-formed syntactic trees into semantic representations, and
      equally an account of how contextual or pragmatic information can
      resolve indeterminacies in these semantic representations.
      </para>

    <para>As we indicated above, an important aspect of theories of
   generative grammar is that they are intended to model the linguistic
   knowledge of speakers and hearers; they are not intended to explain
   how humans actually process linguistic information. This is, in part,
   reflected in the claim that a generative grammer encodes the
   <emphasis>competence</emphasis> of an idealized native speaker,
   rather than the speaker's <emphasis>competence</emphasis>. A closely
   related distinction is to say that a generative grammar encodes
   <glossterm>declarative</glossterm> rather than
   <glossterm>procedural</glossterm> knowledge.  As you might expect,
   computational linguistics has the crucial role of proposing
   procedural models of language. A central example is
   <glossterm>parsing</glossterm>, where we have to develop
   computational mechanisms which convert, say, strings of words into
   structural representations such as syntax trees. Nevertheless, it is
   widely accepted that well-engineered computational models of language
   contain both declarative and procedural aspects. Thus, a full account
   of parsing will say how declarative knowledge in the form of a
   grammar and lexicon combines with procedural knowledge which
   determines how a syntactic analysis should be assigned to a given
   string of words. This procedural knowledge will be expressed as an
   <glossterm>algorithm</glossterm>: that is, an explicit recipe for
   mapping some input into an appropriate output in a finite number of
   steps.
    </para>

    <para>
   A simple parsing algorithm for context-free gramars, for
   instance, looks first for a rule of the form <literal>S &rarr;
    X<subscript>1</subscript> &mldr;
    X<subscript>n</subscript></literal>, and builds a partial tree
   structure. It then steps through the grammar rules one-by-one,
   looking for a rule of the form <literal>X<subscript>1</subscript>
    &rarr; Y<subscript>1</subscript> ...
    Y<subscript>j</subscript></literal> which will expand the leftmost
   daughter introduced by the <literal>S</literal> rule, and further
   extends the partial tree. This process continues, for example by
   looking  for a rule of the form <literal>Y<subscript>1</subscript>
    &rarr; Z<subscript>1</subscript> ...
    Z<subscript>k</subscript></literal> and expanding the partial tree
   appropriately,  until the leftmost node label in the partial tree is
   a lexical category; the parser then checks to see if the first word
   of the input can belong to the category. To illustrate, let's suppose
   that the first grammer rule chosen by the parser is <literal>S &rarr;
    NP VP</literal> and the second rule chosen is <literal>NP &rarr; Det
    N</literal>; then the partial tree will the one in
   <xref linkend="partialtree"/>.
 
    </para>

    <figure id="partialtree">
      <title>Partial Parse Tree</title>
      <graphic fileref="images/partialtree" scale="15"/>
    </figure>

    <para>
      If we assume that the input string we are trying to parse is
      <emphasis>the cat slept</emphasis>,  we will succeed in
      identifying <emphasis>the</emphasis> as a word which can belong to
      the category <literal>Det</literal>. In this case, the parser goes
      on to the next node of the tree (e.g., <literal>N</literal>) and
      next input word (e.g., <emphasis>cat</emphasis>). However, if we
      had built the same partial tree with an input string <emphasis>did
	the cat sleep</emphasis>, the parse would fail at this point
      (since <emphasis>did</emphasis> is not of category
      <literal>Det</literal>); so it would throw away the structure
      built so far, and look for an alternative way of going from the
      <literal>S</literal> node down to a leftmost lexical category
      (e.g., using a rule <literal>S &rarr; V NP VP</literal>). The
      important point for now is not the details of this or other
      parsing algorithms; we discuss this topic much more fully in
      (parsing-chap xref). Rather, we just want to illustrate the idea
      that an algorithm can be broken down into a fixed number of steps
      which produce a definite result at the end.
    </para>

    <para>In <xref linkend="dialogue"/> we further illustrate some of
    these points in the context of a spoken dialogue system, such as our
    earlier example of an application that offers the user information
    about movies currently on show.  
    </para>
    <figure id="dialogue">
      <title>Architecture of Spoken Dialogue System</title>

		<graphic fileref="images/dialogue" scale="20"/>

    </figure>

    <para> Down the lefthand side of the diagram we have shown a
      pipeline of some representative speech understanding <emphasis>component</emphasis>s.
      These map from speech input via syntactic parsing to some kind of
      meaning representation. Up the righthand side is an inverse
      pipeline of components for concept-to-speech generation. These
      components constitute the procedural aspect of the system's
      natural language processing. In the central column of the diagram
      are some representative declaratives aspects: the repositories of
      language-related information which are called upon by the
      processing components.
    </para>

    <para>In addition to embodying the declarative/procedural
      distinction, the diagram also illustrates that linguistically
      motivated ways of modularizing linguistic knowledge are often
      reflected in computational systems. That is, the various
      components are organized so that the data which they exchange
      corresponds roughly to different levels of representation. For
      example, the output of the speech analysis component will contain
      sequences of phonological representations of words, and the output
      of the parser will be a logical form that represents the
      compositional semantics of the output. Of course the parallel is
      not precise, in part because it is often a matter of practical
      expedience where to place the boundaries between different
      processing components. For example, we can assume that within the
      parsing component of <xref
      linkend="dialogue"/>, there is a level of syntactic
      representation; however, we have chosen not to expose this at the
      level of the system diagram. Perhaps the most important points to
      note are that first, computational systems typically assume that
      the processing of natural language inputs can be usefully broken
      down into a series of discrete steps; and second, that in the
      process of natural language understanding, these steps go from
      more concrete levels to more abstract ones, while in natural
      language production, the direction is reversed.</para>

  </section> <!-- The Architecture of linguistic and NLP systems -->

<!--
  <section><title>Linguistics and Natural Language Processing (draft)</title>

  <para>
    [What is the relationship between linguistics and NLP?
    Goal of (generative) linguistics to account for the grammaticality
    judgements of the ideal monolingual speaker/hearer, vs goal of
    NLP to build systems to map between the (linguistic) systems of
    humans and machines.  Challenge of linguistics is to balance
    descriptive and explanatory adequacy; challenge of NLP to balance
    expressiveness and tractability.]
  </para>

    <para>
    [Grammar as a definition of well-formed
    sentences along with a semantic translation, versus
    an implementation which (say) maps from sentences to
    meanings (parser) or vice versa (generator).
    declarative vs procedural;
    system of rewriting rules vs automaton;
    perspective on NLP: relating the declarative to the procedural;
    distinguish this constrast from competence vs performance.]
    </para>

    <para>
    In the late 1980s and early 1990s there was a promising
    convergence between the fields of linguistics and NLP.  (This had
    been a feature of the 1960s, e.g. with the application of the SPE
    model in speech synthesis systems.)  Computational linguists often
    looked to linguistics as a source of knowledge about language.
    Over the last decade we have seen a new divergence, as
    computational linguists have discovered that linguistic analyses
    often failed to account for the linguistic patterns attested in
    the large corpora used to develop their systems.  However, once
    linguists learn to work with these large datasets, their own
    analytical work will benefit, leading to broader coverage of their
    theories, and earlier refutation of false hypotheses.  The result,
    we expect, will be new opportunities for cross-fertilization
    between linguistics and NLP.
    </para>

    <para>
    [Opportunities for linguists to contribute their insights to the
    future development of NLP and, in the reverse direction, to apply
    the results of NLP research back in linguistics.]
    </para>

    <para>
    [rationalism vs empiricism; realism vs idealism; other balancing acts]
    </para>

<para>
rationalism vs empiricism:
significant distinction coming out of the history of the natural
sciences:
to what extent does our experience of the world provide the basis for
our knowledge?

rationalism - all truth has its origins in human thought
  (doesn't require input from supernatural beings or the
  experience of our senses)
Descartes, Leibniz
``innate ideas'' implanted in our minds from birth.
E.g. principles of Euclidean geometry arose through the
process of reason, and not divine revelation or sensory experience.
Enlightenment - priority of human reason over revelation/experience.

empiricism
John Locke - primary source of knowledge is the experience of our faculties
- reason is secondary - reflecting on that experience

linguistic implications:
Chomsky, poverty of the stimulus, innate language faculty
to account for universal grammar

vs humans have general learning methods (analogical, memory based),
and use these to identify meaningful patterns in language
and ground them in sensory experience.

</para>

<para>
realism vs idealism;
Kant - phenomena (appearances and representations that we can
experience)
vs the ``things in themselves'' which can never been known directly

what is the metaphysical status of the constructs of our NLP models?
Are things like ``noun phrase'' real world entities, or just
theory-internal constructs, products of thought?
useful fictions
unobservables (e.g. null elements, underlying forms)

realism - theories can establish constructs and these are
existent entities, which actually cause the observed phenomena.

- entities exist in the real world independently of human perception
and human reason
</para>

  </section>
-->

  <section><title>Learning NLP with the Natural Language Toolkit</title>

    <para>
    The Natural Language Toolkit (NLTK) was originally created as
    part of a computational linguistics course in the Department of
    Computer and Information Science at the University of
    Pennsylvania in 2001.  Since then it has been developed and
    expanded with the help of dozens of contributors.  It has now
    been adopted in courses in dozens of universities, and serves
    as the basis of many research projects.  In this section we will
    discuss some of the benefits of learning (and teaching) NLP
    using NLTK.
    </para>

    <para>
    NLP is often taught within the confines of a single-semester
    course, either at advanced undergraduate level, or at postgraduate
    level.  Unfortunately, it turns out to be rather difficult to
    cover both the theoretical and practical sides of the subject in
    such a short span of time.  Some courses focus on theory to the
    exclusion of practical exercises, and deprive students of the
    challenge and excitement of writing programs to automatically
    process natural language.  Other courses are simply designed to
    teach programming for linguists, and get too caught up in the
    mechanics of programming to get very far with NLP.  NLTK was
    developed, in part, to address this problem, making it feasible
    to cover a substantial amount of theory and practice within a
    single-semester course.
    </para>

    <para>
    A significant fraction of any NLP course is made up of fundamental
    data structures and algorithms.  These are usually taught with the
    help of formal notations and complex diagrams.  Large trees and
    charts are copied onto the board and edited in tedious slow
    motion, or laboriously prepared for presentation slides.  A more
    effective method is to use live demonstrations.  NLTK provides
    interactive graphical user interfaces, making it possible to view
    program state, and to study program execution step-by-step.  Most
    NLTK modules have a demonstration mode, where they will perform an
    interesting task without requiring any special input from the
    user.  It is even possible to make minor modifications to programs
    in response to ``what if'' questions from the audience.  Not only
    do students learn the mechanics of NLP more quickly, they gain
    deeper insights into the data structures and algorithms, and they
    acquire new problem-solving skills.
    </para>

    <para>
    Thus NLTK offers a fresh approach to NLP pedagogy, in which
    theoretical content is tightly integrated with application.
    Practical work is supported by significant corpora distributed
    with NLTK, including samples from the Linguistic Data Consortium,
    the leading publisher of linguistic data.
    </para>
 
    <para>
    NLTK supports assignments of varying difficulty and scope.  In the
    simplest assignments, students experiment with existing components
    to perform a wide variety of NLP tasks.  This may involve no
    programming at all, in the case of the existing demonstrations, or
    simply changing a line or two of program code.  As students become
    more familiar with the toolkit they can be asked to modify
    existing components or to create complete systems out of existing
    components.  NLTK also provides students with a flexible framework
    for advanced projects, such as developing a multi-component
    system, by integrating and extending NLTK components, and adding
    on entirely new components.  Here NLTK helps by providing standard
    implementations of all the basic data structures and algorithms,
    interfaces to standard corpora, and a flexible and extensible
    architecture.
    </para>

  </section> <!-- Learning NLP with the Natural Language Toolkit -->

  <section><title>The Python Programming Language</title>

    <para>
    NLTK is written in the <glossterm>Python</glossterm> language, a simple yet powerful
    scripting language with excellent functionality for processing
    linguistic data.  Python can be downloaded for free from
    <literal>www.python.org</literal>.
    Here is a five-line Python program which
    takes text input and prints all the words ending in <literal>ing</literal>:
    </para>

<programlisting><![CDATA[
import sys                            # load the system library
for line in sys.stdin.readlines():    # for each line of input
    for word in line.split():         # for each word in the line
        if word.endswith('ing'):      # does the word end in 'ing'?
            print word                # if so, print the word
]]></programlisting>

    <para>
    This program illustrates some of the main features of Python.
    First, whitespace is used to <emphasis>nest</emphasis> lines of
    code, thus the line starting with <literal>if</literal> falls
    inside the scope of the previous line starting with
    <literal>for</literal>, so the <literal>ing</literal> test is
    performed for each word.  Second, Python is
    <emphasis>object-oriented</emphasis>; each variable is an entity
    which has certain defined attributes and methods.  For example,
    <literal>line</literal> is more than a sequence of characters.  It
    has a method (or operation) called <literal>split</literal> which
    can break a line into its words.  To apply a method to an object,
    we give the object name, followed by a period, followed by the
    method name.  Third, methods have <emphasis>arguments</emphasis>
    expressed inside parentheses.  For instance,
    <literal>split</literal> had no argument because we were splitting
    the string wherever there was white space.  To split a string into
    sentences delimited by a period, we could write
    <literal>split('.')</literal>.  Finally, and most importantly,
    Python is highly readable, so much so that it is fairly easy to guess
    what the above program does without knowing any Python.
    </para>

    <para>
    The readability of Python is particularly striking in comparison
    to other languages which have been used for NLP, such as <glossterm>Perl</glossterm>.
    Here is a Perl program which prints words ending in
    <literal>ing</literal>:
    </para>

<programlisting><![CDATA[
while (<>) {                          # for each line of input
    foreach my $word (split) {        # for each word in a line
        if ($word =~ /ing$/) {        # does the word end in 'ing'?
            print "$word\n";          # if so, print the word
        }
    }
}
]]></programlisting>

    <para>
    Like Python, Perl is a scripting language.  However, it is not an
    object-oriented language, and its syntax is obscure.  For
    instance, it is difficult to guess what kind of entities are
    represented by: <literal>&lt;&gt;</literal>,
    <literal>my</literal>, and <literal>split</literal>.  We agree
    with Hammond, the author of a book on Perl programming for
    linguists, who writes that <emphasis>it is quite easy in Perl to
    write programs that simply look like raving gibberish, even to
    experienced Perl programmers</emphasis> (Hammond 2003:47).  Our
    own experience, as heavy users of Perl since the 1980s, is that it
    is inordinately difficult to maintain and re-use Perl programs of
    any size.  We believe it is not an optimal choice of programming
    language for linguists or for language processing.
    </para>

    <para>
    Several other languages are used for NLP, including Prolog, Java,
    LISP, C and Haskell.  In the appendix we have provided
    translations of our five-line Python program into these languages.
    We chose Python as the implementation language for NLTK for a
    number of reasons: in particular, it has a shallow learning curve,
    its syntax and semantics are transparent, and it has good
    string-handling functionality.  As a scripting language, Python
    facilitates interactive exploration.  As an object-oriented
    language, Python permits data and methods to be encapsulated and
    re-used easily.  Python comes with an extensive standard library,
    including tools for graphical programming and numerical
    processing.
    </para>

   <para>In summary NLTK defines a basic infrastructure that can be
   used to build NLP programs in Python.  It provides: </para>

    <itemizedlist>
      <listitem><para>Basic classes for representing data relevant to
      natural language processing.</para>
      </listitem>
      <listitem><para>Standard interfaces for performing tasks, such
      as tokenization, tagging, and parsing.</para>
      </listitem>
      <listitem><para>Standard implementations for each task, which
      can be combined to solve complex problems.</para>
      </listitem>
      <listitem><para>Extensive documentation, including tutorials
      and reference documentation.</para>
      </listitem>
    </itemizedlist>

  </section> <!-- The Python Programming Language -->

  <section><title>The Design of NLTK</title>

    <para>
    NLTK was designed with six requirements in mind.  First, NLTK is
    <emphasis>easy to use</emphasis>.  The primary purpose of the
    toolkit is to allow students to concentrate on building natural
    language processing systems.  The more time students must spend
    learning to use the toolkit, the less useful it is.  Second, we
    have made a significant effort to ensure that all the data
    structures and interfaces are <emphasis>consistent</emphasis>,
    making it easy to carry out a variety of tasks using a uniform
    framework.  Third, the toolkit is <emphasis>extensible</emphasis>,
    easily accommodating new components, whether those components
    replicate or extend the toolkit's existing functionality.
    Moreover, the toolkit is organized so that it is usually obvious
    where extensions would fit into the toolkit's infrastructure.
    Fourth, the toolkit is designed to be <emphasis>simple</emphasis>,
    providing an intuitive and appealing framework along with
    substantial building blocks, for students to gain a practical
    knowledge of NLP without having to write mountains of code.  Fifth, the toolkit
    is <emphasis>modular</emphasis>, so that the interaction between
    different components of the toolkit is minimized, and uses simple,
    well-defined interfaces.  In particular, it should be possible to
    complete individual projects using small parts of the toolkit,
    without needing to understand how they interact with the rest of
    the toolkit.  This allows students to learn how to use the toolkit
    incrementally throughout a course.  Modularity also makes it
    easier to change and extend the toolkit.  Finally, the toolkit is
    <emphasis>well documented</emphasis>, including nomenclature, data
    structures, and implementations.
    </para>

    <para>
    Contrasting with these requirements are three non-requirements.
    First, while the toolkit provides a wide range of functions, it is
    not intended to be encyclopedic.  There should be a wide variety
    of ways in which students can extend the toolkit.  Second, while
    the toolkit should be efficient enough that students can use their
    NLP systems to perform meaningful tasks, it does not need to be
    highly optimized for runtime performance.  Such optimizations
    often involve more complex algorithms, and sometimes require the
    use of C or C++, making the toolkit harder to install.  Third, we have
    avoided the the use of clever programming tricks, since clear
    implementations are far preferable to ingenious yet indecipherable
    ones.
    </para>

    <para>
    NLTK is organized into a collection of task-specific modules.
    Each module is a combination of data structures for representing a
    particular kind of information (e.g. trees), and implementations
    of standard algorithms involving those structures (e.g. parsing).
    This approach is a standard feature of object-oriented design, in
    which modules encapsulate the resources and methods needed to
    perform a particular task.
    </para>

    <para>
    The most fundamental NLTK modules are for identifying and
    manipulating individual words of text.  These include:
    <literal>tokenizer</literal>, for breaking up strings of
      characters into word tokens;
    <literal>tokenreader</literal>, for reading tokens from
      different kinds of corpora;
    <literal>stemmer</literal>, for stripping affixes from tokens,
      useful in some text retrieval applications;
    and
    <literal>tagger</literal>, for adding part-of-speech tags,
      including regular-expression taggers, n-gram taggers and
      Brill taggers.
    </para>

    <para>
    The second kind of module is for creating and manipulating
    structured linguistic information.  These modules include:
    <literal>tree</literal>, for representing and processing
      parse trees;
    <literal>featurestructure</literal>, for building and
      unifying nested feature structures (also known as
      attribute-value matrices);
    <literal>cfg</literal>, for specifying free grammars;
    and
    <literal>parser</literal>, for creating parse trees over
      input text, including chart parsers, chunk parsers and
      probabilistic parsers.
    </para>

    <para>
    Several utility modules are provided to facilitate processing
    and visualization.  These include:
    <literal>draw</literal>, to visualize NLP structures and
      processes;
    <literal>probability</literal>, to count and collate events,
      and perform statistical estimation;
    and
    <literal>corpus</literal>, to access tagged linguistic corpora.
    </para>

    <para>
    Finally, several advanced modules are provided, demonstrating NLP
    applications of machine learning techniques.  These include:
    <literal>clusterer</literal>, for discovering groups of similar
      items within a large collection, including k-means and
      expectation maximisation;
    <literal>classifier</literal>, for categorising text into
      different types, including naive Bayes and maximum entropy;
    and
    <literal>hmm</literal>, for Hidden Markov Models, useful for a
      variety of sequence classification tasks.
    </para>

    <para>
    A further group of modules is not part of NLTK proper.  These are
    part of the <literal>nltk_contrib</literal> distribution, and
    include a wide selection of third-party contributions, often
    developed as student projects at various institutions where NLTK
    is used.  Several of these student contributions, such as the
    Brill tagger and the HMM module, have now been incorporated into
    NLTK.  Although these modules are not maintained, they may serve
    as a useful starting point for future student projects.
    </para>

  </section> <!-- NLTK -->

  <section id="corpora"><title>Corpora</title>

<figure id="nltk-corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Lexicon Corpus</entry>
          <entry></entry>
          <entry>Words, tags and frequencies from Brown Corpus and WSJ</entry>
          <entry>general purpose</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Rada Mihalcea</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 
</section> <!-- Corpora -->

<!--
<section><title> Further Reading </title>

  <para>
    ACL publications, and the ACL Anthology.
  </para>

  <para>
    Systems with online interfaces to try out, e.g. WordNet
  </para>

  <para>
    Key papers that cover historical developments, and
    
  </para>

  <para>
  Dialogue example is from
  Bob Carpenter and Jennifer Chu-Carroll's ACL-99 Tutorial on Spoken Dialogue Systems
  </para>

  <section><title> NLP Textbooks and Surveys </title>

  <para>
    This section will give a brief overview of other NLP textbooks,
    and field-wide surveys.
  </para>

  <para>
    <citation>Covington94</citation>
    <citation>GazdarMellish89</citation>
  </para>

<bibliography>
<biblioentry>
  <abbrev>Covington94</abbrev>
  <author><surname>Covington</surname><firstname>Michael A.</firstname></author>
  <pubdate>1994</pubdate>
  <title>Natural Language Processing for Prolog Programmers</title>
  <publishername>Prentice-Hall</publishername>
</biblioentry>

<biblioentry>
  <abbrev>GazdarMellish89</abbrev>
  <author><surname>Gazdar</surname><firstname>Gerald</firstname></author>
  <author><surname>Mellish</surname><firstname>Chris</firstname></author>
  <pubdate>1989</pubdate>
  <title>Natural Language Processing in Prolog: An Introduction to Computational Linguistics</title>
  <publishername>Addison-Wesley</publishername>
</biblioentry>

<biblioentry>
  <abbrev>Dale00</abbrev>
  <editor><surname>Dale</surname><firstname>Robert</firstname></editor>
  <editor><surname>Moisl</surname><firstname>Hermann</firstname></editor>
  <editor><surname>Somers</surname><firstname>Harold</firstname></editor>
  <pubdate>2000</pubdate>
  <title>Handbook of Natural Language Processing</title>
  <publishername>Marcel Dekker</publishername>
</biblioentry>

<biblioentry>
  <abbrev>Mitkov03</abbrev>
  <editor><surname>Mitkov</surname><firstname>Ruslan</firstname></editor>
  <pubdate>2003</pubdate>
  <title>The Oxford Handbook of Computational Linguistics</title>
  <publishername>Oxford University Press</publishername>
</biblioentry>

<biblioentry>
  <abbrev></abbrev>
  <author><surname></surname><firstname></firstname></author>
  <author><surname></surname><firstname></firstname></author>
  <pubdate></pubdate>
  <title></title>
  <publishername></publishername>
</biblioentry>

<biblioentry>
  <abbrev></abbrev>
  <author><surname></surname><firstname></firstname></author>
  <author><surname></surname><firstname></firstname></author>
  <pubdate></pubdate>
  <title></title>
  <publishername></publishername>
</biblioentry>

</bibliography>


  </section>

</section>

-->

<section><title>Appendix: NLP in other Programming Languages</title>

<para>
Earlier we explained the thinking that lay behind our choice of the
Python programming language.  We showed a simple Python program that
reads in text and prints the words that end with <literal>ing</literal>.
In this appendix we provide equivalent programs in other languages, so
that readers can gain a sense of the appeal of Python.
</para>

<para>
Prolog is a logic programming language which has been popular for
developing natural language parsers and feature-based grammars, given
the inbuilt support for search and the
<emphasis>unification</emphasis> operation which combines two feature
structures into one.  Unfortunately Prolog is not easy to use for
string processing or input/output, as the following program code
demonstrates:
</para>

<programlisting><![CDATA[
main :-
    current_input(InputStream),
    read_stream_to_codes(InputStream, Codes),
    codesToWords(Codes, Words),
    maplist(string_to_list, Words, Strings),
    filter(endsWithIng, Strings, MatchingStrings),
    writeMany(MatchingStrings),
    halt.

codesToWords([], []).
codesToWords([Head | Tail], Words) :-
    ( char_type(Head, space) ->
        codesToWords(Tail, Words)
    ;
        getWord([Head | Tail], Word, Rest),
        codesToWords(Rest, Words0),
        Words = [Word | Words0]
    ).

getWord([], [], []).
getWord([Head | Tail], Word, Rest) :-
    (
        ( char_type(Head, space) ; char_type(Head, punct) )
    ->  Word = [], Tail = Rest
    ;   getWord(Tail, Word0, Rest), Word = [Head | Word0]
    ).

filter(Predicate, List0, List) :-
    ( List0 = [] -> List = []
    ;   List0 = [Head | Tail],
        ( apply(Predicate, [Head]) ->
            filter(Predicate, Tail, List1),
            List = [Head | List1]
        ;   filter(Predicate, Tail, List)
        )
    ).

endsWithIng(String) :- sub_string(String, _Start, _Len, 0, 'ing').

writeMany([]).
writeMany([Head | Tail]) :- write(Head), nl, writeMany(Tail).

]]></programlisting>

<para>
Java is an object-oriented language incorporating native support for
the internet, that was originally designed to permit the same
executable program to be run on most computer platforms.  Java is
quickly replacing COBOL as the standard language for business
enterprise software.
</para>

<programlisting><![CDATA[
import java.io.*;
public class IngWords {
    public static void main(String[] args) {
        BufferedReader in = new BufferedReader(new
	    InputStreamReader(
                 System.in));
        String line = in.readLine();
        while (line != null) {
            for (String word : line.split(" ")) {
                if (word.endsWith("ing"))
                    System.out.println(word);
            }
            line = in.readLine();
        }
    }
}
]]></programlisting>


<para>
The C programming language is a highly-efficient low-level
language that is popular for operating system software and for
teaching the fundamentals of computer science.
</para>


<programlisting><![CDATA[
#include <sys/types.h>
#include <regex.h>
#include <stdio.h>
#define BUFFER_SIZE 1024

int main(int argc, char **argv) {
     regex_t space_pat, ing_pat;
     char buffer[BUFFER_SIZE];
     regcomp(&space_pat, "[, \t\n]+", REG_EXTENDED);
     regcomp(&ing_pat, "ing$", REG_EXTENDED | REG_ICASE);

     while (fgets(buffer, BUFFER_SIZE, stdin) != NULL) {
         char *start = buffer;
         regmatch_t space_match;
         while (regexec(&space_pat, start, 1, &space_match, 0) == 0) {
             if (space_match.rm_so > 0) {
                 regmatch_t ing_match;
                 start[space_match.rm_so] = '\0';
                 if (regexec(&ing_pat, start, 1, &ing_match, 0) == 0)
                     printf("%s\n", start);
             }
             start += space_match.rm_eo;
         }
     }
     regfree(&space_pat);
     regfree(&ing_pat);

     return 0;
}
]]></programlisting>

<para>
LISP is a so-called functional programming language, in which all objects
are lists, and all operations are performed by (nested) functions
of the form <literal>(function arg1 arg2 ...)</literal>.
Many of the earliest NLP systems were implemented in LISP.
</para>

<programlisting><![CDATA[
(defpackage "REGEXP-TEST" (:use "LISP" "REGEXP"))
(in-package "REGEXP-TEST")

(defun has-suffix (string suffix)
  "Open a file and look for words ending in _ing."
  (with-open-file (f string)
     (with-loop-split (s f " ")
        (mapcar #'(lambda (x) (has_suffix suffix x)) s))))

(defun has_suffix (suffix string)
  (let* ((suffix_len (length suffix))
   (string_len (length string))
    (base_len (- string_len suffix_len)))
    (if (string-equal suffix string :start1 0 :end1 NIL :start2 base_len :end2 NIL)
        (print string))))

(has-suffix "test.txt" "ing")

]]></programlisting>

<para>
Haskell is another functional programming language which permits
a much more compact solution of our simple task.
</para>


<programlisting><![CDATA[
module Main
  where main = interact (unlines.(filter ing).(map (filter isAlpha)).words)
    where ing = (=="gni").(take 3).reverse
]]></programlisting>



<para>
We are grateful to the following people for providing program samples:
Tim Baldwin,
Trevor Cohn,
Rod Farmer
Edward Ivanovic,
Olivia March,
and
Lars Yencken.
</para>



</section> <!-- Appendix -->


&index;
</article>

<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:2
sgml-indent-data:t
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
End:
-->
