<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Introduction to Natural Language Processing in Python</title>
    &versiondate; &copyright;
  </articleinfo>

  <section id="goals">
    <title> Goals </title>

    <para>
      Language is the chief manifestation of human intelligence.
      Through it, we express basic needs and lofty aspirations,
      technical know-how and flights of fantasy.  Ideas are
      shared, often over great separations of distance and time.
      Here is a handful of samples from English, just one of
      about 7,000 human languages:
    </para>

<orderedlist><listitem><para>
  <orderedlist>
    <listitem><para>
      Overhead the day drives level and grey, hiding the sun by a
      flight of grey spears.
      (William Faulkner, <emphasis>As I Lay Dying</emphasis>, 1935)
    </para></listitem>
    <listitem><para>
      When using the toaster please ensure that the exhaust fan is
      turned on.
    </para></listitem>
    <listitem><para>
      Amiodarone weakly inhibited CYP2C9, CYP2D6, and CYP3A4-mediated
      activities with Ki values of 45.1-271.6 &mu;M (Medline)
    </para></listitem>
    <listitem><para>
      Iraqi Head Seeks Arms (spoof headline,
        <literal>http://www.snopes.com/humor/nonsense/head97.htm</literal>)
    </para></listitem>
    <listitem><para>
      The earnest prayer of a righteous man has great power and wonderful
      results. (James 5:16b)
    </para></listitem>
    <listitem><para>
      Twas brillig, and the slithy toves did gyre and gimble in the wabe
      (Lewis Carroll, <emphasis>Jabberwocky</emphasis>, 1872)
    </para></listitem>
    <listitem><para>
       There are two ways to do this, AFAIK :smile:
    </para></listitem>
  </orderedlist>
</para></listitem></orderedlist>

    <para>
      Thanks to this richness, the study of language is part of many
      disciplines outside of linguistics, including translation,
      literary criticism, philosophy, anthropology and psychology.
      Many less obvious disciplines investigate language use, such as
      law, hermeneutics, forensics, telephony, pedagogy, archaeology,
      cryptanalysis and speech pathology.  Each applies distinctive
      methodologies to gather observations, develop theories and test
      hypotheses.  Yet all serve to deepen our understanding of
      language, and of the intellect which is manifested in language.
    </para>

    <para>
      The importance of language to science and the arts is matched in
      significance by the cultural treasure that is inherent in
      language.  Each language is rich in unique respects, in its oral
      histories and creation legends, down to its grammatical
      constructions and its very words and their nuances of meaning,
      providing a window onto human pre-history.  Threatened remnant
      cultures have words to distinguish plant subspecies according to
      therapeutic uses which are unknown to science.  Languages evolve
      over time as they come into contact with each other, while
      technological change gives rise to new words, even morphemes
      like <emphasis>e-</emphasis> and <emphasis>cyber-</emphasis>.
      In many parts of the world, small linguistic variations from one
      town to the next add up to a completely different language in
      the space of a half-hour drive.  For its breathtaking complexity
      and diversity, human language is as a colourful tapestry
      stretching through time and space.  This heritage is routinely
      recorded, transcribed, digitized, and archived for future
      scientific and humanitarian endeavours.
    </para>

    <para>
      The striking consequences of each new wave of computing
      technology are the challenges for language analysis.
      Early machine languages gave way to high-level programming
      languages which are automatically parsed and interpreted.
      Databases are interrogated using linguistic expressions
      like <literal>SELECT age FROM employee</literal>.
      More recently, computing devices have become ubiquitous,
      and keyboards are being replaced by multimodal interfaces
      incorporating text, speech, and dialogue technologies.
      Building systems for natural linguistic interaction requires
      sophisticated language analysis.
    </para>

    <para>
      The greatest challenge for language analysis is presented by the
      explosion of text and multimedia content on the world-wide web.
      For many people, a large and growing fraction of work and
      leisure time is spent navigating and accessing this universe of
      information.  What tourist sites can I visit between
      Philadelphia and Pittsburgh on a limited budget?  What do expert
      critics say about Canon digital cameras?  What predictions about
      the steel market were made by credible commentators in the past
      week?  Answering such questions may require a combination of
      language processing tasks such as information extraction, collation,
      inference, and summarisation.
    </para>

    <para>
      The large and growing scale of language analysis requires
      automatic processing.  Thus, <emphasis>natural language
      processing</emphasis>, or NLP, is important for scientific,
      cultural, and economic reasons.  It is experiencing rapid growth
      as its theories and methods are applied to a rich variety of new
      language technologies.  Linguists and computer scientists need
      to have a working knowledge of NLP in order to participate in
      this exciting new endeavour.
    </para>

    <para>
    In the late 1980s and early 1990s there was a promising
    convergence between the fields of linguistics and NLP.  (This had
    been a feature of the 1960s, e.g. with the application of the SPE
    model in speech synthesis systems.)  Computational linguists often
    looked to linguistics as a source of knowledge about language.
    Over the last decade we have seen a new divergence, as
    computational linguists have discovered that linguistic analyses
    often failed to account for the linguistic patterns attested in
    the large corpora used to develop their systems.  However, once
    linguists learn to work with these large datasets, their own
    analytical work will benefit, leading to broader coverage of their
    theories, and earlier refutation of false hypotheses.  The result,
    we expect, will be new opportunities for cross-fertilization
    between linguistics and NLP.
    </para>

    <para>
    Our goal in developing the Natural Language Toolkit is to
    equip linguists to work with these large datasets,
    create robust models of linguistic phenomena, and deploy them in
    working language technologies.  In order to do this, we present
    a carefully balanced selection of theoretical foundations and
    practical application.  For example, we believe that readers must
    understand parsing algorithms, but that they must also be able to
    implement a parser.  The intended audience of these materials is much
    broader than linguists, however.  We believe they will have broad
    appeal from humanities computing all the way to artificial
    intelligence, and from corpus linguistics to information
    extraction.
    There are important
    opportunities for linguists to contribute their insights to the
    future development of NLP and, in the reverse direction, to apply
    the results of NLP research back in linguistics.
    </para>

  </section>

  <section>
    <title>A Brief History of Natural Language Processing</title>

    <para>A long-standing challenge within computer science has been
    to build intelligent machines.  The chief measure of machine
    intelligence has been a linguistic one, namely the
    <emphasis>Turing Test</emphasis>...  Today, there is substantial
    ongoing research and development in such areas as machine
    translation and spoken dialogue, and significant commercial
    systems are in widespread use...  However, these systems are all
    limited to the narrowly domains for which they were developed
    (e.g. cinema schedules).  As the following dialogue fragment
    demonstrates, they can perform simple reasoning (inferring that
    the user doesn't simply want to know when the movie is showing,
    but wants to see the movie), and draw on simple world knowledge
    (in this case, knowledge of the cinema locations).  However,
    such systems cannot perform general common sense reasoning
    or draw on world knowledge.
    </para>

<programlisting>
S: How may I help you?

U: When is Saving Private Ryan playing?

S: For what theater?

U: The Paramount theater.

S: Saving Private Ryan is not playing at the
   Paramount theater, but it's playing at the Madison
   theater at 3:00, 5:30, 8:00, and 10:30. 
</programlisting>

    <para>
    Since the very beginning, then, the fundamental question in NLP
    research has been to make progress on the holy grail of natural
    linguistic interaction without recourse to unlimited knowledge and
    reasoning.
    </para>

    <para>
    The very notion that natural language could be treated in a
    computational manner grew out of a research program, dating back
    to the early 1900s, to reconstruct mathematical talk using logic
    (Frege, Russell, Wittgenstein, Tarski, Lambek, Carnap).  This led
    to the notion of language as a formal system amenable
    to computational translation.  Three further developments laid the
    foundation for natural language processing:
    formal language theory (syntax),
    symbolic logic (semantics, inference), hypothesis of the recursive
    computation of meaning (e.g. rule-to-rule hypothesis,
    correspondence between syntax and semantics, Montague).
    </para>

<para>
declarative vs procedural (grammar vs parser)
</para>

<para>
reaction from ``low church'' AI:
semantic grammars, templates
scripts for specialized domains
(eschewed the declarative vs procedural distinction)
</para>

<para>
neither approach worked - both left out the same thing
</para>

<para>
rationalism vs empiricism:
significant distinction coming out of the history of the natural
sciences:
to what extent does our experience of the world provide the basis for
our knowledge?

rationalism - all truth has its origins in human thought
  (doesn't require input from supernatural beings or the
  experience of our senses)
Descartes, Leibniz
``innate ideas'' implanted in our minds from birth.
E.g. principles of Euclidean geometry arose through the
process of reason, and not divine revelation or sensory experience.
Enlightenment - priority of human reason over revelation/experience.

empiricism
John Locke - primary source of knowledge is the experience of our faculties
- reason is secondary - reflecting on that experience

linguistic implications:
Chomsky, poverty of the stimulus, innate language faculty
to account for universal grammar

vs humans have general learning methods (analogical, memory based),
and use these to identify meaningful patterns in language
and ground them in sensory experience.

</para>

<para>
realism vs idealism;
Kant - phenomena (appearances and representations that we can
experience)
vs the ``things in themselves'' which can never been known directly

what is the metaphysical status of the constructs of our NLP models?
Are things like ``noun phrase'' real world entities, or just
theory-internal constructs, products of thought?
useful fictions
unobservables (e.g. null elements, underlying forms)

realism - theories can establish constructs and these are
existent entities, which actually cause the observed phenomena.

- entities exist in the real world independently of human perception
and human reason
</para>

<para>
Balancing acts:
statistical vs symbolic
(artificial: should be statistical vs ? (gradient vs binary)
symbolic vs non-symbolic
deep vs shallow processing;
science vs engineering
</para>

  </section>


  <section>
    <title>SECTION ON ARCHITECTURE OF LINGUISTICS AND NLP SYSTEMS</title>

  <para>correspondence between computational processing architecture
    and the cognitive architecture assumed by linguists...
  </para>
 

  </section>
    

  <section>
    <title>NLTK: The Natural Language Toolkit</title>

    <para>
    The Natural Language Toolkit (NLTK) was developed in conjunction
    with a computational linguistics course at the University of
    Pennsylvania in 2001.  It can be downloaded for free from
    <literal>nltk.sourceforge.net</literal>.
    </para>

    <para>
    We chose Python because it has a shallow learning curve, its
    syntax and semantics are transparent, and it has good
    string-handling functionality.  As an interpreted language, Python
    facilitates interactive exploration.  As an object-oriented
    language, Python permits data and methods to be encapsulated and
    re-used easily.  Python comes with an extensive standard library,
    including tools for graphical programming and numerical
    processing.
    </para>

    <para>
    NLTK was designed with six requirements in mind.  First, NLTK is
    <emphasis>easy to use</emphasis>.  The primary purpose of the
    toolkit is to allow students to concentrate on building natural
    language processing systems.  The more time students must spend
    learning to use the toolkit, the less useful it is.  Second, we
    have made a significant effort to ensure that all the data
    structures and interfaces are <emphasis>consistent</emphasis>.
    Third, the toolkit is <emphasis>extensible</emphasis>, easily
    accommodating new components, whether those components replicate
    or extend the toolkit's existing functionality.  Moreover, the
    toolkit is organized so that it is usually obvious where
    extensions would fit into the toolkit's infrastructure.  Fourth,
    the toolkit is designed to be <emphasis>simple</emphasis>, not
    hiding the complexities of building an NLP system, but providing
    intuitive structures.  Fifth, the toolkit is
    <emphasis>modular</emphasis>, so that the interaction between
    different components of the toolkit is minimized, and uses simple,
    well-defined interfaces.  In particular, it should be possible to
    complete individual projects using small parts of the toolkit,
    without needing to understand how they interact with the rest of
    the toolkit.  This allows students to learn how to use the toolkit
    incrementally throughout a course.  Modularity also makes it
    easier to change and extend the toolkit.  Finally, the toolkit is
    <emphasis>well documented</emphasis>, including nomenclature,
    data structures, and implementations.
    </para>

    <para>
    Contrasting with these requirements are three non-requirements.
    First, while the toolkit provides a wide range of functions, it is
    not intended to be encyclopedic.  There should be a wide variety
    of ways in which students can extend the toolkit.  Second, while
    the toolkit should be efficient enough that students can use their
    NLP systems to perform meaningful tasks, it does not need to be
    highly optimized for runtime performance.  Such optimizations
    often involve more complex algorithms, and sometimes require the
    use of C++, making the toolkit harder to install.  Third, we have
    avoided the the use of clever programming tricks, since clear
    implementations are far preferable to ingenious yet indecipherable
    ones.
    </para>


    <para> The Natural Language Toolkit (NLTK) defines a basic
    infrastructure that can be used to build NLP programs in Python.
    It provides: </para>

    <itemizedlist>
      <listitem><para>Basic classes for representing data relevant to
      natural language processing.</para>
      </listitem>
      <listitem><para>Standard interfaces for performing tasks, such
      as tokenization, tagging, and parsing.</para>
      </listitem>
      <listitem><para>Standard implementations for each task, which
      can be combined to solve complex problems.</para>
      </listitem>
    </itemizedlist>

    <para> This tutorial introduces natural language processing in Python,
      NLTK, and the <literal>nltk.token</literal> module. </para>

  </section> <!-- Goals -->

  <section id="overview">
    <title> Overview of NLTK </title>

    <para>NLTK provides basic classes for representing data relevant to NLP.
    NLTK also provides standard interfaces for performing NLP tasks, along
    with standard implementations of each task.</para>

    <para>NLTK is organized into a collection of task-specific modules and
    packages.  Each contains data-oriented classes to represent NLP information,
    and task-oriented classes to encapsulate the resources and methods needed
    to perform a particular task.</para>

    <para>NLTK has the following modules:

<itemizedlist>
<listitem><para>token: classes for representing and processing
  individual elements of text, such as words and
  sentences
</para></listitem>

<listitem><para>probability: classess for representing and processing
  probabilistic information.
</para></listitem>

<listitem><para>tree: classes for representing and processing hierarchical
information over text.
</para></listitem>

<listitem><para>cfg: classes for representing and processing context
free grammars.</para></listitem>

<listitem><para>fsa: finite state automata</para></listitem>

<listitem><para>tagger: tagging each word with a part-of-speech, a sense, etc
</para></listitem>

<listitem><para>parser: building trees over text (includes chart, chunk and
  probabilistic parsers)
</para></listitem>

<listitem><para>classifier: classify text into categories
  (includes feature, featureSelection, maxent, naivebayes</para></listitem>

<listitem><para>draw: visualize NLP structures and processes</para></listitem>

<listitem><para>corpus: access (tagged) corpus data</para></listitem>
</itemizedlist>
    </para>

  </section> <!-- overview -->

  <section id="python">
    <title> Programming Fundamentals and Python </title>

    <para> Introduction to processing lists and strings. </para>

    <para> First, list initialization, length, indexing, slicing: </para>

<programlisting><![CDATA[
>>> a = ['colourless', 'green', 'ideas']
>>> print a
['colourless', 'green', 'ideas']
>>> a
['colourless', 'green', 'ideas']
>>> len(a)
3
>>> a[1]
'green'
>>> a[-1]
'ideas'
>>> a[1:]
['green', 'ideas']
]]></programlisting>

    <para> Note that we had to explicitly print the result of the
      assignment above, using <literal>print a</literal>.  We achieved
      the same result by giving the variable name, which Python evaluates
      and prints.  (For conciseness we henceforth omit these print statements.)
      Below we see use of list concatenation, sorting and reversal.  The final
      command concatenates two list elements.
    </para>

<programlisting><![CDATA[
>>> b = a + ['sleep', 'furiously']
['colourless', 'green', 'ideas', 'sleep', 'furiously']
>>> b.sort()
['colourless', 'furiously', 'green', 'ideas', 'sleep']
>>> b.reverse()
['sleep', 'ideas', 'green', 'furiously', 'colourless']
>>> b[2] + b[1]
'greenideas'
]]></programlisting>

    <para> Simple for loop: </para>

<programlisting><![CDATA[
>>> for w in b:
...    print w[0]
...
's'
'i'
'g'
'f'
'c'
]]></programlisting>

    <para> Miscellaneous further interesting examples:</para>

<programlisting><![CDATA[
>>> b[2][1]
'r'
>>> b.index('green')
2
>>> b[5]
IndexError: list index out of range
>>> b[0] * 3
'sleepsleepsleep'
>>> c = ' '.join(b)
'sleep ideas green furiously colourless'
>>> c.split('r')
['sleep ideas g', 'een fu', 'iously colou', 'less']
>>> map(lambda x: len(x), b)
[5, 5, 5, 9, 10]
>>> [(x, len(x)) for x in b]
[('sleep', 5), ('ideas', 5), ('green', 5), ('furiously', 9), ('colourless', 10)]
]]></programlisting>

    <para> Next we'll take a look at Python ``dictionaries'' (or associative arrays). </para>

<programlisting><![CDATA[
>>> d = {}
>>> d['colourless'] = 'adj'
>>> d['furiously'] = 'adv'
>>> d['ideas'] = 'n'
>>> d.keys()
['colourless', 'furiously', 'ideas']
>>> d.values()
['adv', 'adj', 'n']
>>> d
{'furiously': 'adv', 'colourless': 'adj', 'ideas': 'n'}
>>> d.has_key('ideas')
1
>>> d.get('sleep')
None
>>> for w in d.keys():
...    print "%s [%s]," % (w, d[w]),
furiously [adv], colourless [adj], ideas [n],
]]></programlisting>

    <para> We can use dictionaries to count word occurrences.  For example,
      the following code reads <emphasis>Macbeth</emphasis> and counts the
      frequency of each word. </para>

<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg
# initialize a dictionary 
>>> words = {}
# tokenize Macbeth 
>>> book = gutenberg.read('shakespeare-macbeth.txt')
# process the individual tokens 
>>> for token in book['WORDS']:
    # get the token's text and normalize to lowercase 
...    word = token['TEXT'].lower()
    # get the current frequency count (or zero if it's undefined) and increment 
...    words[word] = words.get(word, 0) + 1
>>> print words['scotland']
18
# create a list of word frequencies
>>> frequencies = [(freq, word) for (word, freq) in words.items()]
# sort by frequency and reverse
>>> frequencies.sort(); frequencies.reverse()
>>> print frequencies[:10]
[(2262, 'the'), (1788, 'and'), (1356, 'to'), (1275, 'of'), (978, 'i'), (852, 'a'),
(714, 'that'), (687, 'you'), (657, 'in'), (615, 'my')]
]]></programlisting>

    <para> Finally, we look at Python's regular expression module, for
    substituting and searching within strings.  We use a utility
    function <literal>re_show</literal> to show how regular
    expressions match against substrings.
    </para>

<programlisting><![CDATA[
>>> import re
>>> from nltk.util import re_show
>>> string = "colourless green ideas sleep furiously"
>>> re_show('l', string)
co{l}our{l}ess green ideas s{l}eep furious{l}y
>>> re.sub('l', 's', string)
'cosoursess green ideas sseep furioussy'
>>> re_show('green', string)
colourless {green} ideas sleep furiously
>>> re.sub('green', 'red', string)
'colourless red ideas sleep furiously'
>>> re_show('[^aeiou][aeiou]', string)
{co}{lo}ur{le}ss g{re}en{ i}{de}as s{le}ep {fu}{ri}ously
>>> re.findall('[^aeiou][aeiou]', string)
['co', 'lo', 'le', 're', ' i', 'de', 'le', 'fu', 'ri']
>>> re.findall('([^aeiou])([aeiou])', string)
[('c', 'o'), ('l', 'o'), ('l', 'e'), ('r', 'e'), (' ', 'i'), ('d', 'e'),
('l', 'e'), ('f', 'u'), ('r', 'i')]
>>> re_show('(green|sleep)', string)
colourless {green} ideas {sleep} furiously
>>> re.findall('(green|sleep)', string)
['green', 'sleep']
]]></programlisting>

  </section>


  <section id="accessing">
    <title> Accessing NLTK </title>

    <para> NLTK consists of a set of Python
    <glossterm>modules</glossterm>, each of which defines classes and
    functions related to a single data structure or task.  Before you
    can use a module, you must <glossterm>import</glossterm> its
    contents.  The simplest way to import the contents of a module is
    to use the
    "<literal>from&nbsp;<replaceable>module</replaceable>&nbsp;import&nbsp;*</literal>"
    command.  For example, to import the contents of the
    <literal>nltk.token</literal> module, which is discussed in this
    tutorial, type: </para>

<programlisting><![CDATA[
>>> from nltk.token import *
]]></programlisting>
    
    <para> A disadvantage of the
    "<literal>from&nbsp;<replaceable>module</replaceable>&nbsp;import&nbsp;*</literal>" command is
    that it does not specify what objects are imported; and it is
    possible that some of the import objects will unintentionally
    cause conflicts.  To avoid this disadvantage, you can explicitly
    list the objects you wish to import.  For example, to import the
    <literal>Token</literal> and <literal>Location</literal> classes
    from the <literal>nltk.token</literal> module, type: </para>

<programlisting><![CDATA[
>>> from nltk.token import Token, CharSpanLocation
]]></programlisting>

    <para> Another option is to import the module itself, rather than
    its contents.  For example, to import the
    <literal>nltk.token</literal> module, type: </para>

<programlisting><![CDATA[
>>> import nltk.token
]]></programlisting>

    <para> Once a module is imported, its contents can then be accessed
    using fully qualified dotted names: </para>

<programlisting><![CDATA[
>>> nltk.token.Token(TEXT='dog')
'dog'@[?]
>>> nltk.token.CharSpanLocation(3,5,source='source.txt')
@[3:5]
]]></programlisting>

    <para> For more information about importing, see any Python
    textbook. </para>

    <para> NLTK is distributed with several corpora, listed in
      <xref linkend="corpora"/>.  Many of these corpora are
      supported by the NLTK <literal>corpus</literal> module.
      The following code listing shows how some of these corpora
      can be accessed.
    </para>

<programlisting><![CDATA[
>>> from nltk.corpus import gutenberg 
>>> gutenberg.items() 
['austen-emma.txt', 'bible-kjv.txt', 'chesterton-brown.txt',
'austen-persuasion.txt', 'shakespeare-macbeth.txt', 'shakespeare-caesar.txt',
'shakespeare-hamlet.txt', 'chesterton-ball.txt', 'milton-paradise.txt',
'chesterton-thursday.txt', 'austen-sense.txt', 'blake-songs.txt',
'blake-poems.txt', 'whitman-leaves.txt']
>>> print gutenberg.read('milton-paradise.txt') 
<[<**This>@[2:8c], <is>@[9:11c], <the>@[12:15c], <Project>@[16:23c], <Gutenberg>@[24:33c],
<Etext>@[34:39c], <of>@[40:42c], <Paradise>@[43:51c], <Lost(Raben)**> ... ]>
>>> from nltk.corpus import brown
>>> brown.items()
['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09',
'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', ... ]
>>> brown.read('ca01')
<[<The/at>, <Fulton/np-tl>, <County/nn-tl>, <Grand/jj-tl>,
<Jury/nn-tl>, <said/vbd>, <Friday/nr>, <an/at>, <investigation/nn>,
<of/in>, <Atlanta's/np$>, <recent/jj>, <primary/nn>, <election/nn>,
<produced/vbd>, <``/``>, <no/at>, <evidence/nn>, <''/''>, <that/cs>,
<any/dti>, <irregularities/nns>, <took/vbd>, <place/nn>, <./.>, ...]>
>>> from nltk.corpus import treebank
>>> treebank.items()
['wsj_0001.prd', 'wsj_0002.prd', 'wsj_0003.prd', ...]
>>> treebank.read('parsed/wsj_0001.prd')
[ (S: (NP-SBJ: (NP: <Pierre> <Vinken>) ...)
        (VP: will ...)),
  (S: (NP-SBJ: <Mr.> <Vinken>)
        (VP: <is> ...)) ]
]]></programlisting>

  </section> <!-- Accessing NLTK -->

  <section id="corpora">
    <title> Corpora </title>

<figure id="nltk-corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Lexicon Corpus</entry>
          <entry></entry>
          <entry>Words, tags and frequencies from Brown Corpus and WSJ</entry>
          <entry>general purpose</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Rada Mihalcea</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 
</section>

  <section id="tour">
    <title> A Tour of NLTK Modules </title>

    <para>[To be written]</para>
  </section> <!-- tour -->

<section> <title>NLTK Interfaces</title>

<para> <literal>TokenizerI</literal> is the first "interface"
      class we've encountered; at this point, we'll take a short
      digression to explain how interfaces are implemented in
      NLTK. </para>

<para> An <glossterm>interface</glossterm> gives a partial
      specification of the behavior of a class, including
      specifications for methods that the class should implement.  For
      example, a "comparable" interface might specify that a class
      must implement a comparison method.  Interfaces do not give a
      complete specification of a class; they only specify a minimum
      set of methods and behaviors which should be implemented by the
      class.  For example, the <literal>TokenizerI</literal> interface
      specifies that a tokenizer class must implement a
      <literal>tokenize</literal> method, which takes a
      <literal>string</literal>, and returns a list of
      <literal>Token</literal>s; but it does not specify what other
      methods the class should implement (if any).  </para>

<para> The notion of "interfaces" can be very useful in ensuring
      that different classes work together correctly.  Although the
      concept of "interfaces" is supported in many languages, such as
      Java, there is no native support for interfaces in
      Python. </para>

<para> NLTK therefore implements interfaces using classes, all
      of whose methods raise the
      <literal>NotImplementedError</literal> exception.  To
      distinguish interfaces from other classes, they are always named
      with a trailing "<literal>I</literal>".  If a class implements
      an interface, then it should be a subclass of the interface.
      For example, the <literal>WhitespaceTokenizer</literal> class implements
      the <literal>TokenizerI</literal> interface, and so it is a
      subclass of <literal>TokenizerI</literal>.  </para>

</section> <!-- NLTK Interfaces -->

<section>
  <title>Teaching with NLTK</title>

    <para>
    Integrating theory and practice is a difficult in the framework
    of a single semester, introductory course on NLP.  Common approaches
    are to focus on theory to the exclusion of practical exercises, or
    to focus exclusively on teaching linguists how to write programs.
    However, we believe that neither approach is suitable for a
    one-semester course on NLP.  The former provides the students with
    no practical experience, while the latter usually gets bogged down
    in the mechanics of programming and doesn't succeed in teaching
    any significant NLP.  The teacher of a one-semester course faces an
    acute dilemma: forget the practical component and just teach the
    theory or teach programming and try to fit in some CL at the end
    once they can manage all the "housekeeping" functions (file I/O,
    representing and displaying grammars and trees, etc).  It is clear
    that these considerable overheads and shortcomings warrant the
    fresh approach to NLP pedagogy offered here.
    </para>

    <para>
    Apart from the practical component, CL courses may also depend on
    software for in-class demonstrations.  This context calls for
    interactive graphical user interfaces, making it possible to view
    program state (e.g. the chart of a chart parser), observe program
    execution step-by-step (e.g. execution of a finite-state machine),
    and even make minor modifications to programs in response to
    ``what if'' questions from the class.  Because of these
    difficulties it is common to avoid live demonstrations, and keep
    classes for theoretical presentations only.  Apart from being
    dull, this approach leaves students to solve important practical
    problems on their own, or to deal with them less efficiently in
    office hours.
    </para>
 
    <para>
    NLTK supports assignments of varying difficulty and scope.  In the
    simplest assignments, students experiment with existing components
    to perform a wide variety of NLP tasks.  As students become more
    familiar with the toolkit, they can be asked to modify existing
    components, or to create complete systems out of existing
    components.
    NLTK also provides students with a flexible framework for advanced
    projects.  Typical projects might involve implementing a new
    algorithm, developing a new component, or implementing a new task.
    </para>

    <para>
    This book confronts these problems by tightly integrating the
    theoretical content with a practical component based on the
    Natural Language Toolkit.  The toolkit was developed in
    conjunction with a NLP course at the University of Pennsylvania, a
    leading centre for NLP teaching and research.  The practical
    component is supported by significant corpora distributed with
    NLTK, including samples from the Linguistic Data Consortium, the
    leading publisher of linguistic data.  Key features of the toolkit
    are that students augment and replace existing components, they
    learn structured programming by example, and they manipulate
    sophisticated models from the outset.  The integrated material
    will continue to be tested in courses in 2004 at the universities
    of Pennsylvania, Edinburgh and Melbourne, along with a long list
    of other institutions listed on the NLTK website.
    </para>

</section> <!-- teaching -->

<section>
  <title> Further Reading </title>

  <para>Development of NLTK:</para>

  <para>
  Edward Loper and Steven Bird (2002).
  NLTK: The Natural Language Toolkit,
  <emphasis>Proceedings of the ACL Workshop on Effective Tools and
    Methodologies for Teaching Natural Language Processing and Computational
    Linguistics</emphasis>,
  Somerset, NJ: Association for Computational Linguistics,
  pp. 62-69, http://arXiv.org/abs/cs/0205028
  </para>

  <para>
  BirdLoper04
  </para>

  <para>
  Edward Loper (2004).
  NLTK: Building a Pedagogical Toolkit in Python,
  <emphasis>PyCon DC 2004</emphasis>
  Python Software Foundation,
  http://www.python.org/pycon/dc2004/papers/
  </para>

  <para>Python:</para>

  <para>
  Guido Van Rossum (2003).
  <emphasis>An Introduction to Python</emphasis>,
  Network Theory Ltd;
  Guido Van Rossum (2003).
  <emphasis>The Python Language Reference</emphasis>,
  Network Theory Ltd,
  </para>

  <para>
  Dialogue example is from
  Bob Carpenter and Jennifer Chu-Carroll's ACL-99 Tutorial on Spoken Dialogue Systems
  </para>

</section>

  <section id="exercises">
    <title> Exercises </title>

    <para>Using the Python interpreter in interactive mode, experiment
with words, texts, tokens, locations and tokenizers, and satisfy
yourself that you understand all the examples in the tutorial.
Now complete the following questions.</para>

<orderedlist>
  <listitem>
    <para>
    Describe the class of strings matched by the following regular
    expressions:
      <orderedlist>
        <listitem><para><literal>[a-zA-Z]+</literal></para></listitem>
        <listitem><para><literal>[A-Z][a-z]*</literal></para></listitem>
        <listitem><para><literal>\d+(\.\d+)?</literal></para></listitem>
        <listitem><para><literal>([bcdfghjklmnpqrstvwxyz][aeiou][bcdfghjklmnpqrstvwxyz])*</literal></para></listitem>
        <listitem><para><literal>\w+|[^\w\s]+</literal></para></listitem>
      </orderedlist>
    </para>
  </listitem>
  <listitem>
    <para>
    Write regular expressions to match the following classes of strings:
      <orderedlist>
        <listitem><para>A single determiner (assume that ``a,'' ``an,'' and ``the''
    are the only determiners).</para></listitem>
        <listitem><para>An arithmetic expression using integers, addition, and
    multiplication, such as <literal>2*3+8</literal>.</para></listitem>
      </orderedlist>
    </para>
  </listitem>
  <listitem>
    <para>
      Create and print a token with the text <emphasis>parrot</emphasis>
      and occupying the location from character 32-37 inclusive.
    </para>
  </listitem>
  <listitem>
    <para>
      Use the corpus module to tokenize <literal>austin-persuasion.txt</literal>.
      How many words does this book have?
    </para>
  </listitem>
  <listitem>
    <para>
      The historical overview described ``high church'' and ``low
      church'' AI.  Discuss why neither approach to natural language
      processing worked.  What did they both leave out?
    </para>
  </listitem>
</orderedlist>


  </section> <!-- Exercises -->


&index;
</article>

