<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <title>NLTK Tutorial: Introduction to Natural Language Processing in Python</title>
    &copyright;
  </articleinfo>

  <section id="goals">
    <title> Goals </title>

    <para> The Natural Language Toolkit (NLTK) defines a basic
    infrastructure that can be used to build NLP programs in Python.
    It provides: </para>

    <itemizedlist>
      <listitem><para>Basic classes for representing data relevant to
      natural language processing.</para>
      </listitem>
      <listitem><para>Standard interfaces for performing tasks, such
      as tokenization, tagging, and parsing.</para>
      </listitem>
      <listitem><para>Standard implementations for each task, which
      can be combined to solve complex problems.</para>
      </listitem>
    </itemizedlist>

    <para> This tutorial introduces natural language processing in Python,
      NLTK, and the <literal>nltk.token</literal> module. </para>

  </section> <!-- Goals -->

  <section id="overview">
    <title> Overview of NLTK </title>

    <para>NLTK provides basic classes for representing data relevant to NLP.
    NLTK also provides standard interfaces for performing NLP tasks, along
    with standard implementations of each task.</para>

    <para>NLTK is organized into a collection of task-specific modules and
    packages.  Each contains data-oriented classes to represent NLP information,
    and task-oriented classes to encapsulate the resources and methods needed
    to perform a particular task.</para>

    <para>NLTK has the following modules:

<itemizedlist>
<listitem><para>token: classes for representing and processing
  individual elements of text, such as words and
  sentences
</para></listitem>

<listitem><para>probability: classess for representing and processing
  probabilistic information.
</para></listitem>

<listitem><para>tree: classes for representing and processing hierarchical
information over text.
</para></listitem>

<listitem><para>cfg: classes for representing and processing context
free grammars.</para></listitem>

<listitem><para>fsa: finite state automata</para></listitem>

<listitem><para>tagger: tagging each word with a part-of-speech, a sense, etc
</para></listitem>

<listitem><para>parser: building trees over text (includes chart, chunk and
  probabilistic parsers)
</para></listitem>

<listitem><para>classifier: classify text into categories
  (includes feature, featureSelection, maxent, naivebayes</para></listitem>

<listitem><para>draw: visualize NLP structures and processes</para></listitem>

<listitem><para>corpus: access (tagged) corpus data</para></listitem>
</itemizedlist>
    </para>

  </section> <!-- overview -->

  <section id="python">
    <title> Python Overview </title>

    <para> Introduction to processing lists and strings. </para>

    <para> First, list initialization, length, indexing, slicing: </para>

<programlisting><![CDATA[
>>>a = ['colourless', 'green', 'ideas']
>>>print a
['colourless', 'green', 'ideas']
>>>a
['colourless', 'green', 'ideas']
>>>len(a)
3
>>>a[1]
'green'
>>>a[-1]
'ideas'
>>>a[1:]
['green', 'ideas']
]]></programlisting>

    <para> Note that we had to explicitly print the result of the
      assignment above, using <literal>print a</literal>.  We achieved
      the same result by giving the variable name, which Python evaluates
      and prints.  (For conciseness we henceforth omit these print statements.)
      Below we see use of list concatenation, sorting and reversal.  The final
      command concatenates two list elements.
    </para>

<programlisting><![CDATA[
>>>b = a + ['sleep', 'furiously']
['colourless', 'green', 'ideas', 'sleep', 'furiously']
>>>b.sort()
['colourless', 'furiously', 'green', 'ideas', 'sleep']
>>>b.reverse()
['sleep', 'ideas', 'green', 'furiously', 'colourless']
>>>b[2] + b[1]
'greenideas'
]]></programlisting>

    <para> Simple for loop: </para>

<programlisting><![CDATA[
>>>for w in b:
...    print w[0]
...
's'
'i'
'g'
'f'
'c'
]]></programlisting>

    <para> Miscellaneous further interesting examples:</para>

<programlisting><![CDATA[
>>>b[2][1]
'r'
>>>b.index('green')
2
>>>b[5]
IndexError: list index out of range
>>>b[0] * 3
'sleepsleepsleep'
>>>c = ' '.join(b)
'sleep ideas green furiously colourless'
>>>c.split('r')
['sleep ideas g', 'een fu', 'iously colou', 'less']
>>>map(lambda x: len(x), b)
[5, 5, 5, 9, 10]
>>>[(x, len(x)) for x in b]
[('sleep', 5), ('ideas', 5), ('green', 5), ('furiously', 9), ('colourless', 10)]
]]></programlisting>

    <para> Next we'll take a look at Python ``dictionaries'' (or associative arrays). </para>

<programlisting><![CDATA[
>>>d = {}
>>>d['colourless'] = 'adj'
>>>d['furiously'] = 'adv'
>>>d['ideas'] = 'n'
>>>d.keys()
['colourless', 'furiously', 'ideas']
>>>d.values()
['adv', 'adj', 'n']
>>>d
{'furiously': 'adv', 'colourless': 'adj', 'ideas': 'n'}
>>>d.has_key('ideas')
1
>>>d.get('sleep')
None
>>>for w in d.keys():
...    print "%s [%s]," % (w, d[w]),
furiously [adv], colourless [adj], ideas [n],
]]></programlisting>

    <para> We can use dictionaries to count word occurrences.  For example,
      the following code reads <emphasis>Macbeth</emphasis> and counts the
      frequency of each word. </para>

<programlisting><![CDATA[
    >>>from nltk.corpus import gutenberg
    # initialize a dictionary 
    >>>words = {}
    # tokenize Macbeth 
    >>>book = gutenberg.tokenize('shakespeare-macbeth.txt')
    # process the individual tokens 
    >>>for token in book['SUBTOKENS']:
        # get the token's text and normalize to lowercase 
    ...    word = token['TEXT'].lower()
        # get the current frequency count (or zero if it's undefined) and increment 
    ...    words[word] = words.get(word, 0) + 1
    >>>print words['scotland']
    18
    # create a list of word frequencies
    >>>frequencies = [(freq, word) for (word, freq) in words.items()]
    # sort by frequency and reverse
    >>>frequencies.sort(); frequencies.reverse()
    >>>print frequencies[:10]
    [(2262, 'the'), (1788, 'and'), (1356, 'to'), (1275, 'of'), (978, 'i'), (852, 'a'),
    (714, 'that'), (687, 'you'), (657, 'in'), (615, 'my')]
]]></programlisting>

    <para> Finally, we look at Python's regular expression module, for
    substituting and searching within strings.  We use a utility
    function <literal>re_show</literal> to show how regular
    expressions match against substrings.
    </para>

<programlisting><![CDATA[
>>>import re
>>>string = "colourless green ideas sleep furiously"
>>> re_show('l', string)
co{l}our{l}ess green ideas s{l}eep furious{l}y
>>>re.sub('l', 's', string)
'cosoursess green ideas sseep furioussy'
>>> re_show('green', string)
colourless {green} ideas sleep furiously
>>>re.sub('green', 'red', string)
'colourless red ideas sleep furiously'
>>> re_show('[^aeiou][aeiou]', string)
{co}{lo}ur{le}ss g{re}en{ i}{de}as s{le}ep {fu}{ri}ously
>>>re.findall('[^aeiou][aeiou]', string)
['co', 'lo', 'le', 're', ' i', 'de', 'le', 'fu', 'ri']
>>>re.findall('([^aeiou])([aeiou])', string)
[('c', 'o'), ('l', 'o'), ('l', 'e'), ('r', 'e'), (' ', 'i'), ('d', 'e'),
('l', 'e'), ('f', 'u'), ('r', 'i')]
>>> re_show('(green|sleep)', string)
colourless {green} ideas {sleep} furiously
>>>re.findall('(green|sleep)', string)
['green', 'sleep']
]]></programlisting>

  </section>


  <section id="accessing">
    <title> Accessing NLTK </title>

    <para> NLTK consists of a set of Python
    <glossterm>modules</glossterm>, each of which defines classes and
    functions related to a single data structure or task.  Before you
    can use a module, you must <glossterm>import</glossterm> its
    contents.  The simplest way to import the contents of a module is
    to use the
    "<literal>from&nbsp;<replaceable>module</replaceable>&nbsp;import&nbsp;*</literal>"
    command.  For example, to import the contents of the
    <literal>nltk.token</literal> module, which is discussed in this
    tutorial, type: </para>

<programlisting><![CDATA[
>>> from nltk.token import *
]]></programlisting>
    
    <para> A disadvantage of the
    "<literal>from&nbsp;<replaceable>module</replaceable>&nbsp;import&nbsp;*</literal>" command is
    that it does not specify what objects are imported; and it is
    possible that some of the import objects will unintentionally
    cause conflicts.  To avoid this disadvantage, you can explicitly
    list the objects you wish to import.  For example, to import the
    <literal>Token</literal> and <literal>Location</literal> classes
    from the <literal>nltk.token</literal> module, type: </para>

<programlisting><![CDATA[
>>> from nltk.token import Token, CharSpanLocation
]]></programlisting>

    <para> Another option is to import the module itself, rather than
    its contents.  For example, to import the
    <literal>nltk.token</literal> module, type: </para>

<programlisting><![CDATA[
>>> import nltk.token
]]></programlisting>

    <para> Once a module is imported, its contents can then be accessed
    using fully qualified dotted names: </para>

<programlisting><![CDATA[
>>> nltk.token.Token(TEXT='dog')
'dog'@[?]
>>> nltk.token.CharSpanLocation(3,5,source='source.txt')
@[3:5]
]]></programlisting>

    <para> For more information about importing, see any Python
    textbook. </para>

    <para> NLTK is distributed with several corpora, listed in
      <xref linkend="corpora"/>.  Many of these corpora are
      supported by the NLTK <literal>corpus</literal> module.
      The following code listing shows how some of these corpora
      can be accessed.
    </para>

<programlisting><![CDATA[
    >>> from nltk.corpus import gutenberg 
    >>> gutenberg.items() 
    ['austen-emma.txt', 'bible-kjv.txt', 'chesterton-brown.txt',
    'austen-persuasion.txt', 'shakespeare-macbeth.txt', 'shakespeare-caesar.txt',
    'shakespeare-hamlet.txt', 'chesterton-ball.txt', 'milton-paradise.txt',
    'chesterton-thursday.txt', 'austen-sense.txt', 'blake-songs.txt',
    'blake-poems.txt', 'whitman-leaves.txt']
    >>> print gutenberg.tokenize('milton-paradise.txt') 
    <[<**This>@[2:8c], <is>@[9:11c], <the>@[12:15c], <Project>@[16:23c], <Gutenberg>@[24:33c],
    <Etext>@[34:39c], <of>@[40:42c], <Paradise>@[43:51c], <Lost(Raben)**> ... ]>
    >>> from nltk.corpus import brown
    >>> brown.items()
    ['ca01', 'ca02', 'ca03', 'ca04', 'ca05', 'ca06', 'ca07', 'ca08', 'ca09',
    'ca10', 'ca11', 'ca12', 'ca13', 'ca14', 'ca15', 'ca16', 'ca17', ... ]
    >>> brown.tokenize('ca01')
    <[<The/at>, <Fulton/np-tl>, <County/nn-tl>, <Grand/jj-tl>,
    <Jury/nn-tl>, <said/vbd>, <Friday/nr>, <an/at>, <investigation/nn>,
    <of/in>, <Atlanta's/np$>, <recent/jj>, <primary/nn>, <election/nn>,
    <produced/vbd>, <``/``>, <no/at>, <evidence/nn>, <''/''>, <that/cs>,
    <any/dti>, <irregularities/nns>, <took/vbd>, <place/nn>, <./.>, ...]>
    >>> from nltk.corpus import treebank
    >>> treebank.items()
    ['wsj_0001.prd', 'wsj_0002.prd', 'wsj_0003.prd', ...]
    >>> treebank.tokenize('parsed/wsj_0001.prd')
    [ (S: (NP-SBJ: (NP: <Pierre> <Vinken>) ...)
            (VP: will ...)),
      (S: (NP-SBJ: <Mr.> <Vinken>)
            (VP: <is> ...)) ]
]]></programlisting>




<figure id="corpora">
  <title>Corpora and Corpus Samples Distributed with NLTK</title>
  <informaltable frame="all">
    <tgroup cols="4">
      <tbody>
        <row>
          <entry>Corpus</entry>
          <entry>Compiler</entry>
          <entry>Contents</entry>
          <entry>Example Application</entry>
        </row>
        <row>
          <entry>20 Newsgroups (selection)</entry>
          <entry>Ken Lang and Jason Rennie</entry>
          <entry>3 newsgroups, 4000 posts, 780k words</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Brown Corpus</entry>
          <entry>Nelson Francis and Henry Kucera</entry>
          <entry>15 genres, 1.15M words, tagged</entry>
          <entry>training and testing taggers, text classification, language modelling</entry>
        </row>
        <row>
          <entry>CoNLL 2000 Chunking Data</entry>
          <entry>Erik Tjong Kim Sang</entry>
          <entry>270k words, tagged and chunked</entry>
          <entry>training and testing chunk parsers</entry>
        </row>
        <row>
          <entry>Genesis Corpus</entry>
          <entry>Misc web sources</entry>
          <entry>6 texts, 200k words, 6 languages</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>Project Gutenberg (selection)</entry>
          <entry>Michael Hart, Gregory Newby, et al</entry>
          <entry>14 texts, 1.7M words</entry>
          <entry>text classification, language modelling</entry>
        </row>
        <row>
          <entry>NIST 1999 Information Extraction (selection)</entry>
          <entry>John Garofolo</entry>
          <entry>63k words, newswire and named-entity SGML markup</entry>
          <entry>training and testing named-entity recognizers</entry>
        </row>
        <row>
          <entry>Levin Verb Index</entry>
          <entry>Beth Levin</entry>
          <entry>3k verbs with Levin classes</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Names Corpus</entry>
          <entry>Mark Kantrowitz and Bill Ross</entry>
          <entry>8k male and female names</entry>
          <entry>text classification</entry>
        </row>
        <row>
          <entry>PP Attachment Corpus</entry>
          <entry>Adwait Ratnaparkhi</entry>
          <entry>28k prepositional phrases, tagged as noun or verb modifiers</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Roget's Thesaurus</entry>
          <entry>Project Gutenberg</entry>
          <entry>200k words, formatted text</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SEMCOR</entry>
          <entry>Vasile Rus and Mihalcea Rada</entry>
          <entry>880k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>SENSEVAL 2 Corpus</entry>
          <entry>Ted Pedersen</entry>
          <entry>600k words, part-of-speech and sense tagged</entry>
          <entry>word-sense disambiguation</entry>
        </row>
        <row>
          <entry>Stopwords Corpus</entry>
          <entry>Martin Porter et al</entry>
          <entry>2,400 stopwords for 11 languages</entry>
          <entry>text retrieval</entry>
        </row>
        <row>
          <entry>Penn Treebank (selection)</entry>
          <entry>LDC</entry>
          <entry>40k words, tagged and parsed</entry>
          <entry>parser development</entry>
        </row>
        <row>
          <entry>Wordnet 1.7</entry>
          <entry>George Miller and Christiane Fellbaum</entry>
          <entry>180k words in a semantic network</entry>
          <entry>word-sense disambiguation, natural language understanding</entry>
        </row>
        <row>
          <entry>Wordlist Corpus</entry>
          <entry>OpenOffice.org et al</entry>
          <entry>960k words and 20k affixes for 8 languages</entry>
          <entry></entry>
        </row>
      </tbody>
    </tgroup>
  </informaltable>
</figure>
 

  </section> <!-- Accessing NLTK -->

  <section id="tour">
    <title> A Tour of NLTK Modules </title>

    <para>[To be written]</para>
  </section> <!-- tour -->

<section> <title>NLTK Interfaces</title>

<para> <literal>TokenizerI</literal> is the first "interface"
      class we've encountered; at this point, we'll take a short
      digression to explain how interfaces are implemented in
      NLTK. </para>

<para> An <glossterm>interface</glossterm> gives a partial
      specification of the behavior of a class, including
      specifications for methods that the class should implement.  For
      example, a "comparable" interface might specify that a class
      must implement a comparison method.  Interfaces do not give a
      complete specification of a class; they only specify a minimum
      set of methods and behaviors which should be implemented by the
      class.  For example, the <literal>TokenizerI</literal> interface
      specifies that a tokenizer class must implement a
      <literal>tokenize</literal> method, which takes a
      <literal>string</literal>, and returns a list of
      <literal>Token</literal>s; but it does not specify what other
      methods the class should implement (if any).  </para>

<para> The notion of "interfaces" can be very useful in ensuring
      that different classes work together correctly.  Although the
      concept of "interfaces" is supported in many languages, such as
      Java, there is no native support for interfaces in
      Python. </para>

<para> NLTK therefore implements interfaces using classes, all
      of whose methods raise the
      <literal>NotImplementedError</literal> exception.  To
      distinguish interfaces from other classes, they are always named
      with a trailing "<literal>I</literal>".  If a class implements
      an interface, then it should be a subclass of the interface.
      For example, the <literal>WhitespaceTokenizer</literal> class implements
      the <literal>TokenizerI</literal> interface, and so it is a
      subclass of <literal>TokenizerI</literal>.  </para>

</section> <!-- NLTK Interfaces -->

  <section id="exercises">
    <title> Exercises </title>

    <para>Using the Python interpreter in interactive mode, experiment
with words, texts, tokens, locations and tokenizers, and satisfy
yourself that you understand all the examples in the tutorial.
Now complete the following questions.</para>

<orderedlist>
  <listitem>
    <para>
    Describe the class of strings matched by the following regular
    expressions:
      <orderedlist>
        <listitem><para><literal>[a-zA-Z]+</literal></para></listitem>
        <listitem><para><literal>[A-Z][a-z]*</literal></para></listitem>
        <listitem><para><literal>\d+(\.\d+)?</literal></para></listitem>
        <listitem><para><literal>([bcdfghjklmnpqrstvwxyz][aeiou][bcdfghjklmnpqrstvwxyz])*</literal></para></listitem>
        <listitem><para><literal>\w+|[^\w\s]+</literal></para></listitem>
      </orderedlist>
    </para>
  </listitem>
  <listitem>
    <para>
    Write regular expressions to match the following classes of strings:
      <orderedlist>
        <listitem><para>A single determiner (assume that ``a,'' ``an,'' and ``the''
    are the only determiners).</para></listitem>
        <listitem><para>An arithmetic expression using integers, addition, and
    multiplication, such as <literal>2*3+8</literal>.</para></listitem>
      </orderedlist>
    </para>
  </listitem>
  <listitem>
    <para>
      Create and print a token with the text <emphasis>parrot</emphasis>
      and occupying the location from character 32-37 inclusive.
    </para>
  </listitem>
  <listitem>
    <para>
      Use the corpus module to tokenize <literal>austin-persuasion.txt</literal>.
      How many words does this book have?
    </para>
  </listitem>
</orderedlist>


  </section> <!-- Exercises -->


&index;
</article>
