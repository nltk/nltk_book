<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.1//EN" [
<!ENTITY prompt "<prompt>&gt;&gt;&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<!-- Add links to the ref documentation? -->
<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Basics</title>
  </articleinfo>

  <section> <title> Goals </title>

    <para> The Natural Language Processing Toolkit provides a basic
    infrastructure that can be used to build NLP programs in Python.
    blah.. </para>

  </section> <!-- Goals -->

  <section> <title> Words </title>

    <para> There are a number of reasonable ways to represent words in
    Python.  Perhaps the simplest is as string values, such as
    <literal>'dog'</literal>; this is how words are typically
    represented when using the toolkit.  

<screen>
    &prompt;<command> words = ['the', 'cat', 'climbed', 'the', 'tree']</command>
    ['the', 'cat', 'climbed', 'the', 'tree']
</screen>

    However, the toolkit also allows for other representations.  For
    example, you could store words as integers, with some mapping
    between integers and words. </para>

    <section> <title> Types and Tokens </title>

      <para> The term "word" can actually be used in two different
      ways: to refer to an individual occurance of a word; or to refer
      to an abstract vocabulary item.  For example, the phrase "my dog
      likes his dog" contains five occurances of words, but only four
      vocabulary items (since the vocabulary item "dog" appears
      twice).  In order to make this distinction clear, we will use
      the term <glossterm>word token</glossterm> to refer to
      occurances of words, and the term <glossterm>word
      type</glossterm> to refer to vocabulary items. </para>

      <para> The terms <glossterm>token</glossterm> and
      <glossterm>type</glossterm> can also be applied in other
      domains.  For example, a <glossterm>sentence token</glossterm>
      is an individual occurance of a sentence; but a
      <glossterm>sentence type</glossterm> is an abstract sentence,
      without context.  If someone repeats a sentence twice, they have
      uttered two sentence tokens, but only one sentence type.  When
      the kind of token or type is obvious from context, we will
      simply use the terms <glossterm>token</glossterm> and
      <glossterm>type</glossterm>. </para>

      <para> In the toolkit, tokens are constructed from their types,
      using the <literal>Token</literal> constructor:

<screen>
    &prompt;<command> my_word_type = 'dog' </command>
    'dog'
    &prompt;<command> my_word_token = Token(my_word_type) </command>
    'dog'@[?]
</screen>

      The reason that the token is displayed this way will be
      explained in the next two sections. </para>

    </section> <!-- Tokens and Types -->

    <section> <title> Text Locations </title>

      <para> <glossterm>Text locations</glossterm> specify regions of
      texts, using a <glossterm>start index</glossterm> and an
      <glossterm>end index</glossterm>.  A location with start index
      <replaceable>s</replaceable> and end index
      <replaceable>e</replaceable> is written
      <literal>[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>]</literal>,
      and specifies the region of the text beginning at
      <replaceable>s</replaceable>, and including everything up to
      (but not including) the text at <replaceable>e</replaceable>.
      Locations are created using the <literal>Location</literal>
      constructor:

<screen>
    &prompt;<command> my_loc = Location(1, 5) </command>
    @[1:5]
    &prompt;<command> another_loc = Location(0, 2) </command>
    @[0:2]
    &prompt;<command> yet_another_loc = Location(22, 22) </command>
    @[22:22]
</screen>
</para>

      <para> Note that a text location does <emphasis>not</emphasis> include
      the text at its end location.  This convention may seem
      unintuitive at first, but it has a number of advantages.  It is
      consistant with Python's slice notation (e.g.,
      <literal>x[1:3]</literal> specifies elements 1 and 2 of x).

        <footnote><para>But unlike Python slices, text locations do
        <emphasis>not</emphasis> support negative indexes.</para></footnote>

      It allows text locations to specify points between tokens,
      instead of just ranges; for example,
      <literal>Location(3,3)</literal> specifies the point just before
      the text at index 3.  And it simplifies arithmatic on indices;
      for example, the length of <literal>Location(5,10)</literal> is
      <literal>10-5</literal>, and two locations are contiguous if the
      start of one equals the end of the other. </para>

      <para> To create a text location specifying the text at a single
      index, you can use the <literal>Location</literal> constructor
      with a single argument.  For example, the fourth word in a text
      could be specified with <literal>loc1</literal>:

<screen>
    &prompt;<command> loc1 = Location(4) </command>
    @[4]
</screen>

      The toolkit uses the shorthand notation
      <literal>@[<replaceable>s</replaceable>]</literal> for locations
      whose width is one.  Note that
      <literal>Location(<replaceable>s</replaceable>)</literal> is
      equivalant to <literal>Location(<replaceable>s</replaceable>,
      <replaceable>s+1</replaceable>)</literal>, <emphasis>not</emphasis>
      <literal>Location(<replaceable>s</replaceable>,
      <replaceable>s</replaceable>)</literal>:

<screen>
    &prompt;<command> loc2 = Location(4, 5) </command>
    @[4]
    &prompt;<command> loc3 = Location(4, 4) </command>
    @[4:4]
</screen>
</para>

      <section> <title> Units </title>

        <para> The start and end indices can be based on a variety of
        different <glossterm>units</glossterm>, such as character
        number, word number, or sentence number.  By default, the unit
        of a text location is left unspecified, but locations can be
        explicitly tagged with information about what unit their
        indices use:

<screen>
    &prompt;<command> my_loc = Location(1, 5, unit='w') </command>
    @[1w:5w]
    &prompt;<command> another_loc = Location(3, 72, unit='c') </command>
    @[3c:72c]
    &prompt;<command> my_loc = Location(6, unit='s') </command>
    @[6s]
    &prompt;<command> my_loc = Location(10, 11, unit='s') </command>
    @[10s]
</screen>

        Unit labels take the form of case-insensitive
        <literal>string</literal>s.  Typical examples of unit labels
        are <literal>'c'</literal> (for character number),
        <literal>'w'</literal> (for word number), and
        <literal>'s'</literal> (for sentence number). </para>

      </section>  <!-- Units -->
      <section> <title> Sources </title>

        <para> A text location may also be tagged with a
        <glossterm>source</glossterm>, which gives an indication of
        where the text was derived from.  A typical example of a
        source would be a <literal>string</literal> containing the
        name of the file from which the element of text was read.

<screen>
    &prompt;<command> my_loc = Location(1, 5, source='foo.txt') </command>
    @[1:5]@'foo.txt'
    &prompt;<command> another_loc = Location(3, 72, unit='c', source='bar.txt') </command>
    @[3c:72c]@'bar.txt'
    &prompt;<command> my_loc = Location(6, unit='s', source='baz.txt') </command>
    @[6s]@'baz.txt'
</screen>

        By default, a text location's source is unspecified. </para>

        <para> Sometimes, it is useful to use text locations as the
        sources for other text locations.  For example, we could
        specify the third character of the fourth word of the first
        sentence in the file <literal>foo.txt</literal> with
        <literal>char_loc</literal>:

<screen>
    &prompt;<command> sentence_loc = Location(0, unit='s', source='foo.txt') </command>
    @[0s]@'foo.txt'
    &prompt;<command> word_loc = Location(3, unit='w', source=sentence_loc) </command>
    @[3w]@[0s]@'foo.txt'
    &prompt;<command> char_loc = Location(2, unit='c', source=word_loc) </command>
    @[2c]@[3w]@[0s]@'foo.txt'
</screen>

        Note that the location indexes are zero-based, so the first
        sentence starts at an index of zero, not one.  </para>

      </section>  <!-- Sources -->
    </section> <!-- Text Locations -->

    <section> <title> Tokens and Locations </title>

      <para> As discussed above, a text token represents a single
      occurance of a text type.  In the NL toolkit, a token is defined
      by a type, together with a location at which that type occurs.  A
      token with type <replaceable>t</replaceable> and location
      <literal>@[<replaceable>l</replaceable>]</literal> can be
      written as
      <literal><replaceable>t</replaceable>@[<replaceable>l</replaceable>]</literal>.
      Tokens are constructed with the <literal>Token</literal>
      constructor:

<screen>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token2 = Token('world', Location(1, unit='w')) </command>
    'world'@[1w]
</screen>
</para>

      <para> Two tokens are only equal if both their type and thier
      location are equal:

<screen>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token2 = Token('hello', Location(1, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token3 = Token('world', Location(0, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token4 = Token('hello', Location(0, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token1 == token2 </command>
    0
    &prompt;<command> token1 == token3 </command>
    0
    &prompt;<command> token1 == token4 </command>
    1
</screen>
</para>

      <para> When a token's location is unknown or unimportant, the
      special location <literal>None</literal> may be used.  A token
      with type <replaceable>t</replaceable> and location
      <literal>None</literal> is written as
      <literal><replaceable>t</replaceable>@[?]</literal>.  If a
      token's location is not specified, it defaults to
      <literal>None</literal>:

<screen>
    &prompt;<command> token1 = Token('hello', None) </command>
    'hello'@[?]
    &prompt;<command> token2 = Token('world') </command>
    'world'@[?]
</screen>
</para>

      <para> A Token with a location of <literal>None</literal> is not
      considered to be equal to any other token.  In particular, even
      if two tokens have the same type, and both have a location of
      <literal>None</literal>, they are not equal:

<screen>
    &prompt;<command> token1 = Token('hello') </command>
    'hello'@[?]
    &prompt;<command> token2 = Token('hello') </command>
    'hello'@[?]
    &prompt;<command> token1 == token2 </command>
    0
</screen>
</para>

    </section>  <!-- Tokens and Locations -->
  </section>  <!-- Words -->

  <section> <title> Texts </title>

    <para> Many natural language processing tasks involve analyzing
    texts of varying sizes, ranging from single sentences to very
    large corpera.  There are a number of ways to represent texts
    using the toolkit.  The simplest is as a single
    <literal>string</literal>.  These strings are typically loaded
    from files:

<screen>
    &prompt;<command> text_str = open('corpus.txt').read() </command>
    'Hello world.  This is a test file.\n'
</screen>
</para>

    <para> It is often more convenient to represent a text as a
    <literal>list</literal> of <literal>Token</literal>s.  These
    <literal>list</literal>s are typically created using a
    <glossterm>tokenizer</glossterm>, such as
    <literal>WSTokenizer</literal> (which splits words apart based on
    whitespace):

<screen>
    &prompt;<command> text_tok_list = WSTokenizer().tokenize(text_str) </command>
    ['Hello'@[0w], 'world.'@[1w], 'This'@[2w], 'is'@[3w], 
     'a'@[4w], 'test'@[5w], 'file.'@[6w]]
</screen>
</para>

    <para> Texts can also be represented as sets of word tokens or
    sets of word types:

<screen>
    &prompt;<command> text_tok_set = Set(*text_tok_list) </command>
    {'This'@[2w], 'a'@[4w], 'Hello'@[0w], 'world.'@[1w], 
     'is'@[3w], 'file.'@[6w], 'test'@[5w]}
</screen>

    For example, this representation might be convenient for a search
    engine which is trying to find all documents containing some
    keyword. (For more information on why a "<literal>*</literal>" is
    used in the call to the Set constructor, see the Set tutorial.)
    <!-- WRITE THE SET TUTORIAL!! -->
    </para>

  </section> <!-- Texts -->

  <section> <title> Tokenization </title>

    <para> As mentioned in the previous section, it is often useful to
    represent a text as a list of tokens.  The process of breaking a
    text up into its constituant tokens is known as
    <glossterm>tokenization</glossterm>.  Tokenization can occur at a
    number of different levels: a text could be broken up into
    paragraphs, sentences, words, syllables, or phonemes.  And for any
    given level of tokenization, there are many different algorithms
    for breaking up the text.  For example, at the word level, it is
    not immediately clear how to treat such strings as "can't,"
    "$22.50," "New York," and "so-called." </para>

    <para> The NL toolkit defines a general interface for tokenizing
    texts, the <literal>TokenizerI</literal> class.  This interface is
    used by all tokenizers, regardless of what level they tokenize at
    or what algorithm they use.  It defines a single method,
    <literal>tokenize</literal>, which takes a
    <literal>string</literal>, and returns a list of
    <literal>Token</literal>s.  </para>

    <section> <title>NLTK Interfaces</title>

      <para> <literal>TokenizerI</literal> is the first "interface"
      class we've encountered; at this point, we'll take a short
      digression to explain how interfaces are implemented in the NL
      toolkit. </para>

      <para> An <glossterm>interface</glossterm> gives a partial
      specification of the behavior of a class, including
      specifications for methods that the class should implement.  For
      example, a "comparable" interface might specify that a class
      must implement a comparison method.  Interfaces do not give a
      complete specification of a class; they only specify a minimum
      set of methods and behaviors which should be implemented by the
      class.  For example, the <literal>TokenizerI</literal> interface
      specifies that a tokenizer class should implement a
      <literal>tokenize</literal> method, which takes a
      <literal>string</literal>, and returns a list of
      <literal>Token</literal>s; but it does not specify what other
      methods the class should implement (if any).  </para>

      <para> The notion of "interfaces" can be very useful in ensuring
      that different classes work together correctly.  Although the
      concept of "interfaces" is supported in many languages, such as
      Java, there is no native support for interfaces in
      Python. </para>

      <para> The NL toolkit therefore implements interfaces using
      classes, all of whose methods raise the
      <literal>NotImplementedError</literal> exception.  To
      distinguish interfaces from other classes, they are always named
      with a trailing "<literal>I</literal>".  If a class implements
      an interface, then it should be a subclass of the interface.
      For example, the <literal>WSTokenizer</literal> class implements
      the <literal>TokenizerI</literal> interface, and so it is a
      subclass of <literal>TokenizerI</literal>.  </para>

    </section> <!-- NLTK Interfaces -->

    <section> <title> The whitespace tokenizer </title> 

      <para> A simple example of a tokenizer is the
      <literal>WSTokenizer</literal>, which breaks a text into words,
      assuming that words are separated by whitespace (space, enter,
      and tab characters).  We can use the
      <literal>WSTokenizer</literal> constructor to build a new
      whitespace tokenizer:

<screen>
    &prompt;<command> tokenizer = WSTokenizer() </command>
</screen>

      Once we have built the tokenizer, we can use it to process texts:

<screen>
    &prompt;<command> tokenizer.tokenize(text_str) </command>
    ['Hello'@[0w], 'world.'@[1w], 'This'@[2w], 'is'@[3w], 
     'a'@[4w], 'test'@[5w], 'file.'@[6w]]
</screen>
</para>

      <para> However, this tokenizer is not ideal for many tasks.  For
      example, we might want punctuation to be included as separate
      tokens; or we might want names like "New York" to be included as
      single tokens. </para>

    </section> <!-- Whitespace tokenizer -->
    <section> <title> The regular expression tokenizer </title>

      <para> The <literal>RETokenizer</literal> is a more powerful
      tokenizer, which uses a regular expression to determine how text
      should be split up.  This regular expression specifies the
      format of a valid word.  For example, if we wanted to mimic the
      behavior or <literal>WSTokenizer</literal>, we could define the
      following <literal>RETokenizer</literal>:

<screen>
    &prompt;<command> tokenizer = RETokenizer(r'[^\s]+') </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['Hello.'@[0w], "Isn't"@[1w], 'this'@[2w], 'fun?'@[3w]]
</screen>

      (The regular expression <literal>\s</literal> matches any
      whitespace character.) </para>

      <para> To define a tokenizer that includes punctuation as
      separate tokens, we could use:

<screen>
    &prompt;<command> regexp = '\w+|[^\w\s]+'</command>
    '\w+|[^\w\s]+'
    &prompt;<command> tokenizer = RETokenizer(regexp) </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['Hello'@[0w], '.'@[1w], 'Isn'@[2w], "'"@[3w], 't'@[4w], 
     'this'@[5w], 'fun'@[6w], '?'@[7w]]
</screen>

      The regular expression in this example will match
      <emphasis>either</emphasis> a sequence of alphanumeric
      characters (letters and numbers); <emphasis>or</emphasis> a
      sequence of punctuation characters. </para>

      <para> There are a number of ways we might want to improve this
      regular expression.  For example, it currently breaks the string
      "$22.50" into four tokens; but we might want it to include this
      as a single token.  One approach to making this change would be
      to add a new clause to the tokenizer's regular expression, which
      is specialized for handling strings of this form:

<screen>
    &prompt;<command> regexp = '\w+|[\$\d+\.\d+]|[^\w\s]+'</command>
    '\w+|[\$\d+\.\d+]|[^\w\s]+'
    &prompt;<command> tokenizer = RETokenizer(regexp) </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
</screen>

      Of course, more general solutions to this problem are also
      possible, using different regular expressions. </para> 
    </section> <!-- Regexp tokenizer -->

  </section> <!-- Tokenization -->

  <section> <title> Processing Tokenized Text </title>

    <para> In this section, we show how you can use the toolkit to
    examine the distribution of word lengths in a document.  This is
    meant to be a simple example of how the tools we have introduced
    can be used to solve a simple NLP problem.  The distribution of
    word lengths in a document can give clues to other properties,
    such as the document's style, or the document's language. </para>

    <para> We present three different approaches to solving this
    problem; each one illustrates different techniques, which might be
    useful for other problems. </para>

    <section> <title> Word Length Distributions 1: Using a List </title>

      <para> To begin with, we'll need to extract the words from a
      corpus that we wish to test.  We'll use the
      <literal>WSTokenizer</literal> to tokenize the corpus:

<screen>
    &prompt;<command> corpus = open('corpus.txt').read() </command>
    &prompt;<command> tokens = WSTokenizer().tokenize(corpus) </command>
</screen>
</para>

      <para> Now, we will construct a list
      <literal>wordlen_count_list</literal>, which gives the number of
      words that have a given length.  In particular,
      <literal>wordlen_count_list[<replaceable>i</replaceable>]</literal>
      is the number of words whose length is
      <replaceable>i</replaceable>. </para>

      <para> When constructing this list, we must be careful not to
      try to add a value past the end of the list.  Therefore,
      whenever we encounter a word that is longer than any previous
      words, we will add enough zeros to
      <literal>wordlen_count_list</literal> that we can store the
      occurance of the new word:

<screen>
    &prompt;<command> wordlen_count_list = []</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     wordlen = len(token.type())</command>
    &prompt2;     <emphasis># Add zeros until wordlen_count_list is long enough</emphasis>
    &prompt2;<command>     while wordlen >= len(wordlen_count_list):</command>
    &prompt2;<command>         wordlen_count_list.append(0)</command>
    &prompt2;     <emphasis># Incrememnt the count for this word length</emphasis>
    &prompt2;<command>     wordlen_count_list[wordlen] += 1</command>
</screen>
</para>

      <para> In order to plot the results, we must create a list of
      points.  These points are simply (wordlen, count) pairs.  We can
      construct this list with:

<screen>
    &prompt;<command> points = [(i, wordlen_count_list[i]) </command>
    &prompt2;<command>           for i in range(len(wordlen_count_list))]</command>
</screen>

      (For more information on list comprehensions, see the "Advanced
      Python Features" tutorial.)</para>

      <para> Finally, we can plot the results, using the
      <literal>nltk.draw.plot_graph</literal> module:

<screen>
    &prompt;<command> plot(Marker(points))</command>
</screen>
</para>

      <para> The complete code for this example, including necessary
      <literal>import</literal> statements, is:

<screen>
from nltk.token import WSTokenizer
from nltk.draw.plot_graph import plot, Marker

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Count up how many times each word length occurs</emphasis>
wordlen_count_list = []
for token in tokens:
     wordlen = len(token.type())
     <emphasis># Add zeros until wordlen_count_list is long enough</emphasis>
     while wordlen >= len(wordlen_count_list):
         wordlen_count_list.append(0)
     <emphasis># Incrememnt the count for this word length</emphasis>
     wordlen_count_list[wordlen] += 1

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = [(i, wordlen_count_list[i]) 
           for i in range(len(wordlen_count_list))]
plot(Marker(points))
</screen>
</para>
      
    </section> <!-- Word length distributions: list -->

    <section> <title> Word Length Distributions 2: Using a Dictionary </title>

      <para> We have been examining the function from word lengths to
      token counts.  In this example, the range of the function (i.e.,
      the set of word lengths) is ordered and relatively small.
      However, we often wish to examine functions whose ranges are not
      so well behaved.  In such cases, dictionaries can be a powerful
      tool.  The following code uses a dictionary to count up the
      number of times each word length occurs: 

<screen>
    &prompt;<command> wordlen_count_dict = {}</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     word_length = len(token.type())</command>
    &prompt2;<command>     if wordlen_count_dict.has_key(word_length):</command>
    &prompt2;<command>         wordlen_count_dict[word_length] += 1</command>
    &prompt2;<command>     else:</command>
    &prompt2;<command>         wordlen_count_dict[word_length] = 1</command>
</screen>
</para>

      <para> To plot the results, we need a list of (wordlen, count)
      pairs.  This is simply the <literal>items</literal> of the
      dictionary:

<screen>
    &prompt;<command> points = wordlen_count_dict.items() </command>
    &prompt;<command> plot(Marker(points))</command>
</screen>
</para>

      <para> The complete code for this example, including necessary
      <literal>import</literal> statements, is:

<screen>
from nltk.token import WSTokenizer
from nltk.draw.plot_graph import plot, Marker

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Construct a dictionary mapping word lengths to token counts</emphasis>
wordlen_count_dict = {}
for token in tokens:
    word_length = len(token.type())
    if wordlen_count_dict.has_key(word_length):
        wordlen_count_dict[word_length] += 1
    else:
        wordlen_count_dict[word_length] = 1

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = wordlen_count_dict.items() 
plot(Marker(points))
</screen>
</para>

    </section> <!-- Word length distributions: dictionary -->

    <section> <title> Word Length Distributions 3: Using a Frequency Distribution </title>

      <para> The <literal>nltk.probability</literal> module defines
      two interfaces, <literal>FreqDistI</literal> and
      <literal>ProbDistI</literal>, for modelling frequency
      distributions and probability distributions, respectively.  In
      this example, we use a frequency distribution to find the
      relationship between word lengths and token counts. </para>

      <para> We will use a <literal>SimpleFreqDist</literal>, which is
      a simple (but sometimes inefficient) implementation of the
      <literal>FreqDistI</literal> interface.  For this example,
      three methods of <literal>SimpleFreqDist</literal> are
      relevant:</para>

        <itemizedlist>
          <listitem><para><literal>inc(<replaceable>sample</replaceable>)</literal>
          increments the frequency of a given sample.</para>
          </listitem>
          <listitem><para><literal>samples()</literal> returns a list of
          the samples covered by a frequency distribution.</para>
          </listitem>
          <listitem><para><literal>count(<replaceable>sample</replaceable>)</literal>
          returns the number of times a given sample occured.</para>
          </listitem>
        </itemizedlist>

      <para> First, we construct the frequency distribution for the
      word lengths:

<screen>
    &prompt;<command> wordlen_freqs = SimpleFreqDist()</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     wordlen_freqs.inc(len(token.type()))</command>
</screen>
</para>

      <para> Next, we extract the set of word lengths that were found
      in the corpus: 

<screen>
    &prompt;<command> wordlens = wordlen_freqs.samples()</command>
</screen>
</para>

      <para> Finally, we construct a list of (wordlen, count) pairs,
      and plot it: 

<screen>
    &prompt;<command> points = [(wordlen, wordlen_freqs.count(wordlen))</command>
    &prompt2;<command>           for wordlen in wordlens]</command>
    &prompt;<command> plot(Marker(points))</command>
</screen>
</para>

      <para> The complete code for this example, including necessary
      <literal>import</literal> statements, is:

<screen>
from nltk.token import WSTokenizer
from nltk.draw.plot_graph import plot, Marker
from nltk.probability import SimpleFreqDist

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Construct a frequency distribution of word lengths</emphasis>
wordlen_freqs = SimpleFreqDist()
for token in tokens:
    wordlen_freqs.inc(len(token.type()))

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = [(wordlen, wordlen_freqs.count(wordlen))
          for wordlen in wordlens]
plot(Marker(points))
</screen>
</para>
      
      <para> For more information about frequency distributions, see
      the Probability Tutorial.</para>

    </section> <!-- Word length distributions: freqdist -->
  </section> <!-- Word length distributions example -->
</article>


