<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<!-- In this tutorial, I'm experimenting with using a flatter 
section hierarchy.  I think this may make the web version easier
to read (each top-level section is a single web-page). -->
<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <title>NLTK Tutorial: Chunking</title>
    &copyright;
  </articleinfo>

  <section id="intro">
    <title> Introduction </title>

    <para> Two of the most common operations in language processing
    are <glossterm>segmentation</glossterm> and <glossterm>labelling</glossterm>.
    For example, tokenization <emphasis>segments</emphasis>
    a sequence of characters into tokens, while tagging
    <emphasis>labels</emphasis> each of these tokens.
    Moreover, these two operations go hand in hand.  We segment a
    stream of characters into linguistically meaningful pieces (e.g.
    as words) only so that we can classify those pieces (e.g. with their
    part-of-speech categories) and then identify higher-level structures.
    The result of such classification is
    usually stored by adding a label to the piece in question.
    Now that we have mapped characters to tagged-tokens, we will
    carry on with segmentation and labelling at a higher level,
    as illustrated in <xref linkend="segmentation"></xref>.
    This process is called <glossterm>chunking</glossterm>, and is
    also known as <glossterm>chunk parsing</glossterm>,
    <glossterm>partial parsing</glossterm>, or <glossterm>light parsing</glossterm>.
    </para>

<figure id="segmentation"><title>Segmentation and Labelling at both the
Token and Chunk Levels</title>
<informaltable>
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/segmentation" scale="30"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

    <para>
    Chunking is like tokenization and tagging in other respects.
    First, chunking can skip over material in the input.  Observe
    in <xref linkend="segmentation"></xref> that only some of the
    tagged tokens have been chunked, while others are left out.
    Compare this with the way that tokenization has omitted spaces
    and punctuation characters.  Second, chunking typically uses
    finite state methods to identify material of interest.  For
    example, the chunk in <xref linkend="segmentation"></xref>
    could have been found by the expression
    <literal>&lt;DT>?&lt;JJ>*&lt;NN></literal> which matches
    an optional determiner, followed by zero or more adjectives,
    followed by a noun.  Compare this with the way that tokenization
    and tagging both make use of regular expressions.  Third,
    chunking, like tokenization and tagging, is very application-specific.
    For example, while linguistic analysis and information extraction
    both need to identify and label salient extents of text, they typically
    require quite different definitions of those extents.
    </para>

    <para>There are two chief motivations for chunking: to locate
    information, or to ignore information.  In the former case,
    we may want to extract all noun phrases so that they can be
    indexed.  A text retrieval system could the use the index to
    support efficient retrieval for queries involving terminological expressions.
    In the latter case we may want to study syntactic patterns,
    finding particular verbs in a corpus and displaying their
    arguments.  For instance, here are uses of the verb <literal>gave</literal>
    in the first 100 files of the Penn Treebank corpus.  NP-chunking
    has been used so that the internal details of each noun phrase can
    be replaced with <literal>NP</literal>.  In this way we can discover
    significant grammatical properties even before a grammar has been
    developed.
    </para>

<programlisting><![CDATA[
gave NP
gave up NP in NP
gave NP up
gave NP help
gave NP to NP
]]></programlisting>

    <para>
    Chunking is akin to parsing in the sense that it can be used to
    build hierarchical structure over text.  There are several
    important differences, however.  First, as noted above, chunking
    is not exhaustive, and typically omits items in the surface string.
    Second, where parsing constructs nested deeply structures,
    chunking creates structures of fixed depth (typically depth 2),
    and as fine-grained as possible, as shown below:</para>

<orderedlist>
  <listitem><para>
[<subscript>NP</subscript>
  [<subscript>NP</subscript>
    G.K. Chesterton ],
  [<subscript>NP</subscript>
    [<subscript>NP</subscript>
      author ] of
    [<subscript>NP</subscript>
      [<subscript>NP</subscript>
        The Man ] who was
      [<subscript>NP</subscript>
        Thursday ]
    ]
  ]
]
</para></listitem><listitem><para>
  [<subscript>NP</subscript> G.K. Chesterton ],
  [<subscript>NP</subscript> author ] of
  [<subscript>NP</subscript> The Man ] who was
  [<subscript>NP</subscript> Thursday ]
</para></listitem>
</orderedlist>

    <para>In this chapter we explore the representation of chunks, and
    show how chunks can be recognized using a <emphasis>chunk parser</emphasis>.
    We describe a chunk parsing method based on regular expressions,
    then show how chunk parsers can be cascaded to create more deeply nested
    structures.
    </para>

  </section> <!-- Introduction -->

  <section id="representing_chunks">
    <title> Representing Chunks: Tags vs Trees </title>

    <para> As befitting its intermediate status between tagging and
    parsing, chunk structures can be represented using tags or trees.
    The most widespread file representation uses so-called ``BIO'' tags.
    In this scheme, each token is tagged with one of three special
    chunk tags, <literal>BEGIN</literal>, <literal>INSIDE</literal>
    or <literal>OUTSIDE</literal>.  A token is tagged as <literal>BEGIN</literal> if it
    is at the beginning of a chunk, and contained within that chunk.
    Subsequent tokens within the chunk are tagged <literal>INSIDE</literal>.
    All other tokens are tagged <literal>OUTSIDE</literal>.  An
    example of this scheme is shown in <xref linkend="tagrep"></xref>.
    </para>

<figure id="tagrep"><title>Tag Representation of Chunk Structures</title>
<informaltable>
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/tagrep" scale="30"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

    <para>The other logical representation for chunk structures is to
    use trees, as shown in <xref linkend="treerep"></xref>.  These
    have the benefit that each chunk is a constituent that can be
    manipulated directly.  NLTK uses this for its internal
    representation of chunks.
    </para>

<figure id="treerep"><title>Tree Representation of Chunk Structures</title>
<informaltable>
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/treerep" scale="30"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

    <para> A <glossterm>chunk parser</glossterm> finds contiguous,
    non-overlapping spans of related tokens and groups them together
    into <glossterm>chunks</glossterm>.  The
    following chunk represents a simple noun phrase:
    <literal>(NP: &lt;the> &lt;big> &lt;dog>)</literal>
    </para>

    <para> The chunk parser combines these individual chunks together,
    along with the intervening tokens, to form a chunk structure.  A
    <glossterm>chunk structure</glossterm> is a two-level tree that
    spans the entire text, and contains both chunks and un-chunked
    tokens.  For example, the following chunk structure captures the
    sample noun phrases in a sentence: </para>

<screen>
(S: (NP: &lt;I>)
    &lt;saw>
    (NP: &lt;the> &lt;big> &lt;dog>)
    &lt;on>
    (NP: &lt;the> &lt;hill>))
</screen>

  </section> <!-- Chunk Structures -->

  <section id="ChunkedTaggedTokenizer">
    <title> Reading Chunked Text </title>

    <para> Chunk parsers often operate on tagged texts, and use the
    tags to help make chunking decisions.  A common file
    representation for chunked tagged text is illustrated below.
    This can be parsed using the
    <literal>ChunkedTaggedTokenReader</literal> as shown.
    </para>

<screen>
[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]
</screen>

<programlisting><![CDATA[
>>> from nltk.tokenreader.tagged import ChunkedTaggedTokenReader
>>> chunked_string = "[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]"
>>> reader = ChunkedTaggedTokenReader(chunk_node='NP', SUBTOKENS='WORDS')
>>> chunk_struct = reader.read_token(chunked_string)
>>> print chunk_struct['TREE']
(S:
  (NP: <the/DT> <little/JJ> <cat/NN>)
  <sat/VBD>
  <on/IN>
  (NP: <the/DT> <mat/NN>))
]]></programlisting>
    
      <para> We can obtain a larger quantity of chunked text from
      the tagged Wall Street Journal in the Penn Treebank corpus.
      NLTK includes a sample of this corpus, and it can be accessed
      as follows: </para>

<programlisting><![CDATA[
>>> from nltk.corpus import treebank
>>> print treebank.groups()
('raw', 'tagged', 'parsed', 'merged')
>>> treebank.items('tagged')
('tagged/wsj_0001.pos', 'tagged/wsj_0002.pos', ...)
]]></programlisting>

      <para> Each file in the <literal>tagged</literal> section of the
      corpus consists of chunked, tagged text.  We can read in
      a file as follows:</para>

<programlisting><![CDATA[
>>> item = treebank.items('tagged')[10]
>>> tree = treebank.read(item)
<PARAS=[<SENTS=[<TREE=(S: (NP_CHUNK: <South/NNP> <Korea/NNP>) <registered/VBD> ...
]]></programlisting>
      
      <para>The result is a set of chunk structures, grouped into
      paragraphs.  We can access the third sentence of the first
      paragraph as shown below:</para>

<programlisting><![CDATA[
>>> tree = treebank.read(item)['PARAS'][0]['SENTS'][2]['TREE']
>>> print tree
(S:
  (NP_CHUNK: <Exports/NNS>)
  <in/IN>
  (NP_CHUNK: <October/NNP>)
  <stood/VBD>
  <at/IN>
  (NP_CHUNK: <$/$> <5.29/CD> <billion/CD>)
  <,/,>
  ...
]]></programlisting>

    <para>We can display this tree graphically using the
      <literal>nltk.draw.tree</literal> module:</para>

<programlisting><![CDATA[
>>> from nltk.draw import tree
>>> tree.draw()
]]></programlisting>


    <section id="ConllChunkedTokenizer">
      <title> The CoNLL Chunking Tokenizer - FIX FOR NLTK 1.4</title>

    <para> Using the <literal>nltk.corpus</literal> module we can load the
    CoNLL chunking data and tokenize it into sentences.  Each sentence is
    a multiline string such as the following:
    </para>

<programlisting><![CDATA[
he PRP B-NP
accepted VBD B-VP
the DT B-NP
position NN I-NP
of IN B-PP
vice NN B-NP
chairman NN I-NP
of IN B-PP
Carlyle NNP B-NP
Group NNP I-NP
, , O
a DT B-NP
merchant NN I-NP
banking NN I-NP
concern NN I-NP
. . O
]]></programlisting>

    <para>Each line consists of a word, its part-of-speech, the chunk category
    <literal>B</literal> (begin), <literal>I</literal> (inside) or
    <literal>O</literal> (outside), and the chunk type
    <literal>NP</literal>, <literal>VP</literal> or <literal>PP</literal>.
    The <literal>ConllChunkedTokenizer</literal> parses this information
    into a chunk structure.  Its initializer permits the selection of one or
    more chunk types to be included.  The following example produces only
    <literal>NP</literal> chunks.  The default (no argument) includes all
    three kinds of chunk.
    </para>

<programlisting><![CDATA[
# Read the chunking corpus, and tokenize it into sentences.
>>> sentences = chunking.tokenize('train.txt')

# Create a CoNLL chunking tokenizer.
>>> chunk_tokenizer = ConllChunkedTokenizer(['NP'])

# Tokenize each sentence
>>> for sent in sentences:
...     tokenizer.tokenize(sent)

# Convert each sentence into a chunk structure. 
>>> chunk_structs = [TreeToken(NODE='S', CHILDREN=sent['SUBTOKENS'])
...                  for sent in sentences]
]]></programlisting>


    </section> 
<!-- The CoNLL Chunking Tokenizer -->

  </section> <!-- The Chunking Tokenizer -->

  <section id="RegexpChunkParser">
    <title> Chunking with Regular Expressions </title>

    <para> For the remainder of this tutorial, we will examine 
    <ulink url="&refdoc;/nltk.parser.chunk.RegexpChunkParser-class.html"
    ><literal>RegexpChunkParser</literal></ulink>.  This chunk parser uses
    a sequence of regular expression based
    <glossterm>rules</glossterm> to chunk a tagged text. </para>

    <note> <para> <literal>RegexpChunkParser</literal>
    <emphasis>only</emphasis> works on tagged texts; if you need to
    chunk an untagged text, then you should either use a tagger to tag
    it first; or use a different chunk parser. </para>
    </note>

    <para> <literal>RegexpChunkParser</literal> works by manipulating a
    <glossterm>chunking hypothesis</glossterm>, which records a
    particular chunking of the text:</para>

    <orderedlist>
      <listitem> <para> The <literal>RegexpChunkParser</literal> begins
      with a chunking hypothesis where no tokens are chunked. </para>
      </listitem>
      <listitem> <para> Each rule is then applied, in turn, to the
      chunking hypothesis.  Applying a rule to a chunking hypothesis
      updates the chunking encoded by the hypothesis. </para>
      </listitem>
      <listitem> <para> After the all of the rules have been applied,
      the chunk structure corresponding to the final chunking
      hypothesis is returned. </para>
      </listitem>
    </orderedlist>

    <para> New <literal>RegexpChunkParser</literal>s are constructed from
    a list of rules, using the <ulink
    url="&refdoc;/nltk.parser.chunk.RegexpChunkParser-class.html#__init__"
    ><literal>RegexpChunkParser</literal> constructor</ulink>.  Optional
    second and third arguments to the constructor specify the node
    labels for chunks and for the top-level node, respectively:</para>

<programlisting><![CDATA[
# Construct a new noun-phrase chunk parser
>>> rules = [rule1, rule2, rule3] 
>>> chunkparser = RegexpChunkParser(rules, chunk_node='NP', top_node='S')
]]></programlisting>

    <note> <para> The following sections discuss how to construct
    rules. </para>
    </note>

    <para> Tagged texts are chunked using the 
    <ulink url="&refdoc;/nltk.parser.chunk.RegexpChunkParser-class.html#parse"
    ><literal>parse</literal></ulink> method:</para>

<programlisting><![CDATA[
>>> sent = Token(TEXT="the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN")
>>> TaggedTokenizer().tokenize(sent)
>>> chunkparser.parse(sent)
>>> print sent
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
    <sat/VBD> <on/IN>
    (NP: <the/DT> <mat/NN>))
]]></programlisting>

  <section id="RegexpChunkParser.TagString">
    <title> Tag Strings and Tag Patterns </title>

    <para> <literal>RegexpChunkParser</literal>
    rules are defined in terms of regular expression patterns
    over "tag strings."  A <glossterm>tag string</glossterm> is a
    string consisting of angle-bracket delimited tag names.  An example
    of a <glossterm>tag string</glossterm> is: </para>

<screen>
&lt;DT>&lt;JJ>&lt;NN>&lt;VBD>&lt;DT>&lt;NN>
</screen>
      
    <note>
      <para> Tag strings do not contain any whitespace. </para>
    </note>

    <para> Most rules are defined using a special kind of regular
    expression pattern, called a <glossterm>tag pattern</glossterm>.
    Tag patterns are identical to <literal>re</literal> regular
    expression patterns in most respects; however, there are a few
    differences, which are intended to simplify their use with
    <literal>RegexpChunkParser</literal>s:</para>

    <itemizedlist>
      <listitem>
        <para> In tag patterns, "&lt;" and "&gt;" act like grouping
        parentheses; so the tag pattern
        "<literal>&lt;NN&gt;+</literal>" matches one or more repetitions of
        "<literal>&lt;NN&gt;</literal>"; and "<literal>&lt;NN|JJ&gt;</literal>"
        matches "<literal>&lt;NN&gt;</literal>" or
        "<literal>&lt;JJ&gt;</literal>."</para>
      </listitem>
      <listitem>
        <para> Whitespace in tag patterns is ignored; so
        "<literal>&lt;DT&gt; | &lt;NN&gt;</literal>" is equivalent to
        "<literal>&lt;DT&gt;|&lt;NN&gt;</literal>".  This allows you to make your
        tag patterns easier to read by inserting whitespace in
        appropriate places. </para>
      </listitem>
      <listitem>
        <para>In tag patterns "<literal>.</literal>" is equivalent to
        "<literal>[^{}&lt;&gt;]</literal>"; so "<literal>&lt;NN.*&gt;</literal>"
        matches any single tag starting with
        "<literal>NN</literal>."  (The use of <literal>{</literal> and
        <literal>}</literal> will be explained later.)</para>
      </listitem>
    </itemizedlist>
        
  </section> <!-- Tag Strings & Tag Patterns -->

  <section id="chunkrule"> <title> Chunk rules </title>

    <para> The simplest type of rule is 
    <ulink url="&refdoc;/nltk.parser.chunk.ChunkRule-class.html"
    ><literal>ChunkRule</literal></ulink>.  A
    <literal>ChunkRule</literal> chunks anything that matches a given
    tag pattern.  <literal>ChunkRule</literal>s are created with the
    <ulink url="&refdoc;/nltk.parser.chunk.ChunkRule-class.html#__init__"
    ><literal>ChunkRule</literal> constructor</ulink>, which takes a
    tag pattern and a description string.  For example, the following
    code creates and uses an <literal>RegexpChunkParser</literal> that
    chunks any sequence of tokens whose tags are all
    "<literal>NN</literal>" or "<literal>DT</literal>": </para>

<programlisting><![CDATA[
>>> rule1 = ChunkRule('<NN|DT>+',
...                   'Chunk sequences of NN and DT')
>>> chunkparser = RegexpChunkParser([rule1], chunk_node='NP', top_node='S')
>>> chunkparser.parse(sent)
>>> print sent['TREE']
(S: (NP: <the/DT>)
    <little/JJ>
    (NP: <cat/NN>)
    <sat/VBD> <on/IN>
    (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <note><para> Each rule must have a "description" associated with it,
      which provides a short explanation of the purpose or the effect
      of the rule.  This description is accessed via the 
      <ulink url="&refdoc;/nltk.parser.chunk.RegexpChunkParserRule-class.html#descr"
      ><literal>descr</literal></ulink> method: </para></note>

<programlisting><![CDATA[
>>> print rule1.descr()
'Chunk sequences of NN and DT'
]]></programlisting>

    <para> We can also use more complex tag patterns.  The following
    code chunks any sequence of tokens beginning with an optional
    determiner ("<literal>DT</literal>"), followed by zero or more
    adverbs of any type ("<literal>JJ.*</literal>"), followed by a
    single noun of any type ("<literal>NN.*</literal>"). </para>

<programlisting><![CDATA[
>>> rule1 = ChunkRule('<DT>?<JJ.*>*<NN.*>',
...                   'Chunk optional det, zero or more adj, and a noun')
>>> chunkparser = RegexpChunkParser([rule1], chunk_node='NP', top_node='S')
>>> chunkparser.parse(sent)
>>> print sent['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
  <sat/VBD> <on/IN>
  (NP: <the/DT> <mat/NN>))
]]></programlisting>

    <para> If a tag pattern matches at multiple overlapping locations,
    the first match takes precedence.  For example, if we apply a rule
    that matches two consecutive nouns to a text containing three
    consecutive nouns, then the first two nouns will be chunked: </para>

<programlisting><![CDATA[
>>> from nltk.tagger import *
>>> nouns = Token(TEXT="dog/NN cat/NN mouse/NN")
>>> TaggedTokenizer().tokenize(nouns)
>>> rule1 = ChunkRule('<NN><NN>',
...                   'Chunk two consecutive nouns')
>>> chunkparser = RegexpChunkParser([rule1], chunk_node='NP', top_node='S')
>>> chunkparser.parse(nouns)
>>> print nouns
(S: (NP: <dog> <cat>) <mouse>)
]]></programlisting>

    </section> <!-- ChunkRule -->

    <section id="chunkrule.tracing"> <title> Tracing </title>

      <para> The <literal>RegexpChunkParser</literal> constructor takes an
      optional "<literal>trace</literal>" argument, which specifies whether
      debugging output should be shown during parsing.  This output
      shows the rules that are applied, and shows the chunking
      hypothesis at each stage of processing.  </para>

<programlisting><![CDATA[
>>> rule1 = ChunkRule('<DT>?<JJ.*>*<NN.*>',
...                   'Chunk optional det, zero or more adj, and a noun')
>>> chunkparser = RegexpChunkParser([rule1], chunk_node='NP', top_node='S', trace=1)
>>> chunkparser.parse(sent)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk optional det, zero or more adj, and a noun:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}

(S: (NP: <the/DT> <little/JJ> <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <note>
        <para>Note that the <literal>parse</literal> method can also be given
        a trace value; this overrides the value given
        to the constructor.</para>

        <para>In the tracing output, chunking is indicated by braces
        ("{" and "}"), instead of the more conventional square
        brackets ("[" and "]").  The reasons for this will be
        discussed when we cover general transformational rules. </para>
        <!-- Add a specific ref? -->
      </note>

      <note><para> <literal>RegexpChunkParser</literal>s can be generated from
      multiple <literal>ChunkRule</literal>s.  In this case, each rule
      will be applied in turn.  For example, the following code chunks
      the example sentence by first finding all sequences of three
      tokens whose tags are "<literal>DT</literal>",
      "<literal>JJ</literal>", and "<literal>NN</literal>"; and then
      looking for any sequence of tokens whose tags are either
      "<literal>DT</literal>" or "<literal>NN</literal>".  </para></note>

<programlisting><![CDATA[
>>> rule1 = ChunkRule('<DT><JJ><NN>',
...                   'Chunk det+adj+noun')
>>> rule2 = ChunkRule('<DT|NN>+',
...                   'Chunk sequences of NN and DT')
>>> chunkparser = RegexpChunkParser([rule1, rule2], chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk det+adj+noun:
               {<DT>  <JJ>  <NN>} <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN and DT:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}

>>> print sent['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <para> When a <literal>ChunkRule</literal> is applied to a
      chunking hypothesis, it will only create chunks that do not
      partially overlap with chunks already in the hypothesis.  Thus,
      if we apply these two rules in reverse order, we will get a
      different result: </para>

<programlisting><![CDATA[
>>> chunkparser = RegexpChunkParser([rule2, rule1], chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN and DT:
               {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
Chunk det+adj+noun:
               {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
>>> print sent['TREE']
(S: (NP: <the/DT>)
      <little/JJ>
      (NP: <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <para> Here, rule 2 ("chunk det+adj+noun") did not find any
      chunks, since all chunks that matched its tag pattern overlapped
      with chunks that were already in the hypothesis. </para>

    </section> <!-- Tracing -->
  </section> <!-- RegexpChunkParser intro -->

  <section id="more_rules">
    <title> More Chunking Rules </title>

    <para> Sometimes it is easier to define what we
    <emphasis>don't</emphasis> want to include in a chunk than it is
    to define what we <emphasis>do</emphasis> want to include.  In
    these cases, it may be easier to build a chunk parser using
    <literal>UnChunkRule</literal> and
    <literal>ChinkRule</literal>. </para>

    <section id="chinkrule">
      <title> The Chink Rule </title>
      
      <para> A <glossterm>chink</glossterm> is a sequence of tokens
      that is not included in a chunk. In the following example,
      "sat/VBD on/IN" is a chink. </para>

<screen>
[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]
</screen>

      <para> <glossterm>Chinking</glossterm> is the process of
      removing a sequence of tokens from a chunk.  If the sequence of
      tokens spans an entire chunk, then the whole chunk is removed;
      if the sequence of tokens is in the middle of the chunk, two new
      chunks are formed; and if the sequence is at the beginning or
      end of the chunk, one new chunk is formed.  These three
      possibilities are illustrated in the following figure: </para>

      <informaltable>
        <tgroup cols="4">
          <thead>
            <row>
              <entry></entry>
              <entry> Chink an entire chunk </entry>
              <entry> Chink the middle of a chunk </entry>
              <entry> Chink the end of a chunk </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><emphasis>Input</emphasis></entry>
              <entry>[a/DT  big/JJ  cat/NN]</entry>
              <entry>[a/DT  big/JJ  cat/NN]</entry>
              <entry>[a/DT  big/JJ  cat/NN]</entry>
            </row>
            <row>
              <entry><emphasis>Operation</emphasis></entry>
              <entry>Chink "a/DT big/JJ cat/NN"</entry>
              <entry>Chink "big/JJ" </entry>
              <entry>Chink "cat/DT" </entry>
            </row>
            <row>
              <entry><emphasis>Output</emphasis></entry>
              <entry> a/DT  big/JJ  cat/NN </entry>
              <entry>[a/DT] big/JJ [cat/NN]</entry>
              <entry>[a/DT  big/JJ] cat/NN </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para> A <ulink url="&refdoc;/nltk.parser.chunk.ChinkRule-class.html"
      ><literal>ChinkRule</literal></ulink> chinks anything that
      matches a given tag pattern.  <literal>ChinkRule</literal>s are
      created with the <ulink
      url="&refdoc;/nltk.parser.chunk.ChinkRule-class.html#__init__"
      ><literal>ChinkRule</literal> constructor</ulink>, which takes a
      tag pattern and a description string.  For example, the
      following rule will chink any sequence of tokens whose tags are
      all "<literal>VBD</literal>" or "<literal>IN</literal>": </para>

<programlisting><![CDATA[
>>> chink_rule = ChinkRule('<VBD|IN>+',
...                        'Chink sequences of VBD and IN')
]]></programlisting>

      <para> Remember that <literal>RegexpChunkParser</literal> begins
      with a chunking hypothesis where nothing is chunked.  So before
      we apply our chink rule, we'll apply another rule that puts the
      entire sentence in a single chunk: </para>

<programlisting><![CDATA[
>>> chunkall_rule = ChunkRule('<.*>+',
...                           'Chunk everything')
]]></programlisting>

      <para> Finally, we can combine these two rules to create a chunk
      parser: </para>

<programlisting><![CDATA[
>>> chunkparser = RegexpChunkParser([chunkall_rule, chink_rule], chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk everything:
               {<DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN>}
Chink sequences of VBD and IN:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}

(S: (NP: <the/DT> <little/JJ> <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <para> If a tag pattern matches at multiple overlapping
      locations, the first match takes precedence. </para>

      <note><para>Cascading Rules:
        <literal>RegexpChunkParser</literal>s can use any number of
        <literal>ChunkRule</literal>s and
        <literal>ChinkRules</literal>, in any order.  As was discussed
        in the <xref linkend="chunkrule"></xref>,
        <literal>ChunkRule</literal>s only create chunks that do not
        partially overlap with chunks already in the chunking
        hypothesis.  Similarly, <literal>ChinkRule</literal>s only
        create chinks that do not partially overlap with chinks that
        are already in the hypothesis.</para></note>

    </section> <!-- ChinkRule -->

    <section id="unchunkrule">
      <title> The Unchunk Rule </title>
    
      <para> An <ulink
      url="&refdoc;/nltk.parser.chunk.UnChunkRule-class.html"
      ><literal>UnChunkRule</literal></ulink> removes any chunk that
      matches a given tag pattern.  <literal>UnChunkRule</literal> is
      very similar to <literal>ChinkRule</literal>; but it will only
      remove a chunk if the tag pattern matches the entire chunk.  In
      contrast, <literal>ChinkRule</literal> can remove sequences of
      tokens from the middle of a chunk. </para>

      <para> <literal>UnChunkRule</literal>s are created with the
      <ulink
      url="&refdoc;/nltk.parser.chunk.UnChunkRule-class.html#__init__"
      ><literal>UnChunkRule</literal> constructor</ulink>, which takes
      a tag pattern and a description string.  For example, the
      following rule will unchunk any sequence of tokens whose tags
      are all "<literal>NN</literal>" or "<literal>DT</literal>":
      </para>

<programlisting><![CDATA[
>>> unchunk_rule = UnChunkRule('<NN|DT>+',
...                            'Unchunk sequences of NN and DT')
]]></programlisting>

      <para> We can combine this rule with a chunking rule to form a
      chunk parser: </para>

<programlisting><![CDATA[
>>> chunk_rule = ChunkRule('<NN|DT|JJ>+',
...                        'Chunk sequences of NN, JJ, and DT')
>>> chunkparser = RegexpChunkParser([chunk_rule, unchunk_rule], chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN, JJ, and DT:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
Unchunk sequences of NN and DT:
           {<DT>  <JJ>  <NN>} <VBD>  <IN>  <DT>  <NN> 
>>> print sent['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
    <sat/VBD>
    <on/IN>
    <the/DT>
    <mat/NN>)
]]></programlisting>

      <para> Note that we would get a different result if we used a
      <literal>ChinkRule</literal> with the same tag pattern (instead
      of an <literal>UnChunkRule</literal>), since
      <literal>ChinkRule</literal>s can remove pieces of a chunk:
      </para>

<programlisting><![CDATA[
>>> unchink_rule = UnChunkRule('<NN|DT>+',
...                            'Chink sequences of NN and DT')
>>> chunk_rule = ChunkRule('<NN|DT|JJ>+',
...                        'Chunk sequences of NN, JJ, and DT')
>>> chunkparser = RegexpChunkParser([chunk_rule, chink_rule], chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN, JJ, and DT:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
Chink sequences of NN and DT:
                <DT> {<JJ>} <NN>  <VBD>  <IN>  <DT>  <NN> 
>>> print sent['TREE']
(S: <the/DT>
    (NP: <little/JJ>)
    <cat/NN>
    <sat/VBD>
    <on/IN>
    <the/DT>
    <mat/NN>)
]]></programlisting>

    </section> <!-- UnChunkRule -->

    <section id="mergerule">
      <title> The Merge Rule</title>

    <para> When constructing complex chunk parsers, it is often
    convenient to perform operations other than chunking, chinking,
    and unchunking.  In this section and the next, we discuss two more complex
    rules which can be used to merge and split chunks. </para>

      <para> <ulink url="&refdoc;/nltk.parser.chunk.MergeRule-class.html"
      ><literal>MergeRule</literal></ulink>s are used to merge two
      contiguous chunks.  Each <literal>MergeRule</literal> is
      parameterized by two tag patterns: a <glossterm>left
      pattern</glossterm> and a <glossterm>right pattern</glossterm>.
      A <literal>MergeRule</literal> will merge two contiguous chunks
      <replaceable>C<subscript>1</subscript></replaceable> and
      <replaceable>C<subscript>2</subscript></replaceable> if the end
      of <replaceable>C<subscript>1</subscript></replaceable> matches
      the left pattern, and the beginning of
      <replaceable>C<subscript>2</subscript></replaceable> matches the
      right pattern. For example, consider the following chunking
      hypothesis: </para>

<screen>
[the/DT little/JJ] [cat/NN]
</screen>

      <para> Where
      <replaceable>C<subscript>1</subscript></replaceable> is
      <literal>[the/DT little/JJ]</literal> and
      <replaceable>C<subscript>2</subscript></replaceable> is
      <literal>[cat/NN]</literal>.  If the left pattern is
      "<literal>JJ</literal>", and the right pattern is
      "<literal>NN</literal>", then
      <replaceable>C<subscript>1</subscript></replaceable> and
      <replaceable>C<subscript>2</subscript></replaceable> will be
      merged to form a single chunk:</para>

<screen>
[the/DT little/JJ cat/NN]
</screen>

      <para> <literal>MergeRule</literal>s are created with the <ulink
      url="&refdoc;/nltk.parser.chunk.MergeRule-class.html#__init__"
      ><literal>MergeRule</literal> constructor</ulink>, which takes a
      left tag pattern, a right tag pattern, and a description string.
      For example, the following rule will merge two contiguous chunks
      if the first one ends in a determiner, noun or adjective; and
      the second one begins in a determiner, noun, or adjective: </para>

<programlisting><![CDATA[
>>> merge_rule = MergeRule('<NN|DT|JJ>', '<NN|DT|JJ>',
...                       'Merge NNs + DTs + JJs')
]]></programlisting>

      <para> To illustrate this rule, we will use a combine it with a
      chunking rule that chunks each individual token, and an
      unchunking rule that unchunks verbs and prepositions: </para>

<programlisting><![CDATA[
>>> chunk_rule = ChunkRule('<.*>',
...                     'Chunk all individual tokens')
>>> unchunk_rule = UnChunkRule('<IN|VB.*>',
...                         'Unchunk VBs and INs')
>>> rules = [chunk_rule, unchunk_rule, merge_rule]
>>> chunkparser = RegexpChunkParser(rules, chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk all individual tokens:
               {<DT>}{<JJ>}{<NN>}{<VBD>}{<IN>}{<DT>}{<NN>}
Unchunk VBs and INs:
               {<DT>}{<JJ>}{<NN>} <VBD>  <IN> {<DT>}{<NN>}
Merge NNs + DTs + JJs:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
>>> print sent
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
    <sat/VBD> <on/IN>
    (NP: <the/DT> <mat/NN>))
]]></programlisting>

    </section> <!-- MergeRule -->

    <section id="splitrule">
      <title> The Split Rule </title>

      <para> <ulink url="&refdoc;/nltk.parser.chunk.SplitRule-class.html"
      ><literal>SplitRule</literal></ulink>s are used to split a
      single chunk into two smaller chunks.  Each
      <literal>SplitRule</literal> is parameterized by two tag
      patterns: a <glossterm>left pattern</glossterm> and a
      <glossterm>right pattern</glossterm>.  A
      <literal>SplitRule</literal> will split a chunk at any point
      <replaceable>p</replaceable>, where the left pattern matches the
      chunk to the left of <replaceable>p</replaceable>, and the right
      pattern matches the chunk to the right of
      <replaceable>p</replaceable>.  For example, consider the
      following chunking hypothesis: </para>

<screen>
[the/DT little/JJ cat/NN the/DT dog/NN]
</screen>

      <para> If the left pattern is "<literal>NN</literal>", and the
      right pattern is "<literal>DT</literal>", then the chunk will be
      split in two between "<literal>cat</literal>" and
      "<literal>dog</literal>", to form two smaller chunks:</para>

<screen>
[the/DT little/JJ cat/NN] [the/DT dog/NN]
</screen>

      <para> <literal>SplitRule</literal>s are created with the <ulink
      url="&refdoc;/nltk.parser.chunk.SplitRule-class.html#__init__"
      ><literal>SplitRule</literal> constructor</ulink>, which takes a
      left tag pattern, a right tag pattern, and a description string.
      For example, the following rule will split any chunk at a
      location that has "<literal>NN</literal>" to the left and
      "<literal>DT</literal>" to the right: </para>

<programlisting><![CDATA[
>>> split_rule = SplitRule('<NN>', '<DT>',
...                     'Split NN followed by DT')
]]></programlisting>

      <para> To illustrate this rule, we will use a combine it with a
      chunking rule that chunks sequences of noun phrases, adjectives,
      and determiners: </para>

<programlisting><![CDATA[
# Tokenize a new test text
>>> from nltk.tagger import *
>>> sent=Token(TEXT='Bob/NNP saw/VBD the/DT man/NN the/DT cat/NN chased/VBD')
>>> TaggedTokenizer().tokenize(sent)

# Create the chunk parser
>>> chunk_rule = ChunkRule('<NN.*|DT|JJ>+',
...                        'Chunk sequences of NN, JJ, and DT')
>>> chunkparser = RegexpChunkParser([chunk_rule, split_rule], chunk_node='NP', top_node='S')

# Parse sent with tracing turned on.
>>> chunkparser.parse(sent, trace=1)
Input:
               <NNP>  <VBD>  <DT>  <NN>  <DT>  <NN>  <VBD> 
Chunk sequences of NN, JJ, and DT:
              {<NNP>} <VBD> {<DT>  <NN>  <DT>  <NN>} <VBD> 
Split NN followed by DT:
              {<NNP>} <VBD> {<DT>  <NN>}{<DT>  <NN>} <VBD> 
>>> print sent['TREE']
(S: (NP: <Bob/NNP>) 
    <saw/VBD>
    (NP: <the/DT> <man/NN>)
    (NP: <the/DT> <cat/NN>)
    <chased/VBD>)
]]></programlisting>

    </section> <!-- SplitRule -->

  </section> <!-- Merge/Split -->

    <section> <title> Chunking Strategies </title>

      <para>There are a surprising number of different ways to chunk a
      sentence.  The chunk rules can add, shift and remove chunk delimiters
      in many ways, and the chunk rules can be combined in many ways.
      One can use a small number of very complex rules, or a long sequence of
      much simpler rules.  This section gives a brief overview of the main
      approaches.</para>

      <section> <title> Static vs Iterative Chunking </title>

        <para>In static chunking we write a single regular expression which
        accomplishes the chunking of a certain type of phrase in a single step.
        We have already seen an example of this, which is repeated below:</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<DT><JJ>*<NN>)', r'{\1}', doc="chunking <DT><JJ>*<NN>")
]]></programlisting>

        <para>This single rule identifies a particular kind of noun phrase chunk
        using a single regular expression.</para>

        <para>It is possible to do the same thing iteratively.  We start by chunking
        nouns, then move the left delimiter leftwards:</para>

<programlisting><![CDATA[
    AbstractChunkRule(r'(<NN.*>)', r'{\1}', doc="chunking NN")
    AbstractChunkRule(r'(<DT|JJ>){', r'{\1', right=r'<JJ|NN.*>', doc="shifting delimiter left"
]]></programlisting>

        <note><para>The second rule is unable to apply to its own output.
        To apply this maximally it is necessary to invoke it many times over.</para></note>

        <para>Another iterative approach which is easier to use begins by
        putting everything into its own chunk, then removing chunk boundaries:</para>

<programlisting><![CDATA[
    AbstractChunkRule(r'(<.*>)', r'{\1}', doc="chunking every tag")
    DT_JJ_NNX = r'<DT|JJ|NN.*>'
    AbstractChunkRule('('+DT_JJ_NNX+')}{', r'\1', right=DT_JJ_NNX,
                      doc="chunk any groups of <DT> <JJ> and <NNX>")
]]></programlisting>

    </section> <!-- Static vs Iterative Chunking -->

    <section> <title> Chunking vs Chinking </title>

      <para>Chunking begins with an unchunked sentence and adds chunk
      delimiters.  The dual approach, known as <emphasis>chinking</emphasis>
      begins with the entire sentence in a single chunk then removes,
      or <emphasis>unchunks</emphasis>, pieces which cannot be part of a
      chunk.  This is convenient in situations where it is relatively
      easy to specify those tags and sequences of tags which
      <emphasis>cannot</emphasis> be in a chunk.</para>

      <para>For example, we could chunk a whole sentence then
      chink the verbs using the following rules:</para>

<programlisting><![CDATA[
    AbstractChunkRule(r'(.*)', r'{\1}', doc='chunk the entire sentence')
    AbstractChunkRule(r'(<VB.*>)', r'}\1{', doc='chink the verbs')
]]></programlisting>

      <para>Note the directionality of the braces in the second argument
      of the second constructor - they face outwards.  Thus, the closing
      brace closes the chunk which started at the beginning of the sentence
      (or after the previous verb).  Similarly, the opening brace begins
      a new chunk which goes to the end of the sentence (or the next verb).</para>

    </section> <!-- Chunking vs Chinking -->
    
  </section> <!-- Chunking Strategies -->

  <section id="evaluation">
    <title> Evaluating Chunk Parsers </title>

    <para> An easy way to evaluate a chunk parser is to take some
    already chunked text, strip off the chunks, rechunk it, and compare
    the result with the original chunked text.  NLTK provides the necessary
    functions:</para>

<programlisting><![CDATA[
unchunked_sent = unchunk(correct_chunks)
chunker = RegexpChunkParser(rules, chunk_node='NP', top_node='S')
chunked_sent = cp.parse(unchunked_sent)
score(correct_chunks, chunked_sent)
]]></programlisting>

    <para>The 
    <ulink url="&refdoc;/nltk.parser.chunk-module.score.html"
    ><literal>score</literal></ulink>
    function takes the correctly chunked sentence as its first argument, and
    the newly chunked version as its second argument, and compares them.
    It reports the fraction of actual chunks that were found (recall), the
    fraction of hypothesized chunks that were correct (precision), and a
    combined score, the F-measure (the harmonic mean of precision and recall).
    </para>

    <para> A number of different metrics can be used to evaluate chunk
    parsers.  We will concentrate on a class of metrics that can be
    derived from two sets: </para>

    <itemizedlist>
      <listitem> <para> <glossterm>guessed</glossterm>: The set of
      chunks returned by the chunk parser. </para>
      </listitem>
      <listitem> <para> <glossterm>correct</glossterm>: The correct
      set of chunks, as defined in the test corpus.</para>
      </listitem>
    </itemizedlist>

    <para> From these two sets, we can define five useful metrics:
    </para>

    <itemizedlist>
      <listitem><para><glossterm>Precision</glossterm>: What
      percentage of guessed chunks were correct?</para>
      </listitem>
      <listitem><para><glossterm>Recall</glossterm>: What percentage
      of correct chunks were guessed?</para>
      </listitem>
      <listitem> <para> <glossterm>F Measure</glossterm>: the harmonic
      mean of precision and recall. </para>
      </listitem>
      <listitem> <para> <glossterm>Missed Chunks</glossterm>:
      What correct chunks were not guessed?</para>
      </listitem>
      <listitem> <para> <glossterm>Incorrect Chunks</glossterm>: What guessed
      chunks were not correct?</para>
      </listitem>
    </itemizedlist>

    <note>
      <para> Note that these metrics do not assign any credit for
      chunks that are "almost" right (e.g., chunks that extend one
      word too long).  It would be possible to design metrics that do
      assign partial credit for such cases, they would be more
      complex.  We decided to keep our metrics simple, so that it is
      easy to understand what a given result means. </para>
    </note>

    <para> During evaluation of a chunk parser, it is useful to
      flatten a chunk structure into a list of tokens.  Since chunk
      structures are represented by <literal>TreeTokens</literal>, we
      can use the <ulink
      url="&refdoc;/nltk.tree.TreeToken-class.html#leaves"
      ><literal>leaves</literal></ulink> method to extract this
      flattened list:
    </para>
      
<programlisting><![CDATA[
# Flatten a chunk structrure into a list of tokens
>>> unchunked_sentence = chunk_structure.leaves()
[<the/DT>, <little/JJ>, <cat/NN>, <sat/VBD>, <on/IN>, <the/DT>, <mat/NN>]
]]></programlisting>

<para>
NOTES: CONLL:
We now extract the word tokens, input them to the chunker, then
compare the resulting chunked sentences with the originals, as
follows:
</para>
 
<programlisting><![CDATA[
>>> from nltk.parser.chunk import *
>>> chunkscore = ChunkScore()

>>> rule = ChunkRule('<PRP|DT|POS|JJ|CD|N.*>+', "Chunk items that often occur in NPs")
>>> chunkparser = RegexpChunkParser([rule], chunk_node='NP', top_node='S')

>>> for cs in chunk_structs:                 # for each chunked sentence
...     tokens = cs.leaves()                 # extract word tokens
...     guess = chunkparser.parse(tokens)    # chunk them
...     chunkscore.score(cs, guess)          # compare with original
>>> print chunkscore
]]></programlisting>


    <section id="evaluation.chunkscore">
      <title> ChunkScore </title>

      <para> <ulink url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html"
      ><literal>ChunkScore</literal></ulink> is a utility class for
      scoring chunk parsers.  It can be used to evaluate the output of
      a chunk parser, using precision, recall, f-measure, missed
      chunks, and incorrect chunks.  It can also be used to combine
      the scores from the parsing of multiple texts.  This is quite
      useful if we are parsing a text one sentence at a time.  </para>

      <para> <literal>ChunkScore</literal>s are created with the
      <ulink url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#__init__"
      ><literal>ChunkScore</literal> constructor</ulink>, which takes
      no arguments: </para>

<programlisting><![CDATA[
>>> chunkscore = ChunkScore()
]]></programlisting>

      <para> The output of a chunk parser can be evaluated using the
      <ulink url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#score"
      ><literal>score</literal></ulink> method: </para>

<programlisting><![CDATA[
>>> unchunked = unchunk(correct)
>>> guess = chunkparser.parse(unchunked)
>>> chunkscore.score(correct, guess)
]]></programlisting>

      <para> The following program listing shows a typical use of the
      <literal>ChunkScore</literal> class.  In this example,
      <literal>chunkparser</literal> is being tested on each sentence
      from the Wall Street Journal tagged files. </para>

<programlisting><![CDATA[
# Import the relevant modules, classes, and functions.
from nltk.tokenizer import RegexpTokenizer
from nltk.parser.chunk import *
from nltk.corpus import treebank
from nltk.tree import TreeToken

# Create a sentence tokenizer. 
sent_tokenizer = RegexpTokenizer('(=======+)?\n\n+', negative=1)
        
# Create the chunk parser that we wish to test.
rule1 = ChunkRule('<DT|JJ|NN>+', "Chunk sequences of DT, JJ, and NN")
chunkparser = RegexpChunkParser([rule1], chunk_node='NP', top_node='S')

# Read the treebank corpus, and tokenize it into sentences.
sentences = []
for item in treebank.items('tagged'): 
    sentence = treebank.tokenize(item, tokenizer=sent_tokenizer) 
    sentences += sentence['SUBTOKENS']

# Convert each sentence into a chunk structure. 
chunk_structs = []
chunk_tokenizer = ChunkedTaggedTokenizer(chunk_node='NP')
for sent in sentences:
    chunk_tokenizer.tokenize(sent)
    chunk_structs.append(TreeToken(NODE='S', CHILDREN=sent['SUBTOKENS']))

# Create a new ChunkScore utility class
chunkscore = ChunkScore()

# Test the chunk parser.
for chunk_struct in chunk_structs:
    test = Token(SUBTOKENS=chunk_struct.leaves())
    chunkparser.parse(test)
    chunkscore.score(chunk_struct, test['TREE'])

# Print the results
print chunkscore
]]></programlisting>

<!--
    <para>Here's another example of chunking code,
    this time for the CoNLL chunking data.</para>

<programlisting><![CDATA[
from nltk.tokenizer import RETokenizer
from nltk.parser.chunk import *
from nltk.corpus import chunking
from nltk.tree import TreeToken

# How many sentences to chunk
samplesize = 10

# Read the treebank corpus, and tokenize it into sentences
print "tokenizing..."
sentences = chunking.tokenize('train.txt')

# Convert each sentence into a chunk structure
print "converting..."
chunk_tokenizer = ConllChunkedTokenizer(['NP'])
c_sents = [TreeToken('S', *chunk_tokenizer.tokenize(s.type()))
           for s in sentences[:samplesize]]

# Create a new ChunkScore utility class
chunkscore = ChunkScore()

# Create the chunk parser that we wish to test.
print "creating parser..."
rule1 = ChunkRule('<PRP|DT|POS|JJ|CD|N.*>+', "Chunk items that often occur in NPs")
chunkparser = RegexpChunkParser([rule1], 'NP', 'S')

# Test the chunk parser
print "testing..."
for c_sent in c_sents:
    tokens = c_sent.leaves()
    guess = chunkparser.parse(tokens)
    chunkscore.score(c_sent, guess)

# Print the results
print chunkscore
]]></programlisting>
-->

      <para> The overall results of the evaluation can be viewed by
      printing the <literal>ChunkScore</literal>.  Each evaluation
      metric is also returned by an accessor method:</para>

      <itemizedlist>
        <listitem><para><ulink
        url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#precision"
        ><literal>precision</literal></ulink></para> </listitem>

        <listitem><para><ulink
        url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#recall"
        ><literal>recall</literal></ulink></para> </listitem>

        <listitem><para><ulink
        url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#f_measure"
        ><literal>f_measure</literal></ulink></para> </listitem>

        <listitem><para><ulink
        url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#missed"
        ><literal>missed</literal></ulink></para> </listitem>

        <listitem><para><ulink
        url="&refdoc;/nltk.parser.chunk.ChunkScore-class.html#incorrect"
        ><literal>incorrect</literal></ulink></para> </listitem>
      
      </itemizedlist>

      <para> The <literal>missed</literal> and
      <literal>incorrect</literal> methods can be especially useful
      when trying to improve the performance of a chunk
      parser: </para>

<programlisting><![CDATA[
from random import randint

print 'Chunks missed by the chunk parser:'
missed = chunkscore.missed()
for i in range(15):
    print missed[randint(0,len(missed)-1)].type()

print 'Incorrect chunks returned by the chunk parser:'
incorrect = chunkscore.incorrect()
for i in range(15):
    print incorrect[randint(0,len(incorrect)-1)].type()
]]></programlisting>

<!--      <note>
        <para> By default, only the first 100 missed chunks and the
        first 100 incorrect chunks will be remembered by the
        <literal>ChunkScore</literal>.  You can tell
        <literal>ChunkScore</literal> to record more chunk examples
        with the <literal>max_fp_examples</literal> (maximum false
        positive examples) and the <literal>max_fn_examples</literal>
        (maximum false negative examples) keyword arguments to the
        <literal>ChunkScore</literal> constructor:</para>

<programlisting><![CDATA[
>>> chunkscore = ChunkScore(max_fp_examples=1000,
...                        max_fn_examples=1000)
]]></programlisting>
      </note> -->

    </section> <!-- ChunkScore -->

  </section> <!-- Evaluating Chunk Parsers-->


  <section>
    <title>Cascaded Chunking</title>

    <para/>

  </section>

  <section>
    <title>Conclusion</title>

    <itemizedlist>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.ChunkRule-class.html"
        ><literal>ChunkRule</literal></ulink> chunks anything that
        matches a given tag pattern.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.ChinkRule-class.html"
        ><literal>ChinkRule</literal></ulink> chinks anything that
        matches a given tag pattern.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.UnChunkRule-class.html"
        ><literal>UnChunkRule</literal></ulink> will un-chunk any chunk
        that matches a given tag pattern.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.MergeRule-class.html"
        ><literal>MergeRule</literal></ulink> can be used to merge two
        contiguous chunks.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.SplitRule-class.html"
        ><literal>SplitRule</literal></ulink> can be used to split a
        single chunk into two smaller chunks.</para>
      </listitem>
    </itemizedlist>

  </section>

  <section>
    <title> Further Reading </title>

    <para/>

  </section>

<section id="exercises">
  <title>Exercises</title>

  <orderedlist>

    <listitem>
      <formalpara>
        <title> Chunking Demonstration </title>
        <para> Run the chunking demonstration...</para>
      </formalpara>

<programlisting><![CDATA[
>>> from nltk.parser.chunk import *
>>> demo()  # the chunk parser
]]></programlisting>

    </listitem>

    <listitem>
      <formalpara>
        <title> BIO Tagging </title>
        <para>A common file representation of chunks uses
        the tags <literal>BEGIN</literal>, <literal>INSIDE</literal>
        and <literal>OUTSIDE</literal>.  Why are three tags
        necessary?  What problem would be caused if we used
        <literal>INSIDE</literal> and <literal>OUTSIDE</literal>
        tags exclusively?
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Predicate structure </title>
        <para> Develop an NP chunker which converts POS-tagged text
          into a list of tuples, where each tuple is a verb followed
          by its arguments.  E.g. <literal>the little cat sat on the mat</literal>
          becomes <literal>('sat', 'on', 'NP')</literal>...
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> NP Chunker </title>
        <para>Develop a noun phrase chunker for the chunking corpus
          using the regular-expression
          chunk parser <literal>RegexpChunkParser</literal>.  Use any
          combination of rules.
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Evaluate your chunker on 100 sentences from a chunked corpus,
          and report the precision, recall and F-measure.
        </para></listitem>
        <listitem><para>
          Use the <literal>chunkscore.missed()</literal> and
          <literal>chunkscore.incorrect()</literal> methods to
          identify the errors made by your chunker.  Discuss.
        </para></listitem>
      </orderedlist>
    </listitem>
  </orderedlist>

</section>    


<!-- ==================================================== 
<section>
<title> The String Representation of Tokenized Sequences </title>

    <para>Before we can apply a regular expression to a sequence of objects,
    those objects must first be encoded in a string.  This presents a difficulty.
    We want to apply the chunker to the output of the tagger, and we want the
    output of the chunker simply add the chunking information.  However, on
    the face of it, we must first project out the tags into a string, chunk
    the string, then somehow get back to the non-string representation.  The
    following sequence of Python objects illustrate the dilemma:</para>

<programlisting><![CDATA[
# tagger output
[ 'the'/'DT'@[1], 'cat'/'NN'@[2] 'sat'/'VBD'@[3], 'on'/'IN'@[4],
'the'/'DT'@[5], 'mat'/'NN'@[6] ]

# the list of tags
[ 'DT', 'NN', 'VBD', 'IN', 'DT', 'NN' ]

# the string of tags ready for chunking
'DT NN VBD IN DT NN'

# the chunked tags
'[DT NN] VBD IN [DT NN]'

# some black magic?
[ [ 'the'/'DT'@[1], 'cat'/'NN'@[2] ], 'sat'/'VBD'@[3], 'on'/'IN'@[4],
[ 'the'/'DT'@[5],'mat'/'NN'@[6] ] ]
]]></programlisting>

    <para>The solution is to encode each token in a string which contains sufficient
    information for the token to be reconstructed, and then write chunking rules
    which only consider the tags, ignoring the types and the locations.
    The <literal>RegexpChunkParser</literal> class provides a convenient interface
    hides this additional layer of complexity from the developer.</para>

    <para>The string representation of a token is illustrated below:</para>

<programlisting><![CDATA[
# a token
'the'/'DT'@[1]

# the string encoding
'>the/DT@[1]<'
]]></programlisting>

    <para>The <literal>rechunkparser</literal> module provides a function
    <literal>tag2str</literal> which takes a list of tagged tokens and converts
    it into a string, concatenating the string encoding of each token.</para>

    <note><para>Note that no spaces are inserted between the encoded tokens.
    In the encoding, tokens are delimited with &lt; and &gt;, not with
    whitespace.</para></note>

<programlisting><![CDATA[
>>> ttokens = [ 'the'/'DT'@[1], 'cat'/'NN'@[2] 'sat'/'VBD'@[3],
'on'/'IN'@[4], 'the'/'DT'@[5], 'mat'/'NN'@[6] ]
>>> tag2str(ttokens)
'<the/DT@[1]><cat/NN@[2]><sat/VBD@[3]><on/IN@[4]><the/DT@[5]><mat/NN@[6]>'
]]></programlisting>

    <para>The chunker operates on this string, not by splitting it, but
    by inserting/removing chunk delimiters.  The <literal>rechunkparser</literal>
    module uses braces as delimiters since, unlike parentheses and square brackets, these
    do not usually need to be backslash-escaped in regular expressions.
    Thus, the chunked representation of the above example is as follows:
    </para>

<programlisting><![CDATA[
'{<the/DT@[1]><cat/NN@[2]>}<sat/VBD@[3]><on/IN@[4]>{<the/DT@[5]><mat/NN@[6]>}'
]]></programlisting>

    <para>The module provides a method called <literal>str2chunks</literal>
    which builds a chunk structure from this string.  This is what is
    returned by the chunkparser for further processing, e.g. by an
    information extraction system.</para>

    <para>It is possible to build an NLTK chunker based solely on the
    infrastructure provided above.  However, we also provide classes to
    make it easier to express chunk rules and chunk parsers.  The
    rest of the section discusses these.</para>

    </section> <<<<< The String Representation of Tokenized Sequences >>>>>

    <section> <title> Chunk Rules and Abstract Chunk Rules </title>

    <para>Chunk rules operate on strings of encoded tokens to insert
    and delete the chunk delimiters.  For instance, a rule might
    create a chunk by inserting <literal>{</literal> before the fifth
    token, and inserting <literal>}</literal> after the sixth token.
    Equally, a rule could combine two adjacent chunks by
    <emphasis>removing</emphasis> <literal>}{</literal> from the string.</para>

    <para>The
    <ulink url="&refdoc;/nltk.parser.chunk.ChunkRule-class.html"
    ><literal>ChunkRule</literal></ulink> class provides a convenient wrapper
    for Python's built-in <literal>re</literal> (regular expression) class.
    The <literal>ChunkRule</literal> constructor takes five arguments:</para>

    <itemizedlist>
      <listitem><para><emphasis>target</emphasis>
        The material that the regular expression must apply to
      </para></listitem>
      <listitem><para><emphasis>action</emphasis>
        The action performed on this material
        (i.e. reproducing the material using the <literal>\1</literal>
        construct and adding or removing braces).
      </para></listitem>
      <listitem><para><emphasis>left</emphasis>
        The left-hand context in which this rule applies.
      </para></listitem>
      <listitem><para><emphasis>right</emphasis>
        The right-hand context in which this rule applies.
      </para></listitem>
      <listitem><para><emphasis>doc</emphasis>
        Brief documentation of the function of the rule
        (e.g. <literal>'chunking groups of JJ|NN'</literal>).
      </para></listitem>
    </itemizedlist>

    <para>Both <literal>target</literal> and <literal>action</literal>
    are explicit arguments.  The remaining arguments are optional,
    keyword arguments.  Here is a chunk rule which inserts the chunk
    delimiters around any tag <literal>NN</literal>.</para>

<programlisting><![CDATA[
ChunkRule(r'(<[^>]*/NN@[^>]*>)', r'{\1}', doc='chunk NNs')
]]></programlisting>

    <note><para>Note that we use Python's raw string notation, so that
    the interpreter does not preprocess the backslash escapes.  In general,
    it is a good idea to use the raw string notation whenever regular
    expressions are involved.  We follow this practice here.</para></note>

    <para>This rule matches tokens like
    <literal>&lt;cat/NN@[1]></literal>,
    and flags them as chunks by wrapping them with the chunk
    delimiters.  This is probably the simplest kind of chunk rule,
    and it is already incomprehensible.  Therefore, the
    <literal>rechunkparser</literal> module defines a more
    convenient interface, namely the <literal>AbstractChunkRule</literal>
    class.</para>

    <para>The
    <ulink url="&refdoc;/nltk.parser.chunk.AbstractChunkRule-class.html"
    ><literal>AbstractChunkRule</literal></ulink>
    class is a simple wrapper for the <literal>ChunkRule</literal> class.
    <literal>AbstractChunkRule</literal> is a derived class whose
    initializer preprocesses its <literal>target</literal>,
    <literal>left</literal> and <literal>right</literal> arguments.
    The calling function can now employ regular expressions
    <emphasis>over the tags only</emphasis>,
    ignoring the fact that the string also contains types and
    locations.  The chunk rule we saw above can now be written
    as follows:</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<NN>)', r'{\1}', doc='chunk NNs')
]]></programlisting>

    <para>Given this simpler format it is now relatively straightforward
    to construct some quite complex chunk rules.  The first generalization
    is to note that the tags themselves have some internal structure that
    we can exploit.  For example, a plural noun is tagged <literal>NNS</literal>.
    Suppose we wished to treat all tags starting with <literal>NN</literal>
    in a single chunk rule.  We could do this as follows:</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<NN.*>)', r'{\1}', doc='chunk NNX')
]]></programlisting>

    <para>In some cases, there is no common prefix and so we are forced
    to use disjunction.  Here is a chunk rule which puts chunk delimiters
    around any individual determiner, adjective or noun.  (Adjacent chunks would then
    need to be merged by another rule.)</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<DT|JJ|NN.*>)', r'{\1}', doc='chunk DT|JJ|NNX')
]]></programlisting>

    <para>Now suppose we wished to create a single chunk which encompassed
    a sequence consisting of a determiner followed by zero or more adjectives,
    followed by a noun.  We can also use the star operator for this:</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<DT><JJ>*<NN.*>)', r'{\1}', doc='chunk DT,JJ,NNX')
]]></programlisting>

    <para>In this example, the scope of the star operator is the preceding tag
    (i.e. the material contained inside the previous pair of angle brackets).
    </para>

    <note><para>Note that the star and other regular expression operators
    behave differently depending on whether they are inside or outside the
    scope of the angle brackets.  Inside the angle brackets, the operators work
    at the character level.  Outside the angle brackets the operators apply to
    complete tags.  In other words, the angle brackets are behaving like
    ordinary parentheses in regular expressions.</para></note>

    <para>The application of chunk rules can be constrained by making use of
    the <literal>left</literal> and <literal>right</literal> context arguments.
    For example, suppose we wanted to chunk a maximal string of tags which ends
    with a verb.  Here is a possible rule:</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<.*>*)', r'{\1}', right = r'<VB.*>')
]]></programlisting>

    <note><para>Contrary to the above, for technical reasons it is
    <emphasis>not</emphasis> possible to use the left context in
    <literal>AbstractChunkRule</literal>s.  (This is because Python's
    look-behind operator requires a fixed-width pattern.)</para></note>

    <note><para>We use a special context argument to permit rules to apply
    to their own output.  The other logical possibility - embedding
    multiple parentheses in the <literal>target</literal> argument, prevents
    rules reapplying to the same context.  This is only an issue in those
    cases where the context of some rule later becomes the target of a
    separate instance of the same rule.</para></note>

    </section> <<<<< Chunk Rules and Abstract Chunk Rules >>>>>

    <section> <title> Chunk Rules and Token Strings </title>

      <para>Chunk Rules are general regular expressions which manipulate token
      strings.  In order to keep token strings well-formed, and corresponding to
      the input, a chunk rule must only add, move or delete chunk delimiters.
      Moreover, the chunk delimiters must be matching and non-nested.  Here is
      an example of a pair of rules which violate this constraint:</para>

<programlisting><![CDATA[
AbstractChunkRule(r'(<NN>)', r'{\1}', doc='chunk NNs'),
AbstractChunkRule(r'(<DT>?<JJ>*<NN>)', r'{\1}', doc='chunk NPs'),
]]></programlisting>

      <para>The first rule chunks single <literal>NN</literal>s, while the second
      rule tries to chunk whole noun phrases.  However, these two rules interact
      in an unintended way.  To see why this is the case, suppose the sequence to
      be chunked includes a determiner, adjective and noun in sequence:
      <literal>&lt;DT>&lt;JJ>&lt;NN></literal>.  The first rule will chunk
      the noun, producing <literal>&lt;DT>&lt;JJ>{&lt;NN>}</literal>.
      The second rule cannot now match on the three tags, since there is now an
      open brace between the <literal>&lt;JJ></literal> and the
      <literal>&lt;NN></literal> which was not accounted for in the rule's
      target expression.  The second rule still applies to the
      <literal>&lt;NN></literal>, and produces
      <literal>&lt;DT>&lt;JJ>{{&lt;NN>}}</literal>.
      This is illegal and will cause the parser to throw an exception.
      In order to diagnose the problem, set the debug flag and step through
      the execution of the parser.</para>

      <para>The problem with the above pair of rules is that they do not form part
      of a legal chunking <emphasis>strategy</emphasis>.  Once a chunk delimiter is
      inserted beside a tag, the only legal operations are to move or delete it.
      For more discussion about how to combine rules into strategies, see the
      section on chunking strategies below.</para>

    </section> <<<<< Chunk Rules and Token Strings >>>>>

    <section> <title> The RegexpChunkParser Class </title>

    <para>The
    <ulink url="&refdoc;/nltk.parser.chunk.RegexpChunkParser-class.html"
    ><literal>RegexpChunkParser</literal></ulink>
    allows the programmer to construct a chunkparser object from a list
    (or <emphasis>cascade</emphasis>) of chunk rules.  Suppose we have a list of
    of rules and a list of tokens.  Then we can construct
    a chunkparser and apply it as follows:</para>

<programlisting><![CDATA[
>>> chunker = RegexpChunkParser(rules, 'NP', 'S')
>>> chunker.parse(tokens)
]]></programlisting>

    <para>The <literal>parse</literal> method converts its argument to the
    string encoding, applies the chunk rules in sequence, then converts the
    result to a chunk structure and returns this structure.  Here is a
    simple example using a rule we created earlier:</para>

<programlisting><![CDATA[
>>> r1 = AbstractChunkRule(r'(<DT><JJ>*<NN>)', r'{\1}', doc="chunking <DT><JJ>*<NN>")
>>> cp = RegexpChunkParser([r1], 'NP', 'S')
>>> chunked_sent = cp.parse(unchunked_sent)
[['the'/'DT'@[1word], 'little'/'JJ'@[2word], 'cat'/'NN'@[3word]],
'sat'/'VBD'@[5word], 'on'/'IN'@[6word], ['the'/'DT'@[8word], 'mat'/'NN'@[9word]]]
]]></programlisting>

    <para>The <literal>RegexpChunkParser</literal> initializer has an optional
    second argument which is a debug flag.  If this is set (e.g. to 1) then
    the chunk parser will display diagnostic output.  We repeat the above
    example with the debug flag set:</para>

<programlisting><![CDATA[
>>> r1 = AbstractChunkRule(r'(<DT><JJ>*<NN>)', r'{\1}', doc="chunking <DT><JJ>*<NN>")
>>> cp = RegexpChunkParser([r1], 'NP', 'S', 1)
>>> chunked_sent = cp.parse(unchunked_sent)

chunking <DT><JJ>*<NN>:
left:
target: ((?:<[^/]*/(?:DT)@\d+>)(?:<[^/]*/(?:JJ)@\d+>)*(?:<[^/]*/(?:NN)@\d+>))
right:
action: {\1}
input:   <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN>
   ->   {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
]]></programlisting>

    </section> <<<<< The RegexpChunkParser Class >>>>>

  -->
  &index;
  
</article>
