<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN"
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<!-- In this tutorial, I'm experimenting with using a flatter 
section hierarchy.  I think this may make the web version easier
to read (each top-level section is a single web-page). -->
<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Chunking</title>
    &versiondate; &copyright;
  </articleinfo>

  <section id="intro">
    <title> Introduction </title>

    <para> Two of the most common operations in language processing
    are <glossterm>segmentation</glossterm> and <glossterm>labelling</glossterm>.
    For example, tokenization <emphasis>segments</emphasis>
    a sequence of characters into tokens, while tagging
    <emphasis>labels</emphasis> each of these tokens.
    Moreover, these two operations go hand in hand.  We segment a
    stream of characters into linguistically meaningful pieces (e.g.
    as words) only so that we can classify those pieces (e.g. with their
    part-of-speech categories) and then identify higher-level structures.
    The result of such classification is
    usually stored by adding a label to the piece in question.
    Now that we have mapped characters to tagged-tokens, we will
    carry on with segmentation and labelling at a higher level,
    as illustrated in <xref linkend="segmentation"></xref>.
    This process is called <glossterm>chunking</glossterm>, and is
    also known as <glossterm>chunk parsing</glossterm>,
    <glossterm>partial parsing</glossterm>, or <glossterm>light parsing</glossterm>.
    </para>

<figure id="segmentation" float="1"><title>Segmentation and Labelling at both the
Token and Chunk Levels</title>
<informaltable><tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/segmentation" scale="30"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

    <para>
    Chunking is like tokenization and tagging in other respects.
    First, chunking can skip over material in the input.  Observe
    in <xref linkend="segmentation"></xref> that only some of the
    tagged tokens have been chunked, while others are left out.
    Compare this with the way that tokenization has omitted spaces
    and punctuation characters.  Second, chunking typically uses
    finite state methods to identify material of interest.  For
    example, the chunk in <xref linkend="segmentation"></xref>
    could have been found by the expression
    <literal>&lt;DT>?&lt;JJ>*&lt;NN></literal> which matches
    an optional determiner, followed by zero or more adjectives,
    followed by a noun.  Compare this with the way that tokenization
    and tagging both make use of regular expressions.  Third,
    chunking, like tokenization and tagging, is very application-specific.
    For example, while linguistic analysis and information extraction
    both need to identify and label salient extents of text, they typically
    require quite different definitions of those extents.
    </para>

    <para>There are two chief motivations for chunking: to locate
    information, or to ignore information.  In the former case,
    we may want to extract all noun phrases so that they can be
    indexed.  A text retrieval system could the use the index to
    support efficient retrieval for queries involving terminological expressions.
    In the latter case we may want to study syntactic patterns,
    finding particular verbs in a corpus and displaying their
    arguments.  For instance, here are uses of the verb <literal>gave</literal>
    in the first 100 files of the Penn Treebank corpus.  NP-chunking
    has been used so that the internal details of each noun phrase can
    be replaced with <literal>NP</literal>.  In this way we can discover
    significant grammatical properties even before a grammar has been
    developed.
    </para>

<screen>
gave NP
gave up NP in NP
gave NP up
gave NP help
gave NP to NP
</screen>

    <para>
    Chunking is akin to parsing in the sense that it can be used to
    build hierarchical structure over text.  There are several
    important differences, however.  First, as noted above, chunking
    is not exhaustive, and typically omits items in the surface string.
    Second, where parsing constructs nested deeply structures,
    chunking creates structures of fixed depth (typically depth 2),
    and as fine-grained as possible, as shown below:</para>

<orderedlist>
  <listitem><para>
[<subscript>NP</subscript>
  [<subscript>NP</subscript>
    G.K. Chesterton ],
  [<subscript>NP</subscript>
    [<subscript>NP</subscript>
      author ] of
    [<subscript>NP</subscript>
      [<subscript>NP</subscript>
        The Man ] who was
      [<subscript>NP</subscript>
        Thursday ]
    ]
  ]
]
</para></listitem><listitem><para>
  [<subscript>NP</subscript> G.K. Chesterton ],
  [<subscript>NP</subscript> author ] of
  [<subscript>NP</subscript> The Man ] who was
  [<subscript>NP</subscript> Thursday ]
</para></listitem>
</orderedlist>

    <para>In this chapter we explore the representation of chunks, and
    show how chunks can be recognized using a <emphasis>chunk parser</emphasis>.
    We describe a chunk parsing method based on regular expressions,
    then show how chunk parsers can be cascaded to create more deeply nested
    structures.
    </para>

  </section> <!-- Introduction -->

  <section id="representing_chunks">
    <title> Representing Chunks: Tags vs Trees </title>

    <para> As befitting its intermediate status between tagging and
    parsing, chunk structures can be represented using tags or trees.
    The most widespread file representation uses so-called ``BIO'' tags.
    In this scheme, each token is tagged with one of three special
    chunk tags, <literal>BEGIN</literal>, <literal>INSIDE</literal>
    or <literal>OUTSIDE</literal>.  A token is tagged as <literal>BEGIN</literal> if it
    is at the beginning of a chunk, and contained within that chunk.
    Subsequent tokens within the chunk are tagged <literal>INSIDE</literal>.
    All other tokens are tagged <literal>OUTSIDE</literal>.  An
    example of this scheme is shown in <xref linkend="tagrep"></xref>.
    </para>

<figure id="tagrep" float="1"><title>Tag Representation of Chunk Structures</title>
<informaltable>
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/tagrep" scale="30"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

    <para>The other logical representation for chunk structures is to
    use trees, as shown in <xref linkend="treerep"></xref>.  These
    have the benefit that each chunk is a constituent that can be
    manipulated directly.  NLTK uses this for its internal
    representation of chunks.
    </para>

<figure id="treerep" float="1"><title>Tree Representation of Chunk Structures</title>
<informaltable>
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/treerep" scale="30"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

    <para> A <glossterm>chunk parser</glossterm> finds contiguous,
    non-overlapping spans of related tokens and groups them together
    into <glossterm>chunks</glossterm>.  The
    following chunk represents a simple noun phrase:
    <literal>(NP: &lt;the> &lt;big> &lt;dog>)</literal>
    </para>

    <para> The chunk parser combines these individual chunks together,
    along with the intervening tokens, to form a chunk structure.  A
    <glossterm>chunk structure</glossterm> is a two-level tree that
    spans the entire text, and contains both chunks and un-chunked
    tokens.  For example, the following chunk structure captures the
    sample noun phrases in a sentence: </para>

<screen>
(S: (NP: &lt;I>)
    &lt;saw>
    (NP: &lt;the> &lt;big> &lt;dog>)
    &lt;on>
    (NP: &lt;the> &lt;hill>))
</screen>

  </section> <!-- Chunk Structures -->

  <section id="ChunkedTaggedTokenizer">
    <title> Reading Chunked Text </title>

    <para> Chunk parsers often operate on tagged texts, and use the
    tags to help make chunking decisions.  A common string
    representation for chunked tagged text is illustrated below.
    </para>

<screen>
[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]
</screen>

    <para>
    This can be parsed using the
    <literal>ChunkedTaggedTokenReader</literal> as shown.
    </para>

<programlisting><![CDATA[
>>> from nltk.tokenreader.tagged import ChunkedTaggedTokenReader
>>> chunked_string = "[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]"
>>> reader = ChunkedTaggedTokenReader(chunk_node='NP', SUBTOKENS='WORDS')
>>> sent_token = reader.read_token(chunked_string)
>>> print sent_token['TREE']
(S:
  (NP: <the/DT> <little/JJ> <cat/NN>)
  <sat/VBD>
  <on/IN>
  (NP: <the/DT> <mat/NN>))
]]></programlisting>
    
    <para>
    We usually want to read structured data from corpora, not strings.
    In the remainder of this section we show how this is done for
    two popular chunked corpus formats.
    </para>

  <section>
    <title>Reading chunk structures from bracketed text</title>

      <para> We can obtain a larger quantity of chunked text from
      the tagged Wall Street Journal in the Penn Treebank corpus.
      NLTK includes a sample of this corpus, and it can be accessed
      as follows: </para>

<programlisting><![CDATA[
>>> from nltk.corpus import treebank
>>> print treebank.groups()
('raw', 'tagged', 'parsed', 'merged')
>>> treebank.items('tagged')
('tagged/wsj_0001.pos', 'tagged/wsj_0002.pos', ...)
]]></programlisting>

      <para> Each file in the <literal>tagged</literal> section of the
      corpus consists of chunked, tagged text.  We can read in
      a file as follows:</para>

<programlisting><![CDATA[
>>> item = treebank.items('tagged')[10]
>>> tree = treebank.read(item)
<SENTS=[<TREE=(S: (NP_CHUNK: <South/NNP> <Korea/NNP>) <registered/VBD> ...
]]></programlisting>
      
      <para>The result is a token with a <literal>SENTS</literal>
      property that contains a list of chunk structures.
      We can access the third sentence as shown below:</para>

<programlisting><![CDATA[
>>> tree = treebank.read(item)['SENTS'][2]['TREE']
>>> print tree
(S:
  (NP_CHUNK: <Exports/NNS>)
  <in/IN>
  (NP_CHUNK: <October/NNP>)
  <stood/VBD>
  <at/IN>
  (NP_CHUNK: <$/$> <5.29/CD> <billion/CD>)
  <,/,>
  ...
]]></programlisting>

    <para>We can display this tree graphically using the
      <literal>nltk.draw.tree</literal> module:</para>

<programlisting><![CDATA[
>>> from nltk.draw.tree import *
>>> tree.draw()
]]></programlisting>

  </section>

    <section id="BIO">
      <title> Reading chunked text from BIO-tagged data </title>

    <para> Using the <literal>nltk.corpus</literal> module we can load
    files that have been chunked using the BIO (<literal>BEGIN/INSIDE/OUTSIDE</literal>)
    notation, such as that provided by the evaluation competitions run
    by CoNLL, the <emphasis>Conference on Natural Language Learning</emphasis>.
    In the CoNLL format, each sentence is represented in a file as a
    multiline string, as shown below:
    </para>

<screen>
he PRP B-NP
accepted VBD B-VP
the DT B-NP
position NN I-NP
of IN B-PP
vice NN B-NP
chairman NN I-NP
of IN B-PP
Carlyle NNP B-NP
Group NNP I-NP
, , O
a DT B-NP
merchant NN I-NP
banking NN I-NP
concern NN I-NP
. . O
</screen>

    <para>Each line consists of a word, its part-of-speech, the chunk category
    <literal>B</literal> (begin), <literal>I</literal> (inside) or
    <literal>O</literal> (outside), and the chunk type
    <literal>NP</literal>, <literal>VP</literal> or <literal>PP</literal>.
    The <literal>ConllTokenReader</literal> parses this information
    into a chunk structure.  Moreover, it permits us to choose any
    subset of the three chunk types to use (by default it includes all
    three).
    The example in <xref linkend="conll"/> produces only <literal>NP</literal> chunks.
    </para>

<figure id="conll" float="1">
<title>Reading NP Chunks from the CoNLL Corpus</title>
<programlisting><![CDATA[
>>> from nltk.tokenreader.conll import *
>>> text = """he PRP B-NP
... accepted VBD B-VP
# ...
... . . O"""
>>> reader = ConllTokenReader(chunk_types=['NP'])
>>> text_tok = reader.read_token(text)
>>> print text_tok['SENTS'][0]['TREE']
(S:
  (NP: <he/PRP>)
  <accepted/VBD>
  (NP: <the/DT> <position/NN>)
  <of/IN>
  (NP: <vice/NN> <chairman/NN>)
  <of/IN>
  (NP: <Carlyle/NNP> <Group/NNP>)
  <,/,>
  (NP: <a/DT> <merchant/NN> <banking/NN> <concern/NN>)
  <./.>)
]]></programlisting>
</figure>

  <para>
    This concludes our discussion of loading chunked data.  In the
    rest of this chapter we will see how chunked data can be created
    from tokenized text, chunked using a chunk parser, then evaluated
    against the so-called gold-standard data.
  </para>

    </section> <!-- BIO tagging -->

  </section> <!-- The Chunking Tokenizer -->

  <section id="RegexpChunkParser">
    <title> Chunking with Regular Expressions </title>

    <para> Earlier we noted that chunking builds flat (or non-nested)
    structures.  In practice, the extents of text to be chunked are
    identified using regular expressions over sequences of part-of-speech
    tags.  In NLTK provides a regular expression chunk parser,
    <literal>RegexpChunkParser</literal> to define the kinds of chunk
    we are interested in, and then to chunk a tagged text.
    This discussion presumes that readers are already familiar with
    regular expressions.
    </para>

    <para> <literal>RegexpChunkParser</literal> works by manipulating a
    <glossterm>chunk structure</glossterm>, which represents a
    particular chunking of the text.  The chunk parser begins
    with a structure in which no tokens are chunked.  Each
    regular-expression pattern (or <emphasis>chunk rule</emphasis>)
    is applied in turn, successively updating the chunk structure.
    Once all of the rules have been applied, the resulting chunk
    structure is stored in the specified property of the token.
    </para>

    <para>
    Chunk rules are defined in terms of regular expression patterns
    over "tag strings."  A <glossterm>tag string</glossterm> is a
    string consisting of tags, delimited with angle-brackets, e.g.:
    <literal>&lt;DT>&lt;JJ>&lt;NN>&lt;VBD>&lt;DT>&lt;NN></literal>.
    (Note that tag strings do not contain any whitespace.)
    Chunk rules are defined using a special kind of regular
    expression pattern, called a <glossterm>tag pattern</glossterm>.
    Tag patterns are identical to the regular expression patterns
    we have already seen, except for three differences which
    make them easier to use for chunk parsing.  First,
    The angle brackets group their contents into atomic units,
    so "<literal>&lt;NN>+</literal>" matches one or more repetitions of
    "<literal>&lt;NN></literal>"; and "<literal>&lt;NN|JJ></literal>"
    matches "<literal>&lt;NN></literal>" or
    "<literal>&lt;JJ></literal>."  Second, the period wildcard
    operator is constrained not to cross tag boundaries, so that
    "<literal>&lt;NN.*&gt;</literal>" matches any single tag
    starting with "<literal>NN</literal>."  Finally,
    whitespace is ignored in tag patterns, so
    "<literal>&lt;DT&gt; | &lt;NN&gt;</literal>" is equivalent to
    "<literal>&lt;DT&gt;|&lt;NN&gt;</literal>".  This allows
    tag patterns to be formatted to improve readability.
    </para>
        
    <para> Now that we can define tag patterns, it is a
    straightforward matter to set up an NLTK chunk parser.
    The simplest type of rule is <literal>ChunkRule</literal>.  This
    chunks anything that matches a given tag pattern.
    The <literal>ChunkRule</literal> constructor takes a
    tag pattern and a description string.  Here is a rule
    which chunks sequences consisting of one or more words
    tagged as <literal>DT</literal> or <literal>NN</literal>
    (i.e. determiners and nouns).
    </para>

<programlisting><![CDATA[
>>> from nltk.parser.chunk import *
>>> rule = ChunkRule('<DT|NN>+',
...                  'Chunk sequences of DT and NN')
]]></programlisting>

    <para> Now we can define a <literal>RegexpChunkParser</literal>
    based on this rule as follows:</para>

<programlisting><![CDATA[
# Construct a new noun-phrase chunk parser
>>> chunkparser = RegexpChunkParser([rule], chunk_node='NP', top_node='S')
]]></programlisting>

    <para>
    Note that the constructor has optional second and third arguments
    that specify the node labels for chunks and for the top-level node,
    respectively.  Now we can use this to chunk a tagged sentence,
    as illustrated by the complete program in <xref linkend="chunk-simple"/>.
      <footnote><para> Each rule must have a "description" associated with it,
      which provides a short explanation of the purpose or the effect
      of the rule.  This description is accessed via the 
      <literal>descr()</literal> method. </para></footnote>
    </para>

<figure id="chunk-simple" float="1"><title>Simple Chunking in NLTK</title>
<programlisting><![CDATA[
>>> from nltk.tokenreader.tagged import *
>>> from nltk.parser.chunk import *

>>> sent = "the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN"
>>> reader = TaggedTokenReader(SUBTOKENS='WORDS')
>>> sent_token = reader.read_token(sent)

>>> rule = ChunkRule('<NN|DT>+',
...                  'Chunk sequences of NN and DT')
>>> parser = RegexpChunkParser([rule], SUBTOKENS='WORDS',
...                            chunk_node='NP', top_node='S')
>>> parser.parse(sent_token)
>>> print sent_token['TREE']
(S:
  (NP: <the/DT>)
  <little/JJ>
  (NP: <cat/NN>)
  <sat/VBD>
  <on/IN>
  (NP: <the/DT> <mat/NN>))
]]></programlisting>
</figure>

    <para> We can also use more complex tag patterns, such as
    <literal>&lt;DT>?&lt;JJ.*>*&lt;NN.*></literal>.  This
    can be used to chunk any sequence of tokens beginning with an optional
    determiner <literal>DT</literal>, followed by zero or more
    adjectives of any type <literal>JJ.*</literal>, followed by a
    single noun of any type <literal>NN.*</literal>. </para>

    <para> If a tag pattern matches at multiple overlapping locations,
    the first match takes precedence.  For example, if we apply a rule
    that matches two consecutive nouns to a text containing three
    consecutive nouns, then the first two nouns will be chunked: </para>

<programlisting><![CDATA[
>>> from nltk.tagger import *
>>> from nltk.tokenreader import *
>>> text = "dog/NN cat/NN mouse/NN"
>>> nouns = TaggedTokenReader(SUBTOKENS='WORDS').read_token(text)
>>> rule = ChunkRule('<NN><NN>',
...                  'Chunk two consecutive nouns')
>>> parser = RegexpChunkParser([rule], chunk_node='NP',
...            top_node='S', SUBTOKENS='WORDS')
>>> parser.parse(nouns)
>>> print nouns['TREE']
(S: (NP: <dog> <cat>) <mouse>)
]]></programlisting>

  </section> <!-- RegexpChunkParser intro -->

  <section id="chunkrule.developing"> <title> Developing Chunk Parsers </title>

      <para> 
      Creating a good chunk parser usually requires several develop
      and test cycles, during which existing rules are refined and
      new rules are added.  In a later section we will describe an
      automatic evaluation process.  Here we show how to trace the
      execution of a chunk parser, to help the developer diagnose
      any problems.</para>

      <para>
      The <literal>RegexpChunkParser</literal> constructor takes an
      optional "<literal>trace</literal>" argument, which specifies whether
      debugging output should be shown during parsing.  This output
      shows the rules that are applied, and shows the chunking
      hypothesis at each stage of processing.
      <footnote>
        <para>Note that the <literal>parse</literal> method can also be given
        a trace value; this overrides the value given
        to the constructor.</para></footnote>
      In the execution trace, chunks are indicated by braces.
      In the following example, two chunking rules are applied to the
      input sentence.  The first rule finds all sequences of three
      tokens whose tags are <literal>DT</literal>,
      <literal>JJ</literal>, and <literal>NN</literal>, and the second
      rule finds any sequence of tokens whose tags are either
      <literal>DT</literal> or <literal>NN</literal>.
      </para>

<programlisting><![CDATA[
>>> sent = "the/DT little/JJ cat/NN sat/VBD on/IN the/DT mat/NN"
>>> reader = TaggedTokenReader(SUBTOKENS='WORDS')
>>> sent_token = reader.read_token(sent)

>>> rule1 = ChunkRule('<DT><JJ><NN>', 'Chunk det+adj+noun')
>>> rule2 = ChunkRule('<DT|NN>+', 'Chunk sequences of NN and DT')
>>> chunkparser = RegexpChunkParser([rule1, rule2], chunk_node='NP',
...                 top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk det+adj+noun:
               {<DT>  <JJ>  <NN>} <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN and DT:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
>>> print sent_token['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <para> When a <literal>ChunkRule</literal> is applied to a
      chunking hypothesis, it will only create chunks that do not
      partially overlap with chunks already in the hypothesis.  Thus,
      if we apply these two rules in reverse order, we will get a
      different result: </para>

<programlisting><![CDATA[
>>> chunkparser = RegexpChunkParser([rule2, rule1], chunk_node='NP',
...                 top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN and DT:
               {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
Chunk det+adj+noun:
               {<DT>} <JJ> {<NN>} <VBD>  <IN> {<DT>  <NN>}
>>> print sent_token['TREE']
(S: (NP: <the/DT>)
      <little/JJ>
      (NP: <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <para> Here, rule 2 ("chunk det+adj+noun") did not find any
      chunks, since all chunks that matched its tag pattern overlapped
      with chunks that were already in the hypothesis. </para>

    </section> <!-- Developing -->

  <section id="more_rules">
    <title> More Chunking Rules </title>

    <para> Sometimes it is easier to define what we
    <emphasis>don't</emphasis> want to include in a chunk than it is
    to define what we <emphasis>do</emphasis> want to include.  In
    these cases, it may be easier to build a chunk parser using
    <literal>UnChunkRule</literal> and
    <literal>ChinkRule</literal>. </para>

    <section id="chinkrule">
      <title> The Chink Rule </title>
      
      <para> A <glossterm>chink</glossterm> is a sequence of tokens
      that is not included in a chunk. In the following example,
      <literal>sat/VBD on/IN</literal> is a chink. </para>

<screen>
[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]
</screen>

      <para> <glossterm>Chinking</glossterm> is the process of
      removing a sequence of tokens from a chunk.  If the sequence of
      tokens spans an entire chunk, then the whole chunk is removed;
      if the sequence of tokens appears in the middle of the chunk,
      these tokens are removed, leaving two chunks where there was
      only one before.  If the sequence is at the beginning or
      end of the chunk, these tokens are removed, and a smaller
      chunk remains.  These three
      possibilities are illustrated in the following table: </para>

      <informaltable>
        <tgroup cols="4">
          <thead>
            <row>
              <entry></entry>
              <entry> Chink an entire chunk </entry>
              <entry> Chink the middle of a chunk </entry>
              <entry> Chink the end of a chunk </entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry><emphasis>Input</emphasis></entry>
              <entry>[a/DT  big/JJ  cat/NN]</entry>
              <entry>[a/DT  big/JJ  cat/NN]</entry>
              <entry>[a/DT  big/JJ  cat/NN]</entry>
            </row>
            <row>
              <entry><emphasis>Operation</emphasis></entry>
              <entry>Chink "a/DT big/JJ cat/NN"</entry>
              <entry>Chink "big/JJ" </entry>
              <entry>Chink "cat/DT" </entry>
            </row>
            <row>
              <entry><emphasis>Output</emphasis></entry>
              <entry> a/DT  big/JJ  cat/NN </entry>
              <entry>[a/DT] big/JJ [cat/NN]</entry>
              <entry>[a/DT  big/JJ] cat/NN </entry>
            </row>
          </tbody>
        </tgroup>
      </informaltable>

      <para> A <ulink url="&refdoc;/nltk.parser.chunk.ChinkRule-class.html"
      ><literal>ChinkRule</literal></ulink> chinks anything that
      matches a given tag pattern.  <literal>ChinkRule</literal>s are
      created with the <ulink
      url="&refdoc;/nltk.parser.chunk.ChinkRule-class.html#__init__"
      ><literal>ChinkRule</literal> constructor</ulink>, which takes a
      tag pattern and a description string.  For example, the
      following rule will chink any sequence of tokens whose tags are
      all "<literal>VBD</literal>" or "<literal>IN</literal>": </para>

<programlisting><![CDATA[
>>> chink_rule = ChinkRule('<VBD|IN>+',
...                        'Chink sequences of VBD and IN')
]]></programlisting>

      <para> Remember that <literal>RegexpChunkParser</literal> begins
      with a chunking hypothesis where nothing is chunked.  So before
      we apply our chink rule, we'll apply another rule that puts the
      entire sentence in a single chunk: </para>

<programlisting><![CDATA[
>>> chunkall_rule = ChunkRule('<.*>+',
...                           'Chunk everything')
]]></programlisting>

      <para> Finally, we can combine these two rules to create a chunk
      parser: </para>

<programlisting><![CDATA[
>>> chunkparser = RegexpChunkParser([chunkall_rule, chink_rule],
...                 chunk_node='NP', top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk everything:
               {<DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN>}
Chink sequences of VBD and IN:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}

>>> print sent_token['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
      <sat/VBD> <on/IN>
      (NP: <the/DT> <mat/NN>))
]]></programlisting>

      <para> If a tag pattern matches at multiple overlapping
      locations, the first match takes precedence. </para>

      <note><para>Cascading Rules:
        <literal>RegexpChunkParser</literal>s can use any number of
        <literal>ChunkRule</literal>s and
        <literal>ChinkRules</literal>, in any order.  As was discussed
        in <xref linkend="RegexpChunkParser"/>,
        <literal>ChunkRule</literal>s only create chunks that do not
        partially overlap with chunks already in the chunking
        hypothesis.  Similarly, <literal>ChinkRule</literal>s only
        create chinks that do not partially overlap with chinks that
        are already in the hypothesis.</para></note>

    </section> <!-- ChinkRule -->

    <section id="unchunkrule">
      <title> The Unchunk Rule </title>
    
      <para> An <ulink
      url="&refdoc;/nltk.parser.chunk.UnChunkRule-class.html"
      ><literal>UnChunkRule</literal></ulink> removes any chunk that
      matches a given tag pattern.  <literal>UnChunkRule</literal> is
      very similar to <literal>ChinkRule</literal>; but it will only
      remove a chunk if the tag pattern matches the entire chunk.  In
      contrast, <literal>ChinkRule</literal> can remove sequences of
      tokens from the middle of a chunk. </para>

      <para> <literal>UnChunkRule</literal>s are created with the
      <ulink
      url="&refdoc;/nltk.parser.chunk.UnChunkRule-class.html#__init__"
      ><literal>UnChunkRule</literal> constructor</ulink>, which takes
      a tag pattern and a description string.  For example, the
      following rule will unchunk any sequence of tokens whose tags
      are all "<literal>NN</literal>" or "<literal>DT</literal>":
      </para>

<programlisting><![CDATA[
>>> unchunk_rule = UnChunkRule('<NN|DT>+',
...                            'Unchunk sequences of NN and DT')
]]></programlisting>

      <para> We can combine this rule with a chunking rule to form a
      chunk parser: </para>

<programlisting><![CDATA[
>>> chunk_rule = ChunkRule('<NN|DT|JJ>+',
...                        'Chunk sequences of NN, JJ, and DT')
>>> chunkparser = RegexpChunkParser([chunk_rule, unchunk_rule],
...                 chunk_node='NP', top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN, JJ, and DT:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
Unchunk sequences of NN and DT:
           {<DT>  <JJ>  <NN>} <VBD>  <IN>  <DT>  <NN> 
>>> print sent_token['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
    <sat/VBD>
    <on/IN>
    <the/DT>
    <mat/NN>)
]]></programlisting>

      <para> Note that we would get a different result if we used a
      <literal>ChinkRule</literal> with the same tag pattern (instead
      of an <literal>UnChunkRule</literal>), since
      <literal>ChinkRule</literal>s can remove pieces of a chunk:
      </para>

<programlisting><![CDATA[
>>> unchink_rule = UnChunkRule('<NN|DT>+',
...                            'Chink sequences of NN and DT')
>>> chunk_rule = ChunkRule('<NN|DT|JJ>+',
...                        'Chunk sequences of NN, JJ, and DT')
>>> chunkparser = RegexpChunkParser([chunk_rule, chink_rule],
...                 chunk_node='NP', top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk sequences of NN, JJ, and DT:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
Chink sequences of NN and DT:
                <DT> {<JJ>} <NN>  <VBD>  <IN>  <DT>  <NN> 
>>> print sent_token['TREE']
(S: <the/DT>
    (NP: <little/JJ>)
    <cat/NN>
    <sat/VBD>
    <on/IN>
    <the/DT>
    <mat/NN>)
]]></programlisting>

    </section> <!-- UnChunkRule -->

    <section id="mergerule">
      <title> The Merge Rule</title>

    <para> When constructing complex chunk parsers, it is often
    convenient to perform operations other than chunking, chinking,
    and unchunking.  In this section and the next, we discuss two more complex
    rules which can be used to merge and split chunks. </para>

      <para> <ulink url="&refdoc;/nltk.parser.chunk.MergeRule-class.html"
      ><literal>MergeRule</literal></ulink>s are used to merge two
      contiguous chunks.  Each <literal>MergeRule</literal> is
      parameterized by two tag patterns: a <glossterm>left
      pattern</glossterm> and a <glossterm>right pattern</glossterm>.
      A <literal>MergeRule</literal> will merge two contiguous chunks
      <replaceable>C<subscript>1</subscript></replaceable> and
      <replaceable>C<subscript>2</subscript></replaceable> if the end
      of <replaceable>C<subscript>1</subscript></replaceable> matches
      the left pattern, and the beginning of
      <replaceable>C<subscript>2</subscript></replaceable> matches the
      right pattern. For example, consider the following chunking
      hypothesis: </para>

<screen>
[the/DT little/JJ] [cat/NN]
</screen>

      <para> Where
      <replaceable>C<subscript>1</subscript></replaceable> is
      <literal>[the/DT little/JJ]</literal> and
      <replaceable>C<subscript>2</subscript></replaceable> is
      <literal>[cat/NN]</literal>.  If the left pattern is
      "<literal>JJ</literal>", and the right pattern is
      "<literal>NN</literal>", then
      <replaceable>C<subscript>1</subscript></replaceable> and
      <replaceable>C<subscript>2</subscript></replaceable> will be
      merged to form a single chunk:</para>

<screen>
[the/DT little/JJ cat/NN]
</screen>

      <para> <literal>MergeRule</literal>s are created with the <ulink
      url="&refdoc;/nltk.parser.chunk.MergeRule-class.html#__init__"
      ><literal>MergeRule</literal> constructor</ulink>, which takes a
      left tag pattern, a right tag pattern, and a description string.
      For example, the following rule will merge two contiguous chunks
      if the first one ends in a determiner, noun or adjective; and
      the second one begins in a determiner, noun, or adjective: </para>

<programlisting><![CDATA[
>>> merge_rule = MergeRule('<NN|DT|JJ>', '<NN|DT|JJ>',
...                       'Merge NNs + DTs + JJs')
]]></programlisting>

      <para> To illustrate this rule, we will use a combine it with a
      chunking rule that chunks each individual token, and an
      unchunking rule that unchunks verbs and prepositions: </para>

<programlisting><![CDATA[
>>> chunk_rule = ChunkRule('<.*>',
...                     'Chunk all individual tokens')
>>> unchunk_rule = UnChunkRule('<IN|VB.*>',
...                         'Unchunk VBs and INs')
>>> rules = [chunk_rule, unchunk_rule, merge_rule]
>>> chunkparser = RegexpChunkParser(rules, chunk_node='NP',
...                 top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
                <DT>  <JJ>  <NN>  <VBD>  <IN>  <DT>  <NN> 
Chunk all individual tokens:
               {<DT>}{<JJ>}{<NN>}{<VBD>}{<IN>}{<DT>}{<NN>}
Unchunk VBs and INs:
               {<DT>}{<JJ>}{<NN>} <VBD>  <IN> {<DT>}{<NN>}
Merge NNs + DTs + JJs:
               {<DT>  <JJ>  <NN>} <VBD>  <IN> {<DT>  <NN>}
>>> print sent_token['TREE']
(S: (NP: <the/DT> <little/JJ> <cat/NN>)
    <sat/VBD> <on/IN>
    (NP: <the/DT> <mat/NN>))
]]></programlisting>

    </section> <!-- MergeRule -->

    <section id="splitrule">
      <title> The Split Rule </title>

      <para> <ulink url="&refdoc;/nltk.parser.chunk.SplitRule-class.html"
      ><literal>SplitRule</literal></ulink>s are used to split a
      single chunk into two smaller chunks.  Each
      <literal>SplitRule</literal> is parameterized by two tag
      patterns: a <glossterm>left pattern</glossterm> and a
      <glossterm>right pattern</glossterm>.  A
      <literal>SplitRule</literal> will split a chunk at any point
      <replaceable>p</replaceable>, where the left pattern matches the
      chunk to the left of <replaceable>p</replaceable>, and the right
      pattern matches the chunk to the right of
      <replaceable>p</replaceable>.  For example, consider the
      following chunking hypothesis: </para>

<screen>
[the/DT little/JJ cat/NN the/DT dog/NN]
</screen>

      <para> If the left pattern is "<literal>NN</literal>", and the
      right pattern is "<literal>DT</literal>", then the chunk will be
      split in two between "<literal>cat</literal>" and
      "<literal>dog</literal>", to form two smaller chunks:</para>

<screen>
[the/DT little/JJ cat/NN] [the/DT dog/NN]
</screen>

      <para> <literal>SplitRule</literal>s are created with the <ulink
      url="&refdoc;/nltk.parser.chunk.SplitRule-class.html#__init__"
      ><literal>SplitRule</literal> constructor</ulink>, which takes a
      left tag pattern, a right tag pattern, and a description string.
      For example, the following rule will split any chunk at a
      location that has "<literal>NN</literal>" to the left and
      "<literal>DT</literal>" to the right: </para>

<programlisting><![CDATA[
>>> split_rule = SplitRule('<NN>', '<DT>',
...                     'Split NN followed by DT')
]]></programlisting>

      <para> To illustrate this rule, we will use a combine it with a
      chunking rule that chunks sequences of noun phrases, adjectives,
      and determiners: </para>

<programlisting><![CDATA[
# Tokenize a new test text
>>> from nltk.tokenreader import *
>>> text = 'Bob/NNP saw/VBD the/DT man/NN the/DT cat/NN chased/VBD'
>>> sent_token = TaggedTokenReader(SUBTOKENS='WORDS').read_token(text)

# Create the chunk parser
>>> chunk_rule = ChunkRule('<NN.*|DT|JJ>+',
...                        'Chunk sequences of NN, JJ, and DT')
>>> chunkparser = RegexpChunkParser([chunk_rule, split_rule],
...                 chunk_node='NP', top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(sent_token, trace=1)
Input:
               <NNP>  <VBD>  <DT>  <NN>  <DT>  <NN>  <VBD> 
Chunk sequences of NN, JJ, and DT:
              {<NNP>} <VBD> {<DT>  <NN>  <DT>  <NN>} <VBD> 
Split NN followed by DT:
              {<NNP>} <VBD> {<DT>  <NN>}{<DT>  <NN>} <VBD> 
>>> print sent_token['TREE']
(S: (NP: <Bob/NNP>) 
    <saw/VBD>
    (NP: <the/DT> <man/NN>)
    (NP: <the/DT> <cat/NN>)
    <chased/VBD>)
]]></programlisting>

    </section> <!-- SplitRule -->

  </section> <!-- Merge/Split -->

  <section id="evaluation">
    <title> Evaluating Chunk Parsers </title>

    <para> An easy way to evaluate a chunk parser is to take some
    already chunked text, strip off the chunks, rechunk it, and compare
    the result with the original chunked text.  The
    <literal>ChunkScore.score()</literal>
    function takes the correctly chunked sentence as its first argument, and
    the newly chunked version as its second argument, and compares them.
    It reports the fraction of actual chunks that were found (recall), the
    fraction of hypothesized chunks that were correct (precision), and a
    combined score, the F-measure (the harmonic mean of precision and recall).
    </para>

    <para> A number of different metrics can be used to evaluate chunk
    parsers.  We will concentrate on a class of metrics that can be
    derived from two sets: </para>

    <itemizedlist>
      <listitem> <para> <glossterm>guessed</glossterm>: The set of
      chunks returned by the chunk parser. </para>
      </listitem>
      <listitem> <para> <glossterm>correct</glossterm>: The correct
      set of chunks, as defined in the test corpus.</para>
      </listitem>
    </itemizedlist>

    <para> From these two sets, we can define five useful metrics:
    </para>

    <itemizedlist>
      <listitem><para><glossterm>Precision</glossterm>: What
      percentage of guessed chunks were correct?</para>
      </listitem>
      <listitem><para><glossterm>Recall</glossterm>: What percentage
      of correct chunks were guessed?</para>
      </listitem>
      <listitem> <para> <glossterm>F Measure</glossterm>: the harmonic
      mean of precision and recall. </para>
      </listitem>
      <listitem> <para> <glossterm>Missed Chunks</glossterm>:
      What correct chunks were not guessed?</para>
      </listitem>
      <listitem> <para> <glossterm>Incorrect Chunks</glossterm>: What guessed
      chunks were not correct?</para>
      </listitem>
    </itemizedlist>

    <note>
      <para> Note that these metrics do not assign any credit for
      chunks that are "almost" right (e.g., chunks that extend one
      word too long).  It would be possible to design metrics that do
      assign partial credit for such cases, they would be more
      complex.  We decided to keep our metrics simple, so that it is
      easy to understand what a given result means. </para>
    </note>

    <para> During evaluation of a chunk parser, it is useful to
      flatten a chunk structure into a list of tokens.  Since chunk
      structures are just trees, we use the
      <literal>leaves()</literal> method to extract this
      flattened list.  Note that we must always include location information
      when evaluating a chunk parser.
    </para>
      
<programlisting><![CDATA[
>>> from nltk.token import *
>>> from nltk.tokenreader.tagged import ChunkedTaggedTokenReader

>>> chunked_string = "[ the/DT little/JJ cat/NN ] sat/VBD on/IN [ the/DT mat/NN ]"
>>> reader = ChunkedTaggedTokenReader(chunk_node='NP', SUBTOKENS='WORDS')
>>> correct = reader.read_token(chunked_string, add_locs=True)
>>> guess = Token(WORDS=correct['TREE'].leaves())
>>> print guess
[<the/DT>@[2:5c], <little/JJ>@[9:15c], <cat/NN>@[19:22c],
<sat/VBD>@[28:31c], <on/IN>@[36:38c], <the/DT>@[44:47c], <mat/NN>@[51:54c]]
]]></programlisting>

  <para>
    Now we run a chunker, and then compare the resulting chunked sentences
    with the originals, as follows:
  </para>
 
<programlisting><![CDATA[
>>> from nltk.parser.chunk import *
>>> chunkscore = ChunkScore()

>>> rule = ChunkRule('<PRP|DT|POS|JJ|CD|N.*>+', "Chunk items that often occur in NPs")
>>> chunkparser = RegexpChunkParser([rule], chunk_node='NP',
...                 top_node='S', SUBTOKENS='WORDS')
>>> chunkparser.parse(guess)
>>> chunkscore.score(correct['TREE'], guess['TREE'])
>>> print chunkscore
ChunkParser score:
    Precision: 100.0%
    Recall:    100.0%
    F-Measure: 100.0%
]]></programlisting>

      <para> <literal>ChunkScore</literal> is a class for
      scoring chunk parsers.  It can be used to evaluate the output of
      a chunk parser, using precision, recall, f-measure, missed
      chunks, and incorrect chunks.  It can also be used to combine
      the scores from the parsing of multiple texts.  This is quite
      useful if we are parsing a text one sentence at a time.
      The following program listing shows a typical use of the
      <literal>ChunkScore</literal> class.  In this example,
      <literal>chunkparser</literal> is being tested on each sentence
      from the Wall Street Journal tagged files. </para>

<programlisting><![CDATA[
>>> from nltk.parser.chunk import *
>>> from nltk.corpus import treebank

>>> rule = ChunkRule('<DT|JJ|NN>+', "Chunk sequences of DT, JJ, and NN")
>>> chunkparser = RegexpChunkParser([rule], chunk_node='NP',
...                 top_node='S', TAG='POS', SUBTOKENS='WORDS')

>>> chunkscore = ChunkScore()

>>> for item in treebank.items('tagged')[:10]:
...     for sent in treebank.read(item, add_locs=True)['SENTS']:
...         test_sent = Token(WORDS = sent['TREE'].leaves())
...         chunkparser.parse(test_sent)
...         chunkscore.score(sent['TREE'], test_sent['TREE'])

>>> print chunkscore
ChunkParser score:
    Precision:  43.5%
    Recall:     30.5%
    F-Measure:  35.8%
]]></programlisting>

  <note><para>It is important to add locations to tokens
    (using <literal>add_locs=True</literal>) so that
    the scorer doesn't confuse tokens taken from different
    regions of the data.</para></note>

      <para> The overall results of the evaluation can be viewed by
      printing the <literal>ChunkScore</literal>.  Each evaluation
      metric is also returned by an accessor method:
      <literal>precision()</literal>, <literal>recall</literal>,
      <literal>f_measure</literal>, <literal>missed</literal>, and
      <literal>incorrect</literal>.  The <literal>missed</literal> and
      <literal>incorrect</literal> methods can be especially useful
      when trying to improve the performance of a chunk
      parser: </para>

<programlisting><![CDATA[
from random import randint

print 'Chunks missed by the chunk parser:'
missed = chunkscore.missed()
for i in range(15):
    print missed[randint(0,len(missed)-1)].type()

print 'Incorrect chunks returned by the chunk parser:'
incorrect = chunkscore.incorrect()
for i in range(15):
    print incorrect[randint(0,len(incorrect)-1)].type()
]]></programlisting>

<!--      <note>
        <para> By default, only the first 100 missed chunks and the
        first 100 incorrect chunks will be remembered by the
        <literal>ChunkScore</literal>.  You can tell
        <literal>ChunkScore</literal> to record more chunk examples
        with the <literal>max_fp_examples</literal> (maximum false
        positive examples) and the <literal>max_fn_examples</literal>
        (maximum false negative examples) keyword arguments to the
        <literal>ChunkScore</literal> constructor:</para>

<programlisting><![CDATA[
>>> chunkscore = ChunkScore(max_fp_examples=1000,
...                        max_fn_examples=1000)
]]></programlisting>
      </note> -->

      <para> When evaluating a chunker it is important to score it
        against the baseline performance of a very simple chunker.
        Perhaps the most naive chunking method is to classify every
        tag in the training data as to whether it occurs inside
        or outside a chunk more often.  We can do this easily using
        a chunked corpus and a conditional frequency distribution
        as shown in <xref linkend="baseline"/>.
      </para>

<figure id="baseline" float="1"><title>Computing Baseline Performance for Chunking</title>
<programlisting><![CDATA[
>>> from nltk.probability import ConditionalFreqDist
>>> from nltk.parser.chunk import *
>>> from nltk.corpus import treebank

>>> cfdist = ConditionalFreqDist()
>>> items = treebank.items('tagged')
>>> split = len(items)*9/10
>>> train, test = items[:split], items[split:]

>>> for item in train:
...     for sent in treebank.read(item)['SENTS']:
...         for t in sent['TREE']:
...             if isinstance(t, Tree):
...                 for tok in t.leaves():
...                     cfdist[tok['POS']].inc(True)
...             else:
...                 cfdist[t['POS']].inc(False)

>>> chunk_tags = [tag for tag in cfdist.conditions() if cfdist[tag].max() == True]
>>> tag_pattern = '<' + '|'.join(chunk_tags) + '>+'
>>> print 'Chunking:', tag_pattern
Chunking: <PRP$|POS|WDT|JJ|WP|DT|$|NN|PRP|NNS|NNP|PDT|LS|CD|EX|WP$|NNPS|JJS|JJR>+

# Now, in the evaluation phase we chunk any sequence of those tags:

>>> rule = ChunkRule(tag_pattern, 'Chunk any sequence involving commonly chunked tags')
>>> chunkparser = RegexpChunkParser([rule], chunk_node='NP',
...                 top_node='S', TAG='POS', SUBTOKENS='WORDS')
>>> chunkscore = ChunkScore()

>>> for item in test:
...     for sent in treebank.read(item, add_locs=True)['SENTS']:
...         test_sent = Token(WORDS = sent['TREE'].leaves())
...         chunkparser.parse(test_sent)
...         chunkscore.score(sent['TREE'], test_sent['TREE'])

>>> print chunkscore
ChunkParser score:
    Precision:  80.2%
    Recall:     81.1%
    F-Measure:  80.7%
]]></programlisting>
</figure>

  </section> <!-- Evaluating Chunk Parsers-->


  <section>
    <title>Cascaded Chunking</title>

    <para> A chunk parser can be used to add multiple levels of
      structure to a sequence of tokens.  For instance, on the first
      pass, NP chunks could be identified.  On the second pass, these
      chunks could be treated as atomic units, and VP chunks could
      be identified.  First we create a tree whose leaves are tagged
      tokens.
    </para>

    <note><para> There should be a way to use the <literal>TreebankTokenReader</literal>
      to do this work.
    </para></note>

<programlisting><![CDATA[
>>> from nltk.tree import Tree
>>> from nltk.parser.chunk import *

>>> the    = Token(TEXT='the', TAG='DT')
>>> little = Token(TEXT='little', TAG='JJ')
>>> cat    = Token(TEXT='cat', TAG='NN')
>>> sat    = Token(TEXT='sat', TAG='VBD')
>>> on     = Token(TEXT='on', TAG='IN')
>>> the    = Token(TEXT='the', TAG='DT')
>>> mat    = Token(TEXT='mat', TAG='NN')

>>> np2 = Tree('NP', [the, mat])
>>> vp = Tree('VP', [sat, on, np2])
>>> np1 = Tree('NP', [the, little, cat])
>>> s = Tree('S', [np1, vp])
>>> print s
(S:
  (NP: <the/DT> <little/JJ> <cat/NN>)
  (VP: <sat/VBD> <on/IN> (NP: <the/DT> <mat/NN>)))
]]></programlisting>

    <para>Now we can extract the leaves of this tree, and run two
    chunk parsers over them, first to find NP chunks, then to find VP
    chunks.  Observe that the first chunk parser puts its result tree
    in a property called <literal>NP-CHUNKS</literal>, while the
    second chunk parser uses the contents of the <literal>NP-CHUNKS</literal>
    property as input.
    </para>

<programlisting><![CDATA[
# find NP chunks
>>> rule = ChunkRule(r'<DT>?<JJ>*<NN.*>', 'Chunk NPs')
>>> parser = RegexpChunkParser([rule], chunk_node='NP',
...            top_node='S', TREE='NP-CHUNKS', SUBTOKENS='WORDS')
>>> text_tok = Token(WORDS=s.leaves())
>>> parser.parse(text_tok)

# find VP chunks
>>> rule = ChunkRule(r'<VB.*><.*>*', 'Chunk VPs')
>>> parser = RegexpChunkParser([rule], chunk_node='VP',
...            top_node='S', SUBTOKENS='NP-CHUNKS')
>>> parser.parse( text_tok )
>>> print text_tok['TREE']
(S:
  (NP: <the/DT> <little/JJ> <cat/NN>)
  (VP: <sat/VBD> <on/IN> (NP: <the/DT> <mat/NN>)))
]]></programlisting>

  </section>

  <section>
    <title>Conclusion</title>

      <para>
      In this chapter we have explored a robust method for identifying
      structure in text using chunk parsers.
      There are a surprising number of different ways to chunk a
      sentence.  The chunk rules can add, shift and remove chunk delimiters
      in many ways, and the chunk rules can be combined in many ways.
      One can use a small number of very complex rules, or a long sequence of
      much simpler rules.  One can hand-craft a collection of rules,
      or train up a brute-force method using existing chunked text.
      </para>


    <itemizedlist>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.ChunkRule-class.html"
        ><literal>ChunkRule</literal></ulink> chunks anything that
        matches a given tag pattern.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.ChinkRule-class.html"
        ><literal>ChinkRule</literal></ulink> chinks anything that
        matches a given tag pattern.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.UnChunkRule-class.html"
        ><literal>UnChunkRule</literal></ulink> will un-chunk any chunk
        that matches a given tag pattern.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.MergeRule-class.html"
        ><literal>MergeRule</literal></ulink> can be used to merge two
        contiguous chunks.</para>
      </listitem>
      <listitem>
        <para>
        <ulink url="&refdoc;/nltk.parser.chunk.SplitRule-class.html"
        ><literal>SplitRule</literal></ulink> can be used to split a
        single chunk into two smaller chunks.</para>
      </listitem>
    </itemizedlist>

  </section>

  <section>
    <title> Further Reading </title>

    <para/>

  </section>

<section id="exercises">
  <title>Exercises</title>

  <orderedlist>

    <listitem>
      <formalpara>
        <title> Chunking Demonstration </title>
        <para> Run the chunking demonstration...</para>
      </formalpara>

<programlisting><![CDATA[
>>> from nltk.parser.chunk import *
>>> demo()  # the chunk parser
]]></programlisting>

    </listitem>

    <listitem>
      <formalpara>
        <title> BIO Tagging </title>
        <para>A common file representation of chunks uses
        the tags <literal>BEGIN</literal>, <literal>INSIDE</literal>
        and <literal>OUTSIDE</literal>.  Why are three tags
        necessary?  What problem would be caused if we used
        <literal>INSIDE</literal> and <literal>OUTSIDE</literal>
        tags exclusively?
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Predicate structure </title>
        <para> Develop an NP chunker which converts POS-tagged text
          into a list of tuples, where each tuple is a verb followed
          by its arguments.  E.g. <literal>the little cat sat on the mat</literal>
          becomes <literal>('sat', 'on', 'NP')</literal>...
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> NP Chunker </title>
        <para>Develop a noun phrase chunker for the CoNLL corpus
          using the regular-expression
          chunk parser <literal>RegexpChunkParser</literal>.  Use any
          combination of rules.
        </para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          Evaluate your chunker on 100 sentences from a chunked corpus,
          and report the precision, recall and F-measure.
        </para></listitem>
        <listitem><para>
          Use the <literal>chunkscore.missed()</literal> and
          <literal>chunkscore.incorrect()</literal> methods to
          identify the errors made by your chunker.  Discuss.
        </para></listitem>
      </orderedlist>
    </listitem>

  </orderedlist>

</section>    

  &index;
  
</article>
