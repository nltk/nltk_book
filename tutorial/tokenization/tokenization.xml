<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <title>NLTK Tutorial: Tokenization</title>
    &copyright;
  </articleinfo>

<section id="intro"><title>Introduction</title>

<para> This chapter addresses the following questions: how do we know
that piece of text is a <glossterm>word</glossterm>, and how do we
represent words and associated information in a machine?
</para>

<!--
<para> This chapter and the following one address the following questions: How do we know
that piece of text is a <glossterm>word</glossterm>? And once we know
that something is a word, what linguistic
<glossterm>category</glossterm> should it be assigned to?
In this chapter, we will also look at a powerful technique for
describing textual patterns called <glossterm>regular expressions</glossterm>.
</para>
-->

<para> It might seem needlessly picky to ask what a word is. Can't we
just say the following?
<orderedlist>
<listitem id="word_def">
<para>A word is a string of characters which has white space
before and after it</para>
</listitem>
</orderedlist>

However, it turns out that things are quite a bit
more complex. To get a flavour of the problems, consider the following text.
<example id="wsj_0034">
<title>Paragraph 12 from <filename>wsj_0034</filename></title>
<literallayout>
It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</literallayout>
</example>
</para>

<para>
Let's start with the string <literal>aren't</literal>. According to
<xref linkend="word_def"/>, it counts as only one word. But consider a
situation where we wanted to check whether all the words in our text
occurred in a dictionary, and our dictionary had entries for <literal
lang="en">are</literal> and <literal lang="en">not</literal>, but not
for <literal lang="en">aren't</literal>.  In this case, we would
probably be happy to say that <literal>aren't</literal> is a contraction
of two distinct words. <!--We can make a similar point about
<literal>1992's</literal>. We might want to run a small program over our
text to extract all words which express dates. In this case, we would
get achieve more generality by first stripping oexcept in this case, we
would not expect to find <literal>1992</literal> in a dictionary.--> 

If we take <xref linkend="word_def"/> literally (as we should, if we are
thinking of implementing it in code), then there are some other minor
but real problems. For example, assuming our file consists of a number
of separate lines, as indicated in <xref linkend="wsj_0034"/>, then all
the words which come at the beginning of a line will fail to be preceded
by whitespace (unless we treat the newline character as a
whitespace). Second, according to our criterion, punctuation symbols
will form part of words; that is, a string like
<literal>investors,</literal> will also count as a word, since there is
no whitespace between <literal>investors</literal> and the following
comma.

<!--
Conversely, we might want to look up names of places in a gazeteer (a
list of places together with their associated geographic location), and
in this case, we would probably want to treat <literal lang="en">South
Korea</literal> as a single <quote>word</quote>.
-->
</para>

<para>
A slightly different challenge is raised by examples such as the
following (drawn from the MedLine [ref] corpus):
<orderedlist>
<listitem>
<para>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
</para>
</listitem>
<listitem>
<para>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.
</para>
</listitem>
</orderedlist>

In these cases, we encounter terms which are unlikely to be found in any
general purpose English lexicon. Moreover, we will have no success in
trying to syntactically analyse these strings using a standard grammar
of English. Now for some applications, we would like to <quote>bundle
up</quote> expressions such as
<literal>alpha-galactosyl-1,4-beta-galactosyl-specific adhesin</literal>
and <literal>4.53 +/- 0.15%</literal> so that they are presented as
unanalysable atoms to the parser. That is, we want to treat them as
single <quote>words</quote> for the purposes of subsequent processing.
</para>

<para>The upshot is that, even if we confine our attention to English
text, the question of what we treat as word may depend a great deal on
what our purposes are. If we turn to languages other than English,
segmenting words can be even more of a challenge. For example, in
Chinese orthography, characters correspond to monosyllabic
morphemes. Many morphemes are words in their own right, but many words
contain more than one morpheme; most of them consist of two
morphemes. However, there is no visual representation of word boundaries
in Chinese text.

<note><para>give example
</para>
</note>

</para>

<para>
Let's look in more detail at the words in the text <xref linkend="wsj_0034"/>.
Suppose we use white space as the delimiter for words, and then list all
the words; we would expect to get something like the following:
<informalexample>
<programlisting><![CDATA[
120
1992
And
Barney
But
European
European
Fund
I
It
It
Japanese
Korea
Mr
Porter
Smith
South
Spain
a
a
...
]]></programlisting>
</informalexample>

<!--
We could also be slightly more clever, and produce a listing which 
<programlisting><![CDATA[
% tr -sc 'A-Za-z0-9' '\012' < wsj_0034 | sort | uniq -c
1 120
1 1992
1 And
1 Barney
1 But
2 European
1 Fund
1 I
2 It
1 Japanese
1 Korea
1 Mr
1 Porter
1 Smith
1 South
1 Spain
3 a
1 alarmed
1 are
1 aren
...
]]></programlisting>

Words according to the Unix <command>wc</command>:
<programlisting><![CDATA[
wc -w wsj_0034_pars12-13
    90 wsj_0034_pars12-13
]]></programlisting>
-->
Now, if we ask a program utility to tell us how many words there in the
text, it will probably come back with something like the following:

<itemizedlist>
<listitem>
<para>There are 90 words in <xref linkend="wsj_0034"/>.</para>
</listitem>	

<!--	  <listitem>
<para>the word `European' occurs twice in <xref
linkend="wsj_0034"/> </para></listitem>  
-->
</itemizedlist>
<indexterm><primary>word token</primary></indexterm>
<indexterm><primary>word type</primary></indexterm>
This calculation depends on treating each of the three occurrences of
<literal>a</literal> as a separate word. Yet what do we mean by saying
that is some object <literal>a</literal> which occurs three times? Are
there three words <literal>a</literal> or just one? We can in fact
answer "Both" if we draw a distinction between a word
<emphasis>token</emphasis> versus a word <emphasis>type</emphasis>.
A word type is
somewhat abstract; it's what we're talking about when we say that we
know the meaning of the word <literal>deprecate</literal>, or when we
say that the words <literal>barf</literal> and <literal>vomit</literal>
are synonyms. On the other hand, a word token is something which exists in
time and space. For example, we could talk about my uttering a token of
the word <literal>grunge</literal> in Edinburgh on July 14 2003;
equally, we can say that the second word token in <xref
linkend="wsj_0034"/> is a token of the word type
<literal>probably</literal>, or that there are two tokens of the type
<literal>European</literal> in the text.
More generally, we want to say that there are 90 word tokens in <xref
linkend="wsj_0034"/>, but only 76 word types. 
</para>

<para> The terms <glossterm>token</glossterm> and
<glossterm>type</glossterm> can also be applied to other linguistic
entities.  For example, a <glossterm>sentence token</glossterm> is an
individual occurrence of a sentence; but a <glossterm>sentence
type</glossterm> is an abstract sentence, without context.  If someone
repeats a sentence twice, they have uttered two sentence tokens, but
only one sentence type.  When the kind of token or type is obvious from
context, we will simply use the terms <glossterm>token</glossterm> and
<glossterm>type</glossterm>.
  </para>

</section>
<!-- ********************** /Introduction ************************* -->



<!-- ********************** Tokenization *************************** -->
<section id="tokenization"> 
<title>Tokenization</title>

<!-- SUBSECTION: Strings -->
<section id="tokenization.strings">
<title>Strings and Tokens</title>

<para> When written language is stored in a machine it is normally
represented as a sequence (or <glossterm>string</glossterm>) of
characters.  Individual words are strings.  A list of words is a string.
Entire texts are also strings. Strings can include special characters
which represent space, tab and newline.</para>

<para> Most computational processing is performed above the level of
characters.  In compiling a programming language, for example, the
compiler expects its input to be a sequence of
<glossterm>tokens</glossterm> that it knows how to deal with; for
example, the classes of identifiers, string constants and numerals.
Analogously, a parser will expect its input to be a sequence of word
tokens rather than a sequence of individual characters.  At its
simplest, then, <glossterm>tokenization</glossterm> of a text involves
searching for locations in the string of characters containing
whitespace (space, tab, or newline) or certain punctuation symbols, and
breaking the string into word tokens at these points.
</para>

</section><!--Strings and Tokens-->

<section><title>Linguistic Issues</title>

<note><para>[More to be written on the linguistic issues involved in
tokenization, including normalization of the text.]</para></note>

</section>
</section>
<!-- ********************** /Tokenization ************************* -->

<section id="nltk-tokenization"> <title> Tokenization in NLTK </title>

<section id="nltk-tokenization.words">
<title>Words</title>

<para>
Earlier we said that written language can be stored in a machine as a character string.
We can represent words in Python using strings, such as <literal>'dog'</literal>.
Furthermore, we can represent a sentence as a list of word strings:</para>

<programlisting><![CDATA[
    >>> words = ['the', 'cat', 'climbed', 'the', 'tree']
    ['the', 'cat', 'climbed', 'the', 'tree']
]]></programlisting>

<para> Another way to represent a word is as an integer, a pointer
into a wordlist.  Although this is less readable, it has the advantage
of permitting us to distinguish homographs.  Two words that are spelled the
same might be given different indices.</para>

<para> It turns out that both of these representations -- strings and integers -- are
too impoverished for our purposes.  They fail to distinguish between word types and
word tokens.  Additionally, they do not make it easy to associate other information
(such as a part-of-speech tag) with a word token.

In NLTK, we usually represent words using
<ulink url="&refdoc;/nltk.token.Token-class.html"><literal>Token</literal>s</ulink>.
In the next section we will learn more about <literal>Tokens</literal>, and how they are
created and manipulated.
</para>
</section> <!-- words -->

<section id="nltk-tokenization.types-and-tokens">
      <title> Types and Tokens in NLTK</title>

<para> In NLTK, word <ulink url="&refdoc;/nltk.token.Token-class.html">
<literal>Token</literal>s</ulink> can be constructed from word strings
using the <ulink url="&refdoc;/nltk.token.Token-class.html#__init__">
<literal>Token</literal> constructor</ulink>, which is defined in the
<ulink url="&refdoc;/nltk.token-module.html"><literal>nltk.token</literal></ulink>
module.  Here is a simple example of the use of this constructor:
</para>

<programlisting><![CDATA[
    >>> from nltk.token import * 
    >>> my_word_type = 'dog' 
    'dog'
    >>> my_word_token = Token(TEXT=my_word_type) 
    <dog>
]]></programlisting>

<para>
A token is a kind of Python dictionary, and we can associate arbitrary
additional properties with a token:
</para>

<programlisting><![CDATA[
    >>> my_word_token['TAG'] = 'Noun' 
    'Noun'
    >>> my_word_token['REFERENT'] = 'entity123' 
    'entity123'
]]></programlisting>

<para>
However, none of this permits us to distinguish two instances of the same word.
For that, we need to use <emphasis>locations</emphasis>, which we turn to next.
</para>

</section> <!-- Tokens and Types -->

<section> <title> Span Locations </title>

<para> <glossterm>Span locations</glossterm> specify regions of
      texts, using a <glossterm>start index</glossterm> and an
      <glossterm>end index</glossterm>.  A location in a stream of
      text starting at character
      <replaceable>s</replaceable> and ending at character
      <replaceable>e</replaceable> is written
      <literal>@[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>c]</literal>.
      This location specifies the region of the text beginning at
      <replaceable>s</replaceable>, and including everything up to
      (but not including) <replaceable>e</replaceable>.
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html"
      ><literal>CharSpanLocations</literal></ulink> are created using the
      <ulink url="&refdoc;/nltk.token.CharSpanLocation-class.html#__init__"
      ><literal>CharSpanLocation</literal> constructor</ulink>, which is
      defined in the <ulink url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module:</para>

<programlisting><![CDATA[
    >>> from nltk.token import * 
    >>> my_loc = CharSpanLocation(1, 5) 
    @[1:5]c@'corpus.txt'
    >>> another_loc = CharSpanLocation(0, 2) 
    @[0:2]c@'corpus.txt'
    >>> yet_another_loc = CharSpanLocation(22, 22) 
    @[22:22]c@'corpus.txt'
]]></programlisting>

<para> Note that a text location does <emphasis>not</emphasis> include
the text at its end location.  This convention may seem unintuitive at
first, but it has a number of advantages.  First, it is consistent with
Python's slice notation (e.g., <literal>x[1:3]</literal> specifies
elements 1 and 2 of <literal>x</literal>).

<footnote><para>Unlike Python slices, text locations do
<emphasis>not</emphasis> support negative indexes.</para></footnote> 

Second, it allows text locations to specify points <emphasis>between</emphasis> tokens,
instead of just ranges.  For example, <literal>Location(3,3)</literal> specifies the
point just before the text at index 3.  Finally, it simplifies arithmetic on
indices; for example, the length of <literal>Location(5,10)</literal> is
<literal>10-5</literal>, and two locations are contiguous if the start
of one equals the end of the other. </para>

<section> <title> Sources </title>

<para>A text location may be tagged with a
<glossterm>source</glossterm>, which gives an indication of where the
text was derived from.  A typical example of a source would be
the name of the file from which the element of text was read. </para>

<programlisting><![CDATA[
    >>> my_loc = CharSpanLocation(1, 5, 'foo.txt') 
    @[1:5c]@'foo.txt'
    >>> another_loc = CharSpanLocation(3, 72, 'bar.txt') 
    @[3:72c]@'bar.txt'
]]></programlisting>

<note>
<para> By default, the location's source is unspecified. </para>
</note>

<para> Sometimes, it is useful to use text locations as the
        sources for other text locations.  For example, we could
        specify the third character of the fourth word of the first
        sentence in the file <literal>foo.txt</literal> with
        <literal>char_loc</literal>: </para>

<programlisting><![CDATA[
    >>> sent_loc = CharSpanLocation(0, 25, 'foo.txt') 
    [0:25]c@'foo.txt'
    >>> word_loc = CharSpanLocation(5, 10, sent_loc) 
    [5:10c]@[0:25c]@'foo.txt'
    >>> char_loc = CharSpanLocation(6, 7, word_loc) 
    [6:7c]@[5:10c]@[0:25c]@'foo.txt'
]]></programlisting>

<note>
<para> The location indexes are zero-based, so the
        first sentence starts at an index of zero, not one.  </para>
</note>

</section>  <!-- Sources -->
</section> <!-- Text Locations -->

<section> <title> Tokens and Locations </title>

<para> As discussed above, a text token represents a single
      occurrence of a piece of text.  In NLTK, a token is defined by a
      type, together with a location at which that type occurs.  A
      token with text <replaceable>t</replaceable> and location
      <literal>@[<replaceable>l</replaceable>]</literal> can be
      written as
      <literal>&lt;<replaceable>t</replaceable>>@[<replaceable>l</replaceable>]</literal>.
      This location information can be used to distinguish two instances of
      the same text.
      Tokens are constructed with the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal> constructor</ulink>: </para>

<programlisting><![CDATA[
    >>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
    'hello'@[0,5c]
    >>> token2 = Token(TEXT='world', LOC=CharSpanLocation(6, 11)) 
    'world'@[6,11c]
]]></programlisting>

<para> Two tokens are only equal if they are equal on all their attributes,
      in this case, for the attributes <literal>text</literal> and <literal>loc</literal>.
</para>

<programlisting><![CDATA[
    >>> token1 = Token(TEXT='hello', LOC=CharSpanLocation(0, 5)) 
    'hello'@[0,5c]
    >>> token2 = Token(TEXT='hello', LOC=CharSpanLocation(6, 11)) 
    'world'@[6,11c]
    >>> token1 == token2 
    False
    >>> token1['TEXT'] == token2['TEXT'] 
    True
]]></programlisting>

<note>
<para> When a token's location is unknown or unimportant, it may be omitted.
However, the distinction between a word token and a word type is lost in this
case.
</para>
</note>

<para> To access a location's start index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#start"
      ><literal>start</literal></ulink> member function; and to access
      a location's end index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#end"
      ><literal>end</literal></ulink> member function: </para>

<programlisting><![CDATA[
    >>> loc1 = CharSpanLocation(0, 8) 
    @[0:8c]
    >>> loc1.start()
    0
    >>> loc1.end()
    8
]]></programlisting>

<para> To access a location's source, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#source"
      ><literal>source</literal></ulink> member function: </para>

<programlisting><![CDATA[
    >>> loc1 = CharSpanLocation(3, 6, 'corpus.txt') 
    @[3,6c]@'corpus.txt'
    >>> loc1.source()
    'corpus.txt'
]]></programlisting>

<para> For more information about tokens and locations, see the
      reference documentation for the <ulink
      url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module. </para>

</section>  <!-- Tokens and Locations -->

<section> <title> Texts </title>

<para> Many natural language processing tasks involve analyzing
    texts of varying sizes, ranging from single sentences to very
    large corpora.  There are a number of ways to represent texts
    using NLTK.  The simplest is as a single
    <literal>string</literal>.  These strings can be loaded directly
    from files: </para>

<programlisting><![CDATA[
    >>> text_str = open('corpus.txt').read() 
    'Hello world.  This is a test file.\n'
]]></programlisting>

<para> However, it is usually preferrable to represent a text as a
    <literal>list</literal> of <literal>Token</literal>s.  These
    <literal>list</literal>s are typically created using a
    <glossterm>tokenizer</glossterm>, such as <ulink
    url="&refdoc;/nltk.tokenizer.WSTokenizer-class.html"
    ><literal>WSTokenizer</literal></ulink> (which splits words apart
    based on whitespace): </para>

<programlisting><![CDATA[
    >>> from nltk.tokenizer import *
    >>> text_token = Token(TEXT='Hello world.  This is a test file.')
    <Hello world.  This is a test file.>
    >>> WSTokenizer().tokenize(text_token)
    >>> print text_token 
    <[<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]>
]]></programlisting>

<para> Observe that the <literal>text_token</literal> initially contains
a single token for the entire text.  We further tokenize this into its
component word tokens, and the result is stored as an annotation of the
original token.  The original source token and the resulting tokens are all stored
together, as the following code illustrates:
</para>

<programlisting><![CDATA[
    >>> print text_token['TEXT'] 
    <Hello world.  This is a test file.>
    >>> print text_token['SUBTOKENS'] 
    [<Hello>, <world.>, <This>, <is>, <a>, <test>, <file.>]
]]></programlisting>

<note><para>The default method for printing a token containing subtokens
is just to print the subtokens.</para></note>

<para> If we want all tokens to contain location data, then we
specify the <literal>addlocs</literal> option:</para>

<programlisting><![CDATA[
    >>> WSTokenizer().tokenize(text_token, addlocs=True) 
    >>> print text_token 
    <[<Hello>@[0:5c], <world.>@[6:12c], <This>@[14:18c], <is>@[19:21c], <a>@[22:23c],
    <test>@[24:28c], <file.>@[29:34c]]>
]]></programlisting>

<para> Texts can also be represented as sets of word tokens, which is
useful in document classification applications: </para>

<programlisting><![CDATA[
    >>> from nltk.set import * 
    >>> print Set(*text_token.freeze()['SUBTOKENS']) 
    {<a>, <This>, <is>, <file.>, <test>, <Hello>, <world.>}
]]></programlisting>

</section> <!-- Texts -->

<section> <title> Normalization </title>

<para> Tokenization may normalize the text, mapping all words to
lowercase, expanding contractions, and possibly even stemming the
words.  An example for stemming is shown below:
</para>

<programlisting><![CDATA[
    >>> text_token = Token(TEXT='stemming can be fun and exciting') 
    >>> WSTokenizer().tokenize(text_token) 
    >>> from nltk.stemmer.porter import * 
    >>> stemmer = PorterStemmer() 
    >>> for word_token in text_token['SUBTOKENS']: 
    ...     stemmer.stem(word_token)
    >>> print text_token 
    <[<TEXT='stemming', STEM='stem'>, <TEXT='can', STEM='can'>, <TEXT='be', STEM='be'>,
    <TEXT='fun', STEM='fun'>, <TEXT='and', STEM='and'>, <TEXT='exciting', STEM='excit'>]>
]]></programlisting>

</section>  <!-- Tokens and Locations -->

<section> <title>NLTK Tokenizers </title>

<!-- As mentioned in the previous section, it is often useful to
represent a text as a list of tokens.  The process of breaking a text up
into its constituent tokens is known as
<glossterm>tokenization</glossterm> -->

<para> As we saw in <xref
	linkend="tokenization"/>, tokenization involves breaking a text
up into its constituent tokens prior to further processing. Tokenization
in NLTK can occur at a number of different levels: a text could be
broken up into paragraphs, sentences, words, syllables, or phonemes. And
for any given level of tokenization, there are many different algorithms
for breaking up the text.  For example, at the word level, it is not
immediately clear how to treat such strings as <literal>can't</literal>
<literal>$22.50</literal> <literal>New York</literal> and
<literal>so-called</literal>.
</para>

<para> NLTK defines a general interface for tokenizing texts, the
    <ulink url="&refdoc;/nltk.tokenizer.TokenizerI-class.html"
    ><literal>TokenizerI</literal></ulink> class.  This interface is
    used by all tokenizers, regardless of what level they tokenize at
    or what algorithm they use.  It defines a single method, <ulink
    url="&refdoc;/nltk.tokenizer.TokenizerI-class.html#tokenize"
    ><literal>tokenize</literal></ulink>, which takes a
    <literal>string</literal>, and returns a list of
    <literal>Token</literal>s.  </para>

<section> <title> The whitespace tokenizer </title> 

<para> A simple example of a tokenizer is the
      <ulink url="&refdoc;/nltk.tokenizer.WSTokenizer-class.html"
      ><literal>WSTokenizer</literal></ulink>, which breaks a text into
      words, assuming that words are separated by whitespace (space,
      enter, and tab characters).  We can use the
      <ulink url="&refdoc;/nltk.tokenizer.WSTokenizer-class.html#__init__"
      ><literal>WSTokenizer</literal> constructor</ulink> to build a
      new whitespace tokenizer: </para>

<programlisting><![CDATA[
    >>> tokenizer = WSTokenizer() 
]]></programlisting>

<para> Once we have built the tokenizer, we can use it to
      process texts: </para>

<programlisting><![CDATA[
    >>> WSTokenizer().tokenize(text_token) 
    >>> print text_token 
    <[<Hello>@[0:5c], <world.>@[6:12c], <This>@[14:18c], <is>@[19:21c], <a>@[22:23c],
    <test>@[24:28c], <file.>@[29:34c]]>
]]></programlisting>

<para> However, this tokenizer is not ideal for many tasks.  For
      example, we might want punctuation to be included as separate
      tokens; or we might want names like "New York" to be included as
      single tokens. </para>

</section> <!-- Whitespace tokenizer -->

<section> <title> The regular expression tokenizer </title>

<para> The <literal>RegexpTokenizer</literal> is a more powerful
      tokenizer.  It uses a regular expression to determine how text
      should be split up.  This regular expression specifies the
      format of a valid word.  For example, if we wanted to mimic the
      behavior or <literal>WSTokenizer</literal>, we could define the
      following <literal>RegexpTokenizer</literal>: </para>

<programlisting><![CDATA[
    >>> text_token = Token(TEXT='Hello. Isn't this fun?') 
    >>> tokenizer = RegexpTokenizer(r'[^\s]+') 
    >>> tokenizer.tokenize(text_token) 
    >>> print text_token 
    <[<Hello.>, <Isn't>, <this>, <fun?>]>
]]></programlisting>

<para> (The regular expression <literal>\s</literal> matches any
      whitespace character.) </para>

<para> To define a tokenizer that includes punctuation as
      separate tokens, we could use: </para>

<programlisting><![CDATA[
    >>> regexp = r'\w+|[^\w\s]+'
    '\w+|[^\w\s]+'
    >>> tokenizer = RegexpTokenizer(regexp) 
    >>> tokenizer.tokenize(text_token) 
    >>> print text_token 
    <[<Hello>, <.>, <Isn>, <'>, <t>, <this>, <fun>, <?>]>
]]></programlisting>

<para> The regular expression in this example will match
      <emphasis>either</emphasis> a sequence of alphanumeric
      characters (letters and numbers); <emphasis>or</emphasis> a
      sequence of punctuation characters. </para>

<para> There are a number of ways we might want to improve this
      regular expression.  For example, it currently breaks the string
      "$22.50" into four tokens; but we might want it to include this
      as a single token.  One approach to making this change would be
      to add a new clause to the tokenizer's regular expression, which
      is specialized for handling strings of this form: </para>

<programlisting><![CDATA[
    >>> text_token = Token(TEXT='That poster costs $22.40.') 
    >>> regexp = r'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
    '(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
    >>> tokenizer = RegexpTokenizer(regexp) 
    >>> tokenizer.tokenize(text_token) 
    >>> print text_token 
    <[<That>, <poster>, <costs>, &lt$22.40>, <.>]>
]]></programlisting>

<para> Of course, more general solutions to this problem are
      also possible, using different regular expressions. </para>

<note><para>It is sometimes more convenient to write a regular expression
matching the material that appears <emphasis>between</emphasis> tokens, such as whitespace
and punctuation.  The <literal>RegexpTokenizer()</literal> constructor permits
an optional parameter <literal>negative</literal> which inverts the meaning
of the regular expression.  For example, the following two tokenizers are
equivalent:
<literal>RegexpTokenizer(r'[^\s]+')</literal>,
<literal>RegexpTokenizer(r'\s+', negative=1)</literal>.
</para></note>

</section> <!-- Regexp tokenizer -->

</section> <!-- Tokenization -->

<section> <title> Tokenization in the corpus module </title>

<para> 
The <literal>nltk.corpus</literal> module provides ready access to several
corpora included with NLTK, along with built-in tokenizers.  Here is an
example of its use:
</para>

<programlisting><![CDATA[
    >>> from nltk.corpus import gutenberg 
    >>> print gutenberg.tokenize('milton-paradise.txt') 
    <[<**This>@[2:8c], <is>@[9:11c], <the>@[12:15c], <Project>@[16:23c], <Gutenberg>@[24:33c],
    <Etext>@[34:39c], <of>@[40:42c], <Paradise>@[43:51c], <Lost(Raben)**> ... ]>
]]></programlisting>

</section>

<section> <title> Example: Processing Tokenized Text </title>

<para> In this section, we see how to use NLTK to
    examine the distribution of word lengths in a document.  This is
    meant to be a simple example of how the tools we have introduced
    can be used to solve a simple NLP problem.  The distribution of
    word lengths in a document can give clues to other properties,
    such as the document's style or difficulty, or even the document's language. </para>

<para> We present three different approaches to solving this
    problem; each one illustrates different techniques, which might be
    useful for other problems. </para>

<section> <title> Word Length Distributions 1: Using a List </title>

<para> To begin with, we'll need to extract the words from a
      corpus that we wish to test.  We'll use the
      <literal>WSTokenizer</literal> to tokenize the corpus: </para>

<programlisting><![CDATA[
    >>> from nltk.corpus import gutenberg 
    >>> text_token = gutenberg.tokenize('milton-paradise.txt') 
]]></programlisting>

<para> Now, we will construct a list
      <literal>wordlen_count_list</literal>, which gives the number of
      words that have a given length.  In particular,
      <literal>wordlen_count_list[<replaceable>i</replaceable>]</literal>
      is the number of words whose length is
      <replaceable>i</replaceable>. </para>

<para> When constructing this list, we must be careful not to
      try to add a value past the end of the list.  Therefore,
      whenever we encounter a word that is longer than any previous
      words, we will add enough zeros to
      <literal>wordlen_count_list</literal> that we can store the
      occurrence of the new word:

<programlisting><![CDATA[
    >>> wordlen_count_list = []
    >>> for token in text_token['SUBTOKENS']:
    ...     wordlen = len(token['TEXT'])
    ...     # Add zeros until wordlen_count_list is long enough
    ...     while wordlen >= len(wordlen_count_list):
    ...         wordlen_count_list.append(0)
    ...     # Increment the count for this word length
    ...     wordlen_count_list[wordlen] += 1
    >>> print wordlen_count_list
    [0, 1224, 11608, 16141, 15382, 11926, 8792, 6429, 4351, 2883, 1634,
    841, 410, 182, 90, 30, 15, 6, 8, 2, 2, 5, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]
]]></programlisting>
</para>

<para> We can plot the results, using the <ulink
      url="&refdoc;/nltk.draw.plot.Plot-class.html"
      ><literal>Plot</literal></ulink> class, defined in the
      <ulink url="&refdoc;/nltk.draw.plot-module.html"
      ><literal>nltk.draw.plot</literal></ulink> module: </para>

<programlisting><![CDATA[
    >>> from nltk.draw.plot import Plot 
    >>> Plot(wordlen_count_list)
]]></programlisting>

<graphic fileref="images/length" scale="75"/>

</section> <!-- Word length distributions: list -->

<section> <title> Word Length Distributions 2: Using a Dictionary </title>

<para> We have been examining the function from word lengths to
      token counts.  In this example, the range of the function (i.e.,
      the set of word lengths) is ordered and relatively small.
      However, we often wish to examine functions whose ranges are not
      so well behaved.  In such cases, dictionaries can be a powerful
      tool.  The following code uses a dictionary to count up the
      number of times each word length occurs: </para>

<programlisting><![CDATA[
    >>> wordlen_count_dict = {}
    >>> for token in text_token['SUBTOKENS']:
    ...     word_length = len(token['TEXT'])
    ...     if wordlen_count_dict.has_key(word_length):
    ...         wordlen_count_dict[word_length] += 1
    ...     else:
    ...         wordlen_count_dict[word_length] = 1
]]></programlisting>

<para> To plot the results, we can use a list of (wordlen,
      count) pairs.  This is simply the <literal>items</literal> of
      the dictionary: </para>

<programlisting><![CDATA[
    >>> points = wordlen_count_dict.items() 
    >>> Plot(points)
]]></programlisting>

</section> <!-- Word length distributions: dictionary -->

<section> <title> Word Length Distributions 3: Using a Frequency Distribution </title>

<para> The <ulink url="&refdoc;/nltk.probability-module.html"
><literal>nltk.probability</literal></ulink> module defines two the
<ulink url="&refdoc;/nltk.probability.FreqDist-class.html"
><literal>FreqDist</literal></ulink> class for modeling frequency
distributions.  In this example, we use a frequency distribution to
find the relationship between word lengths and token counts. For this
example, three methods of <literal>FreqDist</literal> are
relevant:</para>

<itemizedlist>
<listitem><para><ulink url="&refdoc;/nltk.probability.FreqDist-class.html#inc"><literal>inc(<replaceable>sample</replaceable>)</literal></ulink>
          increments the frequency of a given sample.</para>
</listitem>
<listitem><para><ulink url="&refdoc;/nltk.probability.FreqDist-class.html#samples"><literal>samples()</literal></ulink> returns a list of
          the samples covered by a frequency distribution.</para>
</listitem>
<listitem><para><ulink url="&refdoc;/nltk.probability.FreqDist-class.html#count"><literal>count(<replaceable>sample</replaceable>)</literal></ulink>
          returns the number of times a given sample occurred.</para>
</listitem>
</itemizedlist>

<para> First, we construct the frequency distribution for the
      word lengths: </para>

<programlisting><![CDATA[
    >>> wordlen_freqs = FreqDist()
    >>> for token in word_token['SUBTOKENS']:
    ...     wordlen_freqs.inc(len(token['TEXT']))
]]></programlisting>

<para> Next, we extract the set of word lengths that were found
      in the corpus: </para>

<programlisting><![CDATA[
    >>> wordlens = wordlen_freqs.samples()
]]></programlisting>

<para> Finally, we construct a list of (wordlen, count) pairs,
      and plot it: </para>

<programlisting><![CDATA[
    >>> points = [(wordlen, wordlen_freqs.count(wordlen))
    ...           for wordlen in wordlens]
    >>> Plot(points)
]]></programlisting>

<note> <para> The expression
      <literal>[...&nbsp;for&nbsp;...&nbsp;in&nbsp;...]</literal> is
      called a "list comprehension."  For more information on list
      comprehensions, see the <ulink
      url="&tutdoc;/advpython/index.html">"New Python Features"
      tutorial</ulink>. </para>
</note>

<para> For more information about frequency distributions, see
      the Probability Tutorial.</para>

</section> <!-- Word length distributions: freqdist -->
</section> <!-- Word length distributions example -->

</section>  <!-- Tokenization in NLTK -->

&index;
</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
adaptive-fill-mode:nil
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

