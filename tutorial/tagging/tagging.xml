<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">
<!ENTITY tutdoc "http://nltk.sourceforge.net/tutorial">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!ENTITY ling "<prompt>...</prompt>">
]>

<article>
  <articleinfo>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <title>NLTK Tutorial: Tagging</title>
    &copyright;
  </articleinfo>

<section id="intro">
    <title>Introduction</title>

<para> This chapter addresses the following question: Once we know
that something is a word, what linguistic
<glossterm>category</glossterm> should it be assigned to?
</para>

</section>

<section id="pos">
    <title>Word Classes and Parts of Speech</title>
<!-- 
X.2 linguistic overview (for non-linguist readers)
- how have linguists addressed the problem?
- what are the shortcomings of the non-computational approach?

% tr -sc 'A-Za-z0-9' '\012' &lt; wsj_0034 | sort 
-->
<para>There is a long tradition within linguistics of classifying
    words into different categories. These categories are also called
    <firstterm>parts of speech</firstterm>. Familiar examples are
    <type>noun</type>, <type>verb</type>, <type>preposition</type>,
    <type>adjective</type> and <type>adverb</type>. How do we know what
    category a word should belong to? In general, linguists invoke three
    kinds of criteria for making the decision:
    <itemizedlist>
     <listitem>
      <para>formal;</para>
     </listitem>
     <listitem>
      <para>syntactic (or distributional);</para>
     </listitem>
     <listitem>
      <para>notional (or semantic).</para>
     </listitem>
    </itemizedlist>

A <firstterm>formal</firstterm> criterion is one which looks at the
internal structure of a word. For example, <literal>-ness</literal> is a
suffix which combines with an adjective to produce a noun. Examples are
<literal>happy</literal> &gt; <literal>happiness</literal>,
<literal>ill</literal> &gt; <literal>illness</literal>. So if we
encounter a word which ends in <literal>-ness</literal>, this is very
likely to be a noun.
</para>

<para>
A <firstterm>syntactic</firstterm> criterion refers to the syntactic
contexts in which a word can occur. For example, assume that we have
already determined the category of nouns. Then  we might say that a
syntactic criterion for an adjective in English is that it can occur
immediately before a noun, or immediately following the words
<literal>be</literal> or <literal>very</literal>. According to these
tests, <literal>near</literal> should be categorized as an adjective:
<orderedlist>
<listitem>
<para>the near window</para>
</listitem>
<listitem>
<para>The end is (very) near.</para>
</listitem>
</orderedlist>
</para>

<para>A familier example of a <firstterm>notional</firstterm> criterion is
that a noun is <quote>the name of a person, place or thing</quote>.
Within modern linguistics, notional criteria for
word classes have be viewed with considerable suspicion, mainly because
they are hard to formalize. Nevertheless, notional criteria underpin
many of our intuitions about word classes, and enable us to make a good
guess about the categorization of words in languages that we are
unfamiliar with; that is, if we all we know about the Dutch
<literal>verjaardag</literal> is that it means the same as the English
word <literal>birthday</literal>, then we can guess that
<literal>verjaardag</literal> is a noun in Dutch. However, some care is
needed: although we might translate <literal>zij is van dag
jarig</literal> as <literal>it's her birthday today</literal>, the word
<literal>jarig</literal> is in fact an adjective in Dutch, and has no
exact equivalent in English.</para>

</section>

    <section id="tagging">
    <title> Computational Approaches to Tagging </title>
<!--
X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)
-->
   <para> When processing a text, it is often useful to associate
    auxiliary information with each token.  For example, we might
    want to label each token with its part of speech; or we might want
    to disambiguate homonyms by associating them with "word sense"
    labels.  This kind of auxiliary information is typically used in
    later stages of text processing.  For example, part of speech
    labels could be used to derive the internal structure of a
    sentence; and "word sense" labels could be used to allow a
    question-answering system to distinguish homonyms. </para>

    <para> The process of associating labels with each token in a text
    is called <glossterm>tagging</glossterm>, and the labels are
    called <glossterm>tags</glossterm>.  The collection of tags used
    for a particular task is known as a <glossterm>tag
    set</glossterm>. </para>
   
    <para> <glossterm>Part-of-speech tagging</glossterm> is the most
    common example of tagging, and it is the example we will examine
    in this tutorial.  But you should keep in mind that most of the
    techniques we discuss here can also be applied to many other
    tagging problems. </para>

    <para> Part-of-speech tags divide words into categories, based on
    how they can be combined to form sentences.  For example,
    articles can combine with nouns, but not verbs.  Part-of-speech
    tags also give information about the semantic content of a word.
    For example, nouns typically express "things," and prepositions
    express relationships between "things." </para>

    <para> Most part-of-speech tag sets make use of the same basic
    categories, such as "noun," "verb," "adjective," and
    "preposition."  However, tag sets differ both in how finely they
    divide words into categories; and in how define their categories.
    For example, "is" might be tagged as a verb in one tag set; but as
    a form of "to be" in another tag set.  This variation in tag sets
    is reasonable, since part-of-speech tags are used in different
    ways for different tasks. </para>

    <!-- How do I make "Table 1" not use a hard-coded number?? -->
    <para> In this tutorial, we will use the tag set listed in Table
    1.  This tag set is a simplification of the commonly used
    <glossterm>Brown Corpus tag set</glossterm>.  The complete Brown
    Corpus tag set has 87 basic tags.  For more information on tag
    sets, see <citetitle>Foundations of Statistical Natural Language
    Processing</citetitle>
    (<author><surname>Manning</surname></author> &amp;
    <author><surname>Schutze</surname></author>), pp. 139-145,
    or <citetitle>Speech and Language Processing</citetitle>
    (<author><surname>Jurafsky</surname></author> &amp;
    <author><surname>Martin</surname></author>) p. 297.
</para>

    <table id="table.tagset"> <title>Tag Set</title> 
      <tgroup cols="2"><tbody>
          <row>
            <entry>AT</entry>
            <entry>Article</entry>
          </row>
          <row>
            <entry>NN</entry>
            <entry>Noun</entry>
          </row>
          <row>
            <entry>VB</entry>
            <entry>Verb</entry>
          </row>
          <row>
            <entry>JJ</entry>
            <entry>Adjective</entry>
          </row>
          <row>
            <entry>IN</entry>
            <entry>Preposition</entry>
          </row>
          <row>
            <entry>CD</entry>
            <entry>Number</entry>
          </row>
          <row>
            <entry>END</entry>
            <entry>Sentence-ending punctuation</entry>
          </row>
        </tbody>
      </tgroup>
    </table>



    </section>

<!--
    <section> <title> Advanced Topics in Tagging (optional) </title>

    <para></para>

X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions


    </section>
-->

    <section> <title> Tagging in NLTK </title>

<!--
X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->



  <section id="basics">
      <title> The nltk.tagger Module </title>

    <para> The <ulink
    url="&refdoc;/nltk.tagger-module.html"
    ><literal>nltk.tagger</literal></ulink> module defines the classes
    and interfaces used by NLTK to perform tagging.  </para>

    <section id="basics.TaggedType"> <title> TaggedType </title>

      <para> NLTK defines a simple class, <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html"
      ><literal>TaggedType</literal></ulink>, for representing the text
      type of a tagged token.  A <literal>TaggedType</literal>
      consists of a <glossterm>base type</glossterm> and a
      <glossterm>tag</glossterm>.  Typically, the base type and the
      tag will both be strings.  For example, the tagged type for the
      noun "dog" would have the base type <literal>'dog'</literal> and
      the tag <literal>'NN'</literal>.  A tagged type with base type
      <replaceable>b</replaceable> and tag
      <replaceable>t</replaceable> is written
      <replaceable>b</replaceable><literal>/</literal><replaceable>t</replaceable>.
      Tagged types are created with the <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#__init__"
      ><literal>TaggedType</literal></ulink> constructor: </para>

<programlisting>
    &prompt;<command> ttype1 = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttype2 = TaggedType('runs', 'VB') </command>
    'runs'/'VB'
</programlisting>

      <para> A <literal>TaggedType</literal>'s base type is accessed
      via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#base"
      ><literal>base</literal></ulink> member function; and its tag is
      accessed via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#tag"
      ><literal>tag</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> ttype1.base() </command>
    'dog'
    &prompt;<command> ttype2.tag() </command>
    'VB'
</programlisting>

      <para> To construct a tagged token, simply use the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal></ulink> constructor with a
      <literal>TaggedType</literal>: </para>

<programlisting>
    &prompt;<command> ttype = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttoken = Token(ttype, Location(5)) </command>
    'dog'/'NN'@[5]
</programlisting>

    </section> <!-- TaggedType -->

    <section id="basics.corpora"> <title> Reading Tagged Corpora </title>

      <para> Several large corpora (such as the Brown Corpus and
      portions of the Wall Street Journal) have been manually tagged
      with part-of-speech tags.  These corpora are primarily useful
      for testing taggers and for training statistical taggers.
      However, before we can use these corpora, we must read them from
      files and tokenize them. </para>

      <para> Tagged texts are usually stored in files as a sequences
      of whitespace-separated tokens, where each token is of the form
      <replaceable>base</replaceable><literal>/</literal><replaceable>tag</replaceable>.
      Figure 1 shows an example of some tagged text, taken from the
      Brown corpus. </para>

      <figure> <title> An Example of Tagged Text (excerpted from the Brown Corpus) </title>
        <titleabbrev>Example of Tagged Text</titleabbrev>
<screen>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt
it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb
generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in
the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.
</screen>
      </figure>

      <para> To tokenize tagged texts of this form, the
      <literal>nltk.tagger</literal> module defines the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedTokenizer-class.html"
      ><literal>TaggedTokenizer</literal></ulink> class: </para>

<programlisting>
    &prompt;<command> tagged_text_str = open('corpus.txt').read() </command>
    'John/NN saw/VB the/AT book/NN on/IN the/AT 
     table/NN ./END  He/NN sighed/VB ./END'
    &prompt;<command> tokens = TaggedTokenizer().tokenize(tagged_text_str) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

      <para> If <literal>TaggedTokenizer</literal> encounters a word
      without a tag, it will assign it the default tag
      <literal>None</literal>. </para>

    </section> <!-- TaggedTokenizer -->

    <section id="basics.TaggerI"> <title> The TaggerI Interface </title>

      <para> The <literal>nltk.tagger</literal> module defines <ulink
      url="&refdoc;/nltk.tagger.TaggerI-class.html"
      ><literal>TaggerI</literal></ulink>, a general interface for
      tagging texts.  This interface is used by all taggers.  It
      defines a single method, <ulink
      url="&refdoc;/nltk.tagger.TaggerI-class.html#tag"
      ><literal>tag</literal></ulink>, which assigns a tag to each
      token in a list, and returns the resulting list of tagged
      tokens.</para>

<programlisting>
    &prompt;<command> untagged_text_str = open('untagged.txt').read() </command>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str) </command>
    ['John'@[0w], 'saw'@[1w], 'the'@[2w], 'book'@[3w], 
     'on'@[4w], 'the'@[5w], 'table'@[6w], '.'@[7w], 
     'He'@[8w], 'sighed'@[9w], '.'@[10w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

    </section> <!-- TaggerI -->

  </section> <!-- The nltk.tagger module -->

  <section id="taggers"> <title> Taggers </title>

    <para> The <literal>nltk.tagger</literal> module currently defines
    four taggers; this list will likely grow in the future.  This
    section describes the taggers currently implemented by
    <literal>nltk.tagger</literal>, and how they are used. </para>
    
    <section id="taggers.default"> <title> A Default Tagger </title>

      <para> The simplest tagger defined by
      <literal>nltk.tagger</literal> is <ulink
      url="&refdoc;/nltk.tagger.NN_CD_Tagger-class.html"
      ><literal>NN_CD_Tagger</literal></ulink>.  This tagger assigns a
      tag to each token on the basis of its type.  If its type appears
      to be a number, it assigns the type "CD."  Otherwise, it assigns
      the type "NN." </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str) </command>
    ['John'@[0w], 'saw'@[1w], '3'@[2w], 
     'polar'@[3w], 'bears'@[4w], '.'@[5w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'NN'@[1w], '3'/'CD'@[2w], 
     'polar'/'NN'@[3w], 'bears'/'NN'@[4w], '.'/'NN'@[5w]]
</programlisting>

      <para> This is a simple algorithm, but it yields quite poor
      performance when used by itself.  On a typical corpus, it will
      tag only 20%-30% of the tokens correctly.  However, it is a very
      reasonable tagger to use as a default, if a more advanced tagger
      fails to determine a token's tag.  When used in conjunction with
      other taggers, <literal>NN_CD_Tagger</literal> can significantly
      improve performance. </para>
      
    </section> <!-- NN_CD_Tagger -->

    <section id="taggers.unigram"> <title> Unigram Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger-class.html"
      ><literal>UnigramTagger</literal></ulink> class implements a
      simple statistical tagging algorithm: for each token, it assigns
      the tag that is most likely for that token's type.  For example,
      it will assign the tag "JJ" to any occurrence of the word
      "frequent," since "frequent" is used as an adjective (e.g. "a
      frequent word") more often than it is used as a verb (e.g. "I
      frequent this cafe"). </para>

      <para> Before a <literal>UnigramTagger</literal> can be used to
      tag data, it must be trained on a <glossterm>training
      corpus</glossterm>.  It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger-class.html#train"
      ><literal>train</literal></ulink> method, which takes a
      tagged corpus: </para>

<programlisting>
    <emphasis># 'train.txt' is a tagged training corpus</emphasis>
    &prompt;<command> tagged_text_str = open('train.txt').read()</command>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize(tagged_text_str)</command>
    &prompt;<command> tagger = UnigramTagger()</command>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once a <literal>UnigramTagger</literal> has been trained,
      the <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#tag"
      ><literal>tag</literal></ulink> can be used to tag
      untagged corpora: </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str)</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>UnigramTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose type was not
      encountered in the training data. </para>

      <para> Note that, like almost all statistical taggers, the
      performance of <literal>UnigramTagger</literal> is highly
      dependent on the quality of its training set.  In particular, if
      the training set is too small, it will not be able to reliably
      estimate the most likely tag for each word.  Performance will
      also suffer if the training set is significantly different than
      the texts we wish to tag. </para>
      
    </section> <!-- UnigramTagger -->

    <section id="taggers.nthorder"> <title> Nth Order Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.NthOrderTagger-class.html"
      ><literal>NthOrderTagger</literal></ulink> class implements a
      more advanced statistical tagging algorithm.  In addition to
      considering the token's type, it also considers the
      part-of-speech tags of the <replaceable>n</replaceable> preceding
      tokens. </para>

      <para> To decide which tag to assign to a token,
      <literal>NthOrderTagger</literal> first constructs a
      <glossterm>context</glossterm> for the token.  This context
      consists of the token's type, along with the part-of-speech tags
      of the <replaceable>n</replaceable> preceding tags.  It then
      picks the tag which is most likely for that context.  Note that
      a 0th order tagger is equivalent to a unigram tagger, since the
      context used to tag a token is just its type.  1st order taggers
      are sometimes called <glossterm>bigram taggers</glossterm>, and
      2nd order taggers are called <glossterm>trigram
      taggers</glossterm>. </para>

      <para> <literal>NthOrderTagger</literal> uses a tagged training
      corpus to determine which part-of-speech tag is most likely for
      each context:</para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize(tagged_text_str)</command>
    &prompt;<command> tagger = NthOrderTagger(3)</command>         <emphasis># 3rd order tagger</emphasis>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once an <literal>NthOrderTagger</literal> has been trained,
      it can be used to tag untagged corpora: </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str)</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>NthOrderTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose context was not
      encountered in the training data. </para>

      <para> Note that as <replaceable>n</replaceable> gets larger,
      the specificity of the contexts increases; and with it, the
      chance that the data we wish to tag will contain contexts that
      were not present in the training data.  Thus, there is a
      trade-off between the accuracy and the coverage of our results.
      This is a common type of trade-off in natural language
      processing.  It is closely related to the
      <glossterm>precision/recall trade-off</glossterm> that we'll
      encounter later when we discuss information retrieval. </para>

    </section> <!-- NthOrderTagger -->

    <section id="tagger.backoff"> <title> Combining Taggers </title>

      <para> One way to address the trade-off between accuracy and
      coverage is to use the more accurate algorithms when we can, but
      to fall back on algorithms with wider coverage when necessary.
      For example, we could combine the results of a 1st order tagger,
      a 0th order tagger, and an <literal>NN_CD_Tagger</literal>, as
      follows:</para>

      <orderedlist>
        <listitem> 
          <para> Try tagging the token with the 1st order
          tagger. </para>
        </listitem>
        <listitem> 
          <para> If the 1st order tagger is unable to find a tag for
          the token, try finding a tag with the 0th order
          tagger. </para>
        </listitem> 
        <listitem> 
          <para> If the 0th order tagger is also unable to find a tag,
          use the <literal>NN_CD_Tagger</literal> to find a tag. </para>
        </listitem>
      </orderedlist>

      <para> NLTK defines the <ulink
      url="&refdoc;/nltk.tagger.BackoffTagger-class.html"
      ><literal>BackoffTagger</literal></ulink> class for combining
      taggers in this way.  A <literal>BackoffTagger</literal> is
      constructed from an ordered list of one or more
      <glossterm>subtaggers</glossterm>.  For each token in the input,
      the <literal>BackoffTagger</literal> uses the result of the
      first tagger in the list that successfully found a tag.  Taggers
      indicate that they are unable to tag a token by assigning it the
      special tag <literal>None</literal>.  We can use a
      <literal>BackoffTagger</literal> to implement the strategy
      proposed above: </para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize(tagged_text_str)</command>

    <emphasis># Construct the taggers</emphasis>
    &prompt;<command> tagger1 = NthOrderTagger(1)</command>         <emphasis># 1st order tagger</emphasis>
    &prompt;<command> tagger2 = UnigramTagger()</command>           <emphasis># 0th order tagger</emphasis>
    &prompt;<command> tagger3 = NN_CD_Tagger()</command>

    <emphasis># Train the taggers</emphasis>
    &prompt;<command> tagger1.train(train_toks)</command>
    &prompt;<command> tagger2.train(train_toks)</command>

    <emphasis># Combine the taggers</emphasis>
    &prompt;<command> tagger = BackoffTagger([tagger1, tagger2, tagger3])</command>
</programlisting>

      <para> Note that the order in which the taggers are given to
      <literal>BackoffTagger</literal> is important: the taggers
      should be listed in the order that they should be tried.  This
      typically means that more specific taggers should be listed
      before less specific taggers. </para>

      <para> Having defined a combined tagger, we can use it to tag
      new corpora: </para>

<programlisting>
    &prompt;<command> tokens = TaggedTokenizer().tokenize(untagged_text_str)</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>
      
    </section> <!-- Combining Tagger -->

  </section> <!-- Taggers -->

  <section id="impl"> <title> Tagging: A Closer Look </title>

    <para> In the next five sections, we will discuss how each of the
    taggers introduced in the previous section are implemented.  This
    discussion serves several purposes: </para>

    <itemizedlist>
      <listitem>
        <para> It demonstrates how to write classes implementing the
        interfaces defined by NLTK. </para>
      </listitem>
      <listitem>
        <para> It provides you with a better understanding of the
        algorithms and data structures underlying each approach to
        tagging. </para>
      </listitem>
      <listitem>
        <para> It gives you a chance to see some of the code used to
        implement NLTK.  We have tried hard to ensure that the
        implementation of every class in NLTK is easy to understand.
        </para>
      </listitem>
    </itemizedlist>

    <para> Before you read this section, you may wish to read the
    tutorial "<ulink url="&tutdoc;/writing_classes/index.html">
    Writing Classes For NLTK"</ulink>, which describes how to create
    classes that interface with the toolkit. </para>

  </section> <!-- Tagging: A Closer Look -->

  <section id="sequentialtagger"> <title> Sequential Taggers </title>

    <para> The four taggers discussed in this tutorial are implemented
    as sequential taggers.  A <glossterm>sequential tagger</glossterm>
    is a tagger that: </para>

    <orderedlist>
      <listitem><para> Assigns tags to one token at a time, starting
      with the first token of the text, and proceeding in sequential
      order. </para> </listitem>
      <listitem><para> Decides which tag to assign a token on the
      basis of that token, the tokens that preceed it, and the
      predicted tags for the tokens that preceed it. </para>
      </listitem>
    </orderedlist>

    <para> To capture this commonality, we define a common base class,
    <ulink url="&refdoc;/nltk.tagger.SequentialTagger-class.html"
    ><literal>SequentialTagger</literal></ulink>.  This base class
    defines <literal>tag</literal> using a new method,
    <ulink url="&refdoc;/nltk.tagger.SequentialTagger-class.html#tag_next"
    ><literal>tag_next</literal></ulink>, which returns the appropriate
    tag for the next token.  However,
    <literal>SequentialTagger</literal> does not implement this new
    method itself.  Instead, each tagger subclass provides its own
    implementation. </para>

    <para> In addition to capturing the commonality between the four
    taggers, the <literal>SequentialTagger</literal> class has another
    advantage: it will allow us to define
    <literal>BackoffTagger</literal> in such a way that each subtagger
    can use the predictions made by the other taggers as context for
    deciding which tags to assign.  See <xref
    linkend="backoff_impl"></xref> for more
    details.</para>

    <section id="sequentialtagger.tag_next"> <title> SequentialTagger.tag_next </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.SequentialTagger-class.html#tag_next"
      ><literal>tag_next</literal></ulink> method decides which tag to
      assign a token, given the list of tagged tokens that preceeds
      it.  It takes two arguments: a list of tagged tokens preceeding
      the token to be tagged, and the token to be tagged; and it
      returns the appropriate tag for that token. </para>

    </section> <!-- tag_next -->

    <section id="sequentialtagger.tag"> <title> SequentialTagger.tag </title>

      <para> The implementation of the <literal>tag</literal> method
      is relatively streight forward.  It simply loops through the
      untagged text, calling <literal>tag_next</literal> for each
      token.  It uses the result of each call to
      <literal>tag_next</literal> to create a tagged version of that
      token, and collects these together to form the tagged
      text. </para>

<programlisting>
    <command>def tag(self, text):</command>
        tagged_text = []

        for token in text:
            tag = self.next_tag(tagged_text, token)
            tagged_token = Token(TaggedType(token.type(), tag), token.loc())
            tagged_text.append(tagged_token)

        return tagged_text
</programlisting>

    </section> <!-- tag -->

    <section id="sequentialtagger.impl"> <title> The SequentialTagger Implementation </title>

      <para> The complete listing for
      <literal>SequentialTagger</literal> is: </para>

      <figure><title>The SequentialTagger Implementation</title>
<programlisting>
<command>class SequentialTagger(TaggerI):</command>

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        assert 0, "next_tag not defined by SequentialTagger subclass"

    <command>def tag(self, text):</command>
        tagged_text = []

        <emphasis># Tag each token, in sequential order.</emphasis>
        for token in text:
            <emphasis># Get the tag for the next token.</emphasis>
            tag = self.next_tag(tagged_text, token)

            <emphasis># Use tag to build a tagged token, and add it to tagged_text.</emphasis>
            tagged_token = Token(TaggedType(token.type(), tag), token.loc())
            tagged_text.append(tagged_token)

        return tagged_text
</programlisting>
      </figure>
      
      <para> Note that SequentialTagger requires that subclasses
      define the <literal>tag_next</literal> method; otherwise, the
      <literal>assert</literal> statement will raise an
      exception when the user tries to tag a text. </para>

    </section>

    <section id="sequentialtagger.subclasses"> <title> Subclasses </title>

      <para> The next four sections show how the
      <literal>SequentialTagger</literal> base class can be used to
      define <literal>NN_CD_Tagger</literal>,
      <literal>UnigramTagger</literal>,
      <literal>NthOrderTagger</literal>, and
      <literal>BackoffTagger</literal>.</para>

    </section> <!-- Subclasses -->

  </section> <!-- SequentialTagger -->

  <section id="nncd_impl"> <title> NN_CD_Tagger </title>

    <para> <literal>NN_CD_Tagger</literal> assigns the tag
    <literal>"CD"</literal> to any token whose type appears to be a
    number; and <literal>"NN"</literal> to any other token.  It uses a
    simple regular expression to test whether a token's type is a
    number:</para>

<programlisting>
    r'^[0-9]+(.[0-9]+)?$'
</programlisting>

    <para> This regular expression matches one or more digits, followed
    by an optional period and one or more digits (e.g.,
    "<literal>12</literal>" or "<literal>732.42</literal>").  Note the
    use of "<literal>^</literal>" (which matches the beginning of a
    string) and "<literal>$</literal>" (which matches the end of a
    string) to ensure that the regular expression will only match
    complete token types. </para>

    <para> Since <literal>NN_CD_Tagger</literal> is a subclass of
    <literal>SequentialTagger</literal>, it just needs to define the
    <literal>next_tag</literal> method.  In the case of
    <literal>NN_CD_Tagger</literal>, the <literal>next_tag</literal>
    method is quite simple: </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        if re.match(r'^[0-9]+(.[0-9]+)?$', next_token.type()):
            return 'CD'
        else:
            return 'NN'
</programlisting>

    <para> Since <literal>NN_CD_Tagger</literal>s are stateless, and
    have no customization parameters, the <ulink
    url="&refdoc;/nltk.tagger.NN_CD_Tagger-class.html#__init__"
    ><literal>NN_CD_Tagger constructor</literal></ulink> is empty:
    </para>

<programlisting>
    <command>def __init__(self):</command> pass
</programlisting>

    <para> The complete listing for the
    <literal>NN_CD_Tagger</literal> class is:</para>

    <figure><title>The NN_CD_Tagger Implementation</title>
<programlisting> 
<command>class NN_CD_Tagger(SequentialTagger):</command>

    <command>def __init__(self):</command> pass

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Assign the 'CD' tag for numbers; and 'NN' for anything else.</emphasis>
        if re.match(r'^[0-9]+(.[0-9]+)?$', next_token.type()):
            return 'CD'
        else:
            return 'NN'
</programlisting>
    </figure>

    <para> Note that <literal>NN_CD_Tagger</literal> does
    <emphasis>not</emphasis> define <literal>tag</literal>.  When the
    <literal>tag</literal> method is called, the definition given by
    <literal>SequentialTagger</literal> will be used. </para>

  </section> <!-- NN_CD_Tagger -->

  <section id="unigram_impl"> <title> UnigramTagger </title>

    <para> <literal>UnigramTagger</literal> tags each token with the
    tag that is most likely to go with the token's type.  It uses a
    training corpus to decide which tag is most likely for each type.
    In particular, it assumes that the tag that occurs most frequently
    with a type is the most likely tag for that type.  For example, if
    the training corpus contains the word "track" as a noun 18 times,
    and as a verb 7 times, then it will assign the noun tag to any
    tokens whose type is "track." 
        <footnote> <para> We considered using a conditional
        probability distribution, instead of a conditional frequency
        distribution.  However, for most probability distributions,
        the maximum probability sample is always equal to the maximum
        frequency sample in the underlying frequency distributions.
        We decided that the additional complexity involved in using
        <literal>ConditionalProbDist</literal> was not justified. </para>
        </footnote></para>

    <para> UnigramTagger uses a <literal>ConditionalFreqDist</literal>
    to record the most likely tag for each type.  
        <footnote><para>See the <ulink
        url="&tutdoc;/probability/index.html">probability
        tutorial</ulink> for information about constructing and using
        frequency distributions.</para> </footnote>
    The <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#train"
    ><literal>train</literal></ulink> method constructs this
    conditional frequency distribution from a training corpus. </para>

    <section id="unigram_impl.training"> <title> Training the Unigram Tagger </title>

      <para> Tagging is a prediction problem.  In particular, the
      outcome we are interested in is the tag; and the context that we
      will use to predict the outcome is the token's type.  So we will
      construct a <literal>ConditionalFreqDist</literal> whose samples
      are tags, and whose conditions are token types: </para>

<programlisting>
    <command>def train(self, tagged_tokens):</command>
        for token in tagged_tokens:
            outcome = token.type().tag()
            context = token.type().base()
            self._freqdist[context].inc(outcome)
</programlisting>

    </section> <!-- Training -->

    <section id="unigram_impl.tagging"> <title> Tagging with the Unigram Tagger </title>
      
      <para> To find the most likely tag for a given token, we can use
      the the <ulink
      url="&refdoc;/nltk.probability.ConditionalFreqDist-class.html#__getitem__">indexing
      operator</ulink> to access the <literal>FreqDist</literal> for
      the appropriate context; and use the <ulink
      url="&refdoc;/nltk.probability.FreqDist-class.html#max"
      ><literal>max</literal></ulink> method to find the most likely
      outcome for that frequency distribution.  For example, we could
      find the most likely tag for the base type "bank" as
      follows:</para>

<programlisting>
    &prompt;<command> freqdist['bank'].max()</command>
    'NN'
</programlisting>

      <para> The <literal>next_tag</literal> method must decide which
      tag is most likely for a given token.  It simply consults the
      tagger's conditional frequency distribution to find the tag that
      is most likely for the tokens's type. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        context = next_token.type()
        return self._freqdist[context].max()
</programlisting>

      <note> <para> If a context was not encountered in the training
      corpus, then the frequency distribution for that context will be
      empty; so <literal>max()</literal> will return
      <literal>None</literal>.  Thus, <literal>next_tag</literal> will
      return <literal>None</literal> as for any token whose type was
      not encountered in the training corpus. </para> </note>
      
    </section> <!-- Tagging Words -->

    <section id="unigram_impl.init"> <title> Initializing the Unigram Tagger </title>

      <para> The constructor for <literal>UnigramTagger</literal>
      simply initializes <literal>self._freqdist</literal> with a new
      conditional frequency distribution.  </para>

<programlisting>
    <command>def __init__(self):</command>
        self._freqdist = probability.ConditionalFreqDist()
</programlisting>

    </section> <!-- Initializing the UnigramTagger -->

    <section id="unigram_impl.impl"><title>The UnigramTagger Implementation</title>

      <para> The complete listing for the
      <literal>UnigramTagger</literal> class is:</para>
      
      <figure><title>The UnigramTagger Implementation</title>
<programlisting> 
<command>class UnigramTagger(TaggerI):</command>
class UnigramTagger(SequentialTagger):
    <command>def __init__(self):</command>
        self._freqdist = ConditionalFreqDist()
    
    <command>def train(self, tagged_tokens):</command>
        for token in tagged_tokens:
            context = token.type().base()
            feature = token.type().tag()
            self._freqdist[context].inc(feature)

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        context = next_token.type()
        return self._freqdist[context].max()
</programlisting>
      </figure>
    </section> <!-- UnigramTagger Implementation -->
  </section> <!-- UnigramTagger -->

  <section id="nthorder_impl" xreflabel="NthOrderTagger"> 
    <title> NthOrderTagger </title>

    <para> The <literal>NthOrderTagger</literal> is a generalization
    of the <literal>UnigramTagger</literal>.  Instead of using the
    token's base type as a context, it uses a tuple consisting of the
    token's base type and the tags of the <replaceable>n</replaceable>
    preceding tokens.  This generalization creates two new
    issues. </para>

    <para> First, we must decide how to handle the first
    <replaceable>n</replaceable> tokens, since they do not have
    <replaceable>n</replaceable> preceding tokens.  

    <literal>NthOrderTagger</literal> simply uses the tags that are
    available.  For example, in a 3rd order tagger, the context of the
    second token will contain only the token's type and the first
    token's tag.  Another option would be to simply ignore the first
    <replaceable>n</replaceable> tokens.  As it turns out, which
    approach we take will not have much of an impact, since
    <replaceable>n</replaceable> (the order of the tagger) is
    generally much less than
    <replaceable>n<subscript>train</subscript></replaceable> (the
    number of training samples). </para>

    <para> The second issue is that, when tagging a text, we do not
    have access the the actual tags of the
    <replaceable>n</replaceable> preceding tokens.  However, we do
    have access to our predicted values for these tags.
    <literal>NthOrderTagger</literal> uses these predicted tags, since
    they are likely to be correct.  Assuming that our predictions are
    good, the use of predicted tags instead of actual tags will have a
    relatively minor impact on performance. </para>

    <section id="nthorder_impl.init"> <title> Initializing the Nth Order Tagger</title>

      <para> Having addressed these two issues, we can examine the
      implementation of the <literal>NthOrderTagger</literal>.  The
      constructor simply records <replaceable>n</replaceable>, and
      constructs a new conditional frequency distribution: </para>

<programlisting>
    <command>def __init__(self, n):</command>
        self._n = n
        self._freqdist = probability.ConditionalFreqDist()
</programlisting>

    </section> <!-- NthOrderTagger Constructor -->

    <section id="nthorder_impl.train"> <title> Training the Nth Order Tagger </title>

      <para> To train the <literal>NthOrderTagger</literal>, we
      examine each token, and increment the count of the tag for the
      appropriate context.  For contexts, we use a tuple consisting of
      the <replaceable>n</replaceable> previous tags and the current
      token's base type.  We use a variable called
      <literal>prev_tags</literal> to record the rpevious
      <replaceable>n</replaceable> tags; and update it after examining
      each token. </para>

<programlisting>
    <command>def train(self, tagged_tokens):</command>
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = []
        
        for token in tagged_tokens:
            context = tuple(prev_tags + [token.type().base()])
            feature = token.type().tag()
            self._freqdist[context].inc(feature)

            <emphasis># Update prev_tags</emphasis>
            prev_tags.append(token.type().tag())
            if len(prev_tags) == (self._n+1):
                del prev_tags[0]
</programlisting>

    </section> <!-- NthOrderTagger.train -->

    <section id="nthorder_impl.tag"> <title> Tagging with the Nth Order Tagger </title>

      <para> As with the <literal>UnigramTagger</literal>, we can find
      the most likely tag for each token by using the
      <literal>max</literal> method for the frequency distribution
      with the appropriate context.  But instead of using each token's
      base type as a context, we use a tuple consisting of the
      <replaceable>n</replaceable> previous predicted tags and the
      token's base type. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the tags of the n previous tokens.</emphasis>
        prev_tags = []
        start = max(len(tagged_tokens) - self._n, 0)
        for token in tagged_tokens[start:]:
            prev_tags.append(token.type().tag())

        <emphasis># Return the most likely tag for the token's context.</emphasis>
        context = tuple(prev_tags + [next_token.type()])
        return self._freqdist[context].max()
</programlisting>

    </section> <!-- NthOrderTagger.tag -->

    <section id="nthorder_impl.impl"><title>The NthOrderTagger Implementation</title>

      <para> The complete listing for the
      <literal>NthOrderTagger</literal> class is:</para>
      
      <figure><title>The NthOrderTagger Implementation</title>
<programlisting> 
<command>class NthOrderTagger(SequentialTagger):</command>
    <command>def __init__(self, n):</command>
        self._n = n
        self._freqdist = CFFreqDist()

    <command>def train(self, tagged_tokens):</command>
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = []
        
        for token in tagged_tokens:
            context = tuple(prev_tags + [token.type().base()])
            feature = token.type().tag()
            self._freqdist[context].inc(feature)

            <emphasis># Update prev_tags</emphasis>
            prev_tags.append(token.type().tag())
            if len(prev_tags) == (self._n+1):
                del prev_tags[0]

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the tags of the n previous tokens.</emphasis>
        prev_tags = []
        start = max(len(tagged_tokens) - self._n, 0)
        for token in tagged_tokens[start:]:
            prev_tags.append(token.type().tag())

        <emphasis># Return the most likely tag for the token's context.</emphasis>
        context = tuple(prev_tags + [next_token.type()])
        return self._freqdist[context].max()
</programlisting>
      </figure>

    </section> <!-- NthOrderTagger Implementation -->

  </section> <!-- NthOrderTagger -->

  <section id="backoff_impl" xreflabel="BackoffTagger"> 
    <title> BackoffTagger </title>

    <para> The <literal>BackoffTagger</literal> is used to combine the
    results of a list of <glossterm>subtaggers</glossterm>.  For each
    token to be tagged, the <literal>BackoffTagger</literal> consults
    each subtagger, in order.  Each token is assigned the first
    non-<literal>None</literal> tag returned by a subtagger for that
    token.  If all of the subtaggers return the tag
    <literal>None</literal> for a token, then
    <literal>BackoffTagger</literal> will assign it the tag
    <literal>None</literal>. </para>

    <section id="backoff_impl.init"> <title> Initializing a Backoff Tagger </title>

    <para> The <literal>BackoffTagger</literal> constructor simply
    records the list of subtaggers. </para>

<programlisting>
    <command>def __init__(self, subtaggers):</command>
        self._taggers = subtaggers
</programlisting>

    </section> <!-- Initializing a BackoffTagger -->

    <section id="backoff_impl.tag"> <title> Tagging with the Backoff Tagger </title>

      <para> The implementation of <literal>BackoffTagger</literal> is
      relatively straight-forward.  Its <literal>next_tag</literal>
      method simply calls each subtagger's <literal>next_tag</literal>
      method, in order; and returns the first
      non-<literal>None</literal> tag produced by a subtagger. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        for subtagger in self._subtaggers:
            tag = subtagger.next_tag(tagged_tokens, next_token)
            if tag is not None:
                return tag

        <emphasis># Default to None if all subtaggers return None. </emphasis>
        return None
</programlisting>    
    
    </section> <!-- BackoffTagger tagging -->

    <section id="backoff_impl.impl"><title>The BackoffTagger Implementation</title>

      <para> The complete listing for the
      <literal>BackoffTagger</literal> class is:</para>
      
      <figure><title>The BackoffTagger Implementation</title>
<programlisting> 
<command>class BackoffTagger(SequentialTagger):</command>
    <command>def __init__(self, subtaggers):</command>
        self._subtaggers = subtaggers

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        for subtagger in self._subtaggers:
            tag = subtagger.next_tag(tagged_tokens, next_token)
            if tag is not None:
                return tag

        <emphasis># Default to None if all subtaggers return None. </emphasis>
        return None
</programlisting>
      </figure>
    </section> <!-- BackoffTagger impl -->
  </section> <!-- BackoffTagger -->

  <section id="exercises">
    <title>Exercises</title>

    <section id="exercises.combine">
      <title> Combining Taggers with BackoffTagger</title>

      <para> There is typically a trade-off between the accuracy and
      coverage for taggers: taggers that use more specific contexts
      usually produce more accurate results, when they have seen those
      contexts in the training data; but because the training data is
      limited, they are less likely to encounter each context.  The
      <literal>BackoffTagger</literal> addresses this problem by
      trying taggers with more specific contexts first; and falling
      back to the more general taggers when necessary.  In this
      exercise, we examine the effects of using
      <literal>BackoffTagger</literal>. </para>
      
      <orderedlist>
        
        <listitem> <para> Create an <literal>NN_CD_Tagger</literal>, a
        <literal>UnigramTagger</literal>, and a
        <literal>NthOrderTagger</literal>.  Train the
        <literal>UnigramTagger</literal>, and the
        <literal>NthOrderTagger</literal> using a tagged section of
        the Brown corpus.</para> </listitem>

        <listitem>
        <para> Test the performance of each tagger, using a tagged
        section of the Brown corpus.  Record the
        <glossterm>accuracy</glossterm> of the tagger (the
        percentage of tokens that are correctly tagged).  Be sure to
        use a different section of the corpus for testing than you
        used for training. </para> </listitem>
        
        <listitem> <para> Use <literal>BackoffTagger</literal> to
        create three different combinations of the basic taggers.
        Test the accuracy of each combined tagger.  Which
        combinations give the most improvement?  </para>
        </listitem>

        <listitem> <para> Try repeating steps 1-3 with a different
        sized training corpus.  How does it affect your results? </para>
        </listitem>
      </orderedlist>
    </section> <!-- Combine -->
        
    <section id="exercises.context">
      <title> Tagger Context </title>
      
      <para> <literal>NthOrderParser</literal> chooses a tag for a
      token based on its type and the tags of the
      <replaceable>n</replaceable> preceeding tokens.  This is a
      common context to use for parsing, but ceratinly not the only
      possible context. </para>

      <para> Construct a new tagger, subclassed from
      <literal>SequentialTagger</literal>, that uses a different
      context.  If your tagger's context contains multiple elements,
      then you should combine them in a <literal>tuple</literal> (see
      <xref linkend="nthorder_impl"></xref> for an example of this).
      Some possibilities for elements to include are: </para>

      <itemizedlist>
        <listitem> <para> The base type of the current token, or of
        a previous token. </para> </listitem>
        <listitem> <para> The length of the current token's type, or
        of a previous token's type. </para> </listitem>
        <listitem> <para> The first letter of the current token's
        type, or of a previous token's type. </para> </listitem>
        <listitem> <para> The tag a previous token. </para>
        </listitem>
      </itemizedlist>

      <para> Try to choose context elements that you believe will help
      the tagger decide which tag is appropriate.  Keep in mind the
      trade-off between more specific taggers with accurate results;
      and more general taggers with broader coverage. </para>

      <para> Use <literal>BackoffTagger</literal> to combine your
      tagger with other taggers.  How does the combined tagger's
      accuracy compare to the basic tagger?  How does the combined
      tagger's accuracy compare to the combined taggers you created
      in the previous exercise? </para>
    </section> <!-- Tagger Context -->

    <section id="exercises.reverse">
      <title> Reverse Sequential Taggers </title>
      
      <para> Since sequential taggers tag tokens in order, one at a
      time, they can only use the predicted tags to the
      <emphasis>left</emphasis> of the current token to decide what
      tag to assign to a token.  But in some cases, the
      <emphasis>right</emphasis> context can provide more information
      about what tag should be used.  A <glossterm>reverse sequential
      tagger</glossterm> is a tagger that: </para>

      <orderedlist>
        <listitem><para> Assigns tags to one token at a time, starting
        with the last token of the text, and proceeding in
        right-to-left order. </para> </listitem>
        <listitem><para> Decides which tag to assign a token on the
        basis of that token, the tokens that follow it, and the
        predicted tags for the tokens that follow it. </para>
        </listitem>
      </orderedlist>

      <para> There is no need to create new classes to perform reverse
      sequential tagging.  By reversing texts at appropriate times, we
      can use sequential tagging classes to perform reverse sequential
      tagging.  In particular, we should reverse the training text
      before we train the tagger; and reverse the text that we wish to
      tag both before and after we use the sequential tagger. </para>

      <para> Use this technique to create a first order reverse
      sequential tagger.  Measure its accuracy on a tagged section of
      the Brown corpus.  Be sure to use a different section of the
      corpus for testing than you used for training.  How does its
      accuracy compare to a first order sequential tagger, using the
      same training data and test data? </para>
    </section> <!-- Reverse -->

    <section id="example.restart">
      <title> Processing Individual Sentences </title>

      <para> [to be written] Write a modified nth order tagger, that
      ignores tags that are in a previous sentence.  E.g., for a 3nd
      order tagger, if the previous 3 words were "dog/NN ./.  A/DT",
      then just use "DT" and the current token as context. </para>
    </section> <!-- restart at sentences -->

    <section id="example.backoff">
      <title> Alternatives to Backoff </title>

      <para> [to be written] Create a new kind of tagger that combines
      2 or more subtaggers. </para>
    </section> <!-- Alternatives to backoff -->

  </section> <!-- Exercises -->
  </section> <!-- Tagging in NLTK -->

  &index;
</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

