<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "http://nltk.sourceforge.net/ref">
<!ENTITY tutdoc "http://nltk.sourceforge.net/tutorial">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Prompts for Python code samples -->
<!ENTITY prompt "<prompt>&gt;</prompt><prompt>&gt;</prompt><prompt>&gt;</prompt>">
<!ENTITY prompt2 "<prompt>...</prompt>">

<!ENTITY ling "<prompt>...</prompt>">
]>

<article><title>Words: Tokenization and Tagging</title>


<section id="intro">
    <title>Introduction</title>


<para> This chapter addresses the following questions: How do we know
that piece of text is a <glossterm>word</glossterm>? And once we know
that something is a word, what linguistic
<glossterm>category</glossterm> should it be assigned to?
</para>

<para> It might seem needlessly picky to ask what a word is. Can't we
just say that a word is a string of characters which has white space
before and after it?  However, it turns out that things are quite a bit
more complex. To get a flavour of the problems, consider the following text.
<example id="wsj_0034">
	<title>Paragraph 12 from <filename>wsj_0034</filename></title>
<literallayout>
It's probably worth paying a premium for funds that invest in markets
that are partially closed to foreign investors, such as South Korea,
some specialists say.  But some European funds recently have
skyrocketed; Spain Fund has surged to a startling 120% premium.  It has
been targeted by Japanese investors as a good long-term play tied to
1992's European economic integration.  And several new funds that aren't
even fully invested yet have jumped to trade at big premiums.

"I'm very alarmed to see these rich valuations," says Smith Barney's
Mr. Porter.
</literallayout>
</example>

<!-- We note that a string  such as <literal
lang="en">doesn't</literal>. First, a very picky point: if this had
occurred as the first word in the first sentence of the text, then it
would not be preceded by white space! -->

Let's start with the string <literal>aren't</literal>. Should we treat it
as one word or two? We could imagine a situation, for example, where we
wanted to check whether all the words in our text occurred in a
dictionary, and our dictionary had entries for <literal
lang="en">are</literal> and <literal lang="en">not</literal>, but not
for <literal lang="en">aren't</literal>.  We can make a similar point
about <literal>1992's</literal>, except in this case, we would not expect
to find <literal>1992</literal> in a dictionary.

Conversely, we might want to look up names of places in a gazeteer (a
list of places together with their associated geographic location), and
in this case, we would probably want to treat <literal lang="en">South
Korea</literal> as a single <quote>word</quote>.
</para>

<para>
A slightly different challenge is raised by examples such as the following:
<orderedlist>
<listitem>
<para>This is a alpha-galactosyl-1,4-beta-galactosyl-specific adhesin.
</para>
</listitem>
<listitem>
<para>The corresponding free cortisol fractions in these sera were 4.53
+/- 0.15% and 8.16 +/- 0.23%, respectively.
</para>
</listitem>
</orderedlist>

In these cases, we encounter terms which are unlikely to be found in any
general purpose English lexicon. Moreover, we will have no success in
trying to syntactically analyse these strings using a standard grammar
of English. Now for some applications, we would like to<quote>bundle
up</quote> expressions such as
<literal>alpha-galactosyl-1,4-beta-galactosyl-specific adhesin</literal>
and <literal>4.53 +/- 0.15%</literal> so that they are presented as
unanalysable atoms to the parser. That is, we want to treat them as
single <quote>words</quote> for the purposes of subsequent processing.
</para>

<para>The upshot is that, even if we confine our attention to English
text, the question of what we treat as word may depend a great deal on
what our purposes are. If we turn to languages other than English,
segmenting words can be even more of a challenge. For example, in
Chinese orthography, characters correspond to monosyllabic
morphemes. Many morphemes are words in their own right, but many words
contain more than one morpheme; most of them consist of two
morphemes. However, there is no visual representation of word boundaries
in Chinese text.

<note><para>give example
</para>
</note>

</para>

<!--
<section>
      <title>Type and Tokens</title>
-->
<para>
Let's look in more detail at the words in the text <xref linkend="wsj_0034"/>.
Suppose we use white space as the delimiter for words, and then list all
the words; we would expect to get something like the following:
<informalexample>
<programlisting>
 120
 1992
 And
 Barney
 But
 European
 European
 Fund
 I
 It
 It
 Japanese
 Korea
 Mr
 Porter
Smith
 South
 Spain
 a
 a
...
</programlisting>
</informalexample>

<!--
We could also be slightly more clever, and produce a listing which 
<programlisting>
% tr -sc 'A-Za-z0-9' '\012' &lt; wsj_0034 | sort | uniq -c
      1 120
      1 1992
      1 And
      1 Barney
      1 But
      2 European
      1 Fund
      1 I
      2 It
      1 Japanese
      1 Korea
      1 Mr
      1 Porter
      1 Smith
      1 South
      1 Spain
      3 a
      1 alarmed
      1 are
      1 aren
...
</programlisting>

Words according to the Unix <command>wc</command>:
<programlisting>
wc -w wsj_0034_pars12-13
    90 wsj_0034_pars12-13
</programlisting>
-->

Now, it seems that we can make two different sorts of observations about
words in the text:
<itemizedlist>
	  <listitem>
	    <para>There are 90 words in <xref linkend="wsj_0034"/>.</para>
	  </listitem> 

	  <listitem>
	    <para>the word `European' occurs twice in <xref
		linkend="wsj_0034"/> </para></listitem>  
</itemizedlist>
A little reflection will show that we are using <quote>word</quote> in
two different senses, sometimes referred to as word
<emphasis>token</emphasis> versus word <emphasis>type</emphasis>:
      <indexterm><primary>word token</primary></indexterm>
      <indexterm><primary>word type</primary></indexterm>
<variablelist>
<varlistentry><term>Word token</term>
  <listitem><para>an occurrence of a word at a particular
  spatio-temporal location (e.g. a sequential position in a text</para></listitem>
</varlistentry>
<varlistentry><term>Word type</term> <listitem><para>a more abstract
notion, also termed <glossterm>lexeme</glossterm>
<indexterm><primary>lexeme</primary></indexterm>&mdash;we speak of two
tokens belonging to the same type.</para></listitem>
</varlistentry>
</variablelist>
More specifically, we want to say that there are 90 word tokens in <xref
linkend="wsj_0034"/>, but only 76 word types. For example, there are
   five tokens of the type <literal>to</literal>. This distinction
between types and tokens is fundamental, and we will see it being used
many times again.
</para>

<!-- <para> The term "word" can actually be used in two different ways: to refer
to an individual occurrence of a word; or to refer to an abstract
vocabulary item.  For example, the phrase "my dog likes his dog"
contains five occurrences of words, but only four vocabulary items
(since the vocabulary item "dog" appears twice).  In order to make this
distinction clear, we will use the term <glossterm>word
token</glossterm> to refer to occurrences of words, and the term
<glossterm>word type</glossterm> to refer to vocabulary
items. </para> -->

      <indexterm><primary>token</primary></indexterm>
      <indexterm><primary>type</primary></indexterm>
      <indexterm><primary>sentence token</primary></indexterm>
      <indexterm><primary>sentence type</primary></indexterm>
      
<para> The terms <glossterm>token</glossterm> and
   <glossterm>type</glossterm> can also be applied in other
   domains.  For example, a <glossterm>sentence token</glossterm>
   is an individual occurrence of a sentence; but a
   <glossterm>sentence type</glossterm> is an abstract sentence,
   without context.  If someone repeats a sentence twice, they have
   uttered two sentence tokens,
   but only one sentence type.  When
   the kind of token or type is obvious from context, we will
   simply use the terms <glossterm>token</glossterm> and
   <glossterm>type</glossterm>.
  </para>

    
</section><!-- Introduction -->

<section id="pos">
    <title>Parts of speech / word classes (English-oriented)</title>
<!-- 
X.2 linguistic overview (for non-linguist readers)
- how have linguists addressed the problem?
- what are the shortcomings of the non-computational approach?

% tr -sc 'A-Za-z0-9' '\012' &lt; wsj_0034 | sort 
-->
<para>There is a long tradition within linguistics of classifying
    words into different categories. These categories are also called
    <firstterm>parts of speech</firstterm>. Familiar examples are
    <type>noun</type>, <type>verb</type>, <type>preposition</type>,
    <type>adjective</type> and <type>adverb</type>. How do we know what
    category a word should belong to? In general, linguists invoke three
    kinds of criteria for making the decision:
    <itemizedlist>
     <listitem>
      <para>formal;</para>
     </listitem>
     <listitem>
      <para>syntactic (or distributional);</para>
     </listitem>
     <listitem>
      <para>notional (or semantic).</para>
     </listitem>
    </itemizedlist>

A <firstterm>formal</firstterm> criterion is one which looks at the
internal structure of a word. For example, <literal>-ness</literal> is a
suffix which combines with an adjective to produce a noun. Examples are
<literal>happy</literal> &gt; <literal>happiness</literal>,
<literal>ill</literal> &gt; <literal>illness</literal>. So if we
encounter a word which ends in <literal>-ness</literal>, this is very
likely to be a noun.
</para>

<para>
A <firstterm>syntactic</firstterm> criterion refers to the syntactic
contexts in which a word can occur. For example, assume that we have
already determined the category of nouns. Then  we might say that a
syntactic criterion for an adjective in English is that it can occur
immediately before a noun, or immediately following the words
<literal>be</literal> or <literal>very</literal>. According to these
tests, <literal>near</literal> should be categorized as an adjective:
<orderedlist>
<listitem>
<para>the near window</para>
</listitem>
<listitem>
<para>The end is (very) near.</para>
</listitem>
</orderedlist>
</para>

<para>A familier example of a <firstterm>notional</firstterm> criterion is
that a noun is <quote>the name of a person, place or thing</quote>.
Within modern linguistics, notional criteria for
word classes have be viewed with considerable suspicion, mainly because
they are hard to formalize. Nevertheless, notional criteria underpin
many of our intuitions about word classes, and enable us to make a good
guess about the categorization of words in languages that we are
unfamiliar with; that is, if we all we know about the Dutch
<literal>verjaardag</literal> is that it means the same as the English
word <literal>birthday</literal>, then we can guess that
<literal>verjaardag</literal> is a noun in Dutch. However, some care is
needed: although we might translate <literal>zij is van dag
jarig</literal> as <literal>it's her birthday today</literal>, the word
<literal>jarig</literal> is in fact an adjective in Dutch, and has no
exact equivalent in English.</para>

</section>






  <section id="regex"> 
    <title>Regular Expressions</title>


<section id="regex.overview">
<title>Overview</title>

    <para> This section provides an introduction to
    regular expressions illustrated with examples from language
    processing.
    <footnote>
     <para>For a more concise introduction please see the Python documentation
    <ulink url="http://www.python.org/doc/current/lib/module-re.html">
    re -- Regular expression operations</ulink>.</para></footnote>
    </para>

    <para> When written language is stored in a machine it is normally
    represented as a sequence (or <glossterm>string</glossterm>) of
    characters.  Individual words are strings.  A list of words is a
    string.  Entire texts are also strings, including special characters
    for space and newline.  Strings are sometimes formatted, such as a
    <quote>date string</quote> like <literal>2002-06-23</literal>.
    Whole texts may be formatted, such as an email message with header
    fields followed by the message body.  Texts may contain
    <quote>markup</quote>, such as <sgmltag
    class="starttag">abbrev</sgmltag>Phil <sgmltag
    class="endtag">abbrev</sgmltag>, which provides information about
    the interpretation or presentation of some piece of text.  Thus in
    language processing, strings are ubiquitous, and they often contain
    important structure.
    </para>

    <para> Most language processing is performed above the level of
    characters.  For instance, parsing a sentence is a process that
    operates at the level of complete words.  What kinds of processing
    are performed at the character level?  Perhaps word games are the
    most familiar example of such processing.  In completing a
    crossword we may want to know which 3-letter English words end
    with the letter <literal>c</literal>
    (e.g. <literal>arc</literal>).  We might want to know how many
    words can be formed from the letters: <literal>a</literal>,
    <literal>c</literal>, <literal>e</literal>, <literal>o</literal>,
    and <literal>n</literal> (e.g. <literal>ocean</literal>).  We may
    want to find out which unique English word contains the substring
    <literal>gnt</literal> (left as an exercise for the reader).  In
    all these examples, we are considering which word &mdash; drawn from a
    large set of candidates &mdash; matches a given pattern.  There are many
    more serious uses of this so-called <glossterm>pattern
    matching</glossterm>.  We may want to break a text into its
    component words, a process known as
    <glossterm>tokenization</glossterm> (see the <ulink
    url="&tutdoc;/introduction/index.html">introduction</ulink>).  For
    simple tokenization we want to search for locations in the text
    string containing whitespace (space, tab, or newline) or certain
    punctuation symbols, and break the text strings into word strings
    at these points.  Alternatively, we may want to compute the
    relative frequency of particular letters in the text in order to
    guess the language of the text.  For this we must total up the
    number of matches for each character of interest.
    </para>

    <para> So far we have seen elementary examples of pattern
    matching, the matching of individual characters.  More often we
    are interested in matching <emphasis>sequences</emphasis> of
    characters.  For example, part of the operation of a naive
    spell-checker could be to remove a word-final <literal>s</literal>,
    in case it is a plural, and see if the putative singular form
    exists in the dictionary.  For this we must locate
    <literal>s</literal> and remove it, but only if it precedes a word
    boundary.  This requires matching a pattern consisting of two
    characters.
    </para>

    <para> Beyond this pattern matching on the
    <emphasis>content</emphasis> of a text, we often want to process the
    <emphasis>formatting</emphasis> and <emphasis>markup</emphasis> of a
    text.  We may want to check the formatting of a document (e.g. to
    ensure that every sentence begins with a capital letter) or to
    reformat a document (e.g. replacing sequences of space characters
    with a single space).  We may want to find all date strings and
    extract the year.  We may want to extract all words contained inside
    the <sgmltag class="starttag">abbrev</sgmltag> <sgmltag
    class="endtag">abbrev</sgmltag> markup in order to construct a list
    of abbreviations.
    </para>

    <para> Processing the content, format and markup of strings is a
    central task in most kinds of NLP.  The most widespread method for
    string processing uses <glossterm>regular expressions</glossterm>.
    </para>

  </section> <!-- Overview -->

  <section id="regex.simple"> 
    <title> Simple Regular Expressions </title>

    <para>In this section we will see the building blocks for simple
    regular expressions, along with a selection of linguistic
    examples. We can think of a regular expression as a specialised
    notation for describing patterns that we want to match. In order to
    make explicit when we are talking about a pattern
    <varname>patt</varname>, we will use the notation
    &ulcorn;<literal>patt</literal>&drcorn;. The first thing to say
    about regular expressions is that most letters match themselves. For
    example, the pattern &ulcorn;<literal>sing</literal>&drcorn; exactly matches
    the string <literal>sing</literal>. In addition, regular expressions
    provide us with a set of <emphasis>special characters</emphasis>
    which give us a ways to match sets of strings, and we will now look
    at these.
    </para>

    <section id="regex.dot">
      <title>The Wildcard</title>

    <para> The "<literal>.</literal>" symbol is called a
     <firstterm>wildcard</firstterm>: it matches any single character.
      For example, the regular expression &ulcorn;<literal>s.ng</literal>&drcorn;
      matches the following English words:
        <literal>sang</literal>,
        <literal>sing</literal>,
        <literal>song</literal>, and
        <literal>sung</literal>.
      </para>

      <para> We can also use the wildcard symbol for counting characters.
      For instance &ulcorn;<literal>....zy</literal>&drcorn; matches six-letter strings
      that end in <literal>zy</literal>.  The pattern
      &ulcorn;<literal>....berry</literal>&drcorn; finds words like <literal>cranberry</literal>.
      </para>

      <note><para> Note that &ulcorn;<literal>.</literal>&drcorn; matches
      <emphasis>exactly</emphasis> one character, and must be repeated
      for as many characters as should be matched.  To match a
      variable number of characters we must use notation for
      <emphasis>optionality</emphasis>. </para></note>

    </section>

    <section id="regex.qmk">
      <title>Optionality</title>

      <para> The "<literal>?</literal>" symbol indicates that the
      immediately preceding regular expression is optional.  The regular
     expression
      &ulcorn;<literal>colou?r</literal>&drcorn; matches both British and American
      spellings, <literal>colour</literal> and
      <literal>color</literal>.  The expression that precedes the
      <literal>?</literal> may be punctuation, such as an optional
      hyphen.  For instance &ulcorn;<literal>e-?mail</literal>&drcorn; matches both
      <literal>e-mail</literal> and <literal>email</literal>.
      </para>

    </section>

    <section id="regex.plus">
      <title>Repeatability</title>

      <para> The "<literal>+</literal>" symbol indicates that the
      immediately preceding expression is repeatable, up to an
      arbitrary number of times.  For example, the regular expression
      &ulcorn;<literal>coo+l</literal>&drcorn; matches <literal>cool</literal>,
      <literal>coool</literal>, and so on.  This symbol is
      particularly effective when combined with the
      <literal>.</literal> symbol.  For example,
      &ulcorn;<literal>f.+f</literal>&drcorn; matches all strings that begin and end
      with the letter <literal>f</literal>
     (e.g. <literal>foolproof</literal>).  The expression
      &ulcorn;<literal>.+ed</literal>&drcorn; finds strings that potentially have the
      past-tense <literal>-ed</literal> suffix.
      </para>

      <para> The "<literal>*</literal>" symbol indicates that the
      immediately preceding expression is both optional and repeatable.
      For example &ulcorn;<literal>.*gnt.*</literal>&drcorn; matches all strings that
      contain <literal>gnt</literal>. 
      </para>

    

    </section>

    <section id="regex.choices">
      <title>Choices</title>

      <para> Patterns using the wildcard symbol are very effective, but
      there are many instances where we want to limit the set of
      characters that the wildcard can match.  In such cases we can use
      the <literal>[]</literal> notation, which enumerates the set of
      characters to be matched &mdash; this is called a
      <firstterm>character class</firstterm>.  For example, we can match
      any English vowel, but no consonant, using
      &ulcorn;<literal>[aeiou]</literal>&drcorn;. Note that this pattern
      can be interpreted as saying <quote>match <literal>a</literal> or
      <literal>e</literal> or &hellip; or <literal>u</literal></quote>; that is, the pattern
      resembles the wildcard pattern
      &ulcorn;<literal>.</literal>&drcorn; in only matching a string of
      length one; unlike the wildcard, it restricts the characters
      matched to a specific class (in this case, the vowels).  As a
      second example, the expression
      &ulcorn;<literal>p[aeiou]t</literal>&drcorn; matches the words:
      <literal>pat</literal>, <literal>pet</literal>,
      <literal>pit</literal>, <literal>pot</literal>, and
      <literal>put</literal>.
      </para>

      <para> We can combine the <literal>[]</literal> notation with
      our notation for repeatability.  For example, expression
      &ulcorn;<literal>p[aeiou]+t</literal>&drcorn; matches the words listed above,
      along with: <literal>peat</literal>, <literal>poet</literal>,
      and <literal>pout</literal>.
      </para>

      <note><para> Note that the order of vowels in the regular
      expression is insignificant, and we would have had the same
      result with the expression &ulcorn;<literal>p[uoiea]+t</literal>&drcorn;.  Thus,
      inside these brackets, the characters are interpreted not as a
      string but as a set of choices.
      </para></note>

      <para> Often the choices we want to describe cannot be expressed
      at the level of individual characters.  For example, if we were
      processing the output of a tagger to extract noun phrases, we
      might want to find all nouns (<literal>NN.*</literal>), adjectives
      (<literal>JJ.*</literal>), determiners (<literal>DT</literal>) and
      cardinals (<literal>CD</literal>), while excluding all other word
      types (e.g. verbs <literal>VB.*</literal>).  It is possible,
      using a single regular expression, to search for this set of
     candidates using the <firstterm>choice operator</firstterm> <literal>|</literal> as
      follows: &ulcorn;<literal>NN.*|JJ.*|DT|CD</literal>&drcorn;.
      </para>

      <para> As another example of multi-character choices, suppose
      that we wanted to create a program to simplify English prose,
      replacing rare words (like <literal>habitation</literal>) with
      a more frequent, synonymous word (like
      <literal>home</literal>).  In this situation, we need to map
      from a potentially large set of words to an individual word.  We
      can match the set of words using the choice operator.  In the
      case of the word <literal>home</literal>, we would want to
      match the regular expression
      &ulcorn;<literal>dwelling|domicile|abode|habitation</literal>&drcorn;.
      </para>

      <note><para> Note that the choice operator has wide scope, so that
      &ulcorn;<literal>abc|def</literal>&drcorn; is a choice between <literal>abd</literal>
      and <literal>def</literal>, and not between
      <literal>abced</literal> and <literal>abdef</literal>.  The latter
      choice must be written using parentheses: &ulcorn;<literal>ab(c|d)ed</literal>&drcorn;.
      </para></note>

    </section>

  </section>

  <section id="regex.intermediate"> <title> More Complex Regular Expressions
  </title>

    <para>In this section we will cover operators which can be used to
    construct more powerful and useful regular expressions.
    </para>

    <section id="regex.ranges"> <title> Ranges </title>

      <para>In section <xref linkend="choices"/> we saw how the
      <literal>[]</literal> notation could be used to express a set of
      choices between individual characters.  Instead of listing each
      character, it is also possible to express a <emphasis>range</emphasis>
      of characters, using the <literal>-</literal> operator.  For example,
      &ulcorn;<literal>[a-z]</literal>&drcorn; matches any lowercase letter.
      </para>

      <para>As expected, ranges can be combined with other operators.
      For example &ulcorn;<literal>[A-Z][a-z]*</literal>&drcorn; matches words that have
      an initial capital letter followed by any number of lowercase
      letters.  The expression &ulcorn;<literal>20[0-4][0-9]</literal>&drcorn; matches
      years in the range 2000 to 2049.
      </para>

      <para>Ranges can be combined, e.g.
      &ulcorn;<literal>[a-zA-Z]</literal>&drcorn; which matches any lowercase or uppercase
      letter.  The expression &ulcorn;<literal>[b-df-hj-np-tv-z]+</literal>&drcorn; matches
     words consisting only of consonants (e.g. <literal>pygmy</literal>).
      </para>

    </section>

    <section id="regex.complementation"> <title> Complementation </title>

      <para>
     We just saw that the character class 
     &ulcorn;<literal>[b-df-hj-np-tv-z]</literal>&drcorn; allows us to
     match consonants. However, this expression is
     quite cumbersome. A better alternative is to say: let's match
     anything which isn't a vowel. To do this, we need a way of
     expressing <firstterm>complementation</firstterm>. <footnote>
     <para>Complementation is a notion drawn from set theory. Let's
     assume that our domain of all objects is the set
     <varname>U</varname> = &lcub;a, b, c, d, e&rcub;, and let V be the
     set &lcub;a, e&rcub;. Then the complement of <varname>V</varname>,
     often written <varname>V</varname>&apos; is the result of
     <quote>subtracting</quote> <varname>V</varname> from
     <varname>U</varname>; i.e., the set <varname>V</varname>&apos; =
     &lcub;b, c, d &rcub;.</para> </footnote> We do
     this using the symbol <quote><literal>^</literal></quote> as the first character
     within <literal>[]</literal>. Let's look at an
     example. &ulcorn;<literal>[^aeiou]</literal>&drcorn; is just like
     our earlier character class, except now the set of vowels is
     preceded by <literal>^</literal>. The expression as a whole is
     interpreted as matching anything which <emphasis>fails</emphasis>
     to match &ulcorn;<literal>[aeiou]</literal>&drcorn;. In other
     words, it matches all consonants.</para>

    <para>As another example, suppose we want to match any string which
    is enclosed by the HTML tags <sgmltag class="starttag">B</sgmltag>
    and <sgmltag class="endtag">B</sgmltag>, We might try something like
    this:  &ulcorn;<literal>&lt;B&gt;.*&lt;/B&gt;</literal>&drcorn;. This would
    successfully match <literal>&lt;B&gt;important&lt;/B&gt;</literal>, but would
    also match <literal>&lt;B&gt;important&lt;/B&gt; and &lt;B&gt;urgent&lt;/B&gt;</literal>,
    since the &ulcorn;<literal>.*</literal>&drcorn; subpattern will happily
    match all the characters from the end of
    <literal>important</literal> to the end of
    <literal>urgent</literal>. One way of ensuring that we only look at
    matched pairs of tags would be to use the expression
    &ulcorn;<literal>&lt;B&gt;[^<]*&lt;/B&gt;</literal>&drcorn;, where the
    chararacter class matches anything other than a left angle bracket.
    </para>

 
    <para>Finally, note that character class complementation also works
     with for ranges. Thus &ulcorn;<literal>[^a-z]</literal>&drcorn;
     matches anything other than the lower case alphabetic characters
     <literal>a</literal> through <literal>z</literal>.
      </para>

    </section>

    <section id="regex.special"> <title> Common Special Symbols </title>

      <para>^, $, \, \n </para>

    </section>

  </section>

  <section id="regex.advanced"> 
    <title> Advanced Regular Expressions </title>

    <para>greedy vs non-greedy matching</para>

    <para>zero-width assertions</para>

    <para>more special symbols: \b etc</para>

  </section>

  <section id="regex.python_interface"> 
    <title> Python Interface </title>

    <para> How to do regexps in Python - give a couple of code samples
    for the above prose illustrations.  Use /usr/dict/words - local copy. </para>

<programlisting>
<emphasis># Load the regular expression module:</emphasis>
&prompt; <command>from re import *</command>
<emphasis># Read a big list of words:</emphasis>
&prompt; <command>words = open('/home/sb/nltk/data/words', 'r').read()</command>
<emphasis># How many words are there?</emphasis>
&prompt; <command>len(words)</command>
409093
<emphasis># Compile a regexp for 3-letter words ending in c</emphasis>
&prompt; <command>r = compile(r'^..c$', MULTILINE)</command>
<emphasis># Find all matches</emphasis>
&prompt; <command>print r.findall(words)</command>
['arc', 'Doc', 'Lac', 'Mac', 'Vic']
</programlisting>

</section> <!--Python Interface-->
  
</section><!-- Regular Expressions-->



    <section id="tagging">
    <title> Computational Approaches to Tagging </title>
<!--
X.3 computational model (gentle for linguistics ugrads)
    - what are some good data structures and algorithms?
    - just pick one or two approaches, not encyclopedic
    - NLTK demo - watch the execution of the algorithm
      (screen shots to show execution, side bars to say how
       to do it)
-->
   <para> When processing a text, it is often useful to associate
    auxiliary information with each token.  For example, we might
    want to label each token with its part of speech; or we might want
    to disambiguate homonyms by associating them with "word sense"
    labels.  This kind of auxiliary information is typically used in
    later stages of text processing.  For example, part of speech
    labels could be used to derive the internal structure of a
    sentence; and "word sense" labels could be used to allow a
    question-answering system to distinguish homonyms. </para>

    <indexterm><primary>tagging</primary></indexterm>
    <indexterm><primary>tags</primary></indexterm>
    <indexterm><primary>tag set</primary></indexterm>
    <para> The process of associating labels with each token in a text
    is called <glossterm>tagging</glossterm>, and the labels are
    called <glossterm>tags</glossterm>.  The collection of tags used
    for a particular task is known as a <glossterm>tag
    set</glossterm>. </para>
   
    <indexterm><primary>Part-of-speech tagging</primary></indexterm>
    <para> <glossterm>Part-of-speech tagging</glossterm> is the most
    common example of tagging, and it is the example we will examine
    in this tutorial.  But you should keep in mind that most of the
    techniques we discuss here can also be applied to many other
    tagging problems. </para>

    <para> Part-of-speech tags divide words into categories, based on
    how they can be combined to form sentences.  For example,
    articles can combine with nouns, but not verbs.  Part-of-speech
    tags also give information about the semantic content of a word.
    For example, nouns typically express "things," and prepositions
    express relationships between "things." </para>

    <para> Most part-of-speech tag sets make use of the same basic
    categories, such as "noun," "verb," "adjective," and
    "preposition."  However, tag sets differ both in how finely they
    divide words into categories; and in how define their categories.
    For example, "is" might be tagged as a verb in one tag set; but as
    a form of "to be" in another tag set.  This variation in tag sets
    is reasonable, since part-of-speech tags are used in different
    ways for different tasks. </para>

    <!-- How do I make "Table 1" not use a hard-coded number?? -->
    <indexterm><primary>Brown Corpus tag set</primary></indexterm>
    <para> In this tutorial, we will use the tag set listed in Table
    1.  This tag set is a simplification of the commonly used
    <glossterm>Brown Corpus tag set</glossterm>.  The complete Brown
    Corpus tag set has 87 basic tags.  For more information on tag
    sets, see <citetitle>Foundations of Statistical Natural Language
    Processing</citetitle>
    (<author><surname>Manning</surname></author> &amp;
    <author><surname>Schutze</surname></author>), pp. 139-145. </para>

    <table id="table.tagset"> <title>Tag Set</title> 
      <tgroup cols="2"><tbody>
          <row>
            <entry>AT</entry>
            <entry>Article</entry>
          </row>
          <row>
            <entry>NN</entry>
            <entry>Noun</entry>
          </row>
          <row>
            <entry>VB</entry>
            <entry>Verb</entry>
          </row>
          <row>
            <entry>JJ</entry>
            <entry>Adjective</entry>
          </row>
          <row>
            <entry>IN</entry>
            <entry>Preposition</entry>
          </row>
          <row>
            <entry>CD</entry>
            <entry>Number</entry>
          </row>
          <row>
            <entry>END</entry>
            <entry>Sentence-ending punctuation</entry>
          </row>
        </tbody>
      </tgroup>
    </table>



    </section>

    <section> <title> Advanced Topics in Tagging (optional) </title>

    <para></para>
<!--
X.4 advanced topics (optional)
    - other approaches, evaluation, problems
    - challenges for particular languages / language families
    - research questions
-->

    </section>


  <section> <title> Words </title>
      
    <para> There are a number of reasonable ways to represent words in
    Python.  Perhaps the simplest is as string values, such as
    <literal>'dog'</literal>; this is how words are typically
    represented when using NLTK. </para>

<programlisting>
    &prompt;<command> words = ['the', 'cat', 'climbed', 'the', 'tree']</command>
    ['the', 'cat', 'climbed', 'the', 'tree']
</programlisting>

    <para> However, NLTK also allows for other representations.  For
    example, you could store words as integers, with some mapping
    between integers and words. </para>

    <section> <title> Types and Tokens in NLTK</title>
<!--
      <indexterm><primary>word token</primary></indexterm>
      <indexterm><primary>word type</primary></indexterm>
      <para> The term "word" can actually be used in two different
      ways: to refer to an individual occurrence of a word; or to
      refer to an abstract vocabulary item.  For example, the phrase
      "my dog likes his dog" contains five occurrences of words, but
      only four vocabulary items (since the vocabulary item "dog"
      appears twice).  In order to make this distinction clear, we
      will use the term <glossterm>word token</glossterm> to refer to
      occurrences of words, and the term <glossterm>word
      type</glossterm> to refer to vocabulary items. </para>

      <indexterm><primary>token</primary></indexterm>
      <indexterm><primary>type</primary></indexterm>
      <indexterm><primary>sentence token</primary></indexterm>
      <indexterm><primary>sentence type</primary></indexterm>
      <para> The terms <glossterm>token</glossterm> and
      <glossterm>type</glossterm> can also be applied in other
      domains.  For example, a <glossterm>sentence token</glossterm>
      is an individual occurrence of a sentence; but a
      <glossterm>sentence type</glossterm> is an abstract sentence,
      without context.  If someone repeats a sentence twice, they have
      uttered two sentence tokens, but only one sentence type.  When
      the kind of token or type is obvious from context, we will
      simply use the terms <glossterm>token</glossterm> and
      <glossterm>type</glossterm>. </para>
-->
      <para> In NLTK, <ulink url="&refdoc;/nltk.token.Token-class.html"
      ><literal>Token</literal>s</ulink> are constructed from their
      types, using the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal> constructor</ulink>, which is defined
      in the <ulink url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module:</para>

<programlisting>
    &prompt;<command> from nltk.token import * </command>
    &prompt;<command> my_word_type = 'dog' </command>
    'dog'
    &prompt;<command> my_word_token = Token(my_word_type) </command>
    'dog'@[?]
</programlisting>

      <para> The reason that the token is displayed this way will be
      explained in the next two sections. </para>

    </section> <!-- Tokens and Types -->

    <section> <title> Text Locations </title>

      <indexterm><primary>Text locations</primary></indexterm>
      <indexterm><primary>start index</primary></indexterm>
      <indexterm><primary>end index</primary></indexterm>
      <para> <glossterm>Text locations</glossterm> specify regions of
      texts, using a <glossterm>start index</glossterm> and an
      <glossterm>end index</glossterm>.  A location with start index
      <replaceable>s</replaceable> and end index
      <replaceable>e</replaceable> is written
      <literal>@[</literal><replaceable>s</replaceable><literal>:</literal><replaceable>e</replaceable><literal>]</literal>,
      and specifies the region of the text beginning at
      <replaceable>s</replaceable>, and including everything up to
      (but not including) the text at <replaceable>e</replaceable>.
      <ulink url="refdoc;/nltk.token.Location-class.html"
      ><literal>Locations</literal></ulink> are created using the
      <ulink url="refdoc;/nltk.token.Location-class.html#__init__"
      ><literal>Location</literal> constructor</ulink>, which is
      defined in the <ulink url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module:</para>

<programlisting>
    &prompt;<command> from nltk.token import * </command>
    &prompt;<command> my_loc = Location(1, 5) </command>
    @[1:5]
    &prompt;<command> another_loc = Location(0, 2) </command>
    @[0:2]
    &prompt;<command> yet_another_loc = Location(22, 22) </command>
    @[22:22]
</programlisting>

        <para> Note that a text location does <emphasis>not</emphasis> include
      the text at its end location.  This convention may seem
      unintuitive at first, but it has a number of advantages.  It is
      consistent with Python's slice notation (e.g.,
      <literal>x[1:3]</literal> specifies elements 1 and 2 of x).

        <footnote><para>But unlike Python slices, text locations do
        <emphasis>not</emphasis> support negative indexes.</para></footnote>

      It allows text locations to specify points between tokens,
      instead of just ranges; for example,
      <literal>Location(3,3)</literal> specifies the point just before
      the text at index 3.  And it simplifies arithmetic on indices;
      for example, the length of <literal>Location(5,10)</literal> is
      <literal>10-5</literal>, and two locations are contiguous if the
      start of one equals the end of the other. </para>

      <para> To create a text location specifying the text at a single
      index, you can use the <literal>Location</literal> constructor
      with a single argument.  For example, the fourth word in a text
      could be specified with <literal>loc1</literal>: </para>

<programlisting>
    &prompt;<command> loc1 = Location(4) </command>     <emphasis># location width = 1</emphasis>
    @[4]
</programlisting>

      <para> NLTK uses the shorthand notation
      <literal>@[<replaceable>s</replaceable>]</literal> for locations
      whose width is one.  Note that
      <literal>Location(<replaceable>s</replaceable>)</literal> is
      equivalent to <literal>Location(<replaceable>s</replaceable>,
      <replaceable>s+1</replaceable>)</literal>,
      <emphasis>not</emphasis>
      <literal>Location(<replaceable>s</replaceable>,
      <replaceable>s</replaceable>)</literal>:</para>

<programlisting>
    &prompt;<command> loc2 = Location(4, 5) </command>  <emphasis># location width = 1</emphasis>
    @[4]
    &prompt;<command> loc3 = Location(4, 4) </command>  <emphasis># location width = 0</emphasis>
    @[4:4]
</programlisting>

      <section> <title> Units </title>

        <indexterm><primary>units</primary></indexterm>
        <para> The start and end indices can be based on a variety of
        different <glossterm>units</glossterm>, such as character
        number, word number, or sentence number.  By default, the unit
        of a text location is left unspecified, but locations can be
        explicitly tagged with information about what unit their
        indices use: </para>

<programlisting>
    &prompt;<command> my_loc = Location(1, 5, unit='w') </command>
    @[1w:5w]
    &prompt;<command> another_loc = Location(3, 72, unit='c') </command>
    @[3c:72c]
    &prompt;<command> my_loc = Location(6, unit='s') </command>
    @[6s]
    &prompt;<command> my_loc = Location(10, 11, unit='s') </command>
    @[10s]
</programlisting>

        <para> Unit labels take the form of case-insensitive
        <literal>string</literal>s.  Typical examples of unit labels
        are <literal>'c'</literal> (for character number),
        <literal>'w'</literal> (for word number), and
        <literal>'s'</literal> (for sentence number). </para>

      </section>  <!-- Units -->
      <section> <title> Sources </title>

        <indexterm><primary>source</primary></indexterm>
        <para> A text location may also be tagged with a
        <glossterm>source</glossterm>, which gives an indication of
        where the text was derived from.  A typical example of a
        source would be a <literal>string</literal> containing the
        name of the file from which the element of text was
        read. </para>

<programlisting>
    &prompt;<command> my_loc = Location(1, 5, source='foo.txt') </command>
    @[1:5]@'foo.txt'
    &prompt;<command> another_loc = Location(3, 72, unit='c', source='bar.txt') </command>
    @[3c:72c]@'bar.txt'
    &prompt;<command> my_loc = Location(6, unit='s', source='baz.txt') </command>
    @[6s]@'baz.txt'
</programlisting>

        <para> By default, a text location's source is
        unspecified. </para>

        <para> Sometimes, it is useful to use text locations as the
        sources for other text locations.  For example, we could
        specify the third character of the fourth word of the first
        sentence in the file <literal>foo.txt</literal> with
        <literal>char_loc</literal>: </para>

<programlisting>
    &prompt;<command> sentence_loc = Location(0, unit='s', source='foo.txt') </command>
    @[0s]@'foo.txt'
    &prompt;<command> word_loc = Location(3, unit='w', source=sentence_loc) </command>
    @[3w]@[0s]@'foo.txt'
    &prompt;<command> char_loc = Location(2, unit='c', source=word_loc) </command>
    @[2c]@[3w]@[0s]@'foo.txt'
</programlisting>

        <para> Note that the location indexes are zero-based, so the
        first sentence starts at an index of zero, not one.  </para>

      </section>  <!-- Sources -->
    </section> <!-- Text Locations -->
    
    <section> <title> Tokens and Locations </title>
      
      <para> As discussed above, a text token represents a single
      occurrence of a text type.  In NLTK, a token is defined by a
      type, together with a location at which that type occurs.  A
      token with type <replaceable>t</replaceable> and location
      <literal>@[<replaceable>l</replaceable>]</literal> can be
      written as
      <literal><replaceable>t</replaceable>@[<replaceable>l</replaceable>]</literal>.
      Tokens are constructed with the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal> constructor</ulink>: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token2 = Token('world', Location(1, unit='w')) </command>
    'world'@[1w]
</programlisting>

      <para> Two tokens are only equal if both their type and their
      location are equal:</para>

<programlisting>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token2 = Token('hello', Location(1, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token3 = Token('world', Location(0, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token4 = Token('hello', Location(0, unit='w')) </command>
    'world'@[1w]
    &prompt;<command> token1 == token2 </command>
    0
    &prompt;<command> token1 == token3 </command>
    0
    &prompt;<command> token1 == token4 </command>
    1
</programlisting>

      <para> When a token's location is unknown or unimportant, the
      special location <literal>None</literal> may be used.  A token
      with type <replaceable>t</replaceable> and location
      <literal>None</literal> is written as
      <literal><replaceable>t</replaceable>@[?]</literal>.  If a
      token's location is not specified, it defaults to
      <literal>None</literal>: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello', None) </command>
    'hello'@[?]
    &prompt;<command> token2 = Token('world') </command>
    'world'@[?]
</programlisting>

      <para> A Token with a location of <literal>None</literal> is not
      considered to be equal to any other token.  In particular, even
      if two tokens have the same type, and both have a location of
      <literal>None</literal>, they are not equal: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello') </command>
    'hello'@[?]
    &prompt;<command> token2 = Token('hello') </command>
    'hello'@[?]
    &prompt;<command> token1 == token2 </command>
    0
</programlisting>

      <para> To access a token's type, use its <ulink
      url="&refdoc;/nltk.token.Token-class.html#type"
      ><literal>type</literal></ulink> member function; and to access a
      token's location, use its <ulink
      url="&refdoc;/nltk.token.Token-class.html#loc"
      ><literal>loc</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> token1 = Token('hello', Location(0, unit='w')) </command>
    'hello'@[0w]
    &prompt;<command> token1.type()</command>
    'hello'
    &prompt;<command> token1.loc()</command>
    @[0w]
</programlisting>

      <para> To access a location's start index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#start"
      ><literal>start</literal></ulink> member function; and to access
      a location's end index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#end"
      ><literal>end</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> loc1 = Location(0, 8, unit='w') </command>
    @[0w:8w]
    &prompt;<command> loc1.start()</command>
    0
    &prompt;<command> loc1.end()</command>
    8
</programlisting>

      <para> To access a location's unit index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#unit"
      ><literal>unit</literal></ulink> member function; and to access
      a location's source index, use its <ulink
      url="&refdoc;/nltk.token.Location-class.html#source"
      ><literal>source</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> loc1 = Location(3, unit='w', source='foo.txt') </command>
    @[3w]@'foo.txt'
    &prompt;<command> loc1.unit()</command>
    'w'
    &prompt;<command> loc1.source()</command>
    'foo.txt'
</programlisting>

      <para> For more information about tokens and locations, see the
      reference documentation for the <ulink
      url="&refdoc;/nltk.token-module.html"
      ><literal>nltk.token</literal></ulink> module. </para>

    </section>  <!-- Tokens and Locations -->
  </section>  <!-- Words -->

  <section> <title> Texts </title>

    <para> Many natural language processing tasks involve analyzing
    texts of varying sizes, ranging from single sentences to very
    large corpora.  There are a number of ways to represent texts
    using NLTK.  The simplest is as a single
    <literal>string</literal>.  These strings are typically loaded
    from files: </para>

<programlisting>
    &prompt;<command> text_str = open('corpus.txt').read() </command>
    'Hello world.  This is a test file.\n'
</programlisting>

    <indexterm><primary>tokenizer</primary></indexterm>
    <para> It is often more convenient to represent a text as a
    <literal>list</literal> of <literal>Token</literal>s.  These
    <literal>list</literal>s are typically created using a
    <glossterm>tokenizer</glossterm>, such as <ulink
    url="&refdoc;/nltk.token.WSTokenizer-class.html"
    ><literal>WSTokenizer</literal></ulink> (which splits words apart
    based on whitespace): </para>

<programlisting>
    &prompt;<command> text_tok_list = WSTokenizer().tokenize(text_str) </command>
    ['Hello'@[0w], 'world.'@[1w], 'This'@[2w], 'is'@[3w], 
     'a'@[4w], 'test'@[5w], 'file.'@[6w]]
</programlisting>

    <para> Texts can also be represented as sets of word tokens or
    sets of word types: </para>

<programlisting>
    &prompt;<command> text_tok_set = Set(*text_tok_list) </command>
    {'This'@[2w], 'a'@[4w], 'Hello'@[0w], 'world.'@[1w], 
     'is'@[3w], 'file.'@[6w], 'test'@[5w]}
</programlisting>

    <note> <para> For example, this representation might be convenient
    for a search engine which is trying to find all documents
    containing some keyword. (For more information on why a
    "<literal>*</literal>" is used in the call to the Set constructor,
    see the Set tutorial.)  <!-- WRITE THE SET TUTORIAL!! -->
    </para> </note>

  </section> <!-- Texts -->

  <section> <title> Tokenization </title>

    <indexterm><primary>tokenization</primary></indexterm>
    <para> As mentioned in the previous section, it is often useful to
    represent a text as a list of tokens.  The process of breaking a
    text up into its constituent tokens is known as
    <glossterm>tokenization</glossterm>.  Tokenization can occur at a
    number of different levels: a text could be broken up into
    paragraphs, sentences, words, syllables, or phonemes.  And for any
    given level of tokenization, there are many different algorithms
    for breaking up the text.  For example, at the word level, it is
    not immediately clear how to treat such strings as "can't,"
    "$22.50," "New York," and "so-called." </para>

    <para> NLTK defines a general interface for tokenizing texts, the
    <ulink url="&refdoc;/nltk.token.TokenizerI-class.html"
    ><literal>TokenizerI</literal></ulink> class.  This interface is
    used by all tokenizers, regardless of what level they tokenize at
    or what algorithm they use.  It defines a single method, <ulink
    url="&refdoc;/nltk.token.TokenizerI-class.html#tokenize"
    ><literal>tokenize</literal></ulink>, which takes a
    <literal>string</literal>, and returns a list of
    <literal>Token</literal>s.  </para>

    <section> <title>NLTK Interfaces</title>

      <para> <literal>TokenizerI</literal> is the first "interface"
      class we've encountered; at this point, we'll take a short
      digression to explain how interfaces are implemented in
      NLTK. </para>

      <indexterm><primary>interface</primary></indexterm>
      <para> An <glossterm>interface</glossterm> gives a partial
      specification of the behavior of a class, including
      specifications for methods that the class should implement.  For
      example, a "comparable" interface might specify that a class
      must implement a comparison method.  Interfaces do not give a
      complete specification of a class; they only specify a minimum
      set of methods and behaviors which should be implemented by the
      class.  For example, the <literal>TokenizerI</literal> interface
      specifies that a tokenizer class must implement a
      <literal>tokenize</literal> method, which takes a
      <literal>string</literal>, and returns a list of
      <literal>Token</literal>s; but it does not specify what other
      methods the class should implement (if any).  </para>

      <para> The notion of "interfaces" can be very useful in ensuring
      that different classes work together correctly.  Although the
      concept of "interfaces" is supported in many languages, such as
      Java, there is no native support for interfaces in
      Python. </para>

      <para> NLTK therefore implements interfaces using classes, all
      of whose methods raise the
      <literal>NotImplementedError</literal> exception.  To
      distinguish interfaces from other classes, they are always named
      with a trailing "<literal>I</literal>".  If a class implements
      an interface, then it should be a subclass of the interface.
      For example, the <literal>WSTokenizer</literal> class implements
      the <literal>TokenizerI</literal> interface, and so it is a
      subclass of <literal>TokenizerI</literal>.  </para>

    </section> <!-- NLTK Interfaces -->

    <section> <title> The whitespace tokenizer </title> 

      <para> A simple example of a tokenizer is the
      <ulink url="&refdoc;/nltk.token.WSTokenizer-class.html"
      ><literal>WSTokenizer</literal></ulink>, which breaks a text into
      words, assuming that words are separated by whitespace (space,
      enter, and tab characters).  We can use the
      <ulink url="&refdoc;/nltk.token.WSTokenizer-class.html#__init__"
      ><literal>WSTokenizer</literal> constructor</ulink> to build a
      new whitespace tokenizer: </para>

<programlisting>
    &prompt;<command> tokenizer = WSTokenizer() </command>
</programlisting>

      <para> Once we have built the tokenizer, we can use it to
      process texts: </para>

<programlisting>
    &prompt;<command> tokenizer.tokenize(text_str) </command>
    ['Hello'@[0w], 'world.'@[1w], 'This'@[2w], 'is'@[3w], 
     'a'@[4w], 'test'@[5w], 'file.'@[6w]]
</programlisting>

      <para> However, this tokenizer is not ideal for many tasks.  For
      example, we might want punctuation to be included as separate
      tokens; or we might want names like "New York" to be included as
      single tokens. </para>

    </section> <!-- Whitespace tokenizer -->
    <section> <title> The regular expression tokenizer </title>

      <para> The <literal>RETokenizer</literal> is a more powerful
      tokenizer, which uses a regular expression to determine how text
      should be split up.  This regular expression specifies the
      format of a valid word.  For example, if we wanted to mimic the
      behavior or <literal>WSTokenizer</literal>, we could define the
      following <literal>RETokenizer</literal>: </para>

<programlisting>
    &prompt;<command> tokenizer = RETokenizer(r'[^\s]+') </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['Hello.'@[0w], "Isn't"@[1w], 'this'@[2w], 'fun?'@[3w]]
</programlisting>

      <para> (The regular expression <literal>\s</literal> matches any
      whitespace character.) </para>

      <para> To define a tokenizer that includes punctuation as
      separate tokens, we could use: </para>

<programlisting>
    &prompt;<command> regexp = r'\w+|[^\w\s]+'</command>
    '\w+|[^\w\s]+'
    &prompt;<command> tokenizer = RETokenizer(regexp) </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['Hello'@[0w], '.'@[1w], 'Isn'@[2w], "'"@[3w], 't'@[4w], 
     'this'@[5w], 'fun'@[6w], '?'@[7w]]
</programlisting>

      <para> The regular expression in this example will match
      <emphasis>either</emphasis> a sequence of alphanumeric
      characters (letters and numbers); <emphasis>or</emphasis> a
      sequence of punctuation characters. </para>

      <para> There are a number of ways we might want to improve this
      regular expression.  For example, it currently breaks the string
      "$22.50" into four tokens; but we might want it to include this
      as a single token.  One approach to making this change would be
      to add a new clause to the tokenizer's regular expression, which
      is specialized for handling strings of this form: </para>

<programlisting>
    &prompt;<command> example_text = 'That poster costs $22.40.'</command>
    &prompt;<command> regexp = r'(\w+)|(\$\d+\.\d+)|([^\w\s]+)'</command>
    '(\w+)|(\$\d+\.\d+)|([^\w\s]+)'
    &prompt;<command> tokenizer = RETokenizer(regexp) </command>
    &prompt;<command> tokenizer.tokenize(example_text) </command>
    ['That'@[0w], 'poster'@[1w], 'costs'@[2w], '$22.40'@[3w], '.'@[4w]]
</programlisting>

      <para> Of course, more general solutions to this problem are
      also possible, using different regular expressions. </para>
    </section> <!-- Regexp tokenizer -->

  </section> <!-- Tokenization -->

  <section> <title> Example: Processing Tokenized Text </title>

    <para> In this section, we show how you can use NLTK to
    examine the distribution of word lengths in a document.  This is
    meant to be a simple example of how the tools we have introduced
    can be used to solve a simple NLP problem.  The distribution of
    word lengths in a document can give clues to other properties,
    such as the document's style, or the document's language. </para>

    <para> We present three different approaches to solving this
    problem; each one illustrates different techniques, which might be
    useful for other problems. </para>

    <section> <title> Word Length Distributions 1: Using a List </title>

      <para> To begin with, we'll need to extract the words from a
      corpus that we wish to test.  We'll use the
      <literal>WSTokenizer</literal> to tokenize the corpus: </para>

<programlisting>
    &prompt;<command> corpus = open('corpus.txt').read() </command>
    &prompt;<command> tokens = WSTokenizer().tokenize(corpus) </command>
</programlisting>

      <para> Now, we will construct a list
      <literal>wordlen_count_list</literal>, which gives the number of
      words that have a given length.  In particular,
      <literal>wordlen_count_list[<replaceable>i</replaceable>]</literal>
      is the number of words whose length is
      <replaceable>i</replaceable>. </para>

      <para> When constructing this list, we must be careful not to
      try to add a value past the end of the list.  Therefore,
      whenever we encounter a word that is longer than any previous
      words, we will add enough zeros to
      <literal>wordlen_count_list</literal> that we can store the
      occurrence of the new word:

<programlisting>
    &prompt;<command> wordlen_count_list = []</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     wordlen = len(token.type())</command>
    &prompt2;     <emphasis># Add zeros until wordlen_count_list is long enough</emphasis>
    &prompt2;<command>     while wordlen >= len(wordlen_count_list):</command>
    &prompt2;<command>         wordlen_count_list.append(0)</command>
    &prompt2;     <emphasis># Increment the count for this word length</emphasis>
    &prompt2;<command>     wordlen_count_list[wordlen] += 1</command>
</programlisting>
</para>

      <para> We can plot the results, using the <ulink
      url="&refdoc;/nltk.draw.plot.Plot-class.html"
      ><literal>Plot</literal></ulink> class, defined in the
      <ulink url="&refdoc;/nltk.draw.plot-module.html"
      ><literal>nltk.draw.plot</literal></ulink> module: </para>

<programlisting>
    &prompt;<command> Plot(wordlen_count_list)</command>
</programlisting>

      <note> <para> We are currently using a fairly simple class to
      plot functions.  We will likely replace it with a more advanced
      plotting system in the future. </para> </note>

      <para> The complete code for this example, including necessary
      <literal>import</literal> statements, is: </para>

<programlisting>
from nltk.token import WSTokenizer
from nltk.draw.plot import Plot

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Count up how many times each word length occurs</emphasis>
wordlen_count_list = []
for token in tokens:
     wordlen = len(token.type())
     <emphasis># Add zeros until wordlen_count_list is long enough</emphasis>
     while wordlen >= len(wordlen_count_list):
         wordlen_count_list.append(0)
     <emphasis># Increment the count for this word length</emphasis>
     wordlen_count_list[wordlen] += 1

Plot(wordlen_count_list)
</programlisting>
      
    </section> <!-- Word length distributions: list -->

    <section> <title> Word Length Distributions 2: Using a Dictionary </title>

      <para> We have been examining the function from word lengths to
      token counts.  In this example, the range of the function (i.e.,
      the set of word lengths) is ordered and relatively small.
      However, we often wish to examine functions whose ranges are not
      so well behaved.  In such cases, dictionaries can be a powerful
      tool.  The following code uses a dictionary to count up the
      number of times each word length occurs: </para>

<programlisting>
    &prompt;<command> wordlen_count_dict = {}</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     word_length = len(token.type())</command>
    &prompt2;<command>     if wordlen_count_dict.has_key(word_length):</command>
    &prompt2;<command>         wordlen_count_dict[word_length] += 1</command>
    &prompt2;<command>     else:</command>
    &prompt2;<command>         wordlen_count_dict[word_length] = 1</command>
</programlisting>

      <para> To plot the results, we can use a list of (wordlen,
      count) pairs.  This is simply the <literal>items</literal> of
      the dictionary: </para>

<programlisting>
    &prompt;<command> points = wordlen_count_dict.items() </command>
    &prompt;<command> Plot(points)</command>
</programlisting>

      <para> The complete code for this example, including necessary
      <literal>import</literal> statements, is: </para>

<programlisting>
from nltk.token import WSTokenizer
from nltk.draw.plot import Plot

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Construct a dictionary mapping word lengths to token counts</emphasis>
wordlen_count_dict = {}
for token in tokens:
    word_length = len(token.type())
    if wordlen_count_dict.has_key(word_length):
        wordlen_count_dict[word_length] += 1
    else:
        wordlen_count_dict[word_length] = 1

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = wordlen_count_dict.items() 
Plot(points)
</programlisting>

    </section> <!-- Word length distributions: dictionary -->

    <section> <title> Word Length Distributions 3: Using a Frequency Distribution </title>

      <para> The <ulink url="&refdoc;/nltk.probability-module.html"
      ><literal>nltk.probability</literal></ulink> module defines two
      interfaces, <ulink
      url="&refdoc;/nltk.probability.FreqDistI-class.html"
      ><literal>FreqDistI</literal></ulink> and <ulink
      url="&refdoc;/nltk.probability.ProbDistI-class.html"
      ><literal>ProbDistI</literal></ulink>, for modeling frequency
      distributions and probability distributions, respectively.  In
      this example, we use a frequency distribution to find the
      relationship between word lengths and token counts. </para>

      <para> We will use a <ulink
      url="&refdoc;/nltk.probability.SimpleFreqDist-class.html"
      ><literal>SimpleFreqDist</literal></ulink>, which is a simple
      (but sometimes inefficient) implementation of the
      <literal>FreqDistI</literal> interface.  For this example, three
      methods of <literal>SimpleFreqDist</literal> are
      relevant:</para>

        <itemizedlist>
          <listitem><para><ulink url="&refdoc;/nltk.probability.SimpleFreqDist-class.html#inc"><literal>inc(<replaceable>sample</replaceable>)</literal></ulink>
          increments the frequency of a given sample.</para>
          </listitem>
          <listitem><para><ulink url="&refdoc;/nltk.probability.SimpleFreqDist-class.html#samples"><literal>samples()</literal></ulink> returns a list of
          the samples covered by a frequency distribution.</para>
          </listitem>
          <listitem><para><ulink url="&refdoc;/nltk.probability.SimpleFreqDist-class.html#count"><literal>count(<replaceable>sample</replaceable>)</literal></ulink>
          returns the number of times a given sample occurred.</para>
          </listitem>
        </itemizedlist>

      <para> First, we construct the frequency distribution for the
      word lengths: </para>

<programlisting>
    &prompt;<command> wordlen_freqs = SimpleFreqDist()</command>
    &prompt;<command> for token in tokens:</command>
    &prompt2;<command>     wordlen_freqs.inc(len(token.type()))</command>
</programlisting>

      <para> Next, we extract the set of word lengths that were found
      in the corpus: </para>

<programlisting>
    &prompt;<command> wordlens = wordlen_freqs.samples()</command>
</programlisting>

      <para> Finally, we construct a list of (wordlen, count) pairs,
      and plot it: </para>

<programlisting>
    &prompt;<command> points = [(wordlen, wordlen_freqs.count(wordlen))</command>
    &prompt2;<command>           for wordlen in wordlens]</command>
    &prompt;<command> Plot(points)</command>
</programlisting>

      <note> <para> The expression
      <literal>[...&nbsp;for&nbsp;...&nbsp;in&nbsp;...]</literal> is
      called a "list comprehension."  For more information on list
      comprehensions, see the <ulink
      url="&tutdoc;/advpython/index.html">"New Python Features"
      tutorial</ulink>. </para>
      </note>

      <para> The complete code for this example, including necessary
      <literal>import</literal> statements, is: </para>

<programlisting>
from nltk.token import WSTokenizer
from nltk.draw.plot import Plot
from nltk.probability import SimpleFreqDist

<emphasis># Extract a list of words from the corpus</emphasis>
corpus = open('corpus.txt').read() 
tokens = WSTokenizer().tokenize(corpus) 

<emphasis># Construct a frequency distribution of word lengths</emphasis>
wordlen_freqs = SimpleFreqDist()
for token in tokens:
    wordlen_freqs.inc(len(token.type()))

<emphasis># Exctract the set of word lengths found in the corpus</emphasis>
wordlens = wordlen_freqs.samples()

<emphasis># Construct a list of (wordlen, count) and plot the results.</emphasis>
points = [(wordlen, wordlen_freqs.count(wordlen))
          for wordlen in wordlens]
Plot(points)
</programlisting>
      
      <para> For more information about frequency distributions, see
      the Probability Tutorial.</para>

    </section> <!-- Word length distributions: freqdist -->
  </section> <!-- Word length distributions example -->


    <section> <title> Tagging in NLTK </title>

<!--
X.5 implementation
    - how does NLTK do it?
    - simple problems and worked solutions
    - suggested projects (e.g. for your MSc students)
-->



  <section id="basics">
      <title> The nltk.tagger Module </title>

    <para> The <ulink
    url="&refdoc;/nltk.tagger-module.html"
    ><literal>nltk.tagger</literal></ulink> module defines the classes
    and interfaces used by NLTK to perform tagging.  </para>

    <section id="basics.TaggedType"> <title> TaggedType </title>

      <indexterm><primary>base type</primary></indexterm>
      <indexterm><primary>tag</primary></indexterm>
      <para> NLTK defines a simple class, <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html"
      ><literal>TaggedType</literal></ulink>, for representing the text
      type of a tagged token.  A <literal>TaggedType</literal>
      consists of a <glossterm>base type</glossterm> and a
      <glossterm>tag</glossterm>.  Typically, the base type and the
      tag will both be strings.  For example, the tagged type for the
      noun "dog" would have the base type <literal>'dog'</literal> and
      the tag <literal>'NN'</literal>.  A tagged type with base type
      <replaceable>b</replaceable> and tag
      <replaceable>t</replaceable> is written
      <replaceable>b</replaceable><literal>/</literal><replaceable>t</replaceable>.
      Tagged types are created with the <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#__init__"
      ><literal>TaggedType</literal></ulink> constructor: </para>

<programlisting>
    &prompt;<command> ttype1 = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttype2 = TaggedType('runs', 'VB') </command>
    'runs'/'VB'
</programlisting>

      <para> A <literal>TaggedType</literal>'s base type is accessed
      via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#base"
      ><literal>base</literal></ulink> member function; and its tag is
      accessed via the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedType-class.html#tag"
      ><literal>tag</literal></ulink> member function: </para>

<programlisting>
    &prompt;<command> ttype1.base() </command>
    'dog'
    &prompt;<command> ttype2.tag() </command>
    'VB'
</programlisting>

      <para> To construct a tagged token, simply use the <ulink
      url="&refdoc;/nltk.token.Token-class.html#__init__"
      ><literal>Token</literal></ulink> constructor with a
      <literal>TaggedType</literal>: </para>

<programlisting>
    &prompt;<command> ttype = TaggedType('dog', 'NN') </command>
    'dog'/'NN'
    &prompt;<command> ttoken = Token(ttype, Location(5)) </command>
    'dog'/'NN'@[5]
</programlisting>

    </section> <!-- TaggedType -->

    <section id="basics.corpera"> <title> Reading Tagged Corpora </title>

      <para> Several large corpora (such as the Brown Corpus and
      portions of the Wall Street Journal) have been manually tagged
      with part-of-speech tags.  These corpora are primarily useful
      for testing taggers and for training statistical taggers.
      However, before we can use these corpora, we must read them from
      files and tokenize them. </para>

      <para> Tagged texts are usually stored in files as a sequences
      of whitespace-separated tokens, where each token is of the form
      <replaceable>base</replaceable><literal>/</literal><replaceable>tag</replaceable>.
      Figure 1 shows an example of some tagged text, taken from the
      Brown corpus. </para>

      <figure> <title> An Example of Tagged Text (excerpted from the Brown Corpus) </title>
        <titleabbrev>Example of Tagged Text</titleabbrev>
<screen>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt
it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb
generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in
the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.
</screen>
      </figure>

      <para> To tokenize tagged texts of this form, the
      <literal>nltk.tagger</literal> module defines the
      <ulink
      url="&refdoc;/nltk.tagger.TaggedTokenizer-class.html"
      ><literal>TaggedTokenizer</literal></ulink> class: </para>

<programlisting>
    &prompt;<command> tagged_text_str = open('corpus.txt').read() </command>
    'John/NN saw/VB the/AT book/NN on/IN the/AT 
     table/NN ./END  He/NN sighed/VB ./END'
    &prompt;<command> tokens = TaggedTokenizer().tokenize(tagged_text_str) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

      <para> If <literal>TaggedTokenizer</literal> encounters a word
      without a tag, it will assign it the default tag
      <literal>None</literal>. </para>

    </section> <!-- TaggedTokenizer -->

    <section id="basics.TaggerI"> <title> The TaggerI Interface </title>

      <para> The <literal>nltk.tagger</literal> module defines <ulink
      url="&refdoc;/nltk.tagger.TaggerI-class.html"
      ><literal>TaggerI</literal></ulink>, a general interface for
      tagging texts.  This interface is used by all taggers.  It
      defines a single method, <ulink
      url="&refdoc;/nltk.tagger.TaggerI-class.html#tag"
      ><literal>tag</literal></ulink>, which assigns a tag to each
      token in a list, and returns the resulting list of tagged
      tokens.</para>

<programlisting>
    &prompt;<command> untagged_text_str = open('untagged.txt').read() </command>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str) </command>
    ['John'@[0w], 'saw'@[1w], 'the'@[2w], 'book'@[3w], 
     'on'@[4w], 'the'@[5w], 'table'@[6w], '.'@[7w], 
     'He'@[8w], 'sighed'@[9w], '.'@[10w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], 
     'table'/'NN'@[6w], '.'/'END'@[7w], 'He'/'NN'@[8w], 
     'sighed'/'VB'@[9w], '.'/'END'@[10w]]
</programlisting>

    </section> <!-- TaggerI -->

  </section> <!-- The nltk.tagger module -->

  <section id="taggers"> <title> Taggers </title>

    <para> The <literal>nltk.tagger</literal> module currently defines
    four taggers; this list will likely grow in the future.  This
    section describes the taggers currently implemented by
    <literal>nltk.tagger</literal>, and how they are used. </para>
    
    <section id="taggers.default"> <title> A Default Tagger </title>

      <para> The simplest tagger defined by
      <literal>nltk.tagger</literal> is <ulink
      url="&refdoc;/nltk.tagger.NN_CD_Tagger-class.html"
      ><literal>NN_CD_Tagger</literal></ulink>.  This tagger assigns a
      tag to each token on the basis of its type.  If its type appears
      to be a number, it assigns the type "CD."  Otherwise, it assigns
      the type "NN." </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str) </command>
    ['John'@[0w], 'saw'@[1w], '3'@[2w], 
     'polar'@[3w], 'bears'@[4w], '.'@[5w]]
    &prompt;<command> my_tagger.tag(tokens) </command>
    ['John'/'NN'@[0w], 'saw'/'NN'@[1w], '3'/'CD'@[2w], 
     'polar'/'NN'@[3w], 'bears'/'NN'@[4w], '.'/'NN'@[5w]]
</programlisting>

      <para> This is a simple algorithm, but it yields quite poor
      performance when used by itself.  On a typical corpus, it will
      tag only 20%-30% of the tokens correctly.  However, it is a very
      reasonable tagger to use as a default, if a more advanced tagger
      fails to determine a token's tag.  When used in conjunction with
      other taggers, <literal>NN_CD_Tagger</literal> can significantly
      improve performance. </para>
      
    </section> <!-- NN_CD_Tagger -->

    <section id="taggers.unigram"> <title> Unigram Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger-class.html"
      ><literal>UnigramTagger</literal></ulink> class implements a
      simple statistical tagging algorithm: for each token, it assigns
      the tag that is most likely for that token's type.  For example,
      it will assign the tag "JJ" to any occurrence of the word
      "frequent," since "frequent" is used as an adjective (e.g. "a
      frequent word") more often than it is used as a verb (e.g. "I
      frequent this cafe"). </para>

      <indexterm><primary>training
      corpus</primary></indexterm>
      <para> Before a <literal>UnigramTagger</literal> can be used to
      tag data, it must be trained on a <glossterm>training
      corpus</glossterm>.  It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the <ulink
      url="&refdoc;/nltk.tagger.UnigramTagger-class.html#train"
      ><literal>train</literal></ulink> method, which takes a
      tagged corpus: </para>

<programlisting>
    <emphasis># 'train.txt' is a tagged training corpus</emphasis>
    &prompt;<command> tagged_text_str = open('train.txt').read()</command>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize(tagged_text_str)</command>
    &prompt;<command> tagger = UnigramTagger()</command>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once a <literal>UnigramTagger</literal> has been trained,
      the <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#tag"
      ><literal>tag</literal></ulink> can be used to tag
      untagged corpera: </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str)</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>UnigramTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose type was not
      encountered in the training data. </para>

      <para> Note that, like almost all statistical taggers, the
      performance of <literal>UnigramTagger</literal> is highly
      dependent on the quality of its training set.  In particular, if
      the training set is too small, it will not be able to reliably
      estimate the most likely tag for each word.  Performance will
      also suffer if the training set is significantly different than
      the texts we wish to tag. </para>
      
    </section> <!-- UnigramTagger -->

    <section id="taggers.nthorder"> <title> Nth Order Tagging </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.NthOrderTagger-class.html"
      ><literal>NthOrderTagger</literal></ulink> class implements a
      more advanced statistical tagging algorithm.  In addition to
      considering the token's type, it also considers the
      part-of-speech tags of the <replaceable>n</replaceable> preceding
      tokens. </para>

      <indexterm><primary>context</primary></indexterm>
      <indexterm><primary>bigram taggers</primary></indexterm>
      <indexterm><primary>trigram taggers</primary></indexterm>
      <para> To decide which tag to assign to a token,
      <literal>NthOrderTagger</literal> first constructs a
      <glossterm>context</glossterm> for the token.  This context
      consists of the token's type, along with the part-of-speech tags
      of the <replaceable>n</replaceable> preceding tags.  It then
      picks the tag which is most likely for that context.  Note that
      a 0th order tagger is equivalent to a unigram tagger, since the
      context used to tag a token is just its type.  1st order taggers
      are sometimes called <glossterm>bigram taggers</glossterm>, and
      2nd order taggers are called <glossterm>trigram
      taggers</glossterm>. </para>

      <para> <literal>NthOrderTagger</literal> uses a tagged training
      corpus to determine which part-of-speech tag is most likely for
      each context:</para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize(tagged_text_str)</command>
    &prompt;<command> tagger = NthOrderTagger(3)</command>         <emphasis># 3rd order tagger</emphasis>
    &prompt;<command> tagger.train(train_toks)</command>
</programlisting>

      <para> Once an <literal>NthOrderTagger</literal> has been trained,
      it can be used to tag untagged corpora: </para>

<programlisting>
    &prompt;<command> tokens = WSTokenizer().tokenize(untagged_text_str)</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>

      <para> <literal>NthOrderTagger</literal> will assign the default
      tag <literal>None</literal> to any token whose context was not
      encountered in the training data. </para>

      <indexterm><primary>precision/recall trade-off</primary></indexterm>
      <para> Note that as <replaceable>n</replaceable> gets larger,
      the specificity of the contexts increases; and with it, the
      chance that the data we wish to tag will contain contexts that
      were not present in the training data.  Thus, there is a
      trade-off between the accuracy and the coverage of our results.
      This is a common type of trade-off in natural language
      processing.  It is closely related to the
      <glossterm>precision/recall trade-off</glossterm> that we'll
      encounter later when we discuss information retrieval. </para>

    </section> <!-- NthOrderTagger -->

    <section id="tagger.backoff"> <title> Combining Taggers </title>

      <para> One way to address the trade-off between accuracy and
      coverage is to use the more accurate algorithms when we can, but
      to fall back on algorithms with wider coverage when necessary.
      For example, we could combine the results of a 1st order tagger,
      a 0th order tagger, and an <literal>NN_CD_Tagger</literal>, as
      follows:</para>

      <orderedlist>
        <listitem> 
          <para> Try tagging the token with the 1st order
          tagger. </para>
        </listitem>
        <listitem> 
          <para> If the 1st order tagger is unable to find a tag for
          the token, try finding a tag with the 0th order
          tagger. </para>
        </listitem> 
        <listitem> 
          <para> If the 0th order tagger is also unable to find a tag,
          use the <literal>NN_CD_Tagger</literal> to find a tag. </para>
        </listitem>
      </orderedlist>

      <indexterm><primary>subtaggers</primary></indexterm>
      <para> NLTK defines the <ulink
      url="&refdoc;/nltk.tagger.BackoffTagger-class.html"
      ><literal>BackoffTagger</literal></ulink> class for combining
      taggers in this way.  A <literal>BackoffTagger</literal> is
      constructed from an ordered list of one or more
      <glossterm>subtaggers</glossterm>.  For each token in the input,
      the <literal>BackoffTagger</literal> uses the result of the
      first tagger in the list that successfully found a tag.  Taggers
      indicate that they are unable to tag a token by assigning it the
      special tag <literal>None</literal>.  We can use a
      <literal>BackoffTagger</literal> to implement the strategy
      proposed above: </para>

<programlisting>
    &prompt;<command> train_toks = TaggedTokenizer().tokenize(tagged_text_str)</command>

    <emphasis># Construct the taggers</emphasis>
    &prompt;<command> tagger1 = NthOrderTagger(1)</command>         <emphasis># 1st order tagger</emphasis>
    &prompt;<command> tagger2 = UnigramTagger()</command>           <emphasis># 0th order tagger</emphasis>
    &prompt;<command> tagger3 = NN_CD_Tagger()</command>

    <emphasis># Train the taggers</emphasis>
    &prompt;<command> tagger1.train(train_toks)</command>
    &prompt;<command> tagger2.train(train_toks)</command>

    <emphasis># Combine the taggers</emphasis>
    &prompt;<command> tagger = BackoffTagger([tagger1, tagger2, tagger3])</command>
</programlisting>

      <para> Note that the order in which the taggers are given to
      <literal>BackoffTagger</literal> is important: the taggers
      should be listed in the order that they should be tried.  This
      typically means that more specific taggers should be listed
      before less specific taggers. </para>

      <para> Having defined a combined tagger, we can use it to tag
      new corpora: </para>

<programlisting>
    &prompt;<command> tokens = TaggedTokenizer().tokenize(untagged_text_str)</command>
    &prompt;<command> tagger.tag(tokens)</command>
    ['John'/'NN'@[0w], 'saw'/'VB'@[1w], 'the'/'AT'@[2w], 
     'book'/'NN'@[3w], 'on'/'IN'@[4w], 'the'/'AT'@[5w], ...]
</programlisting>
      
    </section> <!-- Combining Tagger -->

  </section> <!-- Taggers -->

  <section id="impl"> <title> Tagging: A Closer Look </title>

    <para> In the next five sections, we will discuss how each of the
    taggers introduced in the previous section are implemented.  This
    discussion serves several purposes: </para>

    <itemizedlist>
      <listitem>
        <para> It demonstrates how to write classes implementing the
        interfaces defined by NLTK. </para>
      </listitem>
      <listitem>
        <para> It provides you with a better understanding of the
        algorithms and data structures underlying each approach to
        tagging. </para>
      </listitem>
      <listitem>
        <para> It gives you a chance to see some of the code used to
        implement NLTK.  We have tried hard to ensure that the
        implementation of every class in NLTK is easy to understand.
        </para>
      </listitem>
    </itemizedlist>

    <para> Before you read this section, you may wish to read the
    tutorial "<ulink url="&tutdoc;/writing_classes/index.html">
    Writing Classes For NLTK"</ulink>, which describes how to create
    classes that interface with the toolkit. </para>

  </section> <!-- Tagging: A Closer Look -->

  <section id="sequentialtagger"> <title> Sequential Taggers </title>

    <indexterm><primary>sequential tagger</primary></indexterm>
    <para> The four taggers discussed in this tutorial are implemented
    as sequential taggers.  A <glossterm>sequential tagger</glossterm>
    is a tagger that: </para>

    <orderedlist>
      <listitem><para> Assigns tags to one token at a time, starting
      with the first token of the text, and proceeding in sequential
      order. </para> </listitem>
      <listitem><para> Decides which tag to assign a token on the
      basis of that token, the tokens that preceed it, and the
      predicted tags for the tokens that preceed it. </para>
      </listitem>
    </orderedlist>

    <para> To capture this commonality, we define a common base class,
    <ulink url="&refdoc;/nltk.tagger.SequentialTagger-class.html"
    ><literal>SequentialTagger</literal></ulink>.  This base class
    defines <literal>tag</literal> using a new method,
    <ulink url="&refdoc;/nltk.tagger.SequentialTagger-class.html#tag_next"
    ><literal>tag_next</literal></ulink>, which returns the appropriate
    tag for the next token.  However,
    <literal>SequentialTagger</literal> does not implement this new
    method itself.  Instead, each tagger subclass provides its own
    implementation. </para>

    <para> In addition to capturing the commonality between the four
    taggers, the <literal>SequentialTagger</literal> class has another
    advantage: it will allow us to define
    <literal>BackoffTagger</literal> in such a way that each subtagger
    can use the predictions made by the other taggers as context for
    deciding which tags to assign.  See <xref
    linkend="backoff_impl"></xref> for more
    details.</para>

    <section id="sequentialtagger.tag_next"> <title> SequentialTagger.tag_next </title>

      <para> The <ulink
      url="&refdoc;/nltk.tagger.SequentialTagger-class.html#tag_next"
      ><literal>tag_next</literal></ulink> method decides which tag to
      assign a token, given the list of tagged tokens that preceeds
      it.  It takes two arguments: a list of tagged tokens preceeding
      the token to be tagged, and the token to be tagged; and it
      returns the appropriate tag for that token. </para>

    </section> <!-- tag_next -->

    <section id="sequentialtagger.tag"> <title> SequentialTagger.tag </title>

      <para> The implementation of the <literal>tag</literal> method
      is relatively streight forward.  It simply loops through the
      untagged text, calling <literal>tag_next</literal> for each
      token.  It uses the result of each call to
      <literal>tag_next</literal> to create a tagged version of that
      token, and collects these together to form the tagged
      text. </para>

<programlisting>
    <command>def tag(self, text):</command>
        tagged_text = []

        for token in text:
            tag = self.next_tag(tagged_text, token)
            tagged_token = Token(TaggedType(token.type(), tag), token.loc())
            tagged_text.append(tagged_token)

        return tagged_text
</programlisting>

    </section> <!-- tag -->

    <section id="sequentialtagger.impl"> <title> The SequentialTagger Implementation </title>

      <para> The complete listing for
      <literal>SequentialTagger</literal> is: </para>

      <figure><title>The SequentialTagger Implementation</title>
<programlisting>
<command>class SequentialTagger(TaggerI):</command>

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        assert 0, "next_tag not defined by SequentialTagger subclass"

    <command>def tag(self, text):</command>
        tagged_text = []

        <emphasis># Tag each token, in sequential order.</emphasis>
        for token in text:
            <emphasis># Get the tag for the next token.</emphasis>
            tag = self.next_tag(tagged_text, token)

            <emphasis># Use tag to build a tagged token, and add it to tagged_text.</emphasis>
            tagged_token = Token(TaggedType(token.type(), tag), token.loc())
            tagged_text.append(tagged_token)

        return tagged_text
</programlisting>
      </figure>
      
      <para> Note that SequentialTagger requires that subclasses
      define the <literal>tag_next</literal> method; otherwise, the
      <literal>assert</literal> statement will raise an
      exception when the user tries to tag a text. </para>

    </section>

    <section id="sequentialtagger.subclasses"> <title> Subclasses </title>

      <para> The next four sections show how the
      <literal>SequentialTagger</literal> base class can be used to
      define <literal>NN_CD_Tagger</literal>,
      <literal>UnigramTagger</literal>,
      <literal>NthOrderTagger</literal>, and
      <literal>BackoffTagger</literal>.</para>

    </section> <!-- Subclasses -->

  </section> <!-- SequentialTagger -->

  <section id="nncd_impl"> <title> NN_CD_Tagger </title>

    <para> <literal>NN_CD_Tagger</literal> assigns the tag
    <literal>"CD"</literal> to any token whose type appears to be a
    number; and <literal>"NN"</literal> to any other token.  It uses a
    simple regular expression to test whether a token's type is a
    number:</para>

<programlisting>
    r'^[0-9]+(.[0-9]+)?$'
</programlisting>

    <para> This regular expression matches one or more digits, followed
    by an optional period and one or more digits (e.g.,
    "<literal>12</literal>" or "<literal>732.42</literal>").  Note the
    use of "<literal>^</literal>" (which matches the beginning of a
    string) and "<literal>$</literal>" (which matches the end of a
    string) to ensure that the regular expression will only match
    complete token types. </para>

    <para> Since <literal>NN_CD_Tagger</literal> is a subclass of
    <literal>SequentialTagger</literal>, it just needs to define the
    <literal>next_tag</literal> method.  In the case of
    <literal>NN_CD_Tagger</literal>, the <literal>next_tag</literal>
    method is quite simple: </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        if re.match(r'^[0-9]+(.[0-9]+)?$', next_token.type()):
            return 'CD'
        else:
            return 'NN'
</programlisting>

    <para> Since <literal>NN_CD_Tagger</literal>s are stateless, and
    have no customization parameters, the <ulink
    url="&refdoc;/nltk.tagger.NN_CD_Tagger-class.html#__init__"
    ><literal>NN_CD_Tagger constructor</literal></ulink> is empty:
    </para>

<programlisting>
    <command>def __init__(self):</command> pass
</programlisting>

    <para> The complete listing for the
    <literal>NN_CD_Tagger</literal> class is:</para>

    <figure><title>The NN_CD_Tagger Implementation</title>
<programlisting> 
<command>class NN_CD_Tagger(SequentialTagger):</command>

    <command>def __init__(self):</command> pass

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Assign the 'CD' tag for numbers; and 'NN' for anything else.</emphasis>
        if re.match(r'^[0-9]+(.[0-9]+)?$', next_token.type()):
            return 'CD'
        else:
            return 'NN'
</programlisting>
    </figure>

    <para> Note that <literal>NN_CD_Tagger</literal> does
    <emphasis>not</emphasis> define <literal>tag</literal>.  When the
    <literal>tag</literal> method is called, the definition given by
    <literal>SequentialTagger</literal> will be used. </para>

  </section> <!-- NN_CD_Tagger -->

  <section id="unigram_impl"> <title> UnigramTagger </title>

    <para> <literal>UnigramTagger</literal> tags each token with the
    tag that is most likely to go with the token's type.  It uses a
    training corpus to decide which tag is most likely for each type.
    In particular, it assumes that the tag that occurs most frequently
    with a type is the most likely tag for that type.  For example, if
    the training corpus contains the word "track" as a noun 18 times,
    and as a verb 7 times, then it will assign the noun tag to any
    tokens whose type is "track." 
        <footnote> <para> We considered using a conditional
        probability distribution, instead of a conditional frequency
        distribution.  However, for most probability distributions,
        the maximum probability sample is always equal to the maximum
        frequency sample in the underlying frequency distributions.
        We decided that the additional complexity involved in using
        <literal>ConditionalProbDist</literal> was not justified. </para>
        </footnote></para>

    <para> UnigramTagger uses a <literal>ConditionalFreqDist</literal>
    to record the most likely tag for each type.  
        <footnote><para>See the <ulink
        url="&tutdoc;/probability/index.html">probability
        tutorial</ulink> for information about constructing and using
        frequency distributions.</para> </footnote>
    The <ulink url="&refdoc;/nltk.tagger.UnigramTagger-class.html#train"
    ><literal>train</literal></ulink> method constructs this
    conditional frequency distribution from a training corpus. </para>

    <section id="unigram_impl.training"> <title> Training the Unigram Tagger </title>

      <para> Tagging is a prediction problem.  In particular, the
      outcome we are interested in is the tag; and the context that we
      will use to predict the outcome is the token's type.  So we will
      construct a <literal>ConditionalFreqDist</literal> whose samples
      are tags, and whose conditions are token types: </para>

<programlisting>
    <command>def train(self, tagged_tokens):</command>
        for token in tagged_tokens:
            outcome = token.type().tag()
            context = token.type().base()
            self._freqdist[context].inc(outcome)
</programlisting>

    </section> <!-- Training -->

    <section id="unigram_impl.tagging"> <title> Tagging with the Unigram Tagger </title>
      
      <para> To find the most likely tag for a given token, we can use
      the the <ulink
      url="&refdoc;/nltk.probability.ConditionalFreqDist-class.html#__getitem__">indexing
      operator</ulink> to access the <literal>FreqDist</literal> for
      the appropriate context; and use the <ulink
      url="&refdoc;/nltk.probability.FreqDist-class.html#max"
      ><literal>max</literal></ulink> method to find the most likely
      outcome for that frequency distribution.  For example, we could
      find the most likely tag for the base type "bank" as
      follows:</para>

<programlisting>
    &prompt;<command> freqdist['bank'].max()</command>
    'NN'
</programlisting>

      <para> The <literal>next_tag</literal> method must decide which
      tag is most likely for a given token.  It simply consults the
      tagger's conditional frequency distribution to find the tag that
      is most likely for the tokens's type. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        context = next_token.type()
        return self._freqdist[context].max()
</programlisting>

      <note> <para> If a context was not encountered in the training
      corpus, then the frequency distribution for that context will be
      empty; so <literal>max()</literal> will return
      <literal>None</literal>.  Thus, <literal>next_tag</literal> will
      return <literal>None</literal> as for any token whose type was
      not encountered in the training corpus. </para> </note>
      
    </section> <!-- Tagging Words -->

    <section id="unigram_impl.init"> <title> Initializing the Unigram Tagger </title>

      <para> The constructor for <literal>UnigramTagger</literal>
      simply initializes <literal>self._freqdist</literal> with a new
      conditional frequency distribution.  </para>

<programlisting>
    <command>def __init__(self):</command>
        self._freqdist = probability.ConditionalFreqDist()
</programlisting>

    </section> <!-- Initializing the UnigramTagger -->

    <section id="unigram_impl.impl"><title>The UnigramTagger Implementation</title>

      <para> The complete listing for the
      <literal>UnigramTagger</literal> class is:</para>
      
      <figure><title>The UnigramTagger Implementation</title>
<programlisting> 
<command>class UnigramTagger(TaggerI):</command>
class UnigramTagger(SequentialTagger):
    <command>def __init__(self):</command>
        self._freqdist = ConditionalFreqDist()
    
    <command>def train(self, tagged_tokens):</command>
        for token in tagged_tokens:
            context = token.type().base()
            feature = token.type().tag()
            self._freqdist[context].inc(feature)

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        context = next_token.type()
        return self._freqdist[context].max()
</programlisting>
      </figure>
    </section> <!-- UnigramTagger Implementation -->
  </section> <!-- UnigramTagger -->

  <section id="nthorder_impl" xreflabel="NthOrderTagger"> 
    <title> NthOrderTagger </title>

    <para> The <literal>NthOrderTagger</literal> is a generalization
    of the <literal>UnigramTagger</literal>.  Instead of using the
    token's base type as a context, it uses a tuple consisting of the
    token's base type and the tags of the <replaceable>n</replaceable>
    preceding tokens.  This generalization creates two new
    issues. </para>

    <para> First, we must decide how to handle the first
    <replaceable>n</replaceable> tokens, since they do not have
    <replaceable>n</replaceable> preceding tokens.  

    <literal>NthOrderTagger</literal> simply uses the tags that are
    available.  For example, in a 3rd order tagger, the context of the
    second token will contain only the token's type and the first
    token's tag.  Another option would be to simply ignore the first
    <replaceable>n</replaceable> tokens.  As it turns out, which
    approach we take will not have much of an impact, since
    <replaceable>n</replaceable> (the order of the tagger) is
    generally much less than
    <replaceable>n<subscript>train</subscript></replaceable> (the
    number of training samples). </para>

    <para> The second issue is that, when tagging a text, we do not
    have access the the actual tags of the
    <replaceable>n</replaceable> preceding tokens.  However, we do
    have access to our predicted values for these tags.
    <literal>NthOrderTagger</literal> uses these predicted tags, since
    they are likely to be correct.  Assuming that our predictions are
    good, the use of predicted tags instead of actual tags will have a
    relatively minor impact on performance. </para>

    <section id="nthorder_impl.init"> <title> Initializing the Nth Order Tagger</title>

      <para> Having addressed these two issues, we can examine the
      implementation of the <literal>NthOrderTagger</literal>.  The
      constructor simply records <replaceable>n</replaceable>, and
      constructs a new conditional frequency distribution: </para>

<programlisting>
    <command>def __init__(self, n):</command>
        self._n = n
        self._freqdist = probability.ConditionalFreqDist()
</programlisting>

    </section> <!-- NthOrderTagger Constructor -->

    <section id="nthorder_impl.train"> <title> Training the Nth Order Tagger </title>

      <para> To train the <literal>NthOrderTagger</literal>, we
      examine each token, and increment the count of the tag for the
      appropriate context.  For contexts, we use a tuple consisting of
      the <replaceable>n</replaceable> previous tags and the current
      token's base type.  We use a variable called
      <literal>prev_tags</literal> to record the rpevious
      <replaceable>n</replaceable> tags; and update it after examining
      each token. </para>

<programlisting>
    <command>def train(self, tagged_tokens):</command>
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = []
        
        for token in tagged_tokens:
            context = tuple(prev_tags + [token.type().base()])
            feature = token.type().tag()
            self._freqdist[context].inc(feature)

            <emphasis># Update prev_tags</emphasis>
            prev_tags.append(token.type().tag())
            if len(prev_tags) == (self._n+1):
                del prev_tags[0]
</programlisting>

    </section> <!-- NthOrderTagger.train -->

    <section id="nthorder_impl.tag"> <title> Tagging with the Nth Order Tagger </title>

      <para> As with the <literal>UnigramTagger</literal>, we can find
      the most likely tag for each token by using the
      <literal>max</literal> method for the frequency distribution
      with the appropriate context.  But instead of using each token's
      base type as a context, we use a tuple consisting of the
      <replaceable>n</replaceable> previous predicted tags and the
      token's base type. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the tags of the n previous tokens.</emphasis>
        prev_tags = []
        start = max(len(tagged_tokens) - self._n, 0)
        for token in tagged_tokens[start:]:
            prev_tags.append(token.type().tag())

        <emphasis># Return the most likely tag for the token's context.</emphasis>
        context = tuple(prev_tags + [next_token.type()])
        return self._freqdist[context].max()
</programlisting>

    </section> <!-- NthOrderTagger.tag -->

    <section id="nthorder_impl.impl"><title>The NthOrderTagger Implementation</title>

      <para> The complete listing for the
      <literal>NthOrderTagger</literal> class is:</para>
      
      <figure><title>The NthOrderTagger Implementation</title>
<programlisting> 
<command>class NthOrderTagger(SequentialTagger):</command>
    <command>def __init__(self, n):</command>
        self._n = n
        self._freqdist = CFFreqDist()

    <command>def train(self, tagged_tokens):</command>
        <emphasis># prev_tags is a list of the previous n tags that we've assigned.</emphasis>
        prev_tags = []
        
        for token in tagged_tokens:
            context = tuple(prev_tags + [token.type().base()])
            feature = token.type().tag()
            self._freqdist[context].inc(feature)

            <emphasis># Update prev_tags</emphasis>
            prev_tags.append(token.type().tag())
            if len(prev_tags) == (self._n+1):
                del prev_tags[0]

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        <emphasis># Find the tags of the n previous tokens.</emphasis>
        prev_tags = []
        start = max(len(tagged_tokens) - self._n, 0)
        for token in tagged_tokens[start:]:
            prev_tags.append(token.type().tag())

        <emphasis># Return the most likely tag for the token's context.</emphasis>
        context = tuple(prev_tags + [next_token.type()])
        return self._freqdist[context].max()
</programlisting>
      </figure>

    </section> <!-- NthOrderTagger Implementation -->

  </section> <!-- NthOrderTagger -->

  <section id="backoff_impl" xreflabel="BackoffTagger"> 
    <title> BackoffTagger </title>

    <indexterm><primary>subtaggers</primary></indexterm>
    <para> The <literal>BackoffTagger</literal> is used to combine the
    results of a list of <glossterm>subtaggers</glossterm>.  For each
    token to be tagged, the <literal>BackoffTagger</literal> consults
    each subtagger, in order.  Each token is assigned the first
    non-<literal>None</literal> tag returned by a subtagger for that
    token.  If all of the subtaggers return the tag
    <literal>None</literal> for a token, then
    <literal>BackoffTagger</literal> will assign it the tag
    <literal>None</literal>. </para>

    <section id="backoff_impl.init"> <title> Initializing a Backoff Tagger </title>

    <para> The <literal>BackoffTagger</literal> constructor simply
    records the list of subtaggers. </para>

<programlisting>
    <command>def __init__(self, subtaggers):</command>
        self._taggers = subtaggers
</programlisting>

    </section> <!-- Initializing a BackoffTagger -->

    <section id="backoff_impl.tag"> <title> Tagging with the Backoff Tagger </title>

      <para> The implementation of <literal>BackoffTagger</literal> is
      relatively straight-forward.  Its <literal>next_tag</literal>
      method simply calls each subtagger's <literal>next_tag</literal>
      method, in order; and returns the first
      non-<literal>None</literal> tag produced by a subtagger. </para>

<programlisting>
    <command>def next_tag(self, tagged_tokens, next_token):</command>
        for subtagger in self._subtaggers:
            tag = subtagger.next_tag(tagged_tokens, next_token)
            if tag is not None:
                return tag

        <emphasis># Default to None if all subtaggers return None. </emphasis>
        return None
</programlisting>    
    
    </section> <!-- BackoffTagger tagging -->

    <section id="backoff_impl.impl"><title>The BackoffTagger Implementation</title>

      <para> The complete listing for the
      <literal>BackoffTagger</literal> class is:</para>
      
      <figure><title>The BackoffTagger Implementation</title>
<programlisting> 
<command>class BackoffTagger(SequentialTagger):</command>
    <command>def __init__(self, subtaggers):</command>
        self._subtaggers = subtaggers

    <command>def next_tag(self, tagged_tokens, next_token):</command>
        for subtagger in self._subtaggers:
            tag = subtagger.next_tag(tagged_tokens, next_token)
            if tag is not None:
                return tag

        <emphasis># Default to None if all subtaggers return None. </emphasis>
        return None
</programlisting>
      </figure>
    </section> <!-- BackoffTagger impl -->
  </section> <!-- BackoffTagger -->

  <section id="exercises">
    <title>Exercises</title>

    <section id="exercises.combine">
      <title> Combining Taggers with BackoffTagger</title>

      <para> There is typically a trade-off between the accuracy and
      coverage for taggers: taggers that use more specific contexts
      usually produce more accurate results, when they have seen those
      contexts in the training data; but because the training data is
      limited, they are less likely to encounter each context.  The
      <literal>BackoffTagger</literal> addresses this problem by
      trying taggers with more specific contexts first; and falling
      back to the more general taggers when necessary.  In this
      exercise, we examine the effects of using
      <literal>BackoffTagger</literal>. </para>
      
      <orderedlist>
        
        <listitem> <para> Create an <literal>NN_CD_Tagger</literal>, a
        <literal>UnigramTagger</literal>, and a
        <literal>NthOrderTagger</literal>.  Train the
        <literal>UnigramTagger</literal>, and the
        <literal>NthOrderTagger</literal> using a tagged section of
        the Brown corpus.</para> </listitem>
      
        <listitem> <indexterm><primary>accuracy</primary></indexterm>
        <para> Test the performance of each tagger, using a tagged
        section of the Brown corpus.  Record the
        <glossterm>accuracy</glossterm> of the tagger (the
        percentage of tokens that are correctly tagged).  Be sure to
        use a different section of the corpus for testing than you
        used for training. </para> </listitem>
        
        <listitem> <para> Use <literal>BackoffTagger</literal> to
        create three different combinations of the basic taggers.
        Test the accuracy of each combined tagger.  Which
        combinations give the most improvement?  </para>
        </listitem>

        <listitem> <para> Try repeating steps 1-3 with a different
        sized training corpus.  How does it affect your results? </para>
        </listitem>
      </orderedlist>
    </section> <!-- Combine -->
        
    <section id="exercises.context">
      <title> Tagger Context </title>
      
      <para> <literal>NthOrderParser</literal> chooses a tag for a
      token based on its type and the tags of the
      <replaceable>n</replaceable> preceeding tokens.  This is a
      common context to use for parsing, but ceratinly not the only
      possible context. </para>

      <para> Construct a new tagger, subclassed from
      <literal>SequentialTagger</literal>, that uses a different
      context.  If your tagger's context contains multiple elements,
      then you should combine them in a <literal>tuple</literal> (see
      <xref linkend="nthorder_impl"></xref> for an example of this).
      Some possibilities for elements to include are: </para>

      <itemizedlist>
        <listitem> <para> The base type of the current token, or of
        a previous token. </para> </listitem>
        <listitem> <para> The length of the current token's type, or
        of a previous token's type. </para> </listitem>
        <listitem> <para> The first letter of the current token's
        type, or of a previous token's type. </para> </listitem>
        <listitem> <para> The tag a previous token. </para>
        </listitem>
      </itemizedlist>

      <para> Try to choose context elements that you believe will help
      the tagger decide which tag is appropriate.  Keep in mind the
      trade-off between more specific taggers with accurate results;
      and more general taggers with broader coverage. </para>

      <para> Use <literal>BackoffTagger</literal> to combine your
      tagger with other taggers.  How does the combined tagger's
      accuracy compare to the basic tagger?  How does the combined
      tagger's accuracy compare to the combined taggers you created
      in the previous exercise? </para>
    </section> <!-- Tagger Context -->

    <section id="exercises.reverse">
      <title> Reverse Sequential Taggers </title>
      
      <para> Since sequential taggers tag tokens in order, one at a
      time, they can only use the predicted tags to the
      <emphasis>left</emphasis> of the current token to decide what
      tag to assign to a token.  But in some cases, the
      <emphasis>right</emphasis> context can provide more information
      about what tag should be used.  A <glossterm>reverse sequential
      tagger</glossterm> is a tagger that: </para>

      <orderedlist>
        <listitem><para> Assigns tags to one token at a time, starting
        with the last token of the text, and proceeding in
        right-to-left order. </para> </listitem>
        <listitem><para> Decides which tag to assign a token on the
        basis of that token, the tokens that follow it, and the
        predicted tags for the tokens that follow it. </para>
        </listitem>
      </orderedlist>

      <para> There is no need to create new classes to perform reverse
      sequential tagging.  By reversing texts at appropriate times, we
      can use sequential tagging classes to perform reverse sequential
      tagging.  In particular, we should reverse the training text
      before we train the tagger; and reverse the text that we wish to
      tag both before and after we use the sequential tagger. </para>

      <para> Use this technique to create a first order reverse
      sequential tagger.  Measure its accuracy on a tagged section of
      the Brown corpus.  Be sure to use a different section of the
      corpus for testing than you used for training.  How does its
      accuracy compare to a first order sequential tagger, using the
      same training data and test data? </para>
    </section> <!-- Reverse -->

    <section id="example.restart">
      <title> Processing Individual Sentences </title>

      <para> [to be written] Write a modified nth order tagger, that
      ignores tags that are in a previous sentence.  E.g., for a 3nd
      order tagger, if the previous 3 words were "dog/NN ./.  A/DT",
      then just use "DT" and the current token as context. </para>
    </section> <!-- restart at sentences -->

    <section id="example.backoff">
      <title> Alternatives to Backoff </title>

      <para> [to be written] Create a new kind of tagger that combines
      2 or more subtaggers. </para>
    </section> <!-- Alternatives to backoff -->

  </section> <!-- Exercises -->
  </section> <!-- Tagging in NLTK -->

  &index;
</article>



<!-- Keep this comment at the end of the file
Local variables:
mode: xml
sgml-omittag:nil
sgml-shorttag:nil
sgml-namecase-general:nil
sgml-general-insert-case:lower
sgml-minimize-attributes:nil
sgml-always-quote-attributes:t
sgml-indent-step:0
sgml-indent-data:nil
sgml-parent-document:nil
sgml-exposed-tags:nil
sgml-local-catalogs:nil
sgml-local-ecat-files:nil
sgml-default-dtd-file:"/home/ewan/cvstree/nltk/doc/docbook/xml.ced"
End:
-->

