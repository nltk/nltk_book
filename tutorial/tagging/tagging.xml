<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">
]>

<article>
  <articleinfo>
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Tagging</title> &copyright;
<!--
    <title>Tagging</title>
-->
  </articleinfo>

<!--
  <para><emphasis>Sample chapter for Introduction to Computational
    Linguistics and Language Technology, by Steven Bird, Ewan Klein
    and Edward Loper, May 2004</emphasis></para>
-->

<section id="intro">
  <title>Introduction</title>

  <para>
    Many natural language expressions are ambiguous, and we need to
    draw on other sources of information to aid interpretation.  For
    instance, our preferred interpretation of <literal>fruit flies like a
    banana</literal> depends on the presence of contextual cues that
    cause us to expect <literal>flies</literal> to be a noun or a
    verb.  Before we can even address such issues, we need to be able
    to represent the required linguistic information.  Here is a
    possible representation:
  </para>

      <table id="table.fruit1">
        <title/>
        <tgroup cols="5">
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <tbody>
            <row>
	    <entry><literal>Fruit</literal></entry>
	    <entry><literal>flies</literal></entry>
	    <entry><literal>like</literal></entry>
	    <entry><literal>a</literal></entry>
	    <entry><literal>banana</literal></entry>
            </row>
            <row>
              <entry>noun</entry>
              <entry>verb</entry>
              <entry>prep</entry>
              <entry>det</entry>
              <entry>noun</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <table id="table.fruit2">
        <title/>
        <tgroup cols="5">
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <tbody>
            <row>
	    <entry><literal>Fruit</literal></entry>
	    <entry><literal>flies</literal></entry>
	    <entry><literal>like</literal></entry>
	    <entry><literal>a</literal></entry>
	    <entry><literal>banana</literal></entry>
            </row>
            <row>
              <entry>noun</entry>
              <entry>noun</entry>
              <entry>verb</entry>
              <entry>det</entry>
              <entry>noun</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

  <para>
    Most language processing systems must recognize and interpret the
    linguistic structures that exist in a sequence of words.  This task
    is virtually impossible if all we know about each word is its text
    representation.  To determine whether a given string of words has
    the structure of, say, a noun phrase, it is infeasible to check
    through a (possibly infinite) list of all strings which can be
    classed as noun phrases.  Instead we want to be able to generalise
    over <emphasis>classes</emphasis> of words. These <firstterm>word
    classes</firstterm> are commonly given labels such as 'determiner',
    'adjective' and 'noun'.  Conversely, to interpret words we need to
    be able to discriminate between different usages, such as
    <literal>deal</literal> as a noun or a verb.  The process of
    classifying words in this way, and labelling them accordingly, is
    known as <glossterm>part-of-speech tagging</glossterm>,
    <glossterm>POS-tagging</glossterm>, or simply
    <glossterm>tagging</glossterm>.  The collection of tags used for a
    particular task is known as a <glossterm>tag set</glossterm>.
  </para>

 
<!--
 <para>
    In the following sections, we will give a more detailed account of
    the linguistic and practical issues that arise in the course of
    part-of-speech tagging, and then survey how tagging is carried out
    in NLTK. Before launching into this, however, we will give the
    reader a flavour of the uses of tagging. That is,
    we consider three kinds of language analysis where tags play
    an important role: parsing, morphological analysis, and
    stylistics.
  </para>

  <para>
    Most natural language parsers depend on <glossterm>part-of-speech
    tags</glossterm>.  Instead of writing rules like <literal>NP &rarr;
    the dog</literal> and <literal>NP &rarr; three red cars</literal>,
    we can write <literal>NP &rarr; DT JJ* NN</literal>.  In this way,
    the terminal symbols of the grammar can be word categories, instead
    of words, greatly reducing the size of the grammar.

  </para>



  <para>
    <glossterm>Morphological analysis</glossterm> is also assisted by part-of-speech tags.
    For instance, if we encounter the word <literal>deals</literal>
    in running text, should this be analysed as the plural form of a
    noun, e.g., <literal>deal<subscript>N</subscript>+PL</literal>
    or the third-person singular form of a verb, e.g.,
    <literal>deal<subscript>V</subscript>+3PS</literal>?
    A tagger will consider the context in which this word appears,
    and will reliably determine whether it is a noun or a verb.
    Then the morphological analyser can be given either
    <literal>deals/NN</literal> or <literal>deals/VB</literal>
    to process.
  </para>
-->
  <para>
    We earlier presented <xref linkend="table.fruit1"/> and <xref
    linkend="table.fruit2"/> as examples of how a string of word tokens can
    be augmented with information about the word classes that the words
    belong to. In effect, we carried out tagging for the string
    <literal>fruit flies like a banana</literal>. However, tags are more
    usually attached inline to the text they are associated with. This
    is illustrated in the following sentence from
    the Brown Corpus:
    <literal>
      The/at Pantheon's/np$ interior/nn ,/, still/rb in/in its/pp$
      original/jj form/nn ,/, is/bez truly/ql majestic/jj and/cc an/at
      architectural/jj triumph/nn ./.
    </literal> 
<!-- *** According to our table later on, ./. should actually be ./end -->
    Here, the sequence <literal>The/at</literal> means that
    the word token <literal>The</literal> is tagged
    <literal>at</literal>, which is the Brown Corpus tag for
    article.<footnote>
	<para>The reader may be initially puzzled by strings
    like <literal>,/,</literal>. This just means that the tag for a
    comma is '<literal>,'</literal>.</para>
	
      </footnote>
    We can think of tagging as one way of
    <firstterm>annotating</firstterm> a text corpus. Annotation is a way
    of adding information to a text &mdash; indeed, we might like to
    think of it as a way of making explicit information which is already
    implicitly present in the text.
  </para>
  <para>
    What is the value of annotating a text in this way? One illustration
    is the use of tagged corpora to study patterns of word
    usage in different genres (<glossterm>stylistics</glossterm>).  For
    example, we can use the tags to identify all words of a certain
    class, such as modals, then tabulate their frequency of occurrence
    in different genres, as shown in <xref linkend="table.stylistics"/>.
  </para>


<table id="table.stylistics">
  <title>Use of Modals in Brown Corpus, by Genre</title>
  <tgroup cols="7">
    <colspec colwidth='3cm'/>
    <colspec colwidth='12mm'/><colspec colwidth='12mm'/><colspec colwidth='12mm'/>
    <colspec colwidth='12mm'/><colspec colwidth='12mm'/><colspec colwidth='12mm'/>
    <tbody>
      <row>
        <entry>Genre</entry>
        <entry>can</entry><entry>could</entry><entry>may</entry>
        <entry>might</entry><entry>must</entry><entry>will</entry>
      </row>
      <row>
        <entry>skill and hobbies</entry>
        <entry>273</entry><entry>59</entry><entry>130</entry>
        <entry>22</entry><entry>83</entry><entry>259</entry>
      </row>
      <row>
        <entry>humor</entry>
        <entry>17</entry><entry>33</entry><entry>8</entry>
        <entry>8</entry><entry>9</entry><entry>13</entry>
      </row>
      <row>
        <entry>fiction: science</entry>
        <entry>16</entry><entry>49</entry><entry>4</entry>
        <entry>12</entry><entry>8</entry><entry>16</entry>
      </row>
      <row>
        <entry>press: reportage</entry>
        <entry>94</entry><entry>86</entry><entry>66</entry>
        <entry>36</entry><entry>50</entry><entry>387</entry>
      </row>
      <row>
        <entry>fiction: romance</entry>
        <entry>79</entry><entry>195</entry><entry>11</entry>
        <entry>51</entry><entry>46</entry><entry>43</entry>
      </row>
      <row>
        <entry>religion</entry>
        <entry>84</entry><entry>59</entry><entry>79</entry>
        <entry>12</entry><entry>54</entry><entry>64</entry>
      </row>
    </tbody>
  </tgroup>
</table>

<!-- this repeats the text after table 2
  <para>
    The process of associating labels with each token in a text is
    called <glossterm>tagging</glossterm>, and the labels are called
    <glossterm>tags</glossterm>.  The collection of tags used for a
    particular task is known as a <glossterm>tag set</glossterm>.
  </para>
-->

  <para>
    This tutorial focuses on part-of-speech tagging, as an early step
    in language processing which does not depend on deep linguistic
    analysis.  Readers should be aware that are many other kinds of
    tagging.  Words can be tagged with directives to a speech
    synthesiser, indicating which words should be emphasised.
    Words can be tagged with sense numbers, indicating which sense
    of the word was used.  Words can also be tagged with morphological
    features.  Examples of each of these kinds of tags are shown in
    <xref linkend="table.taggingexamples"/>.  Note that for space
    reasons, we only show the tag on a single italicised word. Note also
    that the first two examples use XML-style tags, where elements in
    angle brackets enclose the word that is tagged.
  </para>

<table id="table.taggingexamples">
  <title>Examples of Non Part-of-Speech Tagging</title>
  <tgroup cols="2">
    <colspec colwidth='6cm'/>
    <tbody>
      <row>
        <entry>Speech Synthesis Markup Language (W3C SSML)</entry>
        <entry>That is a
        &lt;emphasis><emphasis>big</emphasis>&lt;/emphasis> car!</entry>
      </row>
      <row>
        <entry>SemCor: Brown Corpus tagged with WordNet senses</entry>
        <entry>Space in any
          &lt;wf pos="NN" lemma="form" wnsn="4"><emphasis>form</emphasis>&lt;/wf>
          is completely measured by the three dimensions.
          <emphasis>
            (Wordnet form/nn sense 4: "shape, form, configuration,
            contour, conformation")
          </emphasis>
        </entry>
      </row>
      <row>
        <entry>Morphological tagging, from the Turin University Italian Treebank</entry>
        <entry>
          E' italiano , come progetto e realizzazione , il
          <emphasis>primo</emphasis> (PRIMO ADJ ORDIN M SING)
          porto turistico dell' Albania .
        </entry>
      </row>
    </tbody>
  </tgroup>
</table>

</section> <!-- Introduction -->

<section id="pos">
  <title>Word Classes and Parts of Speech</title>

<!-- 
X.2 linguistic overview (for non-linguist readers)
- how have linguists addressed the problem?
- what are the shortcomings of the non-computational approach?
-->

  <para>
    There is a long tradition within linguistics of classifying words
    into categories called parts of speech. As we have seen,  these
      are sometimes also called  word classes or <firstterm>lexical
	categories</firstterm>.
    Familiar examples are <type>noun</type>, <type>verb</type>,
    <type>preposition</type>, <type>adjective</type> and
    <type>adverb</type>.  In this section we present the standard
    criteria for categorising words in this way, then discuss the main
    classes of words in English.
  </para>

<section id="pos.categorise">
  <title>Categorising Words</title>

  <para>
    How do we decide what category a word should belong to? In general,
    linguists invoke three kinds of criteria for making the decision:
    formal; syntactic (or distributional); notional (or semantic).
    A <firstterm>formal</firstterm> criterion is one which looks at
    the internal structure of a word. For example,
    <literal>-ness</literal> is a suffix which combines with an
    adjective to produce a noun. Examples are <literal>happy</literal>
    &gt; <literal>happiness</literal>, <literal>ill</literal> &gt;
    <literal>illness</literal>.<footnote>
     <para>We use <markup>&gt;</markup> to mean 'is derived from'.</para>
	</footnote>
So if we encounter a word which ends
    in <literal>-ness</literal>, this is very likely to be a noun.
</para>

  <para>
    A <firstterm>syntactic</firstterm> criterion refers to the
    syntactic contexts in which a word can occur. For example, assume
    that we have already determined the category of nouns. Then we
    might say that a syntactic criterion for an adjective in English
    is that it can occur immediately before a noun, or immediately
    following the words <literal>be</literal> or
    <literal>very</literal>. According to these tests,
    <literal>near</literal> should be categorised as an adjective:
  </para>
    <orderedlist>
      <listitem><para>the near window</para></listitem>
      <listitem><para>The end is (very) near.</para></listitem>
    </orderedlist>


  <para>
    A familiar example of a <firstterm>notional</firstterm> criterion
    is that a noun is <quote>the name of a person, place or
    thing</quote>. Within modern linguistics, notional criteria for
    word classes have be viewed with considerable suspicion, mainly
    because they are hard to formalise. Nevertheless, notional
    criteria underpin many of our intuitions about word classes, and
    enable us to make a good guess about the categorisation of words
    in languages that we are unfamiliar with; that is, if we all we
    know about the Dutch <literal>verjaardag</literal> is that it
    means the same as the English word <literal>birthday</literal>,
    then we can guess that <literal>verjaardag</literal> is a noun in
    Dutch. However, some care is needed: although we might translate
    <literal>zij is van dag jarig</literal> as <literal>it's her
    birthday today</literal>, the word <literal>jarig</literal> is in
    fact an adjective in Dutch, and has no exact equivalent in
    English.
  </para>

<!--http://www.askoxford.com/pressroom/archive/odelaunch/-->

  <para>
    All languages acquire new lexical items. A list of words recently
    added to the Oxford Dictionary of English includes
    <literal>cyberslacker, fatoush, blamestorm, SARS, cantopop, bupkis,
    noughties, muggle</literal>, and <literal>robata</literal>. Notice
    that all these new words are nouns, and this is reflected in calling
    nouns an <glossterm>open class</glossterm>. By contrast,
    prepositions are regarded as a <glossterm>closed
    class</glossterm>. That is, there is a limited set of words
    belonging to the class (e.g., <literal>above, along, at, below,
    beside, between, during, for, from, in, near, on, outside, over,
    past, through, towards, under, up, with</literal>), and membership
    of the set only changes very gradually over time.
<!--    
    Some word classes consist of a limited set of so-called
    <firstterm>function</firstterm>
    words. Prepositions are one such class, comprising items like
     etc.  These are called
    <glossterm>closed classes</glossterm>, in the sense that although
    languages acquire new lexical items.  Content words such as
    nouns are not limited in this way, and are continually being
    extended with the invention of new words.  These are called
    <glossterm>open classes</glossterm>.
-->
  </para>

</section>

<section id="pos.english">
  <title>English Word Classes</title>

  <para>
    This section presents a brief overview of English word classes.
    Readers requiring more detail are encouraged to consult a grammar
    of English.
  </para>

  <para>
    Linguists commonly recognize four major categories of open class
    words in English, namely nouns, verbs, adjectives and adverbs.
    Nouns generally refer to people, places, things, or concepts, e.g.:
    <emphasis>woman, Scotland, book, intelligence</emphasis>.  In the
    context of a sentence, nouns can appear after determiners and
    adjectives, and can be the subject or object of the verb:
  </para>

  <table id="table.nouns">
    <title>Syntactic Patterns involving some Nouns</title>
    <tgroup cols="3">
      <colspec colwidth='2cm'/>
      <thead>
        <row>
          <entry>Word</entry>
          <entry>After a determiner</entry>
          <entry>Subject of the verb</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>woman</entry>
          <entry><emphasis>the</emphasis> woman who I saw yesterday ...</entry>
          <entry>the woman <emphasis>sat</emphasis> down</entry>
        </row>
        <row>
          <entry>Scotland</entry>
          <entry><emphasis>the</emphasis> Scotland I remember as a child ...</entry>
          <entry>Scotland <emphasis>has</emphasis> five million people</entry>
        </row>
        <row>
          <entry>book</entry>
          <entry><emphasis>the</emphasis> book I bought yesterday ...</entry>
          <entry>this book <emphasis>recounts</emphasis> the colonisation of Australia</entry>
        </row>
        <row>
          <entry>intelligence</entry>
          <entry><emphasis>the</emphasis> intelligence displayed by the child ...</entry>
          <entry>Mary's intelligence <emphasis>impressed</emphasis> her teachers</entry>
        </row>
      </tbody>
    </tgroup>
  </table>

  <para>
    English nouns can be morphologically complex.  For example, words like
    <literal>books</literal> and <literal>women</literal> are plural.
    As we saw earlier, words with the <literal>-ness</literal> suffix are
    nouns that have been derived from adjectives, e.g. <literal>happiness</literal>
    and <literal>illness</literal>.  The <literal>-ment</literal> suffix
    appears on certain nouns derived from verbs, e.g. <literal>government</literal>
    and <literal>establishment</literal>.
  </para>

  <para>
    Nouns are usually further classified as <glossterm>common
    nouns</glossterm> and <glossterm>proper nouns</glossterm>.  Proper
    nouns identify particular individuals or entities,
    e.g. <literal>Moses</literal> and <literal>Scotland</literal>,
    while common nouns are all the rest.  Another important
    distinction exists between <glossterm>count nouns</glossterm> and
    <glossterm>mass nouns</glossterm>.  Count nouns are thought of as
    distinct entities which can be counted, such as
    <literal>pig</literal> (e.g. <literal>one pig, two pigs, many
    pigs</literal>).  They cannot occur with the word
    <literal>much</literal> (i.e. *<literal>much pigs</literal>).
    Mass nouns, on the other hand, are not thought of as distinct
    entities (e.g.  <literal>sand</literal>).  They cannot be
    pluralised, and do not occur with numbers (e.g. *<literal>two
    sands</literal>, *<literal>many sands</literal>).  However, they
    can occur with <literal>much</literal> (i.e. <literal>much sand</literal>).
  </para>
    
  <para>
    Verbs are words which describe events and actions,
    e.g. <literal>fall</literal>, <literal>eat</literal>.
    In the context of a sentence, verbs express a relation involving the
    referents of one or more
    noun phrases.
  </para>


  <table id="table.verbs">
    <title>Syntactic Patterns involving some Verbs</title>
    <tgroup cols="3">
      <colspec colwidth='2cm'/>
      <colspec colwidth='5cm'/>
      <thead>
        <row>
          <entry>Word</entry>
          <entry>Simple</entry>
          <entry>With modifiers and adjuncts (italicised)</entry>
        </row>
      </thead>
      <tbody>
        <row>
          <entry>fall</entry>
<!-- probably more plausible to think of "last week" as a sentence
modifier -->
          <entry>Rome fell</entry>
	      <entry>Last week, dot com stocks
	      <emphasis>suddenly</emphasis> fell <emphasis>like a
	      stone</emphasis></entry>
        </row>
        <row>
          <entry>eat</entry>
          <entry>Mice eat cheese</entry>
          <entry>John ate the pizza <emphasis>with gusto</emphasis></entry>
        </row>
      </tbody>
    </tgroup>
  </table>

  <para>
    Verbs can be classified according to the number of arguments
    (usually noun phrases) that
    they co-occur with.  The word <literal>fall</literal> is
    <glossterm>intransitive</glossterm>, requiring exactly one
    argument (the entity which falls).  The word
    <literal>eat</literal> is <glossterm>transitive</glossterm>,
    requiring two arguments (the eater and the eaten).  Other verbs
    are more complex; for instance <literal>put</literal> requires
    three arguments, the agent doing the putting, the entity being put
    somewhere, and a location.  The <literal>-ing</literal> suffix
    appears on nouns derived from verbs, e.g. <literal>the
    falling of the leaves</literal> (this is known as the
    <glossterm>gerund</glossterm>).
  </para>

  <para>
    English verbs can be morphologically complex.  For instance,
    the <glossterm>present participle</glossterm> of a verb ends
    in <literal>-ing</literal>, and expresses the idea of ongoing,
    incomplete action (e.g. <literal>falling, eating</literal>).
    The <glossterm>past participle</glossterm> of a verb often
    ends in <literal>-ed</literal>, and expresses the idea of a
    completed action (e.g. <literal>fell, ate</literal>).
  </para>

  <para>
    Two other important word classes are
    <glossterm>adjectives</glossterm> and
    <glossterm>adverbs</glossterm>.  Adjectives describe nouns, and
    can be used as modifiers (e.g. <literal>large</literal> in
    <literal>the large pizza</literal>), or in predicates
    (e.g. <literal>the pizza is large</literal>).  English adjectives
    can be morphologically complex (e.g.
    <literal>fall<subscript>V</subscript>+ing</literal> in
    <literal>the falling stocks</literal>).
    Adverbs modify verbs to specify the time, manner, place or
    direction of the event described by the verb
    (e.g. <literal>quickly</literal> in <literal>the stocks fell quickly</literal>).
    Adverbs may also modify adjectives (e.g. <literal>really</literal>
    in <literal>Mary's teacher was really nice</literal>).
  </para>

  <para>
    English has several categories of closed class words in addition to
    prepositions, and each dictionary and grammar classifies them
    differently.  <xref linkend="table.closed_class"/> gives a sample of
    closed class words, following the classification of the Brown
    Corpus.<footnote>
     <para>Note that part-of-speech tags may be presented as either
     upper-case or lower-case strings &mdash; there is no significance
     attached to this difference.</para>
     </footnote>
  </para>


<table id="table.closed_class">
  <title>Some English Closed Class Words, with Brown Tag</title>
  <tgroup cols="3">
    <colspec colwidth='1.5cm'/>
    <colspec colwidth='6cm'/>
    <tbody>
      <row>
        <entry> ap </entry>
        <entry> determiner/pronoun, post-determiner </entry>
        <entry> many other next more last former little several enough
                 most least only very few fewer past same </entry>
      </row>
      <row>
        <entry> at </entry>
        <entry> article </entry>
        <entry> the an no a every th' ever' ye </entry>
      </row>
      <row>
        <entry> cc </entry>
        <entry> conjunction, coordinating </entry>
        <entry> and or but plus & either neither nor yet 'n' and/or minus an' </entry>
      </row>
      <row>
        <entry> cs </entry>
        <entry> conjunction, subordinating </entry>
        <entry> that as after whether before while like because if since for
                than until so unless though providing once lest
                till whereas whereupon supposing albeit then </entry>
      </row>
      <row>
        <entry> in </entry>
        <entry> preposition </entry>
        <entry> of in for by considering to on among at through with under
                into regarding than since despite ... </entry>
      </row>
      <row>
        <entry> md </entry>
        <entry> modal auxiliary </entry>
        <entry> should may might will would must can could shall ought need 
                 wilt </entry>
      </row>
      <row>
        <entry> pn </entry>
        <entry> pronoun, nominal </entry>
        <entry> none something everything one anyone nothing nobody everybody
                everyone anybody anything someone no-one nothin' </entry>
      </row>
      <row>
        <entry> ppl </entry>
        <entry> pronoun, singular, reflexive </entry>
        <entry> itself himself myself yourself herself oneself ownself </entry>
      </row>
      <row>
        <entry> pp$ </entry>
        <entry> determiner, possessive </entry>
        <entry> our its his their my your her out thy mine thine </entry>
      </row>
      <row>
        <entry> pp$$ </entry>
        <entry> pronoun, possessive </entry>
        <entry> ours mine his hers theirs yours </entry>
      </row>
      <row>
        <entry> pps </entry>
        <entry> pronoun, personal, nominative, 3rd person singular </entry>
        <entry> it he she thee </entry>
      </row>
      <row>
        <entry> ppss </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular </entry>
        <entry> they we I you ye thou you'uns </entry>
      </row>
      <row>
        <entry> wdt </entry>
        <entry> WH-determiner </entry>
        <entry> which what whatever whichever</entry>
      </row>
      <row>
        <entry> wps </entry>
        <entry> WH-pronoun, nominative </entry>
        <entry> that who whoever whosoever what whatsoever </entry>
      </row>
    </tbody>
  </tgroup>
</table>



</section>

  <section id="syntax">

   <title>Tags: Interfacing between Words and Syntax</title>

   <para>We have already noted that part-of-speech tags are closely
   related to the notion of word class used in syntax.
   The assumption in theoretical linguistics  is that every
   distinct word type will be listed in a lexicon (or dictionary), with
   information about its pronunciation, syntactic properties and
   meaning. A key component of the word's syntactic properties will be
   its class. When we carry out a syntactic analysis of our earlier
   example <literal>fruit flies like a banana</literal>, we will look
   up each word in the lexicon, determine its word class, and then
   group it into a hierarchy of phrases, as illustrated in the following
   parse tree.
   </para>

<figure id="syn-tree"><title>Syntactic Parse Tree</title>
<informaltable frame="topbot">
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/syn-tree" scale="6"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

   <para>In this tree, we have used standard syntactic abbreviations for word
    classes. These are shown in <xref linkend="table.class-tag"/>
    together with their counterparts from the Brown tag set.
   </para>

    <table id="table.class-tag">
        <title>Word Class Labels and Tags</title>
        <tgroup cols="3">
            <colspec colwidth='3.5cm'/>
            <colspec colwidth='2cm'/>
            <colspec colwidth='5cm'/>
     <thead>
      <row>
       <entry>Word Class Label</entry>
       <entry>Brown Tag</entry>
       <entry>Word Class</entry>
      </row>
     </thead>
            <tbody>
                <row>
                    <entry>Det</entry>
                    <entry>at</entry>
                    <entry>Article</entry>
                </row>
                <row>
                    <entry>N</entry>
                    <entry>nn</entry>
                    <entry>Noun</entry>
                </row>
                <row>
                    <entry>V</entry>
                    <entry>vb</entry>
                    <entry>Verb</entry>
                </row>
                <row>
                    <entry>Adj</entry>
                    <entry>jj</entry>
                    <entry>Adjective</entry>
                </row>
                <row>
                    <entry>P</entry>
                    <entry>in</entry>
                    <entry>Preposition</entry>
                </row>
                <row>
                    <entry>Card</entry>
                    <entry>cd</entry>
                    <entry>Number</entry>
                </row>
                <row>
                    <entry>&ndash;</entry>
                    <entry>end</entry>
                    <entry>Sentence-ending punctuation</entry>
                </row>
            </tbody>
        </tgroup>
    </table>

   <para>In
    syntactic analyses, there is often a close connection between the
    class a word belongs to and the phrases it forms with neighbouring
    words. So, for example, a noun phrase (labelled NP) will usually
    consist of a noun (N) optional accompanied by an article (Det) and
    some modifiers such as adjectives (Adj). We can  express this
    generalisation in terms of a production rule:
    <simplelist>
     <member><literal>NP &rarr; Det Adj* N</literal></member>
    </simplelist>
    In such a case, we say that the noun is the
    <glossterm>head</glossterm> of the noun phrase; roughly speaking,
    the head of a phrase is a required element which can occur in any
    context that the phrase as a whole can occur in.  However, from a
    purely notational point of view, we are free to use any labels we
    want for word classes, and these could just as well be the labels
    provided by a tag set. For example, we could replace the preceding
    rule with the following:
    <simplelist>
     <member><literal>NP &rarr; at jj* nn</literal></member>
    </simplelist>
    This is useful since some practical tasks, we might use an
    automatic tagger to label the
    words in our example with tags drawn from the Brown tag set, and use
    that as a basis for building a syntactic parse tree like <xref
    linkend="syn-tree"/>. 
   </para>

   <para>
    So far, we have only looked at tags as capturing information about
    word class. However, common tag sets like those used in the Brown
    Corpus also capture a certain amount of
    <glossterm>morpho-syntactic</glossterm> information. Consider, for
    example, the selection of distinct forms of the word <literal>be</literal>
    illustrated in the following sentences:
    <simplelist>
     <member><literal>Be still!</literal></member>
     <member><literal>Being still is hard.</literal></member>
     <member><literal>I am still.</literal></member>
     <member><literal>I have been still all day.</literal></member>
     <member><literal>I was still all day.</literal></member>
    </simplelist>
    We say that these forms are morpho-syntactically distinct because
    they exhibit different morphological inflections and different
    co-occurrence restrictions with neighbouring words. For example,
    <literal>am</literal> cannot replace either of the first two
    examples:
    <simplelist>
     <member><literal>*Am still!</literal></member>
     <member><literal>*Am still is hard.</literal></member>
     </simplelist>
    These differences between the forms are encoded in their Brown
    Corpus tags: <literal>be/be, being/beg, am/bem, been/ben</literal>
    and <literal>was/bedz</literal>. This means that an automatic tagger
    which uses this tag set is in effect carrying out a limited amount of
    morphological analysis.
</para>

  </section>

<section id="tagging">
  <title> Part-of-Speech Tag Sets </title>

  <para>
    Most part-of-speech tag sets make use of the same basic categories,
    such as noun, verb, adjective, and preposition. However, tag sets
    differ both in how finely they divide words into categories; and in
    how they define their categories. For example, <literal>is</literal>
    might be tagged as a verb in one tag set; but as a distinct form of
    <literal>to be</literal> in another tag set &mdash; in fact, we just
    observed the latter situation in the Brown Corpus tag set.  This
    variation in tag sets is unavoidable, since part-of-speech tags are
    used in different ways for different tasks. In other words, there is
    no one 'right way' to assign tags, only more or less useful ways,
    depending on one's goals.
<!--
  <note><para> There are several part-of-speech tag sets in widespread
  use, because there are different schemes for classifying words
  (owing to the different weight given to formal, syntactic and
  notional criteria), and because different processing tasks call for
  finer or coarser classification.</para></note>
-->
  </para>

  <para>
    Observe that the tagging process simultaneously collapses
    distinctions (i.e., lexical identity is usually lost when all
    personal pronouns are tagged <literal>prp</literal>), while
    introducing distinctions and removing ambiguities
    (e.g. <literal>deal</literal> tagged as <literal>vb</literal> or
    <literal>nn</literal>).  This move facilitates classification and
    prediction.  Observe that when we introduce finer distinctions in
    a tag set, we get better information about linguistic context, but
    we have to do more work to classify the current token (there are
    more tags to choose from).  Conversely, with fewer distinctions,
    we have less work to do for classifying the current token, but
    less information about the context to draw on.
  </para>


    <para>
      In this tutorial, we will use the tag set listed in <xref
      linkend="table.class-tag"/> above. As we mentioned, this is a
      radically simplified version of the Brown Corpus tag set, which in
      its entirety has 87 basic tags plus many
      combinations.  A fuller list is given in the Appendix.
    </para>

  </section> <!-- part-of-speech tagsets -->

</section> <!-- Word Classes and Parts of Speech -->


<section id="basics.representation">
  <title> Representing Tagged Tokens and Tagged Corpora </title>

  <para>
    The preceding sections have discussed the nature and use of
    tags in language processing.  In this section the computational
    representation of tags is presented.  First we consider individual
    tagged tokens, and show how they are created and accessed.  Then
    we address tagged corpora.
  </para>

  <para>
    Recall that an NLTK token is a kind of Python dictionary,
    and we can associate arbitrary additional properties with a token.
    By convention, a tagged token is represented by adding a <literal>TAG</literal>
    property to the token.  This can be done when the token is constructed, as follows:
  </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly', TAG='nn')
>>> print tok
<fly/nn>
]]></programlisting>

  <para>
    We can access the properties of this token in the usual way, as
    shown below:
  </para>

<programlisting><![CDATA[
>>> print tok['TEXT']
fly
>>> print tok['TAG']
nn
]]></programlisting>

  <para>
    Sometimes we wish to add a tag to an existing token that lacks
    a tag.  This can be done as follows.  First we create a token
    consisting of text alone, and then add the tag:
  </para>

<programlisting><![CDATA[
>>> tok = Token(TEXT='fly')
<fly>
>>> tok['TAG'] = 'nn'
>>> print tok
<fly/NN>
]]></programlisting>

  <para> Several large corpora (such as the Brown Corpus and portions of the Wall
    Street Journal) have been manually tagged with part-of-speech tags.
    Before we can use these corpora, we must read them from files
    and tokenize them.
  </para>

  <para> Tagged texts are usually stored in files as a sequences of
    whitespace-separated tokens, where each token is of the form
    <literal>text/tag</literal>, as illustrated below for a sample
    from the Brown Corpus.
  </para>

<para><literal>
The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in
other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc
Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps
said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb
accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt
interest/nn of/in both/abx governments/nns ''/'' ./.
</literal></para>

  <para> It is possible to use the <literal>nltk.corpus</literal> module to read and
    tokenize data from a tagged corpus, as shown below: </para>

<programlisting><![CDATA[
>>> from nltk.corpus import brown
>>> tok = brown.tokenize('ca01')
>>> print tok['SUBTOKENS']
[<The/at>, <Fulton/np-tl>, <County/nn-tl>, <Grand/jj-tl>, <Jury/nn-tl>,
<said/vbd>, <Friday/nr>, <an/at>, <investigation/nn>, <of/in>, <Atlanta's/np$>,
<recent/jj>, <primary/nn>, <election/nn>, <produced/vbd>, <``/``>, <no/at>,
<evidence/nn>, <''/''>, <that/cs>, <any/dti>, <irregularities/nns>, <took/vbd>,
<place/nn>, <./.>, ...]
]]></programlisting>

  <para>
    Observe that tokenizing a tagged text produces a single token
    which contains a sequence of tagged
    tokens, stored in its <literal>SUBTOKENS</literal> property.
    Here is another example which constructs a single token for the
    text string, then calls the <literal>TaggedTokenizer()</literal>
    to add the subtokens.
  </para>

<programlisting><![CDATA[
>>> text_str = """
John/nn saw/vb the/at book/nn on/in the/at table/nn ./end  He/nn sighed/vb ./end
"""
>>> text_token = Token(TEXT=text_str)
>>> TaggedTokenizer().tokenize(text_token)
>>> print text_token['SUBTOKENS']
[<John/nn>, <saw/vb>, <the/at>, <book/nn>, <on/in>, <the/at>, 
 <table/nn>, <./end>, <He/nn>, <sighed/vb>, <./end>]
]]></programlisting>

  <important><para> If <literal>TaggedTokenizer</literal> encounters a word without a tag, it
    will assign the word the default tag <literal>None</literal>. </para></important>

  <para>The subtokens are the individual words of interest, and they are accessed
    using the <literal>SUBTOKENS</literal> property.  Each subtoken has a
    <literal>TEXT</literal> and <literal>TAG</literal> property.
  </para>

  <important><para>
    The <literal>properties</literal> member function of a token can be
    used to list the properties defined for that token, in case there is
    any doubt about which property names have been used:
  </para></important>

<programlisting><![CDATA[
>>> print text_token['SUBTOKENS'][1]
<saw/vb>
>>> print text_token['SUBTOKENS'][1]['TAG']
'vb'
>>> print text_token.properties()
['TEXT', 'SUBTOKENS']
>>> print text_token['SUBTOKENS'][1].properties()
['TEXT', 'TAG']
]]></programlisting>

  <para>
    Now that we can load a significant quantity of tagged text, we can process it
    and extract items of interest.  The following code iterates over the fifteen
    genres of the Brown Corpus (accessed using <literal>brown.groups()</literal>).
    The material for each genre lives in a set of files (accessed using
    <literal>brown.items()</literal>).  Each of these is
    tokenized in turn, and stored to <literal>text_token</literal>.
  </para>

<programlisting><![CDATA[
>>> from nltk.corpus import brown
>>> from nltk.probability import ConditionalFreqDist
>>> cfdist = ConditionalFreqDist()
>>> for genre in brown.groups():                   # each genre
...     for item in brown.items(genre):            # each file
...         text_token = brown.tokenize(item)      # tokenize
]]></programlisting>

  <para>
    The next step is to construct a list of the modals that
    were found.  To do this we extract and normalize the text
    from each token found to have the <literal>md</literal> tag.
    For each of these words we increment a count.  This uses the
    conditional frequency distribution, where the condition is the
    current genre, and the event is the modal.
  </para>

<programlisting><![CDATA[
...         found = [token['TEXT'].lower()                 # normalize
...                  for token in text_token['SUBTOKENS']  # each token
...                  if token['TAG'] == 'md']              # that's a modal
...         for modal in found:
...             cfdist[genre].inc(modal)                   # increment count
]]></programlisting>

  <para>
    The conditional frequency distribution is nothing more than
    a mapping from each genre to the
    distribution of modals in that genre.  The following code
    fragment identifies a small set of modals of interest, and
    processes the data structure to output the required counts.
  </para>

<programlisting><![CDATA[
>>> modals = ['can', 'could', 'may', 'might', 'must', 'will']

>>> print "%40s" % 'Genre',              # generate column headings
>>> for modal in modals:
...     print "%6s" % modal,
>>> print

>>> for genre in cfdist.conditions():    # generate rows
...     print "%40s" % genre,
...     for modal in modals:
...         print "%6d" % cfdist[genre].count(modal),
...     print

                                   Genre    can  could    may  might   must   will
                       skill and hobbies    273     59    130     22     83    259
                                   humor     17     33      8      8      9     13
                            popular lore    168    142    165     45     95    163
                          belles-lettres    249    216    213    113    169    222
                        fiction: science     16     49      4     12      8     16
                        press: reportage     94     86     66     36     50    387
miscellaneous: government & house organs    115     37    152     13     99    237
                      fiction: adventure     48    154      6     58     27     48
                        fiction: mystery     44    145     13     57     31     17
                        fiction: romance     79    195     11     51     46     43
                                religion     84     59     79     12     54     64
                                 learned    366    159    325    126    202    330
                          press: reviews     44     40     45     26     18     56
                        press: editorial    122     56     74     37     53    225
                        fiction: general     39    168      8     42     55     50
]]></programlisting>

  <para>
    There are some interesting patterns in this table.  For instance,
    compare the rows for government literature and adventure
    literature; the former is dominated by the use of <literal>can, may, must,
    will</literal> while the latter is characterised by the use
    of <literal>could</literal> and <literal>might</literal>.  With
    some further work it might be possible to guess the genre of a new
    text automatically, according to its distribution of modals.
  </para>

  <para>
    Now that we have seen how tagged tokens and tagged corpora are
    created and accessed, we are ready to take a look at automatic
    tagging.
  </para>

  <important><para>
    In NLTK, tokenization and tagging are operations which
    <emphasis>annotate</emphasis> existing data.  For instance,
    when a document is stored in a single text token, then tokenized
    to create a set of subtokens, the original text is still available
    via the <literal>TEXT</literal> property.  Note that the default
    printing method for tokenized text simply does not print this material.
  </para></important>


</section> <!-- Tagged Tokens and Tagged Corpora -->


<section id="taggers.simple">
  <title> Simple Taggers </title>

  <para>
    In this section we consider three simple taggers.  They all
    process the input tokens one by one, adding a tag to each token.
    In each case they being with tokenized text.  We can easily create
    a sample of tokenized text as follows:
  </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT="John saw 3 polar bears .")
>>> WhitespaceTokenizer().tokenize(text_token)
>>> print text_token
<[<John>, <saw>, <3>, <polar>, <bears>, <.>]>
]]></programlisting>

  <section id="taggers.unigram.default">
    <title> The Default Tagger </title>

    <para>
      The simplest possible tagger assigns the same tag to each token
      regardless of the token's text.  The <literal>DefaultTagger</literal>
      class implements this kind of tagger.  In the following program,
      we create a tagger called <literal>my_tagger</literal> which
      tags everything as a noun.
    </para>

<programlisting><![CDATA[
>>> my_tagger = DefaultTagger('nn')
>>> my_tagger.tag(text_token)
>>> print text_token
<[<John/nn>, <saw/nn>, <3/nn>, <polar/nn>, <bears/nn>, <./nn>]>
]]></programlisting>

    <para>
      This is a simple algorithm, and it performs poorly when used
      on its own. On a typical corpus, it will tag only 20%-30% of
      the tokens correctly. However, it is a very reasonable
      tagger to use as a default, if a more advanced tagger fails
      to determine a token's tag. When used in conjunction with
      other taggers, a <literal>DefaultTagger</literal> can
      significantly improve performance.
    </para>

    <important><para>
      Default taggers assign their tag to every single
      word, even words that have never been encountered before.
      Thus, they help to improve the robustness of a language
      processing system.
    </para></important>

  </section> <!-- DefaultTagger -->

  <section id="taggers.regexp">
    <title> The Regular Expression Tagger </title>

    <para>
      The regular expression tagger assigns tags to tokens on the
      basis of matching patterns in the token's text.  For instance,
      the following tagger assigns <literal>cd</literal> to cardinal
      numbers, and <literal>nn</literal> to everything else.
    </para>

<programlisting><![CDATA[
>>> NN_CD_tagger = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'cd'), (r'.*', 'nn')])
>>> NN_CD_tagger.tag(text_token)
>>> print text_token
<[<John/nn>, <saw/nn>, <3/cd>, <polar/nn>, <bears/nn>, <./nn>]>
]]></programlisting>

    <para>
      We can generalise this method to guess the correct tag for words
      based on the presence of certain prefix or suffix strings.
      For instance, English words beginning with <literal>un-</literal>
      are likely to be adjectives.
    </para>

  </section> <!-- DefaultTagger -->

  <section id="taggers.unigram.unigram">
    <title> The Unigram Tagger </title>

    <para>
      The <literal>UnigramTagger</literal> class implements a simple
      statistical tagging algorithm: for each token, it assigns the
      tag that is most likely for that token's text. For example, it
      will assign the tag <literal>jj</literal> to any occurrence of the word
      <literal>frequent</literal>, since <literal>frequent</literal> is
      used as an adjective (e.g. <literal>a
      frequent word</literal>) more often than it is used as a verb
      (e.g. <literal>I frequent this cafe</literal>).
    </para>

    <para>
      Before a <literal>UnigramTagger</literal> can be used to tag
      data, it must be trained on a <glossterm>training
      corpus</glossterm>. It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the
      <literal>train()</literal> method, which takes a tagged corpus:
    </para>

<programlisting><![CDATA[
>>> from nltk.tagger import *
>>> from nltk.corpus import brown
    # Tokenize ten texts from the Brown Corpus
>>> train_tokens = []
>>> for item in brown.items()[:10]:
...     train_tokens.append(brown.tokenize(item))
    # Initialise and train a unigram tagger
>>> mytagger = UnigramTagger()
>>> for tok in train_tokens: mytagger.train(tok)
]]></programlisting>

    <para>
      Once a <literal>UnigramTagger</literal> has been trained, the
      <literal>tag()</literal> method can be used to tag new text:
    </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT="John saw the book on the table")
>>> WhitespaceTokenizer().tokenize(text_token)
>>> mytagger.tag(text_token)
>>> print text_token
<[<John/np>, <saw/vbd>, <the/at>, <book/None>, <on/in>, <the/at>, <table/nn>]>
]]></programlisting>

    <para>
      As we noted earlier, <literal>UnigramTagger</literal> will assign
      the default tag <literal>None</literal> to any token that was not
      encountered in the training data.  Later we will see how a
      <literal>UnigramTagger</literal> can be combined with a
      <literal>DefaultTagger</literal> to ensure that every word is
      tagged.
    </para>

  </section> <!-- UnigramTagger -->
</section> <!-- Unigram Tagging -->

<section id="evaluation">
  <title>Evaluating Taggers</title>

  <para>
    As we experiment with different taggers, it is important to have
    an objective performance measure.  Fortunately, we already have manually
    verified training data (the original tagged corpus), so we can use
    that to evaluate our taggers.
  </para>

  <para>
    Consider the following sentence from the Brown Corpus.  The 'Gold
    Standard' tags from the corpus are given in the second column,
    while the tags assigned by a unigram tagger appear in the third
    column.  Two mistakes made by the unigram tagger are italicised.
   </para>

  <table id="table.evaluation">
    <title>Evaluating Taggers</title>
    <tgroup cols="3">
      <colspec colwidth='3cm'/>
      <colspec colwidth='3cm'/>
      <colspec colwidth='3cm'/>
      <tbody>
        <row><entry>Sentence:</entry><entry>Gold Standard:</entry><entry>Unigram Tagger:</entry></row>
        <row><entry>The</entry><entry>at</entry><entry>at</entry></row>
        <row><entry>President</entry><entry>nn-tl</entry><entry>nn-tl</entry></row>
        <row><entry>said</entry><entry>vbd</entry><entry>vbd</entry></row>
        <row><entry>he</entry><entry>pps</entry><entry>pps</entry></row>
        <row><entry>will</entry><entry>md</entry><entry>md</entry></row>
        <row><entry>ask</entry><entry>vb</entry><entry>vb</entry></row>
        <row><entry>Congress</entry><entry>np</entry><entry>np</entry></row>
        <row><entry>to</entry><entry>to</entry><entry>to</entry></row>
        <row><entry>increase</entry><entry>vb</entry><entry><emphasis>nn</emphasis></entry></row>
        <row><entry>grants</entry><entry>nns</entry><entry>nns</entry></row>
        <row><entry>to</entry><entry>in</entry><entry><emphasis>to</emphasis></entry></row>
        <row><entry>states</entry><entry>nns</entry><entry>nns</entry></row>
        <row><entry>for</entry><entry>in</entry><entry>in</entry></row>
        <row><entry>vocational</entry><entry>jj</entry><entry>jj</entry></row>
        <row><entry>rehabilitation</entry><entry>nn</entry><entry>nn</entry></row>
        <row><entry>.</entry><entry>.</entry><entry>.</entry></row>
      </tbody>
    </tgroup>
  </table>

  <para>
    The tagger correctly tagged 14 out of 16 words, so it gets
    a score of 14/16, or 87.5%.  Of course, accuracy should be
    judged on the basis of a larger sample of data.  NLTK provides a
    function called <literal>tagger_accuracy</literal> to automate the
    task.  In the simplest case, we can test the tagger using the same
    data it was trained on:
  </para>

<programlisting><![CDATA[
>>> acc = tagger_accuracy(mytagger, train_tokens)
>>> print 'Accuracy = %4.1f%%' % (100 * acc)
94.8%
]]></programlisting>

  <para>
    However, testing a language processing system using the same data it was
    trained on is unwise.  A system which simply memorised the
    training data would get a perfect score without doing any
    linguistic modelling.  Instead, we would like to reward systems
    that make good generalizations, so we should test against
    <emphasis>unseen data</emphasis>, and replace <literal>train_tokens</literal>
    above with <literal>unseen_tokens</literal>.  We can then
    define the two sets of data as follows:
  </para>

<programlisting><![CDATA[
>>> train_tokens = []
>>> for item in brown.items()[:10]:    # texts 0-9
...     train_tokens.append(brown.tokenize(item))
>>> unseen_tokens = []
>>> for item in brown.items()[10:12]:  # texts 10-11
...     unseen_tokens.append(brown.tokenize(item))
]]></programlisting>

  <para>
    Now we train the tagger using <literal>train_tokens</literal>
    and evaluate it using <literal>unseen_tokens</literal>, as
    follows:
  </para>

<programlisting><![CDATA[
>>> for tok in train_tokens: mytagger.train(tok)
>>> acc = tagger_accuracy(mytagger, unseen_tokens)
>>> print 'Accuracy = %4.1f%%' % (100 * acc)
Accuracy = 64.6%
]]></programlisting>

  <para>
    The accuracy scores produced by this evaluation method are lower,
    but they give a more realistic picture of the performance of
    the tagger.  
    Note that the performance of any statistical tagger is highly
    dependent on the quality of its training set. In particular, if
    the training set is too small, it will not be able to reliably
    estimate the most likely tag for each word. Performance will also
    suffer if the training set is significantly different from the
    texts we wish to tag.

  </para>

  <para>
    In the process of developing a tagger, we can use the accuracy
    score as an objective measure of the improvements made to the
    system.  Initially, the accuracy score will go up quickly as we
    fix obvious shortcomings of the tagger.  After a while, however,
    it becomes more difficult and improvements are small.
  </para>

  <para>
    While the accuracy score is certainly useful, it does not tell us
    how to improve the tagger.  For this we need to undertake error
    analysis.  For instance, we could construct a <glossterm>confusion
    matrix</glossterm>, with a row and a column for every possible
    tag, and entries that record how often a word with tag
    <literal>T<subscript>i</subscript></literal> is incorrectly tagged
    as <literal>T<subscript>j</subscript></literal>.  Another approach
    is to analyse the context of errors.
  </para>

<programlisting><![CDATA[
>>> errors = {}                                # to store the error contexts
>>> for gold_doc in unseen_tokens:
...     test_doc = gold_doc.exclude('TAG')     # remove tags
...     mytagger.tag(test_doc)                 # tag the unseen text
...     for i in range(len(tokens)):           # iterate over each token
...         if test_doc['SUBTOKENS'][i] != gold_doc['SUBTOKENS'][i]:    # an error
...             test_context = [tok['TAG'] for tok in test_doc['SUBTOKENS'][i-1:i+1]]
...             gold_context = [tok['TAG'] for tok in gold_doc['SUBTOKENS'][i-1:i+1]]
...             if None not in test_context:   # did we tag all words in the context?
...                 pair = (tuple(test_context), tuple(gold_context))   # save context
...                 errors[pair] = errors.get(pair, 0) + 1
]]></programlisting>

  <para>
    The above program catalogs all errors, along with the tag on the left and their
    frequency of occurrence.  The <literal>errors</literal> dictionary has keys
    of the form <literal>((t1,t2),(g1,g2))</literal>, where
    <literal>(t1,t2)</literal> are the test tags, and <literal>(g1,g2)</literal>
    are the gold-standard tags.  The values in the <literal>errors</literal> dictionary
    are simple counts of how often the error occurred.  With some further processing,
    we construct the list <literal>counted_errors</literal> containing tuples
    consisting of counts and errors, and then do a reverse sort to get the most
    significant errors first:
  </para>
  
<programlisting><![CDATA[
>>> counted_errors = [(errors[k], k) for k in errors.keys()]
>>> counted_errors.sort()
>>> counted_errors.reverse()
>>> for err in counted_errors[:5]:
...     print err
(11, (('at', 'vb'), ('at', 'nn')))
(9, (('nn', 'to'), ('nn', 'in')))
(8, (('cc', 'vbn'), ('cc', 'vbd')))
(7, ((',', 'vbn'), (',', 'vbd')))
(7, ((',', 'np'), (',-hl', 'np-hl')))
]]></programlisting>

  <para>
    The first line of output records the fact that there were 11 cases
    where the unigram tagger mistakenly tagged a noun as a verb,
    following an article.  In fact we already encountered this mistake
    in <xref linkend="table.evaluation"/> for the word
    <literal>increase</literal>.  The unigram tagger tagged
    <literal>increase</literal> as a verb instead of a noun since it
    occurred more often in the training data as a verb.  However, when
    <literal>increase</literal> appears after an article, it is
    invariably a noun.  Evidently, the performance of the tagger would
    improve if it was modified to consider not just the word being
    tagged, but also the tag of the word on the left.  Such taggers are known
    as bigram taggers, and we consider them next.
  </para>

 

</section>

<section id="taggers.ngram">
  <title> Higher Order Taggers </title>

  <para>
    Earlier we encountered the <literal>UnigramTagger</literal>,
    which assigns a tag to a word based on the identity of that word.
    In this section we will look at taggers that exploit a larger amount
    of context when assigning a tag.
  </para>

  <section id="taggers.ngram.bigram">
    <title> Bigram Taggers </title>

  <para>
    As their name suggests, <glossterm>bigram taggers</glossterm> use
    two pieces of information for each tagging decision.  Usually this
    information is the text of the current word together with the tag of
    the previous word. These two pieces of information constitute the
    <glossterm>context</glossterm> for the token to be tagged. Given the
    context, the tagger assigns the most likely tag.  We can visualise
    this process with the help of <xref linkend="table.bigram"/>, a tiny
    fragment of the internal data structure built by a bigram tagger.
    <!-- The selected tags are italicised.-->

  </para>

  <table id="table.bigram">
    <title> Fragment of Bigram Table </title>
    <tgroup cols="8">
      <tbody>
        <row><entry/><entry>ask</entry> <entry>Congress</entry> <entry>to</entry> <entry>increase</entry> <entry>grants</entry> <entry>to</entry> <entry>states</entry></row>
        <row><entry>at</entry><entry/> <entry/> <entry/> <entry>nn</entry> <entry/> <entry/> <entry/></row>
        <row><entry>tl</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry/> <entry>to</entry> <entry/></row>
        <row><entry>bd</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry>nns</entry> <entry>to</entry> <entry/></row>
        <row><entry>md</entry><entry><emphasis>vb</emphasis></entry> <entry/> <entry/> <entry>vb</entry> <entry/> <entry/> <entry/></row>
        <row><entry>vb</entry><entry/> <entry><emphasis>np</emphasis></entry> <entry>to</entry> <entry/> <entry><emphasis>nns</emphasis></entry> <entry>to</entry> <entry>nns</entry></row>
        <row><entry>np</entry><entry/> <entry/> <entry><emphasis>to</emphasis></entry> <entry/> <entry/> <entry>to</entry> <entry/></row>
        <row><entry>to</entry><entry>vb</entry> <entry/> <entry/> <entry><emphasis>vb</emphasis></entry> <entry/> <entry/> <entry/></row>
        <row><entry>nn</entry><entry/> <entry>np</entry> <entry>to</entry> <entry>nn</entry> <entry>nns</entry> <entry>to</entry> <entry/></row>
        <row><entry>nns</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry/> <entry><emphasis>to</emphasis></entry> <entry/></row>
        <row><entry>in</entry><entry/> <entry>np</entry> <entry>in</entry> <entry/> <entry/> <entry>in</entry> <entry><emphasis>nns</emphasis></entry></row>
        <row><entry>jj</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry>nns</entry> <entry>to</entry> <entry>nns</entry></row>
      </tbody>
    </tgroup>
  </table>

<!-- SOURCE CODE FOR BIGRAM TABLE
from nltk.tagger import *
from nltk.corpus import brown
train_tokens = []
for item in brown.items()[:10]:
    train_tokens.append(brown.tokenize(item))
mytagger = NthOrderTagger(1)
for tok in train_tokens: mytagger.train(tok)
words = '''ask Congress to increase grants to states'''.split()
tags = '''at nn-tl vbd md vb np to nn nns in jj'''.split()

print "     ",
for word in words:
    print "<entry>%s</entry>" % word,
print
for tag in tags:
    print "%5s" % tag,
    for word in words:
        guess = mytagger._freqdist[((tag,), word)].max()
        if not guess: guess=""
        print "<entry>%s</entry>" % guess,
    print
-->
    
  <para>The best way to understand the table is to work through an example.
    Suppose we have already processed the sentence
    <literal>The President will ask Congress to increase grants
    to states for vocational rehabilitation .</literal>
    as far as <literal>will/md</literal>.  We can use the table to simply read off the tags that
    should be assigned to the remainder of the sentence.  When preceded by
    <literal>md</literal>, the tagger guesses that the word
    <literal>ask</literal> has the tag <literal>vb</literal> (italicised
    in the table).
    Moving to the next word, we know it is preceded by
    <literal>vb</literal>, and looking across this row we see that
    <literal>Congress</literal> is assigned the tag
    <literal>np</literal>.  The process continues through the rest of
    the sentence.  When we encounter the word
    <literal>increase</literal>, we correctly assign it the tag
    <literal>vb</literal> (unlike the unigram tagger which assigned
    it <literal>nn</literal>).  However, the bigram tagger mistakenly
    assigns the infinitival tag to the word <literal>to</literal>
   immediately preceding <literal>states</literal>, and
    not the preposition tag.  This suggests that we may need to consider even more
    context in order to get the correct tag.
  </para>

<!--
  <note><para>
    To create a bigram tagger in NLTK, use
    <literal>NthOrderTagger(1)</literal>.  In other words,
    a bigram tagger is an nth-order tagger that looks at
    the tag of one word of prior context.
    There is not much point in experimenting with
    these higher order taggers until we have considered how
    taggers can be combined.
  </para></note>
-->    
</section>

<section id="taggers.ngram.nthorder">
  <title> Nth-Order Taggers </title>

  <para>
    As we have just seen, it may be desirable to look at more than just
    the preceding word's tag when making a tagging decision.  An
    <glossterm>nth-order tagger</glossterm> is a generalisation of a
    bigram tagger whose context is the current token's text together
    with the part-of-speech tags of the <replaceable>n</replaceable>
    preceding tokens, as shown in <xref linkend="context"/>. It then
    picks the tag which is most likely for that context. In <xref
    linkend="context"/>, the tag to be chosen,
    <replaceable>t<subscript>k</subscript></replaceable>, is circled,
    and the context is shaded in grey. In this example of an nth order
    tagger, we have <replaceable>n</replaceable>=2; that is, we inspect
    the tags of the two words preceding the current word.
  </para>

<figure id="context"><title>Tagger Context</title>
<informaltable frame="none">
<tgroup cols="1"><tbody><row><entry>
<graphic  fileref="images/context" scale="15"/>
</entry></row></tbody></tgroup></informaltable>
</figure>
 
  <important><para> A 0th order tagger is another term for a
  unigram tagger: i.e., the context used to tag a token is just the text
  of the token itself. First order taggers are also called
  <glossterm>bigram taggers</glossterm>. and second order taggers are
  called <glossterm>trigram taggers</glossterm>. </para></important>

  <para>
    <literal>NthOrderTagger</literal> uses a tagged training corpus to
    determine which part-of-speech tag is most likely for each
    context:
  </para>

<programlisting><![CDATA[
>>> TaggedTokenizer().tokenize(text_token)
>>> tagger = NthOrderTagger(3)                    # 3rd order tagger
>>> tagger.train(tagged_text_token)
]]></programlisting>

  <para>
    Once an <literal>NthOrderTagger</literal> has been trained, it can
    be used to tag untagged corpora:
  </para>

<programlisting><![CDATA[
>>> WhitespaceTokenizer().tokenize(text_token)
>>> tagger.tag(text_token)
>>> print text_token
<[<John/NN>, <saw/VB>, <the/AT>, <book/NN>, <on/IN>, <the/AT>, ...]>
]]></programlisting>

  <para>
    As with the other taggers considered earlier,
    <literal>NthOrderTagger</literal> will assign the default tag
    <literal>None</literal> to any token whose context was not
    encountered in the training data. </para> <para> Note that as
    <replaceable>n</replaceable> gets larger, the specificity of the
    contexts increases; and with it, the chance that the data we wish to
    tag will contain contexts that were not present in the training
   data. This is sometimes referred to as a <glossterm>sparse
   data</glossterm> problem. Thus, there is a trade-off between the
   accuracy and the
    coverage of our results. This is a common type of trade-off in
    natural language processing. It is closely related to the
    <glossterm>precision/recall trade-off</glossterm> that we'll
    encounter later when we discuss information retrieval.
  </para>
</section> <!-- NthOrderTagger -->
</section> <!-- Ngram -->

<section id="tagger.backoff">
  <title> Combining Taggers </title>

  <para>
    One way to address the trade-off between accuracy and coverage is
    to use the more accurate algorithms when we can, but to fall back
    on algorithms with wider coverage when necessary. For example, we
    could combine the results of a first order tagger, a 0th order
    tagger, and an <literal>NN_CD_Tagger</literal>, as follows:
  </para>

  <orderedlist>
    <listitem>
      <para> Try tagging the token with the first order tagger. </para>
    </listitem>
    <listitem>
      <para> If the first order tagger is unable to find a tag for the token, try
        finding a tag with the 0th order tagger. </para>
    </listitem>
    <listitem>
      <para> If the 0th order tagger is also unable to find a tag, use the
        <literal>NN_CD_Tagger</literal> to find a tag. </para>
    </listitem>
  </orderedlist>

  <para>
    NLTK defines the <literal>BackoffTagger</literal> class for
    combining taggers in this way. A <literal>BackoffTagger</literal>
    is constructed from a list of one or more
    <glossterm>subtaggers</glossterm>. For each token in the input,
    the <literal>BackoffTagger</literal> uses the result of the first
    tagger in the list that successfully found a tag. Taggers indicate
    that they are unable to tag a token by assigning it the special
    tag <literal>None</literal>. We can use a
    <literal>BackoffTagger</literal> to implement the strategy
    proposed above:
  </para>

<programlisting><![CDATA[
>>> TaggedTokenizer().tokenize(text_token)
# Construct the taggers
>>> tagger1 = NthOrderTagger(1)              # first order tagger
>>> tagger2 = UnigramTagger()                # zeroth order tagger
>>> tagger3 = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'cd'), (r'.*', 'nn')])
# Train the taggers
>>> tagger1.train(train_toks)
>>> tagger2.train(train_toks)
# Combine the taggers
>>> tagger = BackoffTagger([tagger1, tagger2, tagger3])
]]></programlisting>

  <important><para> The order in which the taggers are given to
  <literal>BackoffTagger</literal> is important: the taggers should be
  listed in the order that they should be tried. This typically means
  that more specific taggers should be listed before less specific
  taggers.  </para></important>

  <para>
    Having defined a combined tagger,
    we can use it to tag new corpora:
  </para>

<programlisting><![CDATA[
>>> TaggedTokenizer().tokenize(text_token)
>>> tagger.tag(text_token)
>>> print text_token
<[<John/NN>, <saw/VB>, <the/AT>, <book/NN>, <on/IN>, <the/AT>, ...]>
]]></programlisting>

  <para> Evaluation: scores for different taggers </para>

  <para> Train and test the taggers: </para>

<programlisting><![CDATA[
>>> train_tokens = []
>>> for item in brown.items()[:20]:
...     train_tokens.append(brown.tokenize(item))
>>> unseen_tokens = []
>>> for item in brown.items()[20:30]:
...     unseen_tokens.append(brown.tokenize(item))

>>> subtagger1 = UnigramTagger()
>>> subtagger2 = NthOrderTagger(1)     # bigram tagger
>>> subtagger3 = NthOrderTagger(2)     # trigram tagger

>>> for tok in train_tokens:
...     subtagger1.train(tok)
...     subtagger2.train(tok)
...     subtagger3.train(tok)

>>> tagger1 = BackoffTagger([subtagger1])
>>> tagger2 = BackoffTagger([subtagger2, subtagger1])
>>> tagger3 = BackoffTagger([subtagger3, subtagger2, subtagger1])

>>> accuracy1 = tagger_accuracy(tagger1, unseen_tokens)
>>> accuracy2 = tagger_accuracy(tagger2, unseen_tokens)
>>> accuracy3 = tagger_accuracy(tagger3, unseen_tokens)

>>> print 'Unigram Accuracy = %4.1f%%' % (100 * accuracy1)
>>> print 'Bigram Accuracy  = %4.1f%%' % (100 * accuracy2)
>>> print 'Trigram Accuracy = %4.1f%%' % (100 * accuracy3)
Unigram Accuracy = 74.7%
Bigram Accuracy  = 75.5%
Trigram Accuracy = 75.5%
]]></programlisting>

<!--
  <para> [Combining taggers using voting?] </para>
-->

</section> <!-- Combining Taggers -->

    <!-- Taggers -->
    <!-- old material on implementation that appeared here to be moved to
an implementation tutorial - SB -->

<section id="brill">
  <title>The Brill Tagger</title>

  <para>
    A potential issue with nth-order tagger is their size.  If tagging
    is to be employed in a variety of language technologies deployed
    on mobile computing devices, it is important to find ways to
    reduce the size of models without overly compromising performance.
    An nth-order tagger with backoff may store trigram and bigram
    tables, large sparse arrays which may have hundreds of millions of
    entries.  As we saw in <xref linkend="context"/>, nth-order models
    only consider the tags of words in the context.  A consequence of
    the size of the models is that it is simply impractical for
    nth-order models to be conditioned on the identities of words in
    the context.  In this section we will examine Brill tagging, a
    statistical tagging method which performs very well using models
    that are only a tiny fraction of the size of nth-order taggers.
  </para>
    
  <para>
    Brill tagging is a kind of <glossterm>transformation-based learning</glossterm>.
    The general idea is very simple: guess the tag of each word, then
    go back and fix the mistakes.  In this way, a Brill tagger
    successively transforms a bad tagging of a text into a good one.
    As with nth-order tagging this is a <glossterm>supervised learning</glossterm>
    method, since we need annotated training data.  However, unlike
    nth-order tagging, it does not count observations but compiles a
    list of transformational correction rules.
  </para>

  <para>
    The process of Brill tagging is usually
    explained by analogy with painting.  Suppose we were painting a
    tree, with all its details of boughs, branches, twigs and leaves,
    against a uniform sky-blue background.  Instead of painting the
    tree first then trying to paint blue in the gaps, it is simpler to
    paint the whole canvas blue, then "correct" the tree section by
    overpainting the blue background.  In the same fashion we might
    paint the trunk a uniform brown before going back to overpaint
    further details with a fine brush.  Brill tagging uses the same
    idea: get the bulk of the painting right with broad brush strokes,
    then fix up the details.  As time goes on, successively finer
    brushes are used, and the scale of the changes becomes arbitrarily
    small.  The decision of when to stop is somewhat arbitrary.
    <xref linkend="table.brill"/> illustrates this process, first
    tagging with the unigram tagger, then fixing the errors.
  </para>

  <table id="table.brill">
    <title>Steps in Brill Tagging</title>
    <tgroup cols="5">
      <colspec colwidth='3cm'/>
      <colspec colwidth='2cm'/>
      <colspec colwidth='2cm'/>
      <tbody>
        <row><entry>Sentence:</entry><entry>Gold Standard:</entry>
             <entry>Unigram Tagger:</entry>
             <entry>Replace <literal>nn</literal> with
               <literal>vb</literal> when the previous word is
               <literal>to</literal></entry>
             <entry>Replace <literal>to</literal> with
               <literal>in</literal> when the next tag is
               <literal>nns</literal></entry>
        </row>
        <row><entry>The</entry><entry>at</entry><entry>at</entry><entry/><entry/></row>
        <row><entry>President</entry><entry>nn-tl</entry><entry>nn-tl</entry><entry/><entry/></row>
        <row><entry>said</entry><entry>vbd</entry><entry>vbd</entry><entry/><entry/></row>
        <row><entry>he</entry><entry>pps</entry><entry>pps</entry><entry/><entry/></row>
        <row><entry>will</entry><entry>md</entry><entry>md</entry><entry/><entry/></row>
        <row><entry>ask</entry><entry>vb</entry><entry>vb</entry><entry/><entry/></row>
        <row><entry>Congress</entry><entry>np</entry><entry>np</entry><entry/><entry/></row>
        <row><entry>to</entry><entry>to</entry><entry>to</entry><entry/><entry/></row>
        <row><entry>increase</entry><entry>vb</entry><entry><emphasis>nn</emphasis></entry><entry><emphasis>vb</emphasis></entry><entry/></row>
        <row><entry>grants</entry><entry>nns</entry><entry>nns</entry><entry/><entry/></row>
        <row><entry>to</entry><entry>in</entry><entry><emphasis>to</emphasis></entry><entry><emphasis>to</emphasis></entry><entry><emphasis>in</emphasis></entry></row>
        <row><entry>states</entry><entry>nns</entry><entry>nns</entry><entry/><entry/></row>
        <row><entry>for</entry><entry>in</entry><entry>in</entry><entry/><entry/></row>
        <row><entry>vocational</entry><entry>jj</entry><entry>jj</entry><entry/><entry/></row>
        <row><entry>rehabilitation</entry><entry>nn</entry><entry>nn</entry><entry/><entry/></row>
        <row><entry>.</entry><entry>.</entry><entry>.</entry><entry/><entry/></row>
      </tbody>
    </tgroup>
  </table>

  <para>
    In <xref linkend="table.brill"/> we saw two rules.  All such rules
    are generated from a template of the following form:
    form "replace <literal>T<subscript>1</subscript></literal> with
      <literal>T<subscript>2</subscript></literal> in the context
      <literal>C</literal>".
    Typical contexts are the identity or the tag of the preceding or
    following word, or the appearance of a tag within 2-3 words of
    of the current word.  During its training phase, the tagger
    guesses values for <literal>T<subscript>1</subscript></literal>,
    <literal>T<subscript>2</subscript></literal> and
    <literal>C</literal>, to create thousands of candidate rules.
    Each rule is then assigned a score based on its net benefit:
    the number of incorrect tags that it corrects, less the
    number of correct tags it incorrectly modifies.  This process
    is best illustrated by a listing of the output from the
    NLTK Brill tagger (here run on tagged Wall Street Journal text
    from the Penn Treebank).<footnote><para>We are grateful to Christopher
	Maloof for developing a Brill tagger for NLTK.  It currently
        resides in the <literal>nltk_contrib</literal> package, but it
        is being migrated into NLTK's tagger package.
    </para></footnote>
  </para>

<programlisting><![CDATA[
Loading tagged data...
Training unigram tagger: [accuracy: 0.820940]
Training Brill tagger on 37168 tokens...
 
Iteration 1: 1482 errors; ranking 23989 rules;
  Found: "Replace POS with VBZ if the preceding word is tagged PRP"
  Apply: [changed 39 tags: 39 correct; 0 incorrect]
 
Iteration 2: 1443 errors; ranking 23662 rules;
  Found: "Replace VBP with VB if one of the 3 preceding words is tagged MD"
  Apply: [changed 36 tags: 36 correct; 0 incorrect]
 
Iteration 3: 1407 errors; ranking 23308 rules;
  Found: "Replace VBP with VB if the preceding word is tagged TO"
  Apply: [changed 24 tags: 23 correct; 1 incorrect]
 
Iteration 4: 1384 errors; ranking 23057 rules;
  Found: "Replace NN with VB if the preceding word is to"
  Apply: [changed 67 tags: 22 correct; 45 incorrect]
...
Iteration 20: 1138 errors; ranking 20717 rules;
  Found: "Replace RBR with JJR if one of the 2 following words is tagged NNS"
  Apply: [changed 14 tags: 10 correct; 4 incorrect]
 
Iteration 21: 1128 errors; ranking 20569 rules;
  Found: "Replace VBD with VBN if the preceding word is tagged VBD"
[insufficient improvement; stopping]
 
Brill accuracy: 0.835145

]]></programlisting>

<!--
  <para>
    [Doing it in NLTK - Maloof's Brill tagger - code fragments;
    List of transformations learned; what linguistic significance
    do these have?]
  </para>
-->

</section>


<section id="conclusion">
  <title>Conclusion</title>

  <para>
    This chapter has introduced the language processing task known as
    tagging, with an emphasis on part-of-speech tagging.  English word
    classes and their corresponding tags were introduced.  We showed
    how tagged tokens and tagged corpora can be represented, then
    discussed a variety of taggers: default tagger, regular expression
    tagger, unigram tagger, nth-order taggers, and the Brill tagger.
    We also described some objective evaluation methods.
    In the process, the reader has been introduced to two important
    paradigms in language processing, namely <glossterm>language modelling</glossterm>
    and <glossterm>transformation-based learning</glossterm>.
    The former is extremely general, and we will encounter it again
    later.  The latter had to be specially tailored to the tagging
    task, but resulted in smaller, linguistically-interpretable
    models.
  </para>

<!--
  <para>
    Discussion of how statistical methods have been used to
    reach a linguistically interpretable, symbolic result.
    Contrast between n-gram and Brill tagger about whether
    we can learn anything from inspecting the model itself
    (n-gram data vs transformational rules).  Comparing accuracy
    of these two methods: 2D graph showing how accuracy changes
    for the two methods as training size increases.
  </para>
-->

  <para>
    There are several other important approaches to tagging involving
    <glossterm>Hidden Markov Models</glossterm> and
    <glossterm>Finite State Transducers</glossterm>, though a
    discussion of these approaches falls outside the scope of this
    chapter.  Later we will see a generalization of tagging called
    <glossterm>chunking</glossterm> in which
    a contiguous sequence of words is assigned a single tag.
  </para>

</section>


<section id="reading">
  <title>Further Reading</title>

  <para>[This section to be expanded to a half-page of discussion and
    pointers to the literature and online resources.]</para>

  <para>Brill tagging: Manning 361ff; Jurafsky 307ff</para>

  <para>HMM tagging: Manning 345ff</para>

  <para>SIL Glossary of Linguistic Terms:
    http://www.sil.org/linguistics/GlossaryOfLinguisticTerms/
  </para>

  <para>
    Language Files: Materials for an
    Introduction to Language and Linguistics (Eighth Edition),
    The Ohio State University Department of Linguistics,
    http://www.ling.ohio-state.edu/publications/files/
  </para>

</section>


<section id="exercises">
  <title>Exercises</title>

  <orderedlist>

    <listitem>
      <formalpara>
        <title> Explorations with part-of-speech tagged corpora </title>
        <para> Tokenize the Brown Corpus and build one or more suitable data structures
          so that you can answer the following questions.</para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          What is the most frequent tag? (This is the tag we would want to
          assign with a <literal>DefaultTagger)</literal></para>
        </listitem>
        <listitem><para>
          Which word has the greatest number of distinct tags?</para>
        </listitem>
        <listitem><para>
          What is the ratio of masculine to feminine pronouns?</para>
        </listitem>
        <listitem><para>
          How many words are ambiguous, in the sense that they appear with at least two tags?</para>
        </listitem>
        <listitem><para>
          What percentage of word <emphasis>occurrences</emphasis> in the Brown Corpus involve
          these ambiguous words?</para>
        </listitem>
        <listitem><para>
          Which nouns are more common in their plural form than their singular form?
          (Only consider regular plurals, formed with the <literal>-s</literal> suffix.)
        </para></listitem>
        <listitem><para>
          Produce an alphabetically sorted list of the distinct words tagged as <literal>md</literal>.
        </para></listitem>
        <listitem><para>
          Identify words which can be plural nouns or third person singular verbs
          (e.g. <literal>deals</literal>).
        </para></listitem>
        <listitem><para>
          Identify three-word prepositional phrases of the form
          preposition + determiner + noun.
        </para></listitem>
        <listitem><para>
          There are 264 distinct words having exactly three possible tags.
          Print a table with the integers 1..10 in one column, and the
          number of distinct words in the corpus having 1..10 distinct tags.
        </para></listitem>
        <listitem><para>
          The word <literal>still</literal> has seven possible tags.
          Print out seven sentences containing this word, each one having
          a different tag.
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Regular Expression Tagging </title>
        <para>
          We defined the <literal>NN_CD_Tagger</literal>, which can be used
          as a fall-back tagger for unknown words.  This tagger only checks for
          cardinal numbers.  By testing for particular prefix or suffix strings,
          it should be possible to guess other tags.  For example, 
          we could tag any word that ends with <literal>-s</literal>
          as a plural noun.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para>
            Define a <literal>RegexpTagger</literal>
            which tests for at least five other patterns in the spelling of words.
            (Use inline documentation to explain the rules.)
          </para>
        </listitem>
        <listitem>
          <para>
            Evaluate the tagger using <literal>tagger_accuracy()</literal>, and
            discuss your findings.
          </para>
        </listitem>
      </orderedlist>
    </listitem>


    <listitem>
      <formalpara>
        <title> Unigram Tagging </title>
        <para>
          Train a unigram tagger and run it on some new text.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para>
            Evaluate the tagger as before and discuss your findings.
          </para>
        </listitem>
        <listitem>
          <para>
            Observe that some words are not assigned a tag.  Why not?
          </para>
        </listitem>
      </orderedlist>
    </listitem>


    <listitem>
      <formalpara>
        <title> Bigram Tagging </title>
        <para>
          Train a bigram tagger and run it on some next text.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para>
            Compare its behaviour with the Unigram tagger.  Is it more
            or less sensitive?  Is it more or less robust?
          </para>
        </listitem>
        <listitem>
          <para>
            Evaluate the tagger as before and discuss your findings.
          </para>
        </listitem>
        <listitem>
          <para>
            Observe that a point is reached where no further words can be tagged.  Why?
          </para>
        </listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Combining taggers with BackoffTagger</title> <para>
        There is typically a trade-off between the accuracy and
        coverage for taggers: taggers that use more specific contexts
        usually produce more accurate results, when they have seen
        those contexts in the training data; but because the training
        data is limited, they are less likely to encounter each
        context. The <literal>BackoffTagger</literal> addresses this
        problem by trying taggers with more specific contexts first;
        and falling back to the more general taggers when
        necessary. In this exercise, we examine the effects of using
        <literal>BackoffTagger</literal>.  Create a
        <literal>DefaultTagger</literal> or a
        <literal>RegexpTagger</literal>, and a
        <literal>UnigramTagger</literal>, and a
        <literal>NthOrderTagger</literal>. Train the
        <literal>UnigramTagger</literal> and the
        <literal>NthOrderTagger</literal> using part of the Brown corpus.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para> Evaluate each tagger on unseen data from the Brown corpus.
            Record the <glossterm>accuracy</glossterm> of the tagger
            (the percentage of tokens that are correctly tagged). Be sure to use a
            different section of the corpus for testing than you used for training. </para>
        </listitem>
        <listitem>
          <para> Use <literal>BackoffTagger</literal> to create three different
            combinations of the basic taggers. Test the accuracy of each combined
            tagger. Which combinations give the most improvement? </para>
        </listitem>
        <listitem>
          <para> Try repeating steps 1-3 with a different sized training corpus. How
            does it affect your results? </para>
        </listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Tagger context </title>
        <para><literal>NthOrderTagger</literal> chooses a tag for a
        token based on its type and the tags of the
        <replaceable>n</replaceable> preceding tokens. This is a
        common context to use for tagging, but certainly not the only
        possible context.  Construct a new tagger, subclassed from
        <literal>SequentialTagger</literal>, that uses a different
        context. If your tagger's context contains multiple elements,
        then you should combine them in a
        <literal>tuple</literal>. Some possibilities for elements to
        include are:
        </para>
      </formalpara>

      <itemizedlist>
        <listitem>
          <para> The text of the current token, or of a previous token. </para>
        </listitem>
        <listitem>
          <para> The length of the current token's text, or of a previous token's
            text. </para>
        </listitem>
        <listitem>
          <para> The first letter of the current token's text, or of a previous
            token's text. </para>
        </listitem>
        <listitem>
          <para> The tag of a previous token. </para>
        </listitem>
      </itemizedlist>

      <para> Try to choose context elements that you believe will help the tagger decide
        which tag is appropriate. Keep in mind the trade-off between more specific
        taggers with accurate results; and more general taggers with broader coverage.
        Combine your tagger with other taggers using <literal>BackoffTagger</literal>.
      </para>

      <orderedlist>
        <listitem>
          <para> How does the combined tagger's accuracy compare to the basic tagger? </para>
        </listitem>
        <listitem>
          <para> How does the combined tagger's accuracy compare to the combined taggers you
            created in the previous exercise? </para>
        </listitem>
      </orderedlist>
 
    </listitem>

    <listitem>
      <formalpara>
        <title> Reverse sequential taggers </title>
        <para> Since sequential taggers tag tokens in order, one at a time, they can only
          use the predicted tags to the <emphasis>left</emphasis> of the current token to
          decide what tag to assign to a token. But in some cases, the
          <emphasis>right</emphasis> context can provide more information about what tag
          should be used. A <glossterm>reverse sequential tagger</glossterm> is a tagger
          that: </para>
      </formalpara>

      <itemizedlist>
        <listitem>
          <para> Assigns tags to one token at a time, starting with the last token of
            the text, and proceeding in right-to-left order. </para>
        </listitem>
        <listitem>
          <para> Decides which tag to assign a token on the basis of that token, the
            tokens that follow it, and the predicted tags for the tokens that follow
            it. </para>
        </listitem>
      </itemizedlist>

      <para> There is no need to create new classes to perform reverse
      sequential tagging.  By reversing texts at appropriate times, we
      can use sequential tagging classes to perform reverse sequential
      tagging. In particular, we should reverse the training text
      before we train the tagger; and reverse the text that we wish to
      tag both before and after we use the sequential tagger. Use this
      technique to create a first order reverse sequential
      tagger. </para>

      <itemizedlist>
        <listitem><para> Measure its accuracy on a tagged section of
        the Brown corpus. Be sure to use a different section of the
        corpus for testing than you used for training.</para>
        </listitem>

        <listitem><para>How does its accuracy compare to a first order
        sequential tagger, using the same training data and test data? 
        </para>
      </listitem>
      </itemizedlist>
    </listitem>
      
    <listitem>
      <formalpara>
        <title> Processing individual sentences </title>
        <para> Write a modified <literal>NthOrderTagger</literal> that ignores tags that are
          in a previous sentence. E.g., for a 3rd order tagger, if the previous 3 words
          were "dog/NN ./. A/DT", then just use "DT" and the current token as context. </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Alternatives to backoff </title> <para> Create a new
        kind of tagger that combines several subtaggers using a new
        mechanism other than backoff (e.g. voting).  For robustness in
        the face of unknown words, include a RegexpTagger, a unigram
        tagger that removes a small number of prefix or suffix
        characters until it recognises a word, or an NthOrderTagger
        that does not consider the text of the token being tagged.
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Comparing nth-order taggers and Brill taggers </title>
        <para> Investigate the relative accuracy of nth-order taggers
        with backoff and Brill taggers as the size of the training data is increased.
        Analyze and compare the errors made by each kind of tagger.
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Application to other languages </title>
        <para> Obtain some tagged data for another language, and train
          and evaluate a variety of taggers on it.  If the language is
          morphologically complex, or if there are any orthographic
          clues (e.g. capitalization) to word classes, consider developing a regular
          expression tagger for it (ordered after the unigram tagger,
          and before the default tagger).  How does the
          accuracy of your tagger(s) compare with the same taggers run
          on English data?  Discuss any issues you encounter in
          applying these methods to the language.
        </para>
      </formalpara>
    </listitem>
  </orderedlist>

</section> <!-- Exercises -->

<appendix>
  <title>Brown Corpus Tag Set</title>

  <para>The following tabulation and examples have been taken from
    the documentation of the AMALGAM tagger.  Note that starred
    tags are the negated version.</para>

<table id="table.browntagset">
  <title>Complete Brown Tag Set</title>
  <tgroup cols="3">
    <colspec colwidth='2cm'/>
    <colspec colwidth='6cm'/>
    <thead>
      <row>
        <entry>Tag</entry>
        <entry>Description</entry>
        <entry>Examples</entry>
      </row>
    </thead>
    <tbody>

      <row>
        <entry> * </entry>
        <entry> negator </entry>
        <entry> not n't </entry>
      </row>
      <row>
        <entry> . </entry>
        <entry> sentence terminator </entry>
        <entry> . ? ; ! : </entry>
      </row>
      <row>
        <entry> : </entry>
        <entry> colon </entry>
        <entry> : </entry>
      </row>
      <row>
        <entry> abl </entry>
        <entry> determiner/pronoun, pre-qualifier </entry>
        <entry> quite such rather </entry>
      </row>
      <row>
        <entry> abn </entry>
        <entry> determiner/pronoun, pre-quantifier </entry>
        <entry> all half many nary </entry>
      </row>
      <row>
        <entry> abx </entry>
        <entry> determiner/pronoun, double conjunction or pre-quantifier </entry>
        <entry> both </entry>
      </row>
      <row>
        <entry> ap </entry>
        <entry> determiner/pronoun, post-determiner </entry>
        <entry> many other next more last former little several enough
                 most least only very few fewer past same </entry>
      </row>
      <row>
        <entry> ap$ </entry>
        <entry> determiner/pronoun, post-determiner, genitive </entry>
        <entry> other's </entry>
      </row>

      <row>
        <entry> be </entry>
        <entry> verb "to be", infinitive or imperative </entry>
        <entry> be </entry>
      </row>
      <row>
        <entry> bed / bed* </entry>
        <entry> verb "to be", past tense, 2nd person singular or all
          persons plural </entry>
        <entry> were / weren't </entry>
      </row>
      <row>
        <entry> bedz / bedz*</entry>
        <entry> verb "to be", past tense, 1st and 3rd person singular </entry>
        <entry> was / wasn't </entry>
      </row>
      <row>
        <entry> beg </entry>
        <entry> verb "to be", present participle or gerund </entry>
        <entry> being </entry>
      </row>
      <row>
        <entry> bem / bem*</entry>
        <entry> verb "to be", present tense, 1st person singular </entry>
        <entry> am / ain't </entry>
      </row>
      <row>
        <entry> ben </entry>
        <entry> verb "to be", past participle </entry>
        <entry> been </entry>
      </row>
      <row>
        <entry> ber / ber* </entry>
        <entry> verb "to be", present tense, 2nd person singular or all
          persons plural </entry>
        <entry> are art / aren't ain't </entry>
      </row>
      <row>
        <entry> bez / bez* </entry>
        <entry> verb "to be", present tense, 3rd person singular </entry>
        <entry> is / isn't ain't </entry>
      </row>
      <row>
        <entry> cc </entry>
        <entry> conjunction, coordinating </entry>
        <entry> and or but plus & either neither nor yet 'n' and/or minus an' </entry>
      </row>
      <row>
        <entry> cd </entry>
        <entry> numeral, cardinal </entry>
        <entry> two 1 1913 three million 87-31 1,119 fifty-three 7.5 billion ... </entry>
      </row>
      <row>
        <entry> cd$ </entry>
        <entry> numeral, cardinal, genitive </entry>
        <entry> 1960's 1961's .404's </entry>
      </row>
      <row>
        <entry> cs </entry>
        <entry> conjunction, subordinating </entry>
        <entry> that as after whether before while like because if since for
                 than until so unless though providing once lest
                 till whereas whereupon supposing albeit then </entry>
      </row>
      <row>
        <entry> do / do*</entry>
        <entry> verb "to do", uninflected present tense, infinitive or imperative </entry>
        <entry> do dost / don't</entry>
      </row>
      <row>
        <entry> dod / dod* </entry>
        <entry> verb "to do", past tense </entry>
        <entry> did done / didn't </entry>
      </row>
      <row>
        <entry> doz / doz* </entry>
        <entry> verb "to do", present tense, 3rd person singular </entry>
        <entry> does / doesn't don't </entry>
      </row>
      <row>
        <entry> dt </entry>
        <entry> determiner/pronoun, singular </entry>
        <entry> this each another that 'nother </entry>
      </row>
      <row>
        <entry> dt$ </entry>
        <entry> determiner/pronoun, singular, genitive </entry>
        <entry> another's </entry>
      </row>
      <row>
        <entry> dt+bez </entry>
        <entry> determiner/pronoun + verb "to be", present tense, 3rd person singular </entry>
        <entry> that's </entry>
      </row>
      <row>
        <entry> dt+md </entry>
        <entry> determiner/pronoun + modal auxiliary </entry>
        <entry> that'll this'll </entry>
      </row>
      <row>
        <entry> dti </entry>
        <entry> determiner/pronoun, singular or plural </entry>
        <entry> any some </entry>
      </row>
      <row>
        <entry> dts </entry>
        <entry> determiner/pronoun, plural </entry>
        <entry> these those them </entry>
      </row>
      <row>
        <entry> dtx </entry>
        <entry> determiner, pronoun or double conjunction </entry>
        <entry> neither either one </entry>
      </row>
      <row>
        <entry> ex </entry>
        <entry> existential there </entry>
        <entry> there </entry>
      </row>
      <row>
        <entry> hv / hv*</entry>
        <entry> verb "to have", uninflected present tense, infinitive or imperative </entry>
        <entry> have hast / haven't ain't </entry>
      </row>
      <row>
        <entry> hv+to </entry>
        <entry> verb "to have", uninflected present tense + infinitival to </entry>
        <entry> hafta </entry>
      </row>
      <row>
        <entry> hvd / hvd*</entry>
        <entry> verb "to have", past tense </entry>
        <entry> had / hadn't </entry>
      </row>
      <row>
        <entry> hvg </entry>
        <entry> verb "to have", present participle or gerund </entry>
        <entry> having </entry>
      </row>
      <row>
        <entry> hvn </entry>
        <entry> verb "to have", past participle </entry>
        <entry> had </entry>
      </row>
      <row>
        <entry> hvz / hvz*</entry>
        <entry> verb "to have", present tense, 3rd person singular </entry>
        <entry> has hath / hasn't ain't </entry>
      </row>
      <row>
        <entry> in </entry>
        <entry> preposition </entry>
        <entry> of in for by considering to on among at through with under
                 into regarding than since despite ... </entry>
      </row>
      <row>
        <entry> jj </entry>
        <entry> adjective </entry>
        <entry> recent over-all possible hard-fought favorable hard meager fit
                 such widespread outmoded inadequate ... </entry>
      </row>
      <row>
        <entry> jj$ </entry>
        <entry> adjective, genitive </entry>
        <entry> Great's </entry>
      </row>
      <row>
        <entry> jjr </entry>
        <entry> adjective, comparative </entry>
        <entry> greater older further earlier later freer franker wider better
                 deeper firmer tougher faster higher bigger ... </entry>
      </row>
      <row>
        <entry> jjs </entry>
        <entry> adjective, semantically superlative </entry>
        <entry> top chief principal northernmost master key head main
                 tops utmost innermost foremost uppermost ... </entry>
      </row>
      <row>
        <entry> jjt </entry>
        <entry> adjective, superlative </entry>
        <entry> best largest coolest calmest latest greatest earliest simplest
                 strongest newest fiercest unhappiest worst ...</entry>
      </row>
      <row>
        <entry> md / md*</entry>
        <entry> modal auxiliary </entry>
        <entry> should may might will would must can could shall ought need 
                 wilt /
                cannot couldn't wouldn't can't won't shouldn't shan't mustn't musn't </entry>      </row>
      <row>
        <entry> nn </entry>
        <entry> noun, singular, common </entry>
        <entry> failure burden court fire appointment awarding compensation
                 Mayor interim committee fact effect ... </entry>
      </row>
      <row>
        <entry> nn$ </entry>
        <entry> noun, singular, common, genitive </entry>
        <entry> season's world's player's night's chapter's golf's football's
                 baseball's club's U.'s coach's bride's ... </entry>
      </row>
      <row>
        <entry> nn+bez </entry>
        <entry> noun, singular, common + verb "to be", present tense, 3rd person singular </entry>
        <entry> water's camera's sky's kid's Pa's heat's throat's father's
                 money's undersecretary's granite's level's ... </entry>
      </row>
      <row>
        <entry> nn+hvz </entry>
        <entry> noun, singular, common + verb "to have", present tense, 3rd person singular </entry>
        <entry> guy's Knife's boat's summer's rain's company's </entry>
      </row>
      <row>
        <entry> nns </entry>
        <entry> noun, plural, common </entry>
        <entry> irregularities presentments thanks reports voters laws
                 legislators years areas adjustments chambers ... </entry>
      </row>
      <row>
        <entry> nns$ </entry>
        <entry> noun, plural, common, genitive </entry>
        <entry> taxpayers' children's members' States' women's cutters'
                 motorists' steelmakers' hours' Nations' ... </entry>
      </row>
      <row>
        <entry> np </entry>
        <entry> noun, singular, proper </entry>
        <entry> Fulton Atlanta September-October Durwood Pye Ivan Allen Jr.
                 Jan. Alpharetta Grady ... </entry>
      </row>
      <row>
        <entry> np$ </entry>
        <entry> noun, singular, proper, genitive </entry>
        <entry> Green's Landis' Smith's Carreon's Allison's Boston's Spahn's
                 Willie's Mickey's Milwaukee's ... </entry>
      </row>
      <row>
        <entry> np+bez </entry>
        <entry> noun, singular, proper + verb "to be", present tense, 3rd person singular </entry>
        <entry> W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's
                 Seaton's Buckhorn's Breed's Penny's ... </entry>
      </row>
      <row>
        <entry> nps </entry>
        <entry> noun, plural, proper </entry>
        <entry> Chases Aderholds Chapelles Armisteads Lockies Carbones French
                 Marskmen Toppers Franciscans ... </entry>
      </row>
      <row>
        <entry> nps$ </entry>
        <entry> noun, plural, proper, genitive </entry>
        <entry> Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees'
                 Stevenses' Geraghtys' Burkes' ...</entry>
      </row>
      <row>
        <entry> nr </entry>
        <entry> noun, singular, adverbial </entry>
        <entry> Friday home Wednesday Tuesday Monday Sunday Thursday yesterday
                 tomorrow tonight West East Saturday west left east downtown
                 north northeast southeast northwest North South right ... </entry>
      </row>
      <row>
        <entry> nr$ </entry>
        <entry> noun, singular, adverbial, genitive </entry>
        <entry> Saturday's Monday's yesterday's tonight's tomorrow's Sunday's
                 Wednesday's Friday's today's Tuesday's West's Today's South's </entry>
      </row>
      <row>
        <entry> nrs </entry>
        <entry> noun, plural, adverbial </entry>
        <entry> Sundays Mondays Saturdays Wednesdays Souths Fridays </entry>
      </row>
      <row>
        <entry> od </entry>
        <entry> numeral, ordinal </entry>
        <entry> first 13th third nineteenth 2d 61st second sixth eighth ninth
                 twenty-first eleventh 50th ... </entry>
      </row>
      <row>
        <entry> pn </entry>
        <entry> pronoun, nominal </entry>
        <entry> none something everything one anyone nothing nobody everybody
                 everyone anybody anything someone no-one nothin </entry>
      </row>
      <row>
        <entry> pn$ </entry>
        <entry> pronoun, nominal, genitive </entry>
        <entry> one's someone's anybody's nobody's everybody's anyone's everyone's </entry>
      </row>
      <row>
        <entry> pp$ </entry>
        <entry> determiner, possessive </entry>
        <entry> our its his their my your her out thy mine thine </entry>
      </row>
      <row>
        <entry> pp$$ </entry>
        <entry> pronoun, possessive </entry>
        <entry> ours mine his hers theirs yours </entry>
      </row>
      <row>
        <entry> ppl </entry>
        <entry> pronoun, singular, reflexive </entry>
        <entry> itself himself myself yourself herself oneself ownself </entry>
      </row>
      <row>
        <entry> ppls </entry>
        <entry> pronoun, plural, reflexive </entry>
        <entry> themselves ourselves yourselves </entry>
      </row>
      <row>
        <entry> ppo </entry>
        <entry> pronoun, personal, accusative </entry>
        <entry> them it him me us you 'em her thee we'uns </entry>
      </row>
      <row>
        <entry> pps </entry>
        <entry> pronoun, personal, nominative, 3rd person singular </entry>
        <entry> it he she thee </entry>
      </row>
      <row>
        <entry> pps+bez </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + verb
          "to be", present tense, 3rd person singular </entry>
        <entry> it's he's she's </entry>
      </row>
      <row>
        <entry> pps+hvd </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + verb "to have", past tense </entry>
        <entry> she'd he'd it'd </entry>
      </row>
      <row>
        <entry> pps+hvz </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + verb
          "to have", present tense, 3rd person singular </entry>
        <entry> it's he's she's </entry>
      </row>
      <row>
        <entry> pps+md </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + modal auxiliary </entry>
        <entry> he'll she'll it'll he'd it'd she'd </entry>
      </row>
      <row>
        <entry> ppss </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular </entry>
        <entry> they we I you ye thou you'uns </entry>
      </row>
      <row>
        <entry> ppss+bem </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular +
          verb "to be", present tense, 1st person singular </entry>
        <entry> I'm Ahm </entry>
      </row>
      <row>
        <entry> ppss+ber </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + 
          verb "to be", present tense, 2nd person singular or all persons plural </entry>
        <entry> we're you're they're </entry>
      </row>
      <row>
        <entry> ppss+hv </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + verb "to have", uninflected present tense </entry>
        <entry> I've we've they've you've </entry>
      </row>
      <row>
        <entry> ppss+hvd </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + verb "to have", past tense </entry>
        <entry> I'd you'd we'd they'd </entry>
      </row>
      <row>
        <entry> ppss+md </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + modal auxiliary </entry>
        <entry> you'll we'll I'll we'd I'd they'll they'd you'd </entry>
      </row>
      <row>
        <entry> ql </entry>
        <entry> qualifier, pre </entry>
        <entry> well less very most so real as highly fundamentally even how much remarkably somewhat more completely too thus ill deeply little overly halfway almost impossibly far severely such ... </entry>
      </row>
      <row>
        <entry> qlp </entry>
        <entry> qualifier, post </entry>
        <entry> indeed enough still 'nuff </entry>
      </row>
      <row>
        <entry> rb </entry>
        <entry> adverb </entry>
        <entry> only often generally also nevertheless upon together back
                 newly no likely meanwhile near then heavily there apparently 
                 yet outright fully aside consistently specifically formally 
                 ever just ...  </entry>
      </row>
      <row>
        <entry> rb+bez </entry>
        <entry> adverb + verb "to be", present tense, 3rd person singular </entry>
        <entry> here's there's </entry>
      </row>
      <row>
        <entry> rbr </entry>
        <entry> adverb, comparative </entry>
        <entry> further earlier better later higher tougher more harder longer
                 sooner less faster easier louder ... </entry>
      </row>
      <row>
        <entry> rbt </entry>
        <entry> adverb, superlative </entry>
        <entry> most best highest uppermost nearest brightest hardest fastest
                 deepest farthest loudest ... </entry>
      </row>
      <row>
        <entry> rn </entry>
        <entry> adverb, nominal </entry>
        <entry> here afar then </entry>
      </row>
      <row>
        <entry> rp </entry>
        <entry> adverb, particle </entry>
        <entry> up out off down over on in about through across after </entry>
      </row>
      <row>
        <entry> to </entry>
        <entry> infinitival to </entry>
        <entry> to t' </entry>
      </row>
      <row>
        <entry> uh </entry>
        <entry> interjection </entry>
        <entry> Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha
                 crunch say oh why see well hello lo alas tarantara 
                 rum-tum-tum gosh hell ... </entry>
      </row>
      <row>
        <entry> vb </entry>
        <entry> verb, base: uninflected present, imperative or infinitive </entry>
        <entry> investigate find act follow inure achieve reduce take remedy
                 re-set distribute realize disable feel receive ... </entry>
      </row>
      <row>
        <entry> vbd </entry>
        <entry> verb, past tense </entry>
        <entry> said produced took recommended commented urged found added
                 praised charged listed ... </entry>
      </row>
      <row>
        <entry> vbg </entry>
        <entry> verb, present participle or gerund </entry>
        <entry> modernizing improving purchasing lacking enabling
                 pricing keeping getting picking ... </entry>
      </row>
      <row>
        <entry> vbn </entry>
        <entry> verb, past participle </entry>
        <entry> conducted charged won received studied revised operated
                 accepted combined experienced ... </entry>
      </row>
      <row>
        <entry> vbz </entry>
        <entry> verb, present tense, 3rd person singular </entry>
        <entry> deserves believes receives takes goes expires says opposes
                 starts permits expects thinks faces ... </entry>
      </row>
      <row>
        <entry> wdt </entry>
        <entry> WH-determiner </entry>
        <entry> which what whatever whichever whichever-the-hell </entry>
      </row>
      <row>
        <entry> wdt+bez </entry>
        <entry> WH-determiner + verb "to be", present tense, 3rd person singular </entry>
        <entry> what's </entry>
      </row>
      <row>
        <entry> wp$ </entry>
        <entry> WH-pronoun, genitive </entry>
        <entry> whose whosever </entry>
      </row>
      <row>
        <entry> wpo </entry>
        <entry> WH-pronoun, accusative </entry>
        <entry> whom that who </entry>
      </row>
      <row>
        <entry> wps </entry>
        <entry> WH-pronoun, nominative </entry>
        <entry> that who whoever whosoever what whatsoever </entry>
      </row>
      <row>
        <entry> wps+bez </entry>
        <entry> WH-pronoun, nominative + verb "to be", present, 3rd person singular </entry>
        <entry> that's who's </entry>
      </row>
      <row>
        <entry> wps+hvd </entry>
        <entry> WH-pronoun, nominative + verb "to have", past tense </entry>
        <entry> who'd </entry>
      </row>
      <row>
        <entry> wps+hvz </entry>
        <entry> WH-pronoun, nominative + verb "to have", present tense, 3rd person singular </entry>
        <entry> who's that's </entry>
      </row>
      <row>
        <entry> wps+md </entry>
        <entry> WH-pronoun, nominative + modal auxiliary </entry>
        <entry> who'll that'd who'd that'll </entry>
      </row>
      <row>
        <entry> wql </entry>
        <entry> WH-qualifier </entry>
        <entry> however how </entry>
      </row>
      <row>
        <entry> wrb </entry>
        <entry> WH-adverb </entry>
        <entry> however when where why whereby wherever how whenever
                 whereon wherein wherewith wheare wherefore whereof howsabout </entry>
      </row>
      <row>
        <entry> wrb+ber </entry>
        <entry> WH-adverb + verb "to be", present, 2nd person singular or all persons plural </entry>
        <entry> where're </entry>
      </row>
    </tbody>
  </tgroup>
</table>

</appendix>



&index;
</article>

