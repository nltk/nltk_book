<?xml version="1.0"?>
<!DOCTYPE article PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" 
"../../docbook/sgml/docbook/xml-dtd-4.2/docbookx.dtd" [
<!-- Base URL for the reference & tutorial documentation -->
<!ENTITY refdoc "../../api/public">
<!ENTITY tutdoc "..">

<!-- Index -->
<!ENTITY index SYSTEM "index.xml">

<!-- Copyright & License -->
<!ENTITY copyright SYSTEM "../copyright.xml">

<!-- Version & Date -->
<!ENTITY versiondate SYSTEM "versiondate.xml">
]>

<article>
  <articleinfo>
<!--
    <author><firstname>Steven</firstname><surname>Bird</surname></author>
    <authorinitials>sb</authorinitials>
    <author><firstname>Ewan</firstname><surname>Klein</surname></author>
    <authorinitials>ek</authorinitials>
    <author><firstname>Edward</firstname><surname>Loper</surname></author>
    <authorinitials>edl</authorinitials>
    <title>NLTK Tutorial: Tagging</title>
    &versiondate; &copyright;
-->
    <title>Tagging</title>
  </articleinfo>

  <para><emphasis>Sample chapter for Natural Language Processing,
    by Steven Bird, Ewan Klein and Edward Loper, April 2005</emphasis></para>

<!-- TODO: ADD DISCUSSION OF HOW TO SAVE A TRAINED TAGGER SO IT
DOESN'T NEED TO BE TRAINED EACH TIME IT IS USED -->

<section id="intro">
  <title>Introduction</title>

  <para>
    Many natural language expressions are ambiguous, and we need to
    draw on other sources of information to aid interpretation.  For
    instance, our preferred interpretation of <literal>fruit flies like a
    banana</literal> depends on the presence of contextual cues that
    cause us to expect <literal>flies</literal> to be a noun or a
    verb.  Before we can even address such issues, we need to be able
    to represent the required linguistic information.  Here is a
    possible representation:
  </para>

      <table id="table.fruit1">
        <title/>
        <tgroup cols="5">
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <tbody>
            <row>
	    <entry><literal>Fruit</literal></entry>
	    <entry><literal>flies</literal></entry>
	    <entry><literal>like</literal></entry>
	    <entry><literal>a</literal></entry>
	    <entry><literal>banana</literal></entry>
            </row>
            <row>
              <entry>noun</entry>
              <entry>verb</entry>
              <entry>prep</entry>
              <entry>det</entry>
              <entry>noun</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

      <table id="table.fruit2">
        <title/>
        <tgroup cols="5">
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <colspec colwidth='1.5cm'/>
          <tbody>
            <row>
	    <entry><literal>Fruit</literal></entry>
	    <entry><literal>flies</literal></entry>
	    <entry><literal>like</literal></entry>
	    <entry><literal>a</literal></entry>
	    <entry><literal>banana</literal></entry>
            </row>
            <row>
              <entry>noun</entry>
              <entry>noun</entry>
              <entry>verb</entry>
              <entry>det</entry>
              <entry>noun</entry>
            </row>
          </tbody>
        </tgroup>
      </table>

  <para>
    Most language processing systems must recognize and interpret the
    linguistic structures that exist in a sequence of words.  This task
    is virtually impossible if all we know about each word is its text
    representation.  To determine whether a given string of words has
    the structure of, say, a noun phrase, it is infeasible to check
    through a (possibly infinite) list of all strings which can be
    classed as noun phrases.  Instead we want to be able to generalise
    over <emphasis>classes</emphasis> of words. These <firstterm>word
    classes</firstterm> are commonly given labels such as 'determiner',
    'adjective' and 'noun'.  Conversely, to interpret words we need to
    be able to discriminate between different usages, such as
    <literal>deal</literal> as a noun or a verb.  The process of
    classifying words in this way, and labelling them accordingly, is
    known as <glossterm>part-of-speech tagging</glossterm>,
    <glossterm>POS-tagging</glossterm>, or simply
    <glossterm>tagging</glossterm>.  The collection of tags used for a
    particular task is known as a <glossterm>tag set</glossterm>.
  </para>

 
<!--
 <para>
    In the following sections, we will give a more detailed account of
    the linguistic and practical issues that arise in the course of
    part-of-speech tagging, and then survey how tagging is carried out
    in NLTK. Before launching into this, however, we will give the
    reader a flavour of the uses of tagging. That is,
    we consider three kinds of language analysis where tags play
    an important role: parsing, morphological analysis, and
    stylistics.
  </para>

  <para>
    Most natural language parsers depend on <glossterm>part-of-speech
    tags</glossterm>.  Instead of writing rules like <literal>NP &rarr;
    the dog</literal> and <literal>NP &rarr; three red cars</literal>,
    we can write <literal>NP &rarr; DT JJ* NN</literal>.  In this way,
    the terminal symbols of the grammar can be word categories, instead
    of words, greatly reducing the size of the grammar.

  </para>



  <para>
    <glossterm>Morphological analysis</glossterm> is also assisted by part-of-speech tags.
    For instance, if we encounter the word <literal>deals</literal>
    in running text, should this be analysed as the plural form of a
    noun, e.g., <literal>deal<subscript>N</subscript>+PL</literal>
    or the third-person singular form of a verb, e.g.,
    <literal>deal<subscript>V</subscript>+3PS</literal>?
    A tagger will consider the context in which this word appears,
    and will reliably determine whether it is a noun or a verb.
    Then the morphological analyser can be given either
    <literal>deals/NN</literal> or <literal>deals/VB</literal>
    to process.
  </para>
-->
  <para>
    We earlier presented <xref linkend="table.fruit1"/> and <xref
    linkend="table.fruit2"/> as examples of how a string of word tokens can
    be augmented with information about the word classes that the words
    belong to. In effect, we carried out tagging for the string
    <literal>fruit flies like a banana</literal>. However, tags are more
    usually attached inline to the text they are associated with. This
    is illustrated in the following sentence from
    the Brown Corpus:
    <literal>
      The/at Pantheon's/np$ interior/nn ,/, still/rb in/in its/pp$
      original/jj form/nn ,/, is/bez truly/ql majestic/jj and/cc an/at
      architectural/jj triumph/nn ./.
    </literal> 
<!-- *** According to our table later on, ./. should actually be ./end -->
    Here, the sequence <literal>The/at</literal> means that
    the word token <literal>The</literal> is tagged
    <literal>at</literal>, which is the Brown Corpus tag for
    article.<footnote>
	<para>The reader may be initially puzzled by strings
    like <literal>,/,</literal>. This just means that the tag for a
    comma is '<literal>,'</literal>.</para>
	
      </footnote>
    We can think of tagging as one way of
    <firstterm>annotating</firstterm> a text corpus. Annotation is a way
    of adding information to a text &mdash; indeed, we might like to
    think of it as a way of making explicit information which is already
    implicitly present in the text.
  </para>
  <para>
    What is the value of annotating a text in this way? One illustration
    is the use of tagged corpora to study patterns of word
    usage in different genres (<glossterm>stylistics</glossterm>).  For
    example, we can use the tags to identify all words of a certain
    class, such as modals, then tabulate their frequency of occurrence
    in different genres, as shown in <xref linkend="table.stylistics"/>.
  </para>


<table id="table.stylistics">
  <title>Use of Modals in Brown Corpus, by Genre</title>
  <tgroup cols="7">
    <colspec colwidth='3cm'/>
    <colspec colwidth='12mm'/><colspec colwidth='12mm'/><colspec colwidth='12mm'/>
    <colspec colwidth='12mm'/><colspec colwidth='12mm'/><colspec colwidth='12mm'/>
    <tbody>
      <row>
        <entry>Genre</entry>
        <entry>can</entry><entry>could</entry><entry>may</entry>
        <entry>might</entry><entry>must</entry><entry>will</entry>
      </row>
      <row>
        <entry>skill and hobbies</entry>
        <entry>273</entry><entry>59</entry><entry>130</entry>
        <entry>22</entry><entry>83</entry><entry>259</entry>
      </row>
      <row>
        <entry>humor</entry>
        <entry>17</entry><entry>33</entry><entry>8</entry>
        <entry>8</entry><entry>9</entry><entry>13</entry>
      </row>
      <row>
        <entry>fiction: science</entry>
        <entry>16</entry><entry>49</entry><entry>4</entry>
        <entry>12</entry><entry>8</entry><entry>16</entry>
      </row>
      <row>
        <entry>press: reportage</entry>
        <entry>94</entry><entry>86</entry><entry>66</entry>
        <entry>36</entry><entry>50</entry><entry>387</entry>
      </row>
      <row>
        <entry>fiction: romance</entry>
        <entry>79</entry><entry>195</entry><entry>11</entry>
        <entry>51</entry><entry>46</entry><entry>43</entry>
      </row>
      <row>
        <entry>religion</entry>
        <entry>84</entry><entry>59</entry><entry>79</entry>
        <entry>12</entry><entry>54</entry><entry>64</entry>
      </row>
    </tbody>
  </tgroup>
</table>

<!-- this repeats the text after table 2
  <para>
    The process of associating labels with each token in a text is
    called <glossterm>tagging</glossterm>, and the labels are called
    <glossterm>tags</glossterm>.  The collection of tags used for a
    particular task is known as a <glossterm>tag set</glossterm>.
  </para>
-->

  <para>
    This tutorial focuses on part-of-speech tagging, as an early step
    in language processing which does not depend on deep linguistic
    analysis.  Readers should be aware that are many other kinds of
    tagging.  Words can be tagged with directives to a speech
    synthesiser, indicating which words should be emphasised.
    Words can be tagged with sense numbers, indicating which sense
    of the word was used.  Words can also be tagged with morphological
    features.  Examples of each of these kinds of tags are shown in
    <xref linkend="table.taggingexamples"/>.  Note that for space
    reasons, we only show the tag on a single italicised word. Note also
    that the first two examples use XML-style tags, where elements in
    angle brackets enclose the word that is tagged.
  </para>

<table id="table.taggingexamples">
  <title>Examples of Non Part-of-Speech Tagging</title>
  <tgroup cols="2">
    <colspec colwidth='6cm'/>
    <tbody>
      <row>
        <entry>Speech Synthesis Markup Language (W3C SSML)</entry>
        <entry>That is a
        &lt;emphasis><emphasis>big</emphasis>&lt;/emphasis> car!</entry>
      </row>
      <row>
        <entry>SemCor: Brown Corpus tagged with WordNet senses</entry>
        <entry>Space in any
          &lt;wf pos="NN" lemma="form" wnsn="4"><emphasis>form</emphasis>&lt;/wf>
          is completely measured by the three dimensions.
          <emphasis>
            (Wordnet form/nn sense 4: "shape, form, configuration,
            contour, conformation")
          </emphasis>
        </entry>
      </row>
      <row>
        <entry>Morphological tagging, from the Turin University Italian Treebank</entry>
        <entry>
          E' italiano , come progetto e realizzazione , il
          <emphasis>primo</emphasis> (PRIMO ADJ ORDIN M SING)
          porto turistico dell' Albania .
        </entry>
      </row>
    </tbody>
  </tgroup>
</table>

 <para>We have already noted that part-of-speech tags are closely
   related to the notion of word class used in syntax.
   The assumption in theoretical linguistics  is that every
   distinct word type will be listed in a lexicon (or dictionary), with
   information about its pronunciation, syntactic properties and
   meaning. A key component of the word's syntactic properties will be
   its class. When we carry out a syntactic analysis of our earlier
   example <literal>fruit flies like a banana</literal>, we will look
   up each word in the lexicon, determine its word class, and then
   group it into a hierarchy of phrases, as illustrated in the following
   parse tree.
   </para>

<figure id="syn-tree"><title>Syntactic Parse Tree</title>
<informaltable frame="topbot">
<tgroup cols="1"><tbody><row><entry>
<graphic fileref="images/syn-tree" scale="6"/>
</entry></row></tbody></tgroup></informaltable>
</figure>

   <para>In this tree, we have used standard syntactic abbreviations for word
    classes. These are shown in <xref linkend="table.class-tag"/>
    together with their counterparts from the Brown tag set.
   </para>

    <table id="table.class-tag">
        <title>Word Class Labels and Tags</title>
        <tgroup cols="3">
            <colspec colwidth='3.5cm'/>
            <colspec colwidth='2cm'/>
            <colspec colwidth='5cm'/>
     <thead>
      <row>
       <entry>Word Class Label</entry>
       <entry>Brown Tag</entry>
       <entry>Word Class</entry>
      </row>
     </thead>
            <tbody>
                <row>
                    <entry>Det</entry>
                    <entry>at</entry>
                    <entry>Article</entry>
                </row>
                <row>
                    <entry>N</entry>
                    <entry>nn</entry>
                    <entry>Noun</entry>
                </row>
                <row>
                    <entry>V</entry>
                    <entry>vb</entry>
                    <entry>Verb</entry>
                </row>
                <row>
                    <entry>Adj</entry>
                    <entry>jj</entry>
                    <entry>Adjective</entry>
                </row>
                <row>
                    <entry>P</entry>
                    <entry>in</entry>
                    <entry>Preposition</entry>
                </row>
                <row>
                    <entry>Card</entry>
                    <entry>cd</entry>
                    <entry>Number</entry>
                </row>
                <row>
                    <entry>&ndash;</entry>
                    <entry>end</entry>
                    <entry>Sentence-ending punctuation</entry>
                </row>
            </tbody>
        </tgroup>
    </table>

   <para>In
    syntactic analyses, there is often a close connection between the
    class a word belongs to and the phrases it forms with neighbouring
    words. So, for example, a noun phrase (labelled NP) will usually
    consist of a noun (N) optional accompanied by an article (Det) and
    some modifiers such as adjectives (Adj). We can  express this
    generalisation in terms of a production rule:
    <simplelist>
     <member><literal>NP &rarr; Det Adj* N</literal></member>
    </simplelist>
    In such a case, we say that the noun is the
    <glossterm>head</glossterm> of the noun phrase; roughly speaking,
    the head of a phrase is a required element which can occur in any
    context that the phrase as a whole can occur in.  However, from a
    purely notational point of view, we are free to use any labels we
    want for word classes, and these could just as well be the labels
    provided by a tag set. For example, we could replace the preceding
    rule with the following:
    <simplelist>
     <member><literal>NP &rarr; at jj* nn</literal></member>
    </simplelist>
    This is useful since some practical tasks, we might use an
    automatic tagger to label the
    words in our example with tags drawn from the Brown tag set, and use
    that as a basis for building a syntactic parse tree like <xref
    linkend="syn-tree"/>. 
   </para>

   <para>
    So far, we have only looked at tags as capturing information about
    word class. However, common tag sets like those used in the Brown
    Corpus also capture a certain amount of
    <glossterm>morpho-syntactic</glossterm> information. Consider, for
    example, the selection of distinct forms of the word <literal>be</literal>
    illustrated in the following sentences:
    <simplelist>
     <member><literal>Be still!</literal></member>
     <member><literal>Being still is hard.</literal></member>
     <member><literal>I am still.</literal></member>
     <member><literal>I have been still all day.</literal></member>
     <member><literal>I was still all day.</literal></member>
    </simplelist>
    We say that these forms are morpho-syntactically distinct because
    they exhibit different morphological inflections and different
    co-occurrence restrictions with neighbouring words. For example,
    <literal>am</literal> cannot replace either of the first two
    examples:
    <simplelist>
     <member><literal>*Am still!</literal></member>
     <member><literal>*Am still is hard.</literal></member>
     </simplelist>
    These differences between the forms are encoded in their Brown
    Corpus tags: <literal>be/be, being/beg, am/bem, been/ben</literal>
    and <literal>was/bedz</literal>. This means that an automatic tagger
    which uses this tag set is in effect carrying out a limited amount of
    morphological analysis.
  </para>


</section> <!-- Introduction -->



<section id="taggers.simple">
  <title> Simple Taggers </title>

  <para>
    In this section we consider three simple taggers.  They all
    process the input tokens one by one, adding a tag to each token.
    In each case they being with tokenized text.  We can easily create
    a sample of tokenized text as follows:
  </para>

<programlisting><![CDATA[
>>> from nltk.tokenizer import *
>>> text_token = Token(TEXT="John saw 3 polar bears .")
>>> WhitespaceTokenizer().tokenize(text_token)
>>> print text_token
<[<John>, <saw>, <3>, <polar>, <bears>, <.>]>
]]></programlisting>

  <section id="taggers.unigram.default">
    <title> The Default Tagger </title>

    <para>
      The simplest possible tagger assigns the same tag to each token
      regardless of the token's text.  The <literal>DefaultTagger</literal>
      class implements this kind of tagger.  In the following program,
      we create a tagger called <literal>my_tagger</literal> which
      tags everything as a noun.
    </para>

<programlisting><![CDATA[
>>> from nltk.tagger import *
>>> my_tagger = DefaultTagger('nn')
>>> my_tagger.tag(text_token)
>>> print text_token
<[<John/nn>, <saw/nn>, <3/nn>, <polar/nn>, <bears/nn>, <./nn>]>
]]></programlisting>

    <para>
      This is a simple algorithm, and it performs poorly when used
      on its own. On a typical corpus, it will tag only 20%-30% of
      the tokens correctly. However, it is a very reasonable
      tagger to use as a default, if a more advanced tagger fails
      to determine a token's tag. When used in conjunction with
      other taggers, a <literal>DefaultTagger</literal> can
      significantly improve performance.
    </para>

    <important><para>
      Default taggers assign their tag to every single
      word, even words that have never been encountered before.
      Thus, they help to improve the robustness of a language
      processing system.
    </para></important>

  </section> <!-- DefaultTagger -->

  <section id="taggers.regexp">
    <title> The Regular Expression Tagger </title>

    <para>
      The regular expression tagger assigns tags to tokens on the
      basis of matching patterns in the token's text.  For instance,
      the following tagger assigns <literal>cd</literal> to cardinal
      numbers, and <literal>nn</literal> to everything else.
    </para>

<programlisting><![CDATA[
>>> NN_CD_tagger = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'cd'), (r'.*', 'nn')])
>>> NN_CD_tagger.tag(text_token)
>>> print text_token
<[<John/nn>, <saw/nn>, <3/cd>, <polar/nn>, <bears/nn>, <./nn>]>
]]></programlisting>

    <para>
      We can generalise this method to guess the correct tag for words
      based on the presence of certain prefix or suffix strings.
      For instance, English words beginning with <literal>un-</literal>
      are likely to be adjectives.
    </para>

  </section> <!-- DefaultTagger -->

  <section id="taggers.unigram.unigram">
    <title> The Unigram Tagger </title>

    <para>
      The <literal>UnigramTagger</literal> class implements a simple
      statistical tagging algorithm: for each token, it assigns the
      tag that is most likely for that token's text. For example, it
      will assign the tag <literal>jj</literal> to any occurrence of the word
      <literal>frequent</literal>, since <literal>frequent</literal> is
      used as an adjective (e.g. <literal>a
      frequent word</literal>) more often than it is used as a verb
      (e.g. <literal>I frequent this cafe</literal>).
    </para>

    <para>
      Before a <literal>UnigramTagger</literal> can be used to tag
      data, it must be trained on a <glossterm>training
      corpus</glossterm>. It uses this corpus to determine which tags
      are most common for each word.
      <literal>UnigramTaggers</literal> are trained using the
      <literal>train()</literal> method, which takes a tagged corpus:
    </para>

<programlisting><![CDATA[
>>> from nltk.tagger import *
>>> from nltk.corpus import brown
    # Tokenize ten texts from the Brown Corpus
>>> train_tokens = []
>>> for item in brown.items()[:10]:
...     train_tokens.append(brown.read(item))
    # Initialise and train a unigram tagger
>>> mytagger = UnigramTagger(SUBTOKENS='WORDS')
>>> for tok in train_tokens: mytagger.train(tok)
]]></programlisting>

    <para>
      Once a <literal>UnigramTagger</literal> has been trained, the
      <literal>tag()</literal> method can be used to tag new text:
    </para>

<programlisting><![CDATA[
>>> text_token = Token(TEXT="John saw the book on the table")
>>> WhitespaceTokenizer(SUBTOKENS='WORDS').tokenize(text_token)
>>> mytagger.tag(text_token)
>>> print text_token
<[<John/np>, <saw/vbd>, <the/at>, <book/None>, <on/in>, <the/at>, <table/nn>]>
]]></programlisting>

    <para>
      As we noted earlier, <literal>UnigramTagger</literal> will assign
      the default tag <literal>None</literal> to any token that was not
      encountered in the training data.  Later we will see how a
      <literal>UnigramTagger</literal> can be combined with a
      <literal>DefaultTagger</literal> to ensure that every word is
      tagged.
    </para>

  </section> <!-- UnigramTagger -->
</section> <!-- Unigram Tagging -->

<section id="evaluation">
  <title>Evaluating Taggers</title>

  <para>
    As we experiment with different taggers, it is important to have
    an objective performance measure.  Fortunately, we already have manually
    verified training data (the original tagged corpus), so we can use
    that to evaluate our taggers.
  </para>

  <para>
    Consider the following sentence from the Brown Corpus.  The 'Gold
    Standard' tags from the corpus are given in the second column,
    while the tags assigned by a unigram tagger appear in the third
    column.  Two mistakes made by the unigram tagger are italicised.
   </para>

  <table id="table.evaluation">
    <title>Evaluating Taggers</title>
    <tgroup cols="3">
      <colspec colwidth='3cm'/>
      <colspec colwidth='3cm'/>
      <colspec colwidth='3cm'/>
      <tbody>
        <row><entry>Sentence:</entry><entry>Gold Standard:</entry><entry>Unigram Tagger:</entry></row>
        <row><entry>The</entry><entry>at</entry><entry>at</entry></row>
        <row><entry>President</entry><entry>nn-tl</entry><entry>nn-tl</entry></row>
        <row><entry>said</entry><entry>vbd</entry><entry>vbd</entry></row>
        <row><entry>he</entry><entry>pps</entry><entry>pps</entry></row>
        <row><entry>will</entry><entry>md</entry><entry>md</entry></row>
        <row><entry>ask</entry><entry>vb</entry><entry>vb</entry></row>
        <row><entry>Congress</entry><entry>np</entry><entry>np</entry></row>
        <row><entry>to</entry><entry>to</entry><entry>to</entry></row>
        <row><entry>increase</entry><entry>vb</entry><entry><emphasis>nn</emphasis></entry></row>
        <row><entry>grants</entry><entry>nns</entry><entry>nns</entry></row>
        <row><entry>to</entry><entry>in</entry><entry><emphasis>to</emphasis></entry></row>
        <row><entry>states</entry><entry>nns</entry><entry>nns</entry></row>
        <row><entry>for</entry><entry>in</entry><entry>in</entry></row>
        <row><entry>vocational</entry><entry>jj</entry><entry>jj</entry></row>
        <row><entry>rehabilitation</entry><entry>nn</entry><entry>nn</entry></row>
        <row><entry>.</entry><entry>.</entry><entry>.</entry></row>
      </tbody>
    </tgroup>
  </table>

  <para>
    The tagger correctly tagged 14 out of 16 words, so it gets
    a score of 14/16, or 87.5%.  Of course, accuracy should be
    judged on the basis of a larger sample of data.  NLTK provides a
    function called <literal>tagger_accuracy</literal> to automate the
    task.  In the simplest case, we can test the tagger using the same
    data it was trained on:
  </para>

<programlisting><![CDATA[
>>> acc = tagger_accuracy(mytagger, train_tokens)
>>> print 'Accuracy = %4.1f%%' % (100 * acc)
94.8%
]]></programlisting>

  <para>
    However, testing a language processing system using the same data it was
    trained on is unwise.  A system which simply memorised the
    training data would get a perfect score without doing any
    linguistic modelling.  Instead, we would like to reward systems
    that make good generalizations, so we should test against
    <emphasis>unseen data</emphasis>, and replace <literal>train_tokens</literal>
    above with <literal>unseen_tokens</literal>.  We can then
    define the two sets of data as follows:
  </para>

<programlisting><![CDATA[
>>> train_tokens = []
>>> for item in brown.items()[:10]:    # texts 0-9
...     train_tokens.append(brown.read(item))
>>> unseen_tokens = []
>>> for item in brown.items()[10:12]:  # texts 10-11
...     unseen_tokens.append(brown.read(item))
]]></programlisting>

  <para>
    Now we train the tagger using <literal>train_tokens</literal>
    and evaluate it using <literal>unseen_tokens</literal>, as
    follows:
  </para>

<programlisting><![CDATA[
>>> for tok in train_tokens: mytagger.train(tok)
>>> acc = tagger_accuracy(mytagger, unseen_tokens)
>>> print 'Accuracy = %4.1f%%' % (100 * acc)
Accuracy = 64.6%
]]></programlisting>

  <para>
    The accuracy scores produced by this evaluation method are lower,
    but they give a more realistic picture of the performance of
    the tagger.  
    Note that the performance of any statistical tagger is highly
    dependent on the quality of its training set. In particular, if
    the training set is too small, it will not be able to reliably
    estimate the most likely tag for each word. Performance will also
    suffer if the training set is significantly different from the
    texts we wish to tag.

  </para>

  <para>
    In the process of developing a tagger, we can use the accuracy
    score as an objective measure of the improvements made to the
    system.  Initially, the accuracy score will go up quickly as we
    fix obvious shortcomings of the tagger.  After a while, however,
    it becomes more difficult and improvements are small.
  </para>

  <para>
    While the accuracy score is certainly useful, it does not tell us
    how to improve the tagger.  For this we need to undertake error
    analysis.  For instance, we could construct a <glossterm>confusion
    matrix</glossterm>, with a row and a column for every possible
    tag, and entries that record how often a word with tag
    <literal>T<subscript>i</subscript></literal> is incorrectly tagged
    as <literal>T<subscript>j</subscript></literal>.  Another approach
    is to analyse the context of errors.
  </para>

<programlisting><![CDATA[
>>> errors = {}                                # to store the error contexts
>>> for gold_doc in unseen_tokens:
...     test_doc = gold_doc.exclude('TAG')     # remove tags
...     mytagger.tag(test_doc)                 # tag the unseen text
...     for i in range(len(test_doc['WORDS'])):     # iterate over each token
...         if test_doc['WORDS'][i] != gold_doc['WORDS'][i]:    # an error
...             test_context = [tok['TAG'] for tok in test_doc['WORDS'][i-1:i+1]]
...             gold_context = [tok['TAG'] for tok in gold_doc['WORDS'][i-1:i+1]]
...             if None not in test_context:   # did we tag all words in the context?
...                 pair = (tuple(test_context), tuple(gold_context))   # save context
...                 errors[pair] = errors.get(pair, 0) + 1
]]></programlisting>

  <para>
    The above program catalogs all errors, along with the tag on the left and their
    frequency of occurrence.  The <literal>errors</literal> dictionary has keys
    of the form <literal>((t1,t2),(g1,g2))</literal>, where
    <literal>(t1,t2)</literal> are the test tags, and <literal>(g1,g2)</literal>
    are the gold-standard tags.  The values in the <literal>errors</literal> dictionary
    are simple counts of how often the error occurred.  With some further processing,
    we construct the list <literal>counted_errors</literal> containing tuples
    consisting of counts and errors, and then do a reverse sort to get the most
    significant errors first:
  </para>
  
<programlisting><![CDATA[
>>> counted_errors = [(errors[k], k) for k in errors.keys()]
>>> counted_errors.sort()
>>> counted_errors.reverse()
>>> for err in counted_errors[:5]:
...     print err
(11, (('at', 'vb'), ('at', 'nn')))
(9, (('nn', 'to'), ('nn', 'in')))
(8, (('cc', 'vbn'), ('cc', 'vbd')))
(7, ((',', 'vbn'), (',', 'vbd')))
(7, ((',', 'np'), (',-hl', 'np-hl')))
]]></programlisting>

  <para>
    The first line of output records the fact that there were 11 cases
    where the unigram tagger mistakenly tagged a noun as a verb,
    following an article.  In fact we already encountered this mistake
    in <xref linkend="table.evaluation"/> for the word
    <literal>increase</literal>.  The unigram tagger tagged
    <literal>increase</literal> as a verb instead of a noun since it
    occurred more often in the training data as a verb.  However, when
    <literal>increase</literal> appears after an article, it is
    invariably a noun.  Evidently, the performance of the tagger would
    improve if it was modified to consider not just the word being
    tagged, but also the tag of the word on the left.  Such taggers are known
    as bigram taggers, and we consider them next.
  </para>

 

</section>

<section id="taggers.ngram">
  <title> Higher Order Taggers </title>

  <para>
    Earlier we encountered the <literal>UnigramTagger</literal>,
    which assigns a tag to a word based on the identity of that word.
    In this section we will look at taggers that exploit a larger amount
    of context when assigning a tag.
  </para>

  <section id="taggers.ngram.bigram">
    <title> Bigram Taggers </title>

  <para>
    As their name suggests, <glossterm>bigram taggers</glossterm> use
    two pieces of information for each tagging decision.  Usually this
    information is the text of the current word together with the tag of
    the previous word. These two pieces of information constitute the
    <glossterm>context</glossterm> for the token to be tagged. Given the
    context, the tagger assigns the most likely tag.  We can visualise
    this process with the help of <xref linkend="table.bigram"/>, a tiny
    fragment of the internal data structure built by a bigram tagger.
    <!-- The selected tags are italicised.-->

  </para>

  <table id="table.bigram">
    <title> Fragment of Bigram Table </title>
    <tgroup cols="8">
      <tbody>
        <row><entry/><entry>ask</entry> <entry>Congress</entry> <entry>to</entry> <entry>increase</entry> <entry>grants</entry> <entry>to</entry> <entry>states</entry></row>
        <row><entry>at</entry><entry/> <entry/> <entry/> <entry>nn</entry> <entry/> <entry/> <entry/></row>
        <row><entry>tl</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry/> <entry>to</entry> <entry/></row>
        <row><entry>bd</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry>nns</entry> <entry>to</entry> <entry/></row>
        <row><entry>md</entry><entry><emphasis>vb</emphasis></entry> <entry/> <entry/> <entry>vb</entry> <entry/> <entry/> <entry/></row>
        <row><entry>vb</entry><entry/> <entry><emphasis>np</emphasis></entry> <entry>to</entry> <entry/> <entry><emphasis>nns</emphasis></entry> <entry>to</entry> <entry>nns</entry></row>
        <row><entry>np</entry><entry/> <entry/> <entry><emphasis>to</emphasis></entry> <entry/> <entry/> <entry>to</entry> <entry/></row>
        <row><entry>to</entry><entry>vb</entry> <entry/> <entry/> <entry><emphasis>vb</emphasis></entry> <entry/> <entry/> <entry/></row>
        <row><entry>nn</entry><entry/> <entry>np</entry> <entry>to</entry> <entry>nn</entry> <entry>nns</entry> <entry>to</entry> <entry/></row>
        <row><entry>nns</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry/> <entry><emphasis>to</emphasis></entry> <entry/></row>
        <row><entry>in</entry><entry/> <entry>np</entry> <entry>in</entry> <entry/> <entry/> <entry>in</entry> <entry><emphasis>nns</emphasis></entry></row>
        <row><entry>jj</entry><entry/> <entry/> <entry>to</entry> <entry/> <entry>nns</entry> <entry>to</entry> <entry>nns</entry></row>
      </tbody>
    </tgroup>
  </table>

<!-- SOURCE CODE FOR BIGRAM TABLE
from nltk.tagger import *
from nltk.corpus import brown
train_tokens = []
for item in brown.items()[:10]:
    train_tokens.append(brown.read(item))
mytagger = NthOrderTagger(1)
for tok in train_tokens: mytagger.train(tok)
words = '''ask Congress to increase grants to states'''.split()
tags = '''at nn-tl vbd md vb np to nn nns in jj'''.split()

print "     ",
for word in words:
    print "<entry>%s</entry>" % word,
print
for tag in tags:
    print "%5s" % tag,
    for word in words:
        guess = mytagger._freqdist[((tag,), word)].max()
        if not guess: guess=""
        print "<entry>%s</entry>" % guess,
    print
-->
    
  <para>The best way to understand the table is to work through an example.
    Suppose we have already processed the sentence
    <literal>The President will ask Congress to increase grants
    to states for vocational rehabilitation .</literal>
    as far as <literal>will/md</literal>.  We can use the table to simply read off the tags that
    should be assigned to the remainder of the sentence.  When preceded by
    <literal>md</literal>, the tagger guesses that the word
    <literal>ask</literal> has the tag <literal>vb</literal> (italicised
    in the table).
    Moving to the next word, we know it is preceded by
    <literal>vb</literal>, and looking across this row we see that
    <literal>Congress</literal> is assigned the tag
    <literal>np</literal>.  The process continues through the rest of
    the sentence.  When we encounter the word
    <literal>increase</literal>, we correctly assign it the tag
    <literal>vb</literal> (unlike the unigram tagger which assigned
    it <literal>nn</literal>).  However, the bigram tagger mistakenly
    assigns the infinitival tag to the word <literal>to</literal>
   immediately preceding <literal>states</literal>, and
    not the preposition tag.  This suggests that we may need to consider even more
    context in order to get the correct tag.
  </para>

<!--
  <note><para>
    To create a bigram tagger in NLTK, use
    <literal>NthOrderTagger(1)</literal>.  In other words,
    a bigram tagger is an nth-order tagger that looks at
    the tag of one word of prior context.
    There is not much point in experimenting with
    these higher order taggers until we have considered how
    taggers can be combined.
  </para></note>
-->    
</section>

<section id="taggers.ngram.nthorder">
  <title> Nth-Order Taggers </title>

  <para>
    As we have just seen, it may be desirable to look at more than just
    the preceding word's tag when making a tagging decision.  An
    <glossterm>nth-order tagger</glossterm> is a generalisation of a
    bigram tagger whose context is the current token's text together
    with the part-of-speech tags of the <replaceable>n</replaceable>
    preceding tokens, as shown in <xref linkend="context"/>. It then
    picks the tag which is most likely for that context. In <xref
    linkend="context"/>, the tag to be chosen,
    <replaceable>t<subscript>k</subscript></replaceable>, is circled,
    and the context is shaded in grey. In this example of an nth order
    tagger, we have <replaceable>n</replaceable>=2; that is, we inspect
    the tags of the two words preceding the current word.
  </para>

<figure id="context"><title>Tagger Context</title>
<informaltable frame="none">
<tgroup cols="1"><tbody><row><entry>
<graphic  fileref="images/context" scale="15"/>
</entry></row></tbody></tgroup></informaltable>
</figure>
 
  <important><para> A 0th order tagger is another term for a
  unigram tagger: i.e., the context used to tag a token is just the text
  of the token itself. First order taggers are also called
  <glossterm>bigram taggers</glossterm>. and second order taggers are
  called <glossterm>trigram taggers</glossterm>. </para></important>

  <para>
    <literal>NthOrderTagger</literal> uses a tagged training corpus to
    determine which part-of-speech tag is most likely for each
    context:
  </para>

<programlisting><![CDATA[
>>> tagger = NthOrderTagger(3, SUBTOKENS='WORDS')       # 3rd order tagger
>>> for item in brown.items()[:10]:
...     tok = brown.read(item)
...     tagger.train(tok)
]]></programlisting>

  <para>
    Once an <literal>NthOrderTagger</literal> has been trained, it can
    be used to tag untagged corpora:
  </para>

<programlisting><![CDATA[
>>> WhitespaceTokenizer().tokenize(text_token)
>>> tagger.tag(text_token)
>>> print text_token
<[<John/NN>, <saw/VB>, <the/AT>, <book/NN>, <on/IN>, <the/AT>, ...]>
]]></programlisting>

  <para>
    As with the other taggers considered earlier,
    <literal>NthOrderTagger</literal> will assign the default tag
    <literal>None</literal> to any token whose context was not
    encountered in the training data. </para> <para> Note that as
    <replaceable>n</replaceable> gets larger, the specificity of the
    contexts increases; and with it, the chance that the data we wish to
    tag will contain contexts that were not present in the training
   data. This is sometimes referred to as a <glossterm>sparse
   data</glossterm> problem. Thus, there is a trade-off between the
   accuracy and the
    coverage of our results. This is a common type of trade-off in
    natural language processing. It is closely related to the
    <glossterm>precision/recall trade-off</glossterm> that we'll
    encounter later when we discuss information retrieval.
  </para>
</section> <!-- NthOrderTagger -->
</section> <!-- Ngram -->

<section id="tagger.backoff">
  <title> Combining Taggers </title>

  <para>
    One way to address the trade-off between accuracy and coverage is
    to use the more accurate algorithms when we can, but to fall back
    on algorithms with wider coverage when necessary. For example, we
    could combine the results of a first order tagger, a 0th order
    tagger, and an <literal>NN_CD_Tagger</literal>, as follows:
  </para>

  <orderedlist>
    <listitem>
      <para> Try tagging the token with the first order tagger. </para>
    </listitem>
    <listitem>
      <para> If the first order tagger is unable to find a tag for the token, try
        finding a tag with the 0th order tagger. </para>
    </listitem>
    <listitem>
      <para> If the 0th order tagger is also unable to find a tag, use the
        <literal>NN_CD_Tagger</literal> to find a tag. </para>
    </listitem>
  </orderedlist>

  <para>
    NLTK defines the <literal>BackoffTagger</literal> class for
    combining taggers in this way. A <literal>BackoffTagger</literal>
    is constructed from a list of one or more
    <glossterm>subtaggers</glossterm>. For each token in the input,
    the <literal>BackoffTagger</literal> uses the result of the first
    tagger in the list that successfully found a tag. Taggers indicate
    that they are unable to tag a token by assigning it the special
    tag <literal>None</literal>. We can use a
    <literal>BackoffTagger</literal> to implement the strategy
    proposed above:
  </para>

<programlisting><![CDATA[
# Construct the taggers
>>> tagger1 = NthOrderTagger(1, SUBTOKENS='WORDS')       # first order tagger
>>> tagger2 = UnigramTagger(SUBTOKENS='WORDS')           # zeroth order tagger
>>> tagger3 = RegexpTagger([(r'^[0-9]+(.[0-9]+)?$', 'cd'), (r'.*', 'nn')],
...                SUBTOKENS='WORDS')
# Train the taggers
>>> for tok in train_tokens:
...     tagger1.train(tok)
...     tagger2.train(tok)
# Combine the taggers
>>> tagger = BackoffTagger([tagger1, tagger2, tagger3], SUBTOKENS='WORDS')
]]></programlisting>

  <important><para> The order in which the taggers are given to
  <literal>BackoffTagger</literal> is important: the taggers should be
  listed in the order that they should be tried. This typically means
  that more specific taggers should be listed before less specific
  taggers.  </para></important>

  <para>
    Having defined a combined tagger,
    we can use it to tag new corpora:
  </para>

<programlisting><![CDATA[
>>> tagger.tag(text_token)
>>> print text_token
<[<John/NN>, <saw/VB>, <the/AT>, <book/NN>, <on/IN>, <the/AT>, ...]>
]]></programlisting>

  <para> Evaluation: scores for different taggers </para>

  <para> Train and test the taggers: </para>

<programlisting><![CDATA[
>>> train_tokens = []
>>> for item in brown.items()[:20]:
...     train_tokens.append(brown.read(item))
>>> unseen_tokens = []
>>> for item in brown.items()[20:30]:
...     unseen_tokens.append(brown.read(item))

>>> subtagger1 = UnigramTagger(SUBTOKENS='WORDS')
>>> subtagger2 = NthOrderTagger(1, SUBTOKENS='WORDS')     # bigram tagger
>>> subtagger3 = NthOrderTagger(2, SUBTOKENS='WORDS')     # trigram tagger

>>> for tok in train_tokens:
...     subtagger1.train(tok)
...     subtagger2.train(tok)
...     subtagger3.train(tok)

>>> tagger1 = BackoffTagger([subtagger1], SUBTOKENS='WORDS')
>>> tagger2 = BackoffTagger([subtagger2, subtagger1], SUBTOKENS='WORDS')
>>> tagger3 = BackoffTagger([subtagger3, subtagger2, subtagger1], SUBTOKENS='WORDS')

>>> accuracy1 = tagger_accuracy(tagger1, unseen_tokens)
>>> accuracy2 = tagger_accuracy(tagger2, unseen_tokens)
>>> accuracy3 = tagger_accuracy(tagger3, unseen_tokens)

>>> print 'Unigram Accuracy = %4.1f%%' % (100 * accuracy1)
>>> print 'Bigram Accuracy  = %4.1f%%' % (100 * accuracy2)
>>> print 'Trigram Accuracy = %4.1f%%' % (100 * accuracy3)
Unigram Accuracy = 74.7%
Bigram Accuracy  = 75.5%
Trigram Accuracy = 75.5%
]]></programlisting>

<!--
  <para> [Combining taggers using voting?] </para>
-->

</section> <!-- Combining Taggers -->

    <!-- Taggers -->
    <!-- old material on implementation that appeared here to be moved to
an implementation tutorial - SB -->

<section id="brill">
  <title>The Brill Tagger</title>

  <para>
    A potential issue with nth-order tagger is their size.  If tagging
    is to be employed in a variety of language technologies deployed
    on mobile computing devices, it is important to find ways to
    reduce the size of models without overly compromising performance.
    An nth-order tagger with backoff may store trigram and bigram
    tables, large sparse arrays which may have hundreds of millions of
    entries.  As we saw in <xref linkend="context"/>, nth-order models
    only consider the tags of words in the context.  A consequence of
    the size of the models is that it is simply impractical for
    nth-order models to be conditioned on the identities of words in
    the context.  In this section we will examine Brill tagging, a
    statistical tagging method which performs very well using models
    that are only a tiny fraction of the size of nth-order taggers.
  </para>
    
  <para>
    Brill tagging is a kind of <glossterm>transformation-based learning</glossterm>.
    The general idea is very simple: guess the tag of each word, then
    go back and fix the mistakes.  In this way, a Brill tagger
    successively transforms a bad tagging of a text into a good one.
    As with nth-order tagging this is a <glossterm>supervised learning</glossterm>
    method, since we need annotated training data.  However, unlike
    nth-order tagging, it does not count observations but compiles a
    list of transformational correction rules.
  </para>

  <para>
    The process of Brill tagging is usually
    explained by analogy with painting.  Suppose we were painting a
    tree, with all its details of boughs, branches, twigs and leaves,
    against a uniform sky-blue background.  Instead of painting the
    tree first then trying to paint blue in the gaps, it is simpler to
    paint the whole canvas blue, then "correct" the tree section by
    overpainting the blue background.  In the same fashion we might
    paint the trunk a uniform brown before going back to overpaint
    further details with a fine brush.  Brill tagging uses the same
    idea: get the bulk of the painting right with broad brush strokes,
    then fix up the details.  As time goes on, successively finer
    brushes are used, and the scale of the changes becomes arbitrarily
    small.  The decision of when to stop is somewhat arbitrary.
    <xref linkend="table.brill"/> illustrates this process, first
    tagging with the unigram tagger, then fixing the errors.
  </para>

  <table id="table.brill">
    <title>Steps in Brill Tagging</title>
    <tgroup cols="5">
      <colspec colwidth='3cm'/>
      <colspec colwidth='2cm'/>
      <colspec colwidth='2cm'/>
      <tbody>
        <row><entry>Sentence:</entry><entry>Gold Standard:</entry>
             <entry>Unigram Tagger:</entry>
             <entry>Replace <literal>nn</literal> with
               <literal>vb</literal> when the previous word is
               <literal>to</literal></entry>
             <entry>Replace <literal>to</literal> with
               <literal>in</literal> when the next tag is
               <literal>nns</literal></entry>
        </row>
        <row><entry>The</entry><entry>at</entry><entry>at</entry><entry/><entry/></row>
        <row><entry>President</entry><entry>nn-tl</entry><entry>nn-tl</entry><entry/><entry/></row>
        <row><entry>said</entry><entry>vbd</entry><entry>vbd</entry><entry/><entry/></row>
        <row><entry>he</entry><entry>pps</entry><entry>pps</entry><entry/><entry/></row>
        <row><entry>will</entry><entry>md</entry><entry>md</entry><entry/><entry/></row>
        <row><entry>ask</entry><entry>vb</entry><entry>vb</entry><entry/><entry/></row>
        <row><entry>Congress</entry><entry>np</entry><entry>np</entry><entry/><entry/></row>
        <row><entry>to</entry><entry>to</entry><entry>to</entry><entry/><entry/></row>
        <row><entry>increase</entry><entry>vb</entry><entry><emphasis>nn</emphasis></entry><entry><emphasis>vb</emphasis></entry><entry/></row>
        <row><entry>grants</entry><entry>nns</entry><entry>nns</entry><entry/><entry/></row>
        <row><entry>to</entry><entry>in</entry><entry><emphasis>to</emphasis></entry><entry><emphasis>to</emphasis></entry><entry><emphasis>in</emphasis></entry></row>
        <row><entry>states</entry><entry>nns</entry><entry>nns</entry><entry/><entry/></row>
        <row><entry>for</entry><entry>in</entry><entry>in</entry><entry/><entry/></row>
        <row><entry>vocational</entry><entry>jj</entry><entry>jj</entry><entry/><entry/></row>
        <row><entry>rehabilitation</entry><entry>nn</entry><entry>nn</entry><entry/><entry/></row>
        <row><entry>.</entry><entry>.</entry><entry>.</entry><entry/><entry/></row>
      </tbody>
    </tgroup>
  </table>

  <para>
    In <xref linkend="table.brill"/> we saw two rules.  All such rules
    are generated from a template of the following form:
    form "replace <literal>T<subscript>1</subscript></literal> with
      <literal>T<subscript>2</subscript></literal> in the context
      <literal>C</literal>".
    Typical contexts are the identity or the tag of the preceding or
    following word, or the appearance of a tag within 2-3 words of
    of the current word.  During its training phase, the tagger
    guesses values for <literal>T<subscript>1</subscript></literal>,
    <literal>T<subscript>2</subscript></literal> and
    <literal>C</literal>, to create thousands of candidate rules.
    Each rule is then assigned a score based on its net benefit:
    the number of incorrect tags that it corrects, less the
    number of correct tags it incorrectly modifies.  This process
    is best illustrated by a listing of the output from the
    NLTK Brill tagger (here run on tagged Wall Street Journal text
    from the Penn Treebank).<footnote><para>We are grateful to Christopher
	Maloof for developing a Brill tagger for NLTK.  It currently
        resides in the <literal>nltk_contrib</literal> package, but it
        is being migrated into NLTK's tagger package.
    </para></footnote>
  </para>

<programlisting><![CDATA[
Loading tagged data...
Training unigram tagger: [accuracy: 0.820940]
Training Brill tagger on 37168 tokens...
 
Iteration 1: 1482 errors; ranking 23989 rules;
  Found: "Replace POS with VBZ if the preceding word is tagged PRP"
  Apply: [changed 39 tags: 39 correct; 0 incorrect]
 
Iteration 2: 1443 errors; ranking 23662 rules;
  Found: "Replace VBP with VB if one of the 3 preceding words is tagged MD"
  Apply: [changed 36 tags: 36 correct; 0 incorrect]
 
Iteration 3: 1407 errors; ranking 23308 rules;
  Found: "Replace VBP with VB if the preceding word is tagged TO"
  Apply: [changed 24 tags: 23 correct; 1 incorrect]
 
Iteration 4: 1384 errors; ranking 23057 rules;
  Found: "Replace NN with VB if the preceding word is to"
  Apply: [changed 67 tags: 22 correct; 45 incorrect]
...
Iteration 20: 1138 errors; ranking 20717 rules;
  Found: "Replace RBR with JJR if one of the 2 following words is tagged NNS"
  Apply: [changed 14 tags: 10 correct; 4 incorrect]
 
Iteration 21: 1128 errors; ranking 20569 rules;
  Found: "Replace VBD with VBN if the preceding word is tagged VBD"
[insufficient improvement; stopping]
 
Brill accuracy: 0.835145

]]></programlisting>

<!--
  <para>
    [Doing it in NLTK - Maloof's Brill tagger - code fragments;
    List of transformations learned; what linguistic significance
    do these have?]
  </para>
-->

</section>


<section id="conclusion">
  <title>Conclusion</title>

  <para>
    This chapter has introduced the language processing task known as
    tagging, with an emphasis on part-of-speech tagging.  English word
    classes and their corresponding tags were introduced.  We showed
    how tagged tokens and tagged corpora can be represented, then
    discussed a variety of taggers: default tagger, regular expression
    tagger, unigram tagger, nth-order taggers, and the Brill tagger.
    We also described some objective evaluation methods.
    In the process, the reader has been introduced to two important
    paradigms in language processing, namely <glossterm>language modelling</glossterm>
    and <glossterm>transformation-based learning</glossterm>.
    The former is extremely general, and we will encounter it again
    later.  The latter had to be specially tailored to the tagging
    task, but resulted in smaller, linguistically-interpretable
    models.
  </para>

<!--
  <para>
    Discussion of how statistical methods have been used to
    reach a linguistically interpretable, symbolic result.
    Contrast between n-gram and Brill tagger about whether
    we can learn anything from inspecting the model itself
    (n-gram data vs transformational rules).  Comparing accuracy
    of these two methods: 2D graph showing how accuracy changes
    for the two methods as training size increases.
  </para>
-->

  <para>
    There are several other important approaches to tagging involving
    <glossterm>Hidden Markov Models</glossterm>
    (see <literal>nltk.hmm</literal>)
    and <glossterm>Finite State Transducers</glossterm>, though a
    discussion of these approaches falls outside the scope of this
    chapter.  Later we will see a generalization of tagging called
    <glossterm>chunking</glossterm> in which
    a contiguous sequence of words is assigned a single tag.
  </para>

</section>


<section id="reading">
  <title>Further Reading</title>

<!--
  <para>[This section to be expanded to a half-page of discussion and
    pointers to the literature and online resources.]</para>
-->

  <para>Brill tagging: Manning 361ff; Jurafsky 307ff</para>

  <para>HMM tagging: Manning 345ff</para>

</section>


<section id="exercises">
  <title>Exercises</title>

  <orderedlist>

    <listitem>
      <formalpara>
        <title> Explorations with part-of-speech tagged corpora </title>
        <para> Tokenize the Brown Corpus and build one or more suitable data structures
          so that you can answer the following questions.</para>
      </formalpara>
      <orderedlist>
        <listitem><para>
          What is the most frequent tag? (This is the tag we would want to
          assign with a <literal>DefaultTagger)</literal></para>
        </listitem>
        <listitem><para>
          Which word has the greatest number of distinct tags?</para>
        </listitem>
        <listitem><para>
          What is the ratio of masculine to feminine pronouns?</para>
        </listitem>
        <listitem><para>
          How many words are ambiguous, in the sense that they appear with at least two tags?</para>
        </listitem>
        <listitem><para>
          What percentage of word <emphasis>occurrences</emphasis> in the Brown Corpus involve
          these ambiguous words?</para>
        </listitem>
        <listitem><para>
          Which nouns are more common in their plural form than their singular form?
          (Only consider regular plurals, formed with the <literal>-s</literal> suffix.)
        </para></listitem>
        <listitem><para>
          Produce an alphabetically sorted list of the distinct words tagged as <literal>md</literal>.
        </para></listitem>
        <listitem><para>
          Identify words which can be plural nouns or third person singular verbs
          (e.g. <literal>deals</literal>).
        </para></listitem>
        <listitem><para>
          Identify three-word prepositional phrases of the form
          preposition + determiner + noun.
        </para></listitem>
        <listitem><para>
          There are 264 distinct words having exactly three possible tags.
          Print a table with the integers 1..10 in one column, and the
          number of distinct words in the corpus having 1..10 distinct tags.
        </para></listitem>
        <listitem><para>
          The word <literal>still</literal> has seven possible tags.
          Print out seven sentences containing this word, each one having
          a different tag.
        </para></listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Regular Expression Tagging </title>
        <para>
          We defined the <literal>NN_CD_Tagger</literal>, which can be used
          as a fall-back tagger for unknown words.  This tagger only checks for
          cardinal numbers.  By testing for particular prefix or suffix strings,
          it should be possible to guess other tags.  For example, 
          we could tag any word that ends with <literal>-s</literal>
          as a plural noun.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para>
            Define a <literal>RegexpTagger</literal>
            which tests for at least five other patterns in the spelling of words.
            (Use inline documentation to explain the rules.)
          </para>
        </listitem>
        <listitem>
          <para>
            Evaluate the tagger using <literal>tagger_accuracy()</literal>, and
            discuss your findings.
          </para>
        </listitem>
      </orderedlist>
    </listitem>


    <listitem>
      <formalpara>
        <title> Unigram Tagging </title>
        <para>
          Train a unigram tagger and run it on some new text.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para>
            Evaluate the tagger as before and discuss your findings.
          </para>
        </listitem>
        <listitem>
          <para>
            Observe that some words are not assigned a tag.  Why not?
          </para>
        </listitem>
      </orderedlist>
    </listitem>


    <listitem>
      <formalpara>
        <title> Bigram Tagging </title>
        <para>
          Train a bigram tagger and run it on some of the training
          data.  Next, run it on some new data.  What happens to the
          performance of the tagger?  Why?
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Combining taggers with BackoffTagger</title> <para>
        There is typically a trade-off between the accuracy and
        coverage for taggers: taggers that use more specific contexts
        usually produce more accurate results, when they have seen
        those contexts in the training data; but because the training
        data is limited, they are less likely to encounter each
        context. The <literal>BackoffTagger</literal> addresses this
        problem by trying taggers with more specific contexts first;
        and falling back to the more general taggers when
        necessary. In this exercise, we examine the effects of using
        <literal>BackoffTagger</literal>.  Create a
        <literal>DefaultTagger</literal> or a
        <literal>RegexpTagger</literal>, and a
        <literal>UnigramTagger</literal>, and a
        <literal>NthOrderTagger</literal>. Train the
        <literal>UnigramTagger</literal> and the
        <literal>NthOrderTagger</literal> using part of the Brown corpus.
        </para>
      </formalpara>

      <orderedlist>
        <listitem>
          <para> Evaluate each tagger on unseen data from the Brown corpus.
            Record the <glossterm>accuracy</glossterm> of the tagger
            (the percentage of tokens that are correctly tagged). Be sure to use a
            different section of the corpus for testing than you used for training. </para>
        </listitem>
        <listitem>
          <para> Use <literal>BackoffTagger</literal> to create three different
            combinations of the basic taggers. Test the accuracy of each combined
            tagger. Which combinations give the most improvement? </para>
        </listitem>
        <listitem>
          <para> Try repeating steps 1-3 with a different sized training corpus. How
            does it affect your results? </para>
        </listitem>
      </orderedlist>
    </listitem>

    <listitem>
      <formalpara>
        <title> Tagger context </title>
        <para><literal>NthOrderTagger</literal> chooses a tag for a
        token based on its type and the tags of the
        <replaceable>n</replaceable> preceding tokens. This is a
        common context to use for tagging, but certainly not the only
        possible context.  Construct a new tagger, subclassed from
        <literal>SequentialTagger</literal>, that uses a different
        context. If your tagger's context contains multiple elements,
        then you should combine them in a
        <literal>tuple</literal>. Some possibilities for elements to
        include are:
        </para>
      </formalpara>

      <itemizedlist>
        <listitem>
          <para> The text of the current token, or of a previous token. </para>
        </listitem>
        <listitem>
          <para> The length of the current token's text, or of a previous token's
            text. </para>
        </listitem>
        <listitem>
          <para> The first letter of the current token's text, or of a previous
            token's text. </para>
        </listitem>
        <listitem>
          <para> The tag of a previous token. </para>
        </listitem>
      </itemizedlist>

      <para> Try to choose context elements that you believe will help the tagger decide
        which tag is appropriate. Keep in mind the trade-off between more specific
        taggers with accurate results; and more general taggers with broader coverage.
        Combine your tagger with other taggers using <literal>BackoffTagger</literal>.
      </para>

      <orderedlist>
        <listitem>
          <para> How does the combined tagger's accuracy compare to the basic tagger? </para>
        </listitem>
        <listitem>
          <para> How does the combined tagger's accuracy compare to the combined taggers you
            created in the previous exercise? </para>
        </listitem>
      </orderedlist>
 
    </listitem>

    <listitem>
      <formalpara>
        <title> Reverse sequential taggers </title>
        <para> Since sequential taggers tag tokens in order, one at a time, they can only
          use the predicted tags to the <emphasis>left</emphasis> of the current token to
          decide what tag to assign to a token. But in some cases, the
          <emphasis>right</emphasis> context can provide more information about what tag
          should be used. A <glossterm>reverse sequential tagger</glossterm> is a tagger
          that: </para>
      </formalpara>

      <itemizedlist>
        <listitem>
          <para> Assigns tags to one token at a time, starting with the last token of
            the text, and proceeding in right-to-left order. </para>
        </listitem>
        <listitem>
          <para> Decides which tag to assign a token on the basis of that token, the
            tokens that follow it, and the predicted tags for the tokens that follow
            it. </para>
        </listitem>
      </itemizedlist>

      <para> There is no need to create new classes to perform reverse
      sequential tagging.  By reversing texts at appropriate times, we
      can use sequential tagging classes to perform reverse sequential
      tagging. In particular, we should reverse the training text
      before we train the tagger; and reverse the text that we wish to
      tag both before and after we use the sequential tagger. Use this
      technique to create a first order reverse sequential
      tagger. </para>

      <itemizedlist>
        <listitem><para> Measure its accuracy on a tagged section of
        the Brown corpus. Be sure to use a different section of the
        corpus for testing than you used for training.</para>
        </listitem>

        <listitem><para>How does its accuracy compare to a first order
        sequential tagger, using the same training data and test data? 
        </para>
      </listitem>
      </itemizedlist>
    </listitem>
      
    <listitem>
      <formalpara>
        <title> Processing individual sentences </title>
        <para> Write a modified <literal>NthOrderTagger</literal> that ignores tags that are
          in a previous sentence. E.g., for a 3rd order tagger, if the previous 3 words
          were "dog/NN ./. A/DT", then just use "DT" and the current token as context. </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Alternatives to backoff </title> <para> Create a new
        kind of tagger that combines several subtaggers using a new
        mechanism other than backoff (e.g. voting).  For robustness in
        the face of unknown words, include a RegexpTagger, a unigram
        tagger that removes a small number of prefix or suffix
        characters until it recognises a word, or an NthOrderTagger
        that does not consider the text of the token being tagged.
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Comparing nth-order taggers and Brill taggers </title>
        <para> Investigate the relative accuracy of nth-order taggers
        with backoff and Brill taggers as the size of the training data is increased.
        Analyze and compare the errors made by each kind of tagger.
        </para>
      </formalpara>
    </listitem>

    <listitem>
      <formalpara>
        <title> Application to other languages </title>
        <para> Obtain some tagged data for another language, and train
          and evaluate a variety of taggers on it.  If the language is
          morphologically complex, or if there are any orthographic
          clues (e.g. capitalization) to word classes, consider developing a regular
          expression tagger for it (ordered after the unigram tagger,
          and before the default tagger).  How does the
          accuracy of your tagger(s) compare with the same taggers run
          on English data?  Discuss any issues you encounter in
          applying these methods to the language.
        </para>
      </formalpara>
    </listitem>
  </orderedlist>

</section> <!-- Exercises -->

<appendix>
  <title>Brown Corpus Tag Set</title>

  <para>The following tabulation and examples have been taken from
    the documentation of the AMALGAM tagger.  Note that starred
    tags are the negated version.</para>

<table id="table.browntagset">
  <title>Complete Brown Tag Set</title>
  <tgroup cols="3">
    <colspec colwidth='2cm'/>
    <colspec colwidth='6cm'/>
    <thead>
      <row>
        <entry>Tag</entry>
        <entry>Description</entry>
        <entry>Examples</entry>
      </row>
    </thead>
    <tbody>

      <row>
        <entry> * </entry>
        <entry> negator </entry>
        <entry> not n't </entry>
      </row>
      <row>
        <entry> . </entry>
        <entry> sentence terminator </entry>
        <entry> . ? ; ! : </entry>
      </row>
      <row>
        <entry> : </entry>
        <entry> colon </entry>
        <entry> : </entry>
      </row>
      <row>
        <entry> abl </entry>
        <entry> determiner/pronoun, pre-qualifier </entry>
        <entry> quite such rather </entry>
      </row>
      <row>
        <entry> abn </entry>
        <entry> determiner/pronoun, pre-quantifier </entry>
        <entry> all half many nary </entry>
      </row>
      <row>
        <entry> abx </entry>
        <entry> determiner/pronoun, double conjunction or pre-quantifier </entry>
        <entry> both </entry>
      </row>
      <row>
        <entry> ap </entry>
        <entry> determiner/pronoun, post-determiner </entry>
        <entry> many other next more last former little several enough
                 most least only very few fewer past same </entry>
      </row>
      <row>
        <entry> ap$ </entry>
        <entry> determiner/pronoun, post-determiner, genitive </entry>
        <entry> other's </entry>
      </row>

      <row>
        <entry> be </entry>
        <entry> verb "to be", infinitive or imperative </entry>
        <entry> be </entry>
      </row>
      <row>
        <entry> bed / bed* </entry>
        <entry> verb "to be", past tense, 2nd person singular or all
          persons plural </entry>
        <entry> were / weren't </entry>
      </row>
      <row>
        <entry> bedz / bedz*</entry>
        <entry> verb "to be", past tense, 1st and 3rd person singular </entry>
        <entry> was / wasn't </entry>
      </row>
      <row>
        <entry> beg </entry>
        <entry> verb "to be", present participle or gerund </entry>
        <entry> being </entry>
      </row>
      <row>
        <entry> bem / bem*</entry>
        <entry> verb "to be", present tense, 1st person singular </entry>
        <entry> am / ain't </entry>
      </row>
      <row>
        <entry> ben </entry>
        <entry> verb "to be", past participle </entry>
        <entry> been </entry>
      </row>
      <row>
        <entry> ber / ber* </entry>
        <entry> verb "to be", present tense, 2nd person singular or all
          persons plural </entry>
        <entry> are art / aren't ain't </entry>
      </row>
      <row>
        <entry> bez / bez* </entry>
        <entry> verb "to be", present tense, 3rd person singular </entry>
        <entry> is / isn't ain't </entry>
      </row>
      <row>
        <entry> cc </entry>
        <entry> conjunction, coordinating </entry>
        <entry> and or but plus & either neither nor yet 'n' and/or minus an' </entry>
      </row>
      <row>
        <entry> cd </entry>
        <entry> numeral, cardinal </entry>
        <entry> two 1 1913 three million 87-31 1,119 fifty-three 7.5 billion ... </entry>
      </row>
      <row>
        <entry> cd$ </entry>
        <entry> numeral, cardinal, genitive </entry>
        <entry> 1960's 1961's .404's </entry>
      </row>
      <row>
        <entry> cs </entry>
        <entry> conjunction, subordinating </entry>
        <entry> that as after whether before while like because if since for
                 than until so unless though providing once lest
                 till whereas whereupon supposing albeit then </entry>
      </row>
      <row>
        <entry> do / do*</entry>
        <entry> verb "to do", uninflected present tense, infinitive or imperative </entry>
        <entry> do dost / don't</entry>
      </row>
      <row>
        <entry> dod / dod* </entry>
        <entry> verb "to do", past tense </entry>
        <entry> did done / didn't </entry>
      </row>
      <row>
        <entry> doz / doz* </entry>
        <entry> verb "to do", present tense, 3rd person singular </entry>
        <entry> does / doesn't don't </entry>
      </row>
      <row>
        <entry> dt </entry>
        <entry> determiner/pronoun, singular </entry>
        <entry> this each another that 'nother </entry>
      </row>
      <row>
        <entry> dt$ </entry>
        <entry> determiner/pronoun, singular, genitive </entry>
        <entry> another's </entry>
      </row>
      <row>
        <entry> dt+bez </entry>
        <entry> determiner/pronoun + verb "to be", present tense, 3rd person singular </entry>
        <entry> that's </entry>
      </row>
      <row>
        <entry> dt+md </entry>
        <entry> determiner/pronoun + modal auxiliary </entry>
        <entry> that'll this'll </entry>
      </row>
      <row>
        <entry> dti </entry>
        <entry> determiner/pronoun, singular or plural </entry>
        <entry> any some </entry>
      </row>
      <row>
        <entry> dts </entry>
        <entry> determiner/pronoun, plural </entry>
        <entry> these those them </entry>
      </row>
      <row>
        <entry> dtx </entry>
        <entry> determiner, pronoun or double conjunction </entry>
        <entry> neither either one </entry>
      </row>
      <row>
        <entry> ex </entry>
        <entry> existential there </entry>
        <entry> there </entry>
      </row>
      <row>
        <entry> hv / hv*</entry>
        <entry> verb "to have", uninflected present tense, infinitive or imperative </entry>
        <entry> have hast / haven't ain't </entry>
      </row>
      <row>
        <entry> hv+to </entry>
        <entry> verb "to have", uninflected present tense + infinitival to </entry>
        <entry> hafta </entry>
      </row>
      <row>
        <entry> hvd / hvd*</entry>
        <entry> verb "to have", past tense </entry>
        <entry> had / hadn't </entry>
      </row>
      <row>
        <entry> hvg </entry>
        <entry> verb "to have", present participle or gerund </entry>
        <entry> having </entry>
      </row>
      <row>
        <entry> hvn </entry>
        <entry> verb "to have", past participle </entry>
        <entry> had </entry>
      </row>
      <row>
        <entry> hvz / hvz*</entry>
        <entry> verb "to have", present tense, 3rd person singular </entry>
        <entry> has hath / hasn't ain't </entry>
      </row>
      <row>
        <entry> in </entry>
        <entry> preposition </entry>
        <entry> of in for by considering to on among at through with under
                 into regarding than since despite ... </entry>
      </row>
      <row>
        <entry> jj </entry>
        <entry> adjective </entry>
        <entry> recent over-all possible hard-fought favorable hard meager fit
                 such widespread outmoded inadequate ... </entry>
      </row>
      <row>
        <entry> jj$ </entry>
        <entry> adjective, genitive </entry>
        <entry> Great's </entry>
      </row>
      <row>
        <entry> jjr </entry>
        <entry> adjective, comparative </entry>
        <entry> greater older further earlier later freer franker wider better
                 deeper firmer tougher faster higher bigger ... </entry>
      </row>
      <row>
        <entry> jjs </entry>
        <entry> adjective, semantically superlative </entry>
        <entry> top chief principal northernmost master key head main
                 tops utmost innermost foremost uppermost ... </entry>
      </row>
      <row>
        <entry> jjt </entry>
        <entry> adjective, superlative </entry>
        <entry> best largest coolest calmest latest greatest earliest simplest
                 strongest newest fiercest unhappiest worst ...</entry>
      </row>
      <row>
        <entry> md / md*</entry>
        <entry> modal auxiliary </entry>
        <entry> should may might will would must can could shall ought need 
                 wilt /
                cannot couldn't wouldn't can't won't shouldn't shan't mustn't musn't </entry>      </row>
      <row>
        <entry> nn </entry>
        <entry> noun, singular, common </entry>
        <entry> failure burden court fire appointment awarding compensation
                 Mayor interim committee fact effect ... </entry>
      </row>
      <row>
        <entry> nn$ </entry>
        <entry> noun, singular, common, genitive </entry>
        <entry> season's world's player's night's chapter's golf's football's
                 baseball's club's U.'s coach's bride's ... </entry>
      </row>
      <row>
        <entry> nn+bez </entry>
        <entry> noun, singular, common + verb "to be", present tense, 3rd person singular </entry>
        <entry> water's camera's sky's kid's Pa's heat's throat's father's
                 money's undersecretary's granite's level's ... </entry>
      </row>
      <row>
        <entry> nn+hvz </entry>
        <entry> noun, singular, common + verb "to have", present tense, 3rd person singular </entry>
        <entry> guy's Knife's boat's summer's rain's company's </entry>
      </row>
      <row>
        <entry> nns </entry>
        <entry> noun, plural, common </entry>
        <entry> irregularities presentments thanks reports voters laws
                 legislators years areas adjustments chambers ... </entry>
      </row>
      <row>
        <entry> nns$ </entry>
        <entry> noun, plural, common, genitive </entry>
        <entry> taxpayers' children's members' States' women's cutters'
                 motorists' steelmakers' hours' Nations' ... </entry>
      </row>
      <row>
        <entry> np </entry>
        <entry> noun, singular, proper </entry>
        <entry> Fulton Atlanta September-October Durwood Pye Ivan Allen Jr.
                 Jan. Alpharetta Grady ... </entry>
      </row>
      <row>
        <entry> np$ </entry>
        <entry> noun, singular, proper, genitive </entry>
        <entry> Green's Landis' Smith's Carreon's Allison's Boston's Spahn's
                 Willie's Mickey's Milwaukee's ... </entry>
      </row>
      <row>
        <entry> np+bez </entry>
        <entry> noun, singular, proper + verb "to be", present tense, 3rd person singular </entry>
        <entry> W.'s Ike's Mack's Jack's Kate's Katharine's Black's Arthur's
                 Seaton's Buckhorn's Breed's Penny's ... </entry>
      </row>
      <row>
        <entry> nps </entry>
        <entry> noun, plural, proper </entry>
        <entry> Chases Aderholds Chapelles Armisteads Lockies Carbones French
                 Marskmen Toppers Franciscans ... </entry>
      </row>
      <row>
        <entry> nps$ </entry>
        <entry> noun, plural, proper, genitive </entry>
        <entry> Republicans' Orioles' Birds' Yanks' Redbirds' Bucs' Yankees'
                 Stevenses' Geraghtys' Burkes' ...</entry>
      </row>
      <row>
        <entry> nr </entry>
        <entry> noun, singular, adverbial </entry>
        <entry> Friday home Wednesday Tuesday Monday Sunday Thursday yesterday
                 tomorrow tonight West East Saturday west left east downtown
                 north northeast southeast northwest North South right ... </entry>
      </row>
      <row>
        <entry> nr$ </entry>
        <entry> noun, singular, adverbial, genitive </entry>
        <entry> Saturday's Monday's yesterday's tonight's tomorrow's Sunday's
                 Wednesday's Friday's today's Tuesday's West's Today's South's </entry>
      </row>
      <row>
        <entry> nrs </entry>
        <entry> noun, plural, adverbial </entry>
        <entry> Sundays Mondays Saturdays Wednesdays Souths Fridays </entry>
      </row>
      <row>
        <entry> od </entry>
        <entry> numeral, ordinal </entry>
        <entry> first 13th third nineteenth 2d 61st second sixth eighth ninth
                 twenty-first eleventh 50th ... </entry>
      </row>
      <row>
        <entry> pn </entry>
        <entry> pronoun, nominal </entry>
        <entry> none something everything one anyone nothing nobody everybody
                 everyone anybody anything someone no-one nothin </entry>
      </row>
      <row>
        <entry> pn$ </entry>
        <entry> pronoun, nominal, genitive </entry>
        <entry> one's someone's anybody's nobody's everybody's anyone's everyone's </entry>
      </row>
      <row>
        <entry> pp$ </entry>
        <entry> determiner, possessive </entry>
        <entry> our its his their my your her out thy mine thine </entry>
      </row>
      <row>
        <entry> pp$$ </entry>
        <entry> pronoun, possessive </entry>
        <entry> ours mine his hers theirs yours </entry>
      </row>
      <row>
        <entry> ppl </entry>
        <entry> pronoun, singular, reflexive </entry>
        <entry> itself himself myself yourself herself oneself ownself </entry>
      </row>
      <row>
        <entry> ppls </entry>
        <entry> pronoun, plural, reflexive </entry>
        <entry> themselves ourselves yourselves </entry>
      </row>
      <row>
        <entry> ppo </entry>
        <entry> pronoun, personal, accusative </entry>
        <entry> them it him me us you 'em her thee we'uns </entry>
      </row>
      <row>
        <entry> pps </entry>
        <entry> pronoun, personal, nominative, 3rd person singular </entry>
        <entry> it he she thee </entry>
      </row>
      <row>
        <entry> pps+bez </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + verb
          "to be", present tense, 3rd person singular </entry>
        <entry> it's he's she's </entry>
      </row>
      <row>
        <entry> pps+hvd </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + verb "to have", past tense </entry>
        <entry> she'd he'd it'd </entry>
      </row>
      <row>
        <entry> pps+hvz </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + verb
          "to have", present tense, 3rd person singular </entry>
        <entry> it's he's she's </entry>
      </row>
      <row>
        <entry> pps+md </entry>
        <entry> pronoun, personal, nominative, 3rd person singular + modal auxiliary </entry>
        <entry> he'll she'll it'll he'd it'd she'd </entry>
      </row>
      <row>
        <entry> ppss </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular </entry>
        <entry> they we I you ye thou you'uns </entry>
      </row>
      <row>
        <entry> ppss+bem </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular +
          verb "to be", present tense, 1st person singular </entry>
        <entry> I'm Ahm </entry>
      </row>
      <row>
        <entry> ppss+ber </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + 
          verb "to be", present tense, 2nd person singular or all persons plural </entry>
        <entry> we're you're they're </entry>
      </row>
      <row>
        <entry> ppss+hv </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + verb "to have", uninflected present tense </entry>
        <entry> I've we've they've you've </entry>
      </row>
      <row>
        <entry> ppss+hvd </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + verb "to have", past tense </entry>
        <entry> I'd you'd we'd they'd </entry>
      </row>
      <row>
        <entry> ppss+md </entry>
        <entry> pronoun, personal, nominative, not 3rd person singular + modal auxiliary </entry>
        <entry> you'll we'll I'll we'd I'd they'll they'd you'd </entry>
      </row>
      <row>
        <entry> ql </entry>
        <entry> qualifier, pre </entry>
        <entry> well less very most so real as highly fundamentally even how much remarkably somewhat more completely too thus ill deeply little overly halfway almost impossibly far severely such ... </entry>
      </row>
      <row>
        <entry> qlp </entry>
        <entry> qualifier, post </entry>
        <entry> indeed enough still 'nuff </entry>
      </row>
      <row>
        <entry> rb </entry>
        <entry> adverb </entry>
        <entry> only often generally also nevertheless upon together back
                 newly no likely meanwhile near then heavily there apparently 
                 yet outright fully aside consistently specifically formally 
                 ever just ...  </entry>
      </row>
      <row>
        <entry> rb+bez </entry>
        <entry> adverb + verb "to be", present tense, 3rd person singular </entry>
        <entry> here's there's </entry>
      </row>
      <row>
        <entry> rbr </entry>
        <entry> adverb, comparative </entry>
        <entry> further earlier better later higher tougher more harder longer
                 sooner less faster easier louder ... </entry>
      </row>
      <row>
        <entry> rbt </entry>
        <entry> adverb, superlative </entry>
        <entry> most best highest uppermost nearest brightest hardest fastest
                 deepest farthest loudest ... </entry>
      </row>
      <row>
        <entry> rn </entry>
        <entry> adverb, nominal </entry>
        <entry> here afar then </entry>
      </row>
      <row>
        <entry> rp </entry>
        <entry> adverb, particle </entry>
        <entry> up out off down over on in about through across after </entry>
      </row>
      <row>
        <entry> to </entry>
        <entry> infinitival to </entry>
        <entry> to t' </entry>
      </row>
      <row>
        <entry> uh </entry>
        <entry> interjection </entry>
        <entry> Hurrah bang whee hmpf ah goodbye oops oh-the-pain-of-it ha
                 crunch say oh why see well hello lo alas tarantara 
                 rum-tum-tum gosh hell ... </entry>
      </row>
      <row>
        <entry> vb </entry>
        <entry> verb, base: uninflected present, imperative or infinitive </entry>
        <entry> investigate find act follow inure achieve reduce take remedy
                 re-set distribute realize disable feel receive ... </entry>
      </row>
      <row>
        <entry> vbd </entry>
        <entry> verb, past tense </entry>
        <entry> said produced took recommended commented urged found added
                 praised charged listed ... </entry>
      </row>
      <row>
        <entry> vbg </entry>
        <entry> verb, present participle or gerund </entry>
        <entry> modernizing improving purchasing lacking enabling
                 pricing keeping getting picking ... </entry>
      </row>
      <row>
        <entry> vbn </entry>
        <entry> verb, past participle </entry>
        <entry> conducted charged won received studied revised operated
                 accepted combined experienced ... </entry>
      </row>
      <row>
        <entry> vbz </entry>
        <entry> verb, present tense, 3rd person singular </entry>
        <entry> deserves believes receives takes goes expires says opposes
                 starts permits expects thinks faces ... </entry>
      </row>
      <row>
        <entry> wdt </entry>
        <entry> WH-determiner </entry>
        <entry> which what whatever whichever whichever-the-hell </entry>
      </row>
      <row>
        <entry> wdt+bez </entry>
        <entry> WH-determiner + verb "to be", present tense, 3rd person singular </entry>
        <entry> what's </entry>
      </row>
      <row>
        <entry> wp$ </entry>
        <entry> WH-pronoun, genitive </entry>
        <entry> whose whosever </entry>
      </row>
      <row>
        <entry> wpo </entry>
        <entry> WH-pronoun, accusative </entry>
        <entry> whom that who </entry>
      </row>
      <row>
        <entry> wps </entry>
        <entry> WH-pronoun, nominative </entry>
        <entry> that who whoever whosoever what whatsoever </entry>
      </row>
      <row>
        <entry> wps+bez </entry>
        <entry> WH-pronoun, nominative + verb "to be", present, 3rd person singular </entry>
        <entry> that's who's </entry>
      </row>
      <row>
        <entry> wps+hvd </entry>
        <entry> WH-pronoun, nominative + verb "to have", past tense </entry>
        <entry> who'd </entry>
      </row>
      <row>
        <entry> wps+hvz </entry>
        <entry> WH-pronoun, nominative + verb "to have", present tense, 3rd person singular </entry>
        <entry> who's that's </entry>
      </row>
      <row>
        <entry> wps+md </entry>
        <entry> WH-pronoun, nominative + modal auxiliary </entry>
        <entry> who'll that'd who'd that'll </entry>
      </row>
      <row>
        <entry> wql </entry>
        <entry> WH-qualifier </entry>
        <entry> however how </entry>
      </row>
      <row>
        <entry> wrb </entry>
        <entry> WH-adverb </entry>
        <entry> however when where why whereby wherever how whenever
                 whereon wherein wherewith wheare wherefore whereof howsabout </entry>
      </row>
      <row>
        <entry> wrb+ber </entry>
        <entry> WH-adverb + verb "to be", present, 2nd person singular or all persons plural </entry>
        <entry> where're </entry>
      </row>
    </tbody>
  </tgroup>
</table>

</appendix>



&index;
</article>

