\documentclass{beamer}             % for slides
% \documentclass[handout]{beamer}    % for handout
\input{beamer}

\title{Accessing and Analyzing Linguistic Field Data}

\author{Steven Bird}
\institute{
  University of Melbourne, AUSTRALIA
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Introduction}
  \begin{itemize}
  \item variety of data types, e.g.
  \item \textbf{lexicon:} database of words, minimally containing part
    of speech information and glosses
  \item \textbf{paradigm:} any kind of rational
    tabulation of words or phrases to illustrate contrasts and systematic
    variation
  \item \textbf{text:} any larger unit such as a narrative or a
    conversation
  \item descriptions: field notes, grammars and analytical papers
  \item complex inter-relations (new word; lexicon update, paradigm;
    add to field notes; extend grammar, ...)
  \item tasks: sorting, tabulating, garnering statistics, searching,
  verifying transcriptions
  \item principle challenge is computational
  \end{itemize}

\end{frame}

\begin{frame}
\frametitle{Tools and Technologies}
\begin{itemize}
\item General-purpose tools
  \begin{itemize}
  \item word processors
  \item spreadsheets
  \item databases
  \end{itemize}
\item Special purpose tools
  \begin{itemize}
  \item Toolbox
  \end{itemize}
\item broader question: data management in support of best practices
\item Bird, Steven and Gary Simons (2003).  Seven Dimensions of Portability
    for Language Documentation and Description, \textit{Language} 79: 557-582.
\end{itemize}
\end{frame}

\section{General Purpose Tools}

\subsection{Word Processors}

\begin{frame}
\frametitle{General Purpose Tools: Word Processors}

\begin{itemize}
\item office software widely used in computer-based language
  documentation work
\item familiar, ready availability (though may be too expensive?)
\item Word processors for managing dictionaries:
  \begin{itemize}
  \item consistency of content and format --- time consuming
  \item how would we check the following constraint: \\
    \textit{each entry must have a part-of-speech field, drawn from a set of 20
     possibilities, displayed after the pronunciation field, and rendered
     in 11-point bold}
  \item generally a bad idea!
  \end{itemize}
\item save to a non-proprietary format (e.g. RTF, HTML, or XML)
\item write programs to do this checking automatically
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Microsoft Word}
\begin{itemize}
\item sleep \verb|[sli:p]| \textbf{vi} \textit{condition of body and mind...}
\item We can enter this in MSWord, then "Save as Web Page"
\item resulting HTML:

\begin{verbatim}
<p class=MsoNormal>
  sleep
  <span style='mso-spacerun:yes'> </span>
  [<span class=SpellE>sli:p</span>]
  <span style='mso-spacerun:yes'> </span>
  <b><span style='font-size:11.0pt'>vi</span></b>
  <span style='mso-spacerun:yes'> </span>
  <i>a condition of body and mind ...</i>
</p>
\end{verbatim}

\item Observations:
  \begin{itemize}
  \item entry: HTML paragraph using the \verb|<p>| element
  \item pos: inside a \verb|<span style='font-size:11.0pt'>| element
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Example: Validating Dictionaries stored in MSWord}
\scriptsize

\begin{verbatim}
>>> import re
>>> legal_pos = set(['n', 'v.t.', 'v.i.', 'adj', 'det'])
>>> pattern = re.compile(r"'font-size:11.0pt'>([a-z.]+)<")
>>> document = open("dict.htm").read()
>>> used_pos = set(re.findall(pattern, document))
>>> illegal_pos = used_pos.difference(legal_pos)
>>> print list(illegal_pos)
['v.intr', 'v.i', 'intrans']
\end{verbatim}
\end{frame}

\subsection{Spreadsheets}

\begin{frame}[fragile]
\frametitle{Example: Converting MSWord dictionary to Excel}
\small

\begin{verbatim}
>>> import re
>>> document = open("dict.htm").read()
>>> document = re.sub("[\r\n]", "", document)
>>> word_pattern = re.compile(r">([\w]+)")
>>> pron_pattern = re.compile(r"\[.*>([a-z:]+)<.*\]")
>>> for entry in document.split("<p"):
...     word_match = word_pattern.search(entry)
...     pron_match = pron_pattern.search(entry)
...     if word_match and pron_match:
...         lex = word_match.group(1)
...         pos = pron_match.group(1)
...         print '"%s","%s"' % (lex, pos)
"sleep","sli:p"
"walk","wo:k"
"wake","weik"
\end{verbatim}
\end{frame}

\begin{frame}
\frametitle{Spreadsheets}
\begin{itemize}
\item wordlists, paradigms
\item e.g. comparative wordlist (\url{http://www.rosettaproject.org/}):
  \begin{itemize}
  \item row for each cognate set
  \item column for each language
  \end{itemize}
\item export spreadsheets in \textit{CSV format}
\item read them using Python's \texttt{csv} module
\item e.g. find cognates having an edit-distance of at least three from each other
\end{itemize}
\end{frame}

\subsection{Databases}

\begin{frame}
\frametitle{Databases}
\begin{itemize}
  \item lexicons are stored in a full-fledged relational databases
  \item databases can implement many well-formedness constraints\\
    e.g. ensure that all parts-of-speech come from an
    \textit{enumerated type}
  \item However, the relational model is often too restrictive
  \item optional, repeatable fields (e.g.
    dictionary sense definitions and example sentences).
  \item SQL cannot express many linguistically-motivated queries\\
    \textit{Find all words that appear in example sentences for which no dictionary entry is provided}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Database Example}  
\scriptsize

\begin{verbatim}
>>> import csv
>>> lexemes = []
>>> defn_words = []
>>> for row in csv.reader(open("dict.csv")):
...     lexeme, pron, pos, defn = row
...     lexemes.append(lexeme)
...     defn_words += defn.split()
>>> undefined = list(set(defn_words).difference(set(lexemes)))
>>> undefined.sort()
>>> print undefined
['...', 'a', 'and', 'body', 'by', 'cease', 'condition', 'down',
'each', 'foot', 'lifting', 'mind', 'of', 'progress', 'setting', 'to']
\end{verbatim}
\end{frame}

\section{Processing Toolbox Data}

\begin{frame}
\frametitle{Processing Toolbox Data}

\begin{itemize}
\item single most popular tool for managing linguistic field data
\item many kinds of validation and formatting not supported by Toolbox software
\item simple file format
\item each file is a collection of \textit{entries} (aka
  \textit{records})
\item each entry is made up of one or more \textit{fields}
\item challenges:
  \begin{enumerate}
  \item different entry types, implications for which fields are present
  \item field order only sometimes significant
  \end{enumerate}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Toolbox Example}
\small

\begin{verbatim}
\lx kaa
\ps N.M
\cl isi
\ge cooking banana
\gp banana bilong kukim
\sf FLORA
\dt 12/Feb/2005
\ex Taeavi iria kaa isi kovopaueva kaparapasia.
\xp Taeavi i bin planim gaden banana bilong kukim tasol long paia.
\xe Taeavi planted banana in order to cook it.
\end{verbatim}
\end{frame}

\subsection{Accessing Toolbox Data}

\begin{frame}[fragile]
\frametitle{Accessing Toolbox Data}

\begin{itemize}
\item scan the file, convert into tree object
\item preserves order of fields, gives array and XPath-style access
\end{itemize}

\begin{verbatim}
>>> from nltk_lite.corpora import toolbox
>>> lexicon = toolbox.parse_corpus('rotokas.dic', key='lx')
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Accessing with Indexes}

\begin{verbatim}
>>> lexicon[3][0] 
<Element lx at 77bd28>
>>> lexicon[3][0].tag
'lx'
>>> lexicon[3][0].text
'kaa'
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Accessing with Indexes (cont)}

\scriptsize
\begin{verbatim}
>>> toolbox.to_sfm_string(lexicon[3])
\lx kaa
\ps N.M
\cl isi
\ge cooking banana
\gp banana bilong kukim
\sf FLORA
\dt 12/Feb/2005
\ex Taeavi iria kaa isi kovopaueva kaparapasia.
\xp Taeavi i bin planim gaden banana bilong kukim tasol long paia.
\xe Taeavi planted banana in order to cook it.
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Accessing with Paths}

\begin{itemize}
\item lexicon is a series of \texttt{record} objects
\item each contains field objects, such as \texttt{lx} and \texttt{ps}
\item address all the lexemes: \texttt{record/lx}
\end{itemize}

\small
\begin{verbatim}
>>> [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]
['kaa', 'kaa', 'kaa', 'kaakaaro', 'kaakaaviko', 'kaakaavo', 'kaakaoko',
'kaakasi', 'kaakau', 'kaakauko', 'kaakito', 'kaakuupato', ..., 'kuvuto']
\end{verbatim}
\end{frame}

\subsection{Adding and Removing Fields}

\begin{frame}[fragile]
\frametitle{Adding New Fields}
\small

\begin{itemize}
\item Example: add CV field
\item Aside: utility function to do CV template
\end{itemize}

\begin{verbatim}
>>> import re
>>> def cv(s):
...     s = s.lower()
...     s = re.sub(r'[^a-z]',     r'_', s)
...     s = re.sub(r'[aeiou]',    r'V', s)
...     s = re.sub(r'[^V_]',      r'C', s)
...     return (s)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Adding New Fields (cont)}
\small

\begin{verbatim}
>>> from nltk_lite.etree.ElementTree import SubElement
>>> for entry in lexicon:
...     for field in entry:
...	    if field.tag == 'lx':
...         cv_field = SubElement(entry, 'cv')
...         cv_field.text = cv(field.text)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Adding New Fields (cont)}
\small
\begin{verbatim}
>>> toolbox.to_sfm_string(lexicon[50])
\lx kaeviro
\cv CVVCVCV
\ps V.A
\ge lift off
\ge take off
\gp go antap
\nt used to describe action of plane
\dt 12/Feb/2005
\ex Pita kaeviroroe kepa kekesia oa vuripierevo kiuvu.
\xp Pita i go antap na lukim haus win i bagarapim.
\xe Peter went to look at the house that the wind destroyed.
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Removing Fields}
\scriptsize

\begin{itemize}
\item sanitise our data before giving it to others
\end{itemize}

\small
\begin{verbatim}
>>> retain = ('lx', 'ps', 'ge')
>>> for entry in lexicon:
...     entry[:] = [f for f in entry if f.tag in retain]
>>> toolbox.to_sfm_string(lexicon[50])
\lx kaeviro
\ps V.A
\ge lift off
\ge take off
\end{verbatim}
\end{frame}

\subsection{Formatting}

\begin{frame}[fragile]
\frametitle{Formatting Entries}

\begin{itemize}
\item request specific fields without concern for ordering
\end{itemize}
\scriptsize

\begin{verbatim}
>>> lexicon = toolbox.parse_corpus('rotokas.dic', key='lx')
>>> for entry in lexicon[70:80]:
...     lx = entry.findtext('lx')
...     ps = entry.findtext('ps')
...     ge = entry.findtext('ge')
...     print "%s (%s) '%s'" % (lx, ps, ge)
kakapikoto (N.N2) 'newborn baby'
kakapu (V.B) 'place in sling for purpose of carrying'
kakapua (N.N) 'sling for lifting'
kakara (N.N) 'arm band'
Kakarapaia (N.PN) 'village name'
kakarau (N.F) 'frog'
Kakarera (N.PN) 'name'
Kakareraia (N.???) 'name'
kakata (N.F) 'cockatoo'
kakate (N.F) 'bamboo water tube'
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating HTML Tables}

\begin{itemize}
\item Publish shoebox lexicon on the web
\end{itemize}
\scriptsize

\begin{verbatim}
>>> html = "<table>\n"
>>> for entry in lexicon[70:80]:
...     lx = entry.findtext('lx')
...     ps = entry.findtext('ps')
...     ge = entry.findtext('ge')
...     html += "  <tr><td>%s</td><td>%s</td><td>%s</td></tr>\n" % (lx, ps, ge)
>>> html += "</table>"
>>> print html
\end{verbatim}

\tiny

\begin{verbatim}
<table>
  <tr><td>kakapikoto</td><td>N.N2</td><td>newborn baby</td></tr>
  <tr><td>kakapu</td><td>V.B</td><td>place in sling for purpose of carrying</td></tr>
  <tr><td>kakapua</td><td>N.N</td><td>sling for lifting</td></tr>
  <tr><td>kakara</td><td>N.N</td><td>bracelet</td></tr>
  <tr><td>Kakarapaia</td><td>N.PN</td><td>village name</td></tr>
  <tr><td>kakarau</td><td>N.F</td><td>stingray</td></tr>
  <tr><td>Kakarera</td><td>N.PN</td><td>name</td></tr>
  <tr><td>Kakareraia</td><td>N.???</td><td>name</td></tr>
  <tr><td>kakata</td><td>N.F</td><td>cockatoo</td></tr>
  <tr><td>kakate</td><td>N.F</td><td>bamboo tube for water</td></tr>
</table>
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating XML}

\scriptsize
\begin{verbatim}
>>> import sys
>>> from nltk_lite.etree.ElementTree import ElementTree
>>> tree = ElementTree(lexicon[3])
>>> tree.write(sys.stdout)
<record>
  <lx>kaakaaro</lx>
  <ps>N.N</ps>
  <ge>mixtures</ge>
  <gp>???</gp>
  <eng>mixtures</eng>
  <eng>charm used to keep married men and women youthful and attractive</eng>
  <cmt>Check vowel length. Is it kaakaaro or kaakaro?</cmt>
  <dt>14/May/2005</dt>
  <ex>Kaakaroto ira purapaiveira aue iava opita, voeao-pa airepa oraouirara, ra va aiopaive.</ex>
  <xp>Kokonas ol i save wokim long ol kain samting bilong ol nupela marit, bai ol i ken kaikai.</xp>
  <xe>Mixtures are made from coconut, ???.</xe>
</record>
\end{verbatim}
\end{frame}

\section{Linguistic Exploration}

\subsection{Reduplication}

\begin{frame}[fragile]
\frametitle{Reduplication}
\scriptsize
\begin{itemize}
\item create a table of lexemes and their glosses

\begin{verbatim}
>>> lexgloss = {}
>>> for entry in lexicon:
...     lx = entry.findtext('lx')
...     if lx and entry.findtext('ps')[0] == 'V':
...         lexgloss[lx] = entry.findtext('ge')
\end{verbatim}

\item For each lexeme, check if the lexicon contains the reduplicated form:

\begin{verbatim}
>>> for lex in lexgloss:
...     if lex+lex in lexgloss:
...         print "%s (%s); %s (%s)" % (lex, lexgloss[lex], lex+lex, lexgloss[lex+lex])
\end{verbatim}

\tiny
\begin{verbatim}
kuvu (fill.up); kuvukuvu (stamp the ground)
kitu (save); kitukitu (scrub clothes)
kopa (ingest); kopakopa (gulp.down)
kasi (burn); kasikasi (angry)
koi (high pitched sound); koikoi (groan with pain)
kee (chip); keekee (shattered)
kauo (jump); kauokauo (jump up and down)
kea (deceived); keakea (lie)
kove (drop); kovekove (drip repeatedly)
kape (unable to meet); kapekape (grip with arms not meeting)
kapo (fasten.cover.strip); kapokapo (fasten.cover.strips)
koa (skin); koakoa (remove the skin)
kipu (paint); kipukipu (rub.on)
koe (spoon out a solid); koekoe (spoon out)
kovo (work); kovokovo (surround)
kiru (have sore near mouth); kirukiru (crisp)
kotu (bite); kotukotu (grind teeth together)
kavo (collect); kavokavo (work black magic)
kuri (scrape); kurikuri (scratch repeatedly)
karu (unhook); karukaru (open)
kare (return); karekare (return)
kari (break); karikari (shred)
kiro (write); kirokiro (write)
kae (carry); kaekae (tempt)
koru (make return); korukoru (obstruct)
ku (finished with); kuku (spoonfeed)
kosi (exit); kosikosi (exit)
\end{verbatim}
\end{itemize}
\end{frame}

\subsection{Complex Search Criteria}

\begin{frame}[fragile]
\frametitle{Complex Search Criteria}
\begin{itemize}
\item for phonological description, identify segments, alternations,
  syllable canon...
\item what syllable types occur in lexemes (MSC, conspiracies)?
\scriptsize

\begin{verbatim}
>>> from nltk_lite.tokenize import regexp
>>> from nltk_lite.probability import FreqDist
>>> fd = FreqDist()
>>> lexemes = [lexeme.text.lower() for lexeme in lexicon.findall('record/lx')]
>>> for lex in lexemes:
...     for syl in regexp(lex, pattern=r'[^aeiou][aeiou]'):
...         fd.inc(syl)
\end{verbatim}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Complex Search Criteria (cont)}

\begin{itemize}
\item Tabulate the results:

{\scriptsize
\begin{verbatim}
>>> for vowel in 'aeiou':
...     for cons in 'ptkvsr':
...          print '%s%s:%4d ' % (cons, vowel, fd.count(cons+vowel)),
...     print
pa:  84  ta:  43  ka: 414  va:  87  sa:   0  ra: 185 
pe:  32  te:   8  ke: 139  ve:  25  se:   1  re:  62 
pi:  97  ti:   0  ki:  88  vi:  96  si:  95  ri:  83 
po:  31  to: 140  ko: 403  vo:  42  so:   3  ro:  86 
pu:  49  tu:  35  ku: 169  vu:  44  su:   1  ru:  72 
\end{verbatim}}
\item NB \texttt{t} and \texttt{s} columns
\item \texttt{ti} not attested, while \texttt{si} is frequent: palatalization?
\item which lexeme contains \texttt{su}?  \textit{kasuari}
\end{itemize}
\end{frame}


\begin{frame}[fragile]
\frametitle{Prosodically-motivated search}
\small
\begin{itemize}
\item segmental and prosodic constraints
\item well-formed morphemes and lexemes
\item highly-specific query (e.g. find things predicted not to exist
  by our theoretical model?)
\item \textit{Find all trisyllabic lexemes ending in a long vowel}
\item bottom-up design: need to count syllables, and test for vowels
\end{itemize}

\begin{verbatim}
>>> def num_cons(word):
...     template = cv(word)
...     return template.count('C')
>>> def is_vowel(char):
...     return (char in 'aeiou')
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Prosodically-motivated search (cont)}
\tiny

\begin{verbatim}
>>> for entry in lexicon:
...     lx = entry.findtext('lx')
...     if lx:
...         ps = entry.findtext('ps')
...         if num_cons(lx) == 3 and ps[0] == 'V'\
...           and is_vowel(lx[-1]) and is_vowel(lx[-2]):
...             ge = entry.findtext('ge')
...             print "%s (%s) '%s'" % (lx, ps, ge)
kaetupie (V.B) 'tighten'
kakupie (V.B) 'shout'
kapatau (V.B) 'add to'
kapuapie (V.B) 'wound'
kapupie (V.B) 'close tight'
kapuupie (V.B) 'close'
karepie (V.B) 'return'
karivai (V.A) 'have an appetite'
kasipie (V.B) 'care for'
kaukaupie (V.B) 'shine intensely'
kavorou (V.A) 'covet'
kavupie (V.B) 'leave.behind'
...
kuverea (V.A) 'incomplete'
\end{verbatim}
\end{frame}

\subsection{Minimal Sets}

\begin{frame}
\frametitle{Finding Minimal Sets}
\begin{itemize}
\item E.g. mace vs maze, face vs faze
\item minimal set parameters: context, target, display
\vfil
\small

\begin{tabular}{l|lll}
\textbf{Minimal Set} &     \textbf{Context} &  \textbf{Target} & \textbf{Display} \\ \hline
\textit{bib, bid, big} &   first two letters&  third letter    & word\\
\textit{deal (N), deal (V)} & whole word    &  pos             & word (pos)
\end{tabular}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Finding Minimal Sets: Example 1}
\scriptsize

\begin{verbatim}
>>> from nltk_lite.utilities import MinimalSet
>>> pos = 1
>>> ms = MinimalSet((lex[:pos] + '_' + lex[pos+1:], lex[pos], lex)
...                 for lex in lexemes if len(lex) == 4)
>>> for context in ms.contexts(3):
...     print context + ':',
...     for target in ms.targets():
...         print "%-4s" % ms.display(context, target, "-"),
...     print
k_si: kasi -    kesi -    kosi
k_ru: karu kiru keru kuru koru
k_pu: kapu kipu -    -    kopu
k_ro: karo kiro -    -    koro
k_ri: kari kiri keri kuri kori
k_pa: kapa -    kepa -    kopa
k_ra: kara kira kera -    kora
k_ku: kaku -    -    kuku koku
k_ki: kaki kiki -    -    koki
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Finding Minimal Sets: Example 2}
\scriptsize

\begin{verbatim}
>>> entries = [(e.findtext('lx'), e.findtext('ps'), e.findtext('ge'))
...               for e in lexicon
...               if e.findtext('lx') and e.findtext('ps') and e.findtext('ge')]
>>> ms = MinimalSet((lx, ps[0], "%s (%s)" % (ps[0], ge))
...          for (lx, ps, ge) in entries)
>>> for context in ms.contexts()[:10]:
...     print "%10s:" % context, "; ".join(ms.display_all(context))
  kokovara: N (unripe coconut); V (unripe)
     kapua: N (sore); V (have sores)
      koie: N (pig); V (get pig to eat)
      kovo: C (garden); N (garden); V (work)
    kavori: N (crayfish); V (collect crayfish or lobster)
    korita: N (cutlet?); V (dissect meat)
      keru: N (bone); V (harden like bone)
  kirokiro: N (bush used for sorcery); V (write)
    kaapie: N (hook); V (snag)
       kou: C (heap); V (lay egg)
\end{verbatim}
\end{frame}

\subsection{Example Application}

\begin{frame}
\frametitle{Example Application: Improving Access to Lexical Resources}
\begin{itemize}
\item lexicon as a \textit{language resource}
\item existence of standardized writing system?  literacy?
\item lookup by approximate spelling
\item soundex
\item edit distance
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Computing Soundex}
\scriptsize

\begin{verbatim}
>>> group = {
...     ' ':0,                     # blank (for short words)
...     'p':1,  'b':1,  'v':1,     # labials
...     't':2,  'd':2,  's':2,     # alveolars
...     'l':3,  'r':3,             # sonorant consonants
...     'i':4,  'e':4,             # high front vowels
...     'u':5,  'o':5,             # high back vowels
...     'a':6                      # low vowels
... }
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Computing Soundex (cont)}
\scriptsize

\begin{verbatim}
>>> def soundex(word):
...     if len(word) == 0: return word  # sanity check
...     word += '    '                  # ensure word long enough
...     c0 = word[0].upper()
...     c1 = group[word[1]]
...     cons = filter(lambda x: x in 'pbvtdslr ', word[2:])
...     c2 = group[cons[0]]
...     c3 = group[cons[1]]
...     return "%s%d%d%d" % (c0, c1, c2, c3)
>>> print soundex('kalosavi')
K632
>>> print soundex('ti')
T400
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Soundex Index}
\begin{verbatim}
>>> soundex_idx = {}
>>> for lex in lexemes:
...     code = soundex(lex)
...     if code not in soundex_idx:
...         soundex_idx[code] = set()
...     soundex_idx[code].add(lex)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Sorting and Lookup}
\scriptsize
\begin{verbatim}
>>> from nltk_lite.utilities import edit_dist
>>> def fuzzy_spell(target):
...     scored_candidates = []
...     code = soundex(target)
...     for word in soundex_idx[code]:
...         dist = edit_dist(word, target)
...         scored_candidates.append((dist, word))
...     scored_candidates.sort()
...     return [w for (d,w) in scored_candidates[:10]]
\end{verbatim}

\begin{verbatim}
>>> fuzzy_spell('kokopouto')
['kokopeoto', 'kokopuoto', 'kokepato', 'koovoto', 'koepato',
 'kooupato', 'kopato', 'kopiito', 'kovuto', 'koavaato']
>>> fuzzy_spell('kogou')
['kogo', 'koou', 'kokeu', 'koko', 'kokoa', 'kokoi', 'kokoo',
 'koku', 'kooe', 'kooku']
\end{verbatim}
\end{frame}

\section{Generating Reports}

\subsection{Summaries}

\begin{frame}[fragile]
\frametitle{Generating Summary Reports}
\begin{itemize}
\item overall picture of the quality and organisation of the data
\item e.g. average number of fields per record
\end{itemize}

\begin{verbatim}
>>> sum(len(entry) for entry in lexicon) / len(lexicon)
10
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating Summary Reports (cont)}
\begin{itemize}
\item E.g. entry patterns
\end{itemize}

\small
\begin{verbatim}
>>> fd = FreqDist()
>>> for entry in lexicon:
...     for field in entry:
...	        fd.inc(field.tag)
>>> fd.sorted_samples()[:10]
['ge', 'ex', 'xe', 'xp', 'gp', 'lx', 'ps', 'dt', 'rt', 'eng']
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating Summary Reports (cont)}
\small

\begin{verbatim}
>>> fd = FreqDist()
>>> for entry in lexicon:
...	    fd.inc(':'.join(field.tag for field in entry))
>>> top_ten = fd.sorted_samples()[:10]
>>> print '\n'.join(top_ten)
lx:rt:ps:ge:gp:dt:ex:xp:xe
lx:ps:ge:gp:dt:ex:xp:xe
lx:ps:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:rt:ps:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:ps:ge:gp:nt:dt:ex:xp:xe
lx:ps:ge:gp:dt
lx:ps:ge:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:rt:ps:ge:ge:gp:dt:ex:xp:xe:ex:xp:xe
lx:ps:ge:ge:gp:dt:ex:xp:xe
lx:rt:ps:ge:ge:gp:dt:ex:xp:xe
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Generating Summary Reports (cont)}
\small

\begin{verbatim}
>>> fd = FreqDist()
>>> for entry in lexicon:
...     previous = "0"
...     for field in entry:
...         current = field.tag
...         fd.inc("%s->%s" % (previous, current))
...         previous = current
>>> fd.sorted_samples()[:10]
['ex->xp', 'xp->xe', '0->lx', 'ge->gp', 'ps->ge', 'dt->ex', 'lx->ps',
'gp->dt', 'xe->ex', 'lx->rt']
\end{verbatim}
\end{frame}

\subsection{Grammar}

\begin{frame}[fragile]
\frametitle{Grammar}
\small

\begin{verbatim}
>>> from nltk_lite import parse
>>> grammar = parse.cfg.parse_grammar('''
...   S -> Head "ps" Glosses Comment "dt" Examples
...   Head -> "lx" | "lx" "rt"
...   Glosses -> Gloss Glosses
...   Glosses ->
...   Gloss -> "ge" | "gp"
...   Examples -> Example Examples
...   Examples ->
...   Example -> "ex" "xp" "xe"
...   Comment -> "cmt"
...   Comment ->
...   ''')
>>> rd_parser = parse.RecursiveDescent(grammar)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Grammar}
\small

\begin{verbatim}
>>> for entry in lexicon[10:20]:
...     marker_list = [field.tag for field in entry]
...     if rd_parser.get_parse_list(marker_list):
...         print "+", ':'.join(marker_list)
...     else:
...         print "-", ':'.join(marker_list)
- lx:ps:ge:gp:sf:nt:dt:ex:xp:xe:ex:xp:xe
- lx:rt:ps:ge:gp:nt:dt:ex:xp:xe:ex:xp:xe
- lx:ps:ge:gp:nt:dt:ex:xp:xe:ex:xp:xe
- lx:ps:ge:gp:nt:sf:dt
- lx:ps:ge:gp:dt:cmt:ex:xp:xe:ex:xp:xe
+ lx:ps:ge:ge:ge:gp:cmt:dt:ex:xp:xe
+ lx:rt:ps:ge:gp:cmt:dt:ex:xp:xe:ex:xp:xe
+ lx:rt:ps:ge:ge:gp:dt
- lx:rt:ps:ge:ge:ge:gp:dt:cmt:ex:xp:xe:ex:xp:xe:ex:xp:xe
+ lx:rt:ps:ge:gp:dt:ex:xp:xe
\end{verbatim}
\end{frame}



\subsection{Timestamps}

\begin{frame}[fragile]
\frametitle{Looking at Timestamps}
\scriptsize
\begin{verbatim}
>>> fd = FreqDist()
>>> from string import split
>>> for entry in shoebox.dictionary('rotokas'):
...     if 'dt' in entry:
...         (day, month, year) = split(entry['dt'], '/')
...         fd.inc((month, year))
>>> for time in fd.sorted_samples():
...     print time[0], '/', time[1], ':', fd.count(time)
Feb / 2005 : 307
Dec / 2004 : 151
Jan / 2005 : 123
Feb / 2004 : 64
Sep / 2004 : 49
May / 2005 : 46
Mar / 2005 : 37
Apr / 2005 : 29
Jul / 2004 : 14
Nov / 2004 : 5
Oct / 2004 : 5
Aug / 2004 : 4
May / 2003 : 2
Jan / 2004 : 1
May / 2004 : 1
\end{verbatim}
\end{frame}

\end{document}
