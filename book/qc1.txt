pxi 5up Chapters -3 -> Chapters 1-3

pxii 2d 8-Chapter 10 -> 8-10
     4d afterword -> Afterword

pxiii [?] Given van Rossum's intro remarks at PyCon 2009, should we also
explicitly say that we are committed to porting NLTK to Python 3.0?  [agreed]

pxiv 13d why is "Graphviz" in bold?

     Table P-2: maxent -> maximum entropy

pxvi data-set -> dataset

pxvii Conventions: URLs should be listed under "Italic", not "Constant width"

pxviii [?] "Using Code Examples" -- needs revising since we already distribute the code
under GPL

pxix [?] If possible, add reference to Edward Clarence Dyason Fellowship

p6 Numpy -> NumPy

p7 5d internet -> Internet

p8 6up called function -> called a function

p17 fig 1-3 reduce scale of figure
    3up careful use -> careful to use

p26 letter "l" -> letter l [italic] (cf p36, ex 24)

p31 13d chap-data-intensive -> Chapter 5

p34 Note that there are three paragraphs but four citations.
    Happy to replace with regular citations and expand bibliography accordingly.

p34 The ACL website is http://www.aclweb.org/

p35 ex 9b: adding it -> adding the string
    ex 10b: b. b) -> b.
    ex 15: letter "b" -> letter b [italic]
    ex 25a with "sh" -> with sh [italic] 

p43 The missing figure citation is to fig 1-2
    9up file[:4] -> fileid[:4]

[?] We're not consistent in the referring to the Inaugural addresses corpus; we
have the following variants:

US Presidential Inaugural Addresses corpus
Inaugural corpus
Inaugural Address Corpus (Table 2-2 and captions for Figure 2-1, Figure 2-3)

Note that we always capitalize corpus in Brown Corpus but not elsewhere -- why?

p44 Happy to omit the second sentence of the table caption
    Fig 2-1 caption: america, citizen s/b roman

[?] Per Ewan: "The 'Contents' column needs to be reviewed by SB."
My concern was that the hyphenation conventions being used here seem unclear.
There are several hyphenated forms that we don't use elsewhere, such as
part-of-speech and named-entity. I don't see the motivation for this. Also, we have
both "part-of-speech tagged" and "POS-tagged".

p45 Content of the table looks fine to me; note that since the CoNLL entry is wrapped,
    we might as well expand "Sel" -> "Selections"

    [?] is "Presidential Addresses"/"Ahrens" correct? Is this meant to be the same as 
    item 39 on http://nltk.googlecode.com/svn/trunk/nltk_data/index.xml?
    
p47 Table 2-3 Delete second occurrence of "nltk.corpus.reader" in table caption.
    It should read: "more documentation can be found using help(nltk.corpus.reader)
    and by reading the online Corpus HOWTO at http://www.nltk.org/howto.

[?]
"Your Turn: Pick a language of interest in udhr.fileids(), and define a
variable raw_text = udhr.raw('Language-Latin1')."

This isn't clear. Presumably 'Language-Latin1' is (in part) a metavariable for the
reader's choice of language. However, we're using italic cw for metavariables, and I
would say that we should also omit the single quotes.

p49 10d [?] omit single quotes -- in context, this is meant to be the location, not a string.

p50 confirming the cw italic font for metavariables is correct

p52 1,2d omit single quotes from america, Americans and make italic font

p53 Ex 2-1 caption: generate_model() s/b cw

p54 [?] File menu, Run menu -- aren't we using cw for these cases?

p58 16up fail -> fails

p59 [?] length constraint -> length constraints

p60 [?] to be consistent, the cited letters (e.g. a, e, i etc) s/b roman 

p67 Fig 2-8 the line joining "motor vehicle" to "go-kart" s/b vertical

p69 11d antonymy s/b bold font

p71 4d [?] function -> method name

    15up insert closing parens after URL

p73 q19 "like the one given above for modals"
    -> "like the one given in section 2.1 for modals"
    or "like the one given on p42 for modals"

p75 it would be nice to avoid these widowed lines if possible.
    (removing "different" from q3 would save one line)

p81 Table 3-1 caption: words in quotes s/b unquoted roman.

p84 Figure 3-1 caption: nltk.Text s/b cw

p89 Figure 3-2 caption: Monty Python s/b unquoted roman

p89 fig 3-2 scale smaller?

p90 [?] since material on strings vs lists overlaps bottom p84 / top p85,
should we add a line to say that we are reinforcing points made earlier?

p91 in 4th sent of sect 3.3, different fonts seem to be being used for the 4 
glyphs illustrating non-ASCII character sets. E.g. 3rd and 4th look larger and 
seem to be in cw font. Can this be fixed? (Similarly, p 93 13up, p94 15d)

p95 LD query: this is meant to be Python output to a terminal window, so I would
say it s/b cw.

p97 Fig 3-5: numbers are missing from middle row. [?] Also it looks like it could be
reduced slightly.

p97--100 [?] We seem to be being inconsistent in presentation of regex symbols: some of
them are in straight double quotes, a couple (p99, 9d) are in single straight quotes,
and some are not quoted at all. My preference would be for no quotes, or failing
that, double curly quotes.

p101 "is a loanword" -> "is borrowed from English"

p106 [?] it looks as tho the WN lemmatizer doesn't handle any verb forms.

p113 [?] Ex 3-5: should we add a line to the caption explaining Python's backtick
operator? 

p116 18up with conversion -> with a conversion

p119 4up [?] this is the first mention of lexeme

p120 delete "proc", lowercase "Normalizing"
[?] do we need second mention of Mertz and J&M?

p121 [?] should we give an example of an AI text?
     ex 4: can also specify -> can specify
           [?] we haven't given a value for msg

p122 ex 7: increase point size of subparts 7(a), 7(b)
     [?] ex 8: "clear setting out" doesn't sound v idiomatic
     [?] ex 9(b): companies -> organizations

p124 [?] ex 27: "500 randomly chosen letters" -- are these meant to be
drawn from the string "aehh "? It's not clear what is meant by
"normalize the whitespace in this string".

     [?] ex 29:  4.71 Î¼w + 0.5 Î¼s - 21.43 -- should this really be cw?

     [?] ex 31: the variable saying was defined on p118, viz as

     >>> saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',
     ... 'more', 'is', 'said', 'than', 'done', '.']

     Are we expecting readers to pick up on this?

p128 Fig 4-1 caption: foo, bar s/b cw (x 2)

p131 [?] the definitions of all(), any() both look wrong. Firstly, their truth depends
not just on whether there are elts in their args, but whether the elts evaluate to
true. And the defs actually seem to make all() and any() equivalent.

     12up [?] might be better to place the tag [1] after "comma operator", since there
     are no parens in the first line of code.

p133 [?] Para starting "Notice in this code sample..." It looks to me as tho this has
been displaced, and is referring to the code sample in the middle of p132 (i.e. the
one starting >>> raw = "I turned off the spectroroute")

p135 13up [?] lexicon s/b cw

p140 (I previously fixed this as a PDF annotation but it was lost!) 
     "It is tempting to adopt idioms from other  languages,
     when Python offers some elegant and highly readable alternatives."
     -> "It is tempting to adopt idioms from other languages."
     "However, Python offers some elegant and highly readable alternatives,
     as we have seen."

p144 6up "defensive programming" --- maybe this should be introduced as a new
term here (i.e. set in bold) rather than waiting until p157.

p149 17up permutations s/b cw

p150 6up [?] keyword arguments -- treat as new terms?

p151 13up the same function, -> the same function (i.e. omit comma)

p154 Fig 4-2 caption: my_program.py s/b roman

p157 [?] "defensive programming" -- see note on p144; remove bold?

p159 eight occurences of numerals on this page s/b non-italic; e.g. in n/2, (n=1)
etc.
     4up 1 s/b proportional font, not cw

p164 [?] tree s/b monochrome

p165 3d previous calls -> previous call

p167 13d [?] rainfall.png -> modals.png

p171 ex 4 assignm0ent -> assignment

p172 [?] ex 16 is the same as ex 24, p123 -- delete?

p173 ex 18 the n most: "n" s/b italic, not c/w

     ex 25 NLTK's the Shakespeare -> NLTK's Shakespeare

     ex 26 How -> In what way

     ex 27 The equation for Catalan numbers needs to be set as a math equation (i.e.,
     italicized subscripts); similarly for C_n at end of 27(a).

p174 ex 34 NGramTagger s/b cw

     ex 35 [?] "Read the linked article" -- rewrite using reference to article URL
     cited at the end of the 2nd sentence?
     
p175 n:math -> n

p178 Slanted font for metavariable is correct

*p178 What's the issue with the second highlighted section?

p201 General N-Gram Tagging

p208 Please delete the highlighted material in parentheses

p210 lexeme BE -> lexeme be

p217 "anti-ngram" -> "anti-n-gram"

p222 ok to make into a paragraph

p223 change reference to corpus in example caption to: Names Corpus

*p231 This is meant to be introducing the terms.  The remainder of 
    the paragraph is describing how these three models work in
    general terms.  I recommend the following rewording to make this
    clear:

       Another solution is to assign scores to all of the possible
       sequences of part-of-speech tags, and to choose the sequence
       whose overall score is highest. This is the approach taken by
       <emphasis role="strong">Hidden Markov Models</emphasis>.
       Hidden Markov Models are similar to consecutive classifiers in
       that they look at both the inputs and the history of predicted
       tags. However, rather than simply finding the single best tag
       for a given word, they generate a probability distribution over
       tags. These probabilities are then combined to calculate
       probability scores for tag sequences, and the tag sequence with
       the highest probability is chosen. Unfortunately, the number of
       possible tag sequences is quite large. Given a tag set with 30
       tags, there are about 600 trillion
       (30<superscript>10</superscript>) ways to label a 10-word
       sentence.  In order to avoid considering all these possible
       sequences separately, Hidden Markov Models require that the
       feature extractor only look at the most recent tag (or the most
       recent <emphasis>n</emphasis> tags, where
       <emphasis>n</emphasis> is fairly small). Given that
       restriction, it is possible to use dynamic programming (Section
       4.7) to efficiently find the most likely tag sequence. In
       particular, for each consecutive word index
       <emphasis>i</emphasis>, a score is computed for each possible
       current and previous tag.  This same basic approach is taken by
       two more advanced models, called <emphasis
       role="strong">Maximum Entropy Markov Models</emphasis> and
       <emphasis role="strong">Linear-Chain Conditional Random Field
       Models</emphasis>; but different algorithms are used to find
       scores for tag sequences.

p233 "data set" -> "dataset" (at least 7 times in chapter 6)

*p234 Change the paragraph "We let words..." to start:

       In our RTE feature detector (Example 6.7), we let words...

*p241 Append the following sentence to the paragraph preceeding example
      6-9 (i.e., the paragraph starting "For example, Figure 6-5 shows.."):

      Example 6-9 demonstrates how to calculate the entropy
      of a list of labels.

*p245 Change the sentence "This process is illustrated in
      Figure 6-7" to say "This process is illustrated in Figures
      6-7 and 6-8."

*p246 ELE and Heldout Estimation should be marked as definition terms
      (i.e., set in bold), and not quoted.
      -- mention NLTK support for estimation here? -> I don't know if 
         this counts as adding content, but I would be fine with ending
         the paragraph with the sentence "The ``nltk.probability`` module
         provides support for a wide variety of smoothing techniques."
      -- mention ELE and Heldout in further readings section?

p254 Add "the Recognizing Textual Entailment competitions" to this list

p255 Set in roman, and capitalize "Q": ynQuestion

p256 q9 "Prepositional Phrase Attachment corpus" -> "PP Attachment Corpus"
     "PP attachment corpus" -> "PP Attachment Corpus"

p275 root node of syntax tree in example (4) s/b boldface

p278 [?] not sure which URLs to give, or where to give them
     - for getty, the wikipedia entry seems best
       http://en.wikipedia.org/wiki/Getty_Thesaurus_of_Geographic_Names
     - for alexandria I don't see anything obvious (should we drop it?)
     - in both cases, the link might be best in the further reading section

p285 hope we can avoid the widowed lines

p290 non-terminals in the tree diagrams s/b boldface

p293 non-terminals s/b boldface

p302 non-terminals s/b boldface

p302 [?] Delete example (11) and the sentence immediately before it
p303 [?] Delete from top down to and including example (13).
     (I don't think this material elucidates the method;
     it also uses the =>* notation which is not defined afaik) 

p309 [?] not sure what this highlighting of "Dependencies" is about

p316 change year of Bresnan and Hay citation from 2006 to 2008

p318 [?] (I previously fixed this as a PDF annotation but it was lost!) 

p328 non-terminals s/b boldface

p329 non-terminals s/b boldface

p332 Fig 9-1 s/b smaller scale

pp334ff directed graphs should be smaller

p351 [?] margin problem

p365 ex (9) [?] alignment and size

p381 ex (29) [?] garbled content, non-terminals s/b boldface

p393 Fig 10-5 [?] screenshot looks weird without a frame (our fault)

p404 "TIMIT corpus" -> "TIMIT Corpus"

p404 delete "an", to leave "has internal structure"

p405 ok not to have comma after the word "variables"

p409 ok not to have comma after the word "window"

p411 agree that GNU WGet should not be italicized

p411 (I previously fixed this as a PDF annotation but it was lost!) 
     "We can enter this in MSWord," -> "We can key in such text using MSWord"

p412 (I previously fixed this as a PDF annotation but it was lost!) 
     "... parts-of-speech as the set difference between used_pos and legal_pos."

p413 ok to delete note about Beautiful Soup package

p417 problem with coordinate structure -- split into two sentences as follows:
     - delete comma after "(SQL)"
     - change comma after "file storage" to period
     - change "and allows" to "It also allows" at start of next sentence

p421 change quoted words "verb" and "noun" to cw, and remove quotes

p429 (I previously fixed this as a PDF annotation but it was lost!)
     "conventions for resource discovery on the Web."
     -> "conventions for finding, sharing, and managing information." 

p433 add URL http://www.python.org/doc/lib/markup.html

p435 adopt proposed replacement text

p440 (I previously fixed this as a PDF annotation but it was lost!)
     Add: "Contributions in the following areas are particularly encouraged:"

p443ff [?] Bracketted author year information in bibliography entries is
     redundant and looks rather ugly -- can we omit this?
     First author's name should be formatted as "Lastname, Firstname"
     
p444 "Ca" s/b "CA" (California)

p444 Replace Bresnan and Hay citation with published version: (italicize "give")

     Joan Bresnan and Jennifer Hay.
       Gradient grammar: An effect of animacy on the syntax of *****give*****
       in New Zealand and American English. Lingua 118: 245Ð59, 2008  
