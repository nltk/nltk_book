.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. standard global imports

    >>> import nltk, re, pprint
    
.. Organization:
    - Supervised Classification -- this introduces basic concepts, and
      runs through lots of interesting examples
    - Evaluation -- talks about evaluation :)
    - Classification Methods (x3)
    - Generative vs Conditional
    - Joint Classification
    - Data modeling -- this talks about abstractly what we can learn 
      about language as linguists
    - ML in Python

.. _chap-data-intensive:

============================
6. Learning to Classify Text
============================

.. nomenclature note: training corpus/test corpus or training set/test set??
.. discuss qc corpus
.. explain that segmentation (e.g. tokenization, sentence segmentation) can
   be viewed as a classification task

Detecting patterns is a central part of |NLP|.
Words ending in `-ed`:lx: tend to be past tense verbs (Chapter chap-tag_).
Frequent use of `will`:lx: is indicative of news text (Chapter chap-words_).
These observable patterns |mdash| word structure and word frequency |mdash|
happen to correlate with particular aspects of meaning, such as tense and topic.
But how did we know where to start looking, which aspects of form to associate
with which aspects of meaning? 

The goal of this chapter is to answer the following questions:

#. How can we identify particular features of language data that
   are salient for classifying it?
#. How can we construct models of language that can
   be used to perform language processing tasks automatically?
#. What can we learn about language from these models?

Along the way we will study some important machine learning techniques,
including as decision trees, naive Bayes' classifiers, maximum entropy classifiers.
We will gloss over the mathematical and statistical underpinnings of these
techniques, focusing instead on how and when to use them (see the
further readings section for more technical background). 
Before looking at these methods, we first need to appreciate the
broad scope of this topic.

-------------------------
Supervised Classification
-------------------------

`Classification`:dt: is the task of choosing the correct `class
label`:dt: for a given input.  In basic classification tasks, each
input is considered in isolation from all other inputs, and set of
labels is defined in advance.  Some examples of classification tasks
are:

* Deciding whether an email is spam or not.
* Deciding what the topic of a news article is, from a fixed list of 
  topic areas such as "sports," "technology," and "politics."
* Deciding whether a given occurrence of the word "bank" is used to
  refer to a river bank, a financial institution, the act of tilting
  to the side, or the act of depositing something in a financial
  institution.

The basic classification task has a number of interesting variants:
for example, in multi-class classification, each instance may be
assigned multiple labels; in open-class classification, the set of
labels is not defined in advance; and in sequence classification, a
list of inputs are jointly classified.

A classifier is called `supervised`:dt: if it is built based on
training corpora containing the correct label for each input.

.. _fig-supervised-classification:
.. figure:: ../images/supervised-classification.png
   :scale: 50:150:60

   Supervised Classification.  (a) During training, a feature
   extractor is used to convert each input value to a feature set.
   Pairs of feature sets and labels are fed into the machine learning
   algorithm to generate a model.  (b) During prediction, the same
   feature extractor is used to convert unseen inputs to feature sets.
   These feature sets are then fed into the model, which generates
   predicted labels.

In the rest of this section, we will look at how classifiers can be
used to solve a wide variety of tasks.  Our discussion is not intended
to be comprehensive, but to give a representative sample of tasks that
can be performed with the help of text classifiers.
   
Gender Identification
---------------------

In Section sec-lexical-resources_ we saw that male and female names
have some distinctive characteristics.  Names ending in `a`:lx:,
`e`:lx: and `i`:lx: are likely to be female, while names ending in
`k`:lx:, `o`:lx:, `r`:lx:, `s`:lx: and `t`:lx: are likely to be male.
Let's build a classifier to model these differences more precisely.

The first step in creating a classifier is deciding what
`features`:dt: of the input are relevant, and how to `encode`:dt:
those features.  For this example, we'll start by just looking at the
final letter of a given name.  The following `feature extractor`:dt:
function builds a dictionary containing relevant information about a
given name:

    >>> def gender_features(word):
    ...     return {'last_letter': word[-1]}
    >>> gender_features('Shrek')
    {'last_letter': 'k'}

The returned dictionary, known as a :dt:`feature set`, maps from
features' names to their values.  Feature names are case-sensitive
strings that typically provide a short human-readable description of
the feature.  Feature values are simple-typed values, such as
booleans, numbers, and strings.

.. note:: 
   Most classification methods require that features be encoded using
   simple value types, such as booleans, numbers, and strings.  But
   note that just because a feature has a simple type, does not
   necessarily mean that the feature's value is simple to express or
   compute; indeed, it is even possible to use very complex and
   informative values, such as the output of a second supervised
   classifier, as features.

Now that we've defined a feature extractor, we need to prepare the
training corpus, consisting of a list of examples and corresponding
class labels.

    >>> from nltk.corpus import names
    >>> import random
    >>> names = ([(name, 'male') for name in names.words('male.txt')] + 
    ...          [(name, 'female') for name in names.words('female.txt')])
    >>> random.shuffle(names)

Next, we use the feature extractor to process the ``names`` data, and
use the resulting feature sets to train a "Naive Bayes" classifier.

    >>> featuresets = [(gender_features(n), g) for (n,g) in names]
    >>> train_set, test_set = featuresets[500:], featuresets[:500]
    >>> classifier = nltk.NaiveBayesClassifier.train(train_set)

.. note:: When working with large corpora, constructing a single list
   that contains the features of every instance can use up a large
   amount of memory.  In these cases, use the function
   ``nltk.classify.apply_features``, which returns an object that acts
   like a list but does not store all the feature sets in memory:

   >>> train = apply_features(extract_features, labeled_words)
   >>> test = apply_features(extract_features, unseen_words)

We will study the naive Bayes classifier later.  For now, let's
just test it out on some names that did not appear in its training data:

    >>> classifier.classify(gender_features('Neo'))
    'male'
    >>> classifier.classify(gender_features('Trinity'))
    'female'

Observe that these character names from *The Matrix* are correctly
classified.  Although this science fiction movie is set in 2199, it
conforms with our expectations about names and genders.  We can
systematically evaluate the classifier on a much larger quantity of
unseen data:

    >>> nltk.classify.accuracy(classifier, test_set)
    0.774

Finally, we can examine the classifier to determine which features it
found most useful in distinguishing the names' genders:

    >>> classifier.show_most_informative_features(5)
    Most Informative Features
             last_letter = 'a'              female : male   =     33.4 : 1.0
             last_letter = 'k'                male : female =     29.7 : 1.0
             last_letter = 'f'                male : female =     25.3 : 1.0
             last_letter = 'v'                male : female =     17.5 : 1.0
             last_letter = 'm'                male : female =     13.6 : 1.0

This listing shows that the names in the training corpus that end in
"a" are female 33.4 times more often than they are male; but names
that end in "k" are male 29.7 times more often than they are female.

.. note:: |TRY|
   Modify the ``gender_features()`` function, to provide the
   classifier features encoding the length of the name, its first
   letter, and any other features that seem like they might be
   informative.  Retrain the classifier with these new features, and
   test its accuracy.  

Choosing The Right Features
---------------------------

Selecting relevant features, and deciding how to encode them for the
learning method, can have an enormous impact on its ability to extract
a good model.  Much of the interesting work in building a classifier
is deciding what features might be relevant, and how we can represent
them.  Although it's often possible to get decent performance by using
a fairly simple and obvious set of features, there are usually
significant gains to be had by using carefully constructed features
based on a thorough understanding of the task at hand.  

Typically, feature extractors are built through a process of
trial-and-error, guided by intuitions about what information is
relevant to the problem at hand.  It's often useful to start with a
"kitchen sink" approach, including all the features that you can think
of, and then checking to see which features actually appear to be
helpful.  However, there are usually limits to the number of features
that you should use with a given learning algorithm |mdash| if you provide
too many features, then the algorithm will have a higher chance of
relying on idiosyncrasies of your training data that don't generalize
well to new examples.  This problem is known as `overfitting`:dt:, and
can especially problematic when working with small training sets.  For
example, the features returned by the feature extractor shown in Figure
gender-features-overfitting_ look like they might be useful.
However, when we train a classifier using all these features, the resulting
accuracy is about 10% lower than the accuracy of a classifier
that only uses the final letter of each name:

    >>> featuresets = [(gender_features2(n), g) for (n,g) in names]
    >>> train_set, test_set = featuresets[500:], featuresets[:500]
    >>> classifier = nltk.NaiveBayesClassifier.train(train_set)
    >>> nltk.classify.accuracy(classifier, test_set)
    0.688

.. pylisting:: gender-features-overfitting
   :caption: A Feature Extractor that Overfits Gender Features

   def gender_features2(name):
       """
       The featuresets returned by this feature extractor contain a
       large number of specific features, leading to overfitting for
       the relatively small 'names' corpus.
       """
       features = {} 
       features["firstletter"] = name[0].lower()
       features["lastletter"] = name[0].lower()
       for letter in 'abcdefghijklmnopqrstuvwxyz':
           features["count(%s)" % letter] = name.lower().count(letter)
           features["has(%s)" % letter] = (letter in name.lower())
       return features

    >>> gender_features('John')
    {'count(j)': 1, 'has(d)': False, 'count(b)': 0, ...}

..  SHOW DONT TELL FOR ERROR ANALYSIS HERE:

Once an initial set of features has been chosen, a very productive
method for refining the feature set is `error analysis`:dt:.  First,
the training corpus is split into two pieces: a training sub-corpus,
and a `development`:dt: sub-corpus.  The model is trained on the
training sub-corpus, and then run on the development sub-corpus.  We can
then examine individual cases in the development sub-corpus where the
model predicted the wrong label, and try to determine what additional
pieces of information would allow it to make the right decision (or
which existing pieces of information are tricking it into making the
wrong decision).  The feature set can then be adjusted accordingly,
and the error analysis procedure can be repeated, ideally using a
different development/training split.

Document Classification
-----------------------

In Section sec-extracting-text-from-corpora_, we saw several examples
of corpora where documents have been labeled with categories.  Using
these corpora, we can build classifiers that will automatically tag
new documents with appropriate category labels.  First, we'll
construct a list of documents, labeled with the appropriate
categories:

    >>> documents = [(nltk.corpus.brown.words(fileid), category)
    ...              for category in brown.categories()
    ...              for fileid in brown.files(category)]
    >>> random.shuffle(documents)

Next, we need to define a feature extractor for documents, so the
classifier will know what information it should make use of.  For
document topic identification, we can define a feature for each word,
indicating whether the document contains that word.  (We use a
`stoplist`:dt: to eliminate common words like "in" and "the," which
don't tell us much about the topic of a document).

    >>> def document_features(document):
    ...     features = {}
    ...     for word in document:
    ...         if word not in stopwords:
    ...             features['contains(%s)' % word] = True
    ...     return features
    >>> print document_features(nltk.corpus.brown.words('cj79'))
    (add output)

Now that we've defined our feature extractor, we can use it to train a
new classifier:
    
    >>> featuresets = [(document_features(d), c) for (d,c) in documents]
    >>> train_set, test_set = featuresets[100:], featuresets[:100]
    >>> classifier = nltk.NaiveBayesClassifier.train(train_set)

To check how reliable the resulting classifier is, we compute its
accuracy on the training data:

    >>> nltk.classify.accuracy(classifier, test_set)

Once again, we can use ``show_most_informative_features`` to find out
which features the classifier found to be especially useful:

    >>> classifier.show_most_informative_features(5)

.. add word frequencies, with binning??  FreqDist(document), then for
   each word not in the stoplist, list the document frequency ..  not
   enough training data for this??

Part-of-Speech Tagging
----------------------

In Chapter chap-tag_ we built a regular expression tagger that chooses
a part-of-speech tag for a word by looking at the internal make-up of
the word.  However, this regular expression tagger had to be
hand-crafted.  Instead, we can train a classifier to work out which
suffixes are most informative.  Let's begin by defining a new feature
extraction function:

    >>> def pos_features(word):
    ...     return {"suffix(1)": word[-1:],
    ...             "suffix(2)": word[-2:],
    ...             "suffix(3)": word[-3:]}

Feature extraction functions behave like tinted glasses, highlighting
some of the properties (colors) in our data and making it impossible
to see other properties.  The classifier will rely exclusively on
these highlighted properties when determining how to label inputs.

    >>> pos_features('studied')
    {'suffix(1)': 'd', 'suffix(3)': 'ied', 'suffix(2)': 'ed'}
    >>> pos_features('if')
    {'suffix(1)': 'f', 'suffix(3)': 'if', 'suffix(2)': 'if'}

Now that we've defined our feature extractor, we can use it to train a
new "decision tree" classifier:
    
    >>> tagged_words = nltk.corpus.brown.tagged_words(categories='news')
    >>> featuresets = [(pos_features(n), g) for (n,g) in tagged_words]

    >>> size = int(len(featuresets) * 0.1)
    >>> train_set, test_set = featuresets[size:], featuresets[:size]

    >>> classifier = nltk.DecisionTreeClassifier.train(train_set, binary=True)
    >>> nltk.classify.accuracy(classifier, test_set)
    0.809864757359

    >>> classifier.classify(pos_features('cats')
    NNS

One nice feature of decision trees is that they are often fairly easy
to interpret -- we can even instruct |NLTK| to print them out as
pseudocode:

>>> print classifier.truncate(depth=3).pseudocode()
if suffix(1) == 's':
  if suffix(2) == 'as':
    if suffix(3) == 'was': return 'BEDZ'
    if suffix(3) != 'was': return 'CS'
  if suffix(2) != 'as':
    if suffix(2) == 'is': return 'BEZ'
    if suffix(2) != 'is': return 'NNS'
if suffix(1) != 's':
  if suffix(1) == 'd':
    if suffix(3) == 'and': return 'CC'
    if suffix(3) != 'and': return 'VBD'
  if suffix(1) != 'd':
    if suffix(2) == ',': return ','
    if suffix(2) != ',': return 'NN'

Here, we can see that the classifier begins by checking whether a word
ends with "s."  If so, it's likely to be a plural noun (NNS), unless
it fits into one of a few special cases (for "was", "as" and "is").
If a word doesn't end in "s," then the classifier next checks whether
it ends in "d" (in which case it's probably a past-tense verb) or in
"," (in which case it will almost always be tagged as ",").  Finally,
if none of the tested features match, it makes the reasonable default
guess that the word is a singular noun (NN).

Making Use of Context
---------------------

By augmenting the feature extraction function, we could modify this
part-of-speech tagger to make use of a variety of other word-internal
features, such as the length of the word, the number of syllables it
contains, or its prefix.  However, as long as the feature extractor
just looks at the target word, we have no way to add features that
depend on the *context* that the word appears in.  But contextual
features often provide powerful clues about the correct tag -- for
example, when tagging the word "fly," knowing that the previous word
is "a" will allow us to determine that it is being used as a noun, not
a verb.

In order to accommodate features that depend on a word's context, we
must revise the pattern that we used define our feature extractor.
Instead of just passing in the word to be tagged, we will pass in a
complete (untagged) sentence, along with the index of the target word:

>>> def pos_features(sentence, i):
...     features = {"suffix(1)": sentence[i][-1:],
...                 "suffix(2)": sentence[i][-2:],
...                 "suffix(3)": sentence[i][-3:]}
...     if i == 0:
...         features["prev-word"] = "<START>"
...     else:
...         features["prev-word"] = sentence[i-1]
...     return features
>>> pos_features(nltk.corpus.brown.sents()[0], 8)
{'suffix(3)': 'ion', 'prev-word': 'an', 'suffix(2)': 'on', 'suffix(1)': 'n'}

Having defined the feature extractor, we next apply it to the words in
a chosen corpus:

>>> tagged_sents = nltk.corpus.brown.tagged_sents(categories='news')
>>> featuresets = []
>>> for tagged_sent in tagged_sents:
...     untagged_sent = nltk.tag.untag(tagged_sent)
...     for i, (word, tag) in enumerate(tagged_sent):
...         featuresets.append( (pos_features(untagged_sent, i), tag) )

Finally, we can use the list of tagged featuresets to train and test a
classifier:

>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]

>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> nltk.classify.accuracy(classifier, test_set)

.. It would be nice to actually show this (using 
   show_most_informative_features or something):

Clearly, making use of contextual features improves the performance
for our part-of-speech tagger.  For example, the classifier learns
that a word is likely to be a noun if it comes immediately after the
word "large" or the word "gubernatorial".  However, it is unable to
learn the generalization that a word is probably a noun if it follows
an adjective, because it doesn't have access to the previous word's
part-of-speech tag.  In general, simple classifiers always treat each
input as independent from all other inputs.  In many contexts, this
makes perfect sense.  For example, decisions about whether names tend
to be male or female can be made on a case-by-case basis.  However,
there are often cases, such as part-of-speech tagging, where we are
interested in solving classification problems that are closely related
to one another.  

Sequence Classification
-----------------------

In order to capture the dependencies between related classification
tasks, we can use `joint classifier`:dt: models, which chooses an
appropriate labeling for a collection of related inputs.  In the case
of part-of-speech tagging, a variety of different `sequence
classifier`:dt: models can be used to jointly choose part-of-speech
tags for all the words in a given sentence.

.. note that consecutive classification isn't a widely-used term,
   but it seems reasonably good to me.

One sequence classification strategy, known as `consecutive
classification`:dt:, is to find the most likely class label for one of
the related inputs; and then using that answer to help find the best
label for the next input.  The process can then be repeated until all
of the inputs have been labeled.  This is the approach that was taken
by the bigram tagger from Section sec-n-gram-tagging_, which began by
choosing a part-of-speech tag for the first word in the sentence, and
then chose the tag for each subsequent word based on the word itself
and the predicted tag for the previous word.

In order to use this strategy with general classifiers models, we must
first augment our feature extractor function to take a ``history``
argument, which provides a list of the tags that we've predicted for
the sentence so far:

>>> def pos_features(sentence, i, history):
...     features = {"suffix(1)": sentence[i][-1:],
...                 "suffix(2)": sentence[i][-2:],
...                 "suffix(3)": sentence[i][-3:]}
...     if i == 0:
...         features["prev-word"] = "<START>"
...         features["prev-tag"] = "<START>"
...     else:
...         features["prev-word"] = sentence[i-1]
...         features["prev-tag"] = history[i-1]
...     return features

Each tag in ``history`` corresponds with a word in ``sentence``.  But
note that ``history`` will only contain tags for words we've already
classified.  Thus, while it is possible to look at words to the right
of the target word, it is not possible to look at the tags for those
words (since we haven't generated them yet).

Having defined a feature extractor, we can proceed to train our
sequence classifier.  During training, we use the annotated tags to
provide the appropriate history to the feature extractor; but when
tagging new sentences, we generate the history list based on the
output of the tagger itself.

.. pylisting:: consecutive-pos-tagger
   :caption: Missing Title

   class ConsecutivePosTagger(nltk.TaggerI):

       def __init__(self, train_sents):
           train_set = []
           for tagged_sent in train_sents:
               untagged_sent = nltk.tag.untag(tagged_sent)
               history = []
               for i, (word, tag) in enumerate(tagged_sent):
                   featureset = pos_features(untagged_sent, i, history)
                   train_set.append( (featureset, tag) )
                   history.append(tag)
           self.classifier = nltk.NaiveBayesClassifier.train(train_set)
       
       def tag(self, sentence):
           history = []
           for i, word in enumerate(sentence):
               featureset = pos_features(sentence, i, history)
               tag = self.classifier.classify(featureset)
               history.append(tag)
           return zip(sentence, history)
    
   >>> tagged_sents = nltk.corpus.brown.tagged_sents(categories='news')
   >>> train_sents, test_sents = tagged_sents[size:], tagged_sents[:size]
   >>> tagger = SimplePosTagger(train_sents)
   >>> print nltk.tag.accuracy(tagger, test_sents)

One shortcoming of this approach is that we commit to every decision
that we make.  For example, if we decide to label a word as a noun,
but later find evidence that it should have been a verb, there's no
way to go back and fix our mistake.  One solution to this problem is
to use a transformational strategy instead.  Transformational joint
classifiers work by creating an initial assignment of labels for the
inputs, and then iteratively refining that assignment in an attempt to
repair inconsistencies between related inputs.  The Brill tagger,
described in Chapter chap-tag_, is a good example of this strategy.

Another solution is to assign scores to all of the possible sequences
of part-of-speech tags, and to choose the sequence whose overall score
is highest.  This is the approach taken by `Hidden Markov Models`:dt:,
`Maximum Entropy Markov Models`:dt:, and `Linear-Chain CRFs`:dt:,
which are all similar to consecutive classifiers in that they look at
both the inputs and the history of predicted tags.  However, rather
than simply finding the single best tag for a given word, they
generate a probability distribution over tags.  These probabilities
are then combined to calculate probability scores for tag sequences;
and the tag sequence with the highest probability is chosen.
Unfortunately, the number of possible tag sequences is quite large |mdash|
given a tag set with 30 tags, there are about 600 trillion (30\
`10`:sup:) ways to label a 10 word sentence.  In order to avoid
considering all these possible sequences separately, these models all
require that the feature extractor only look at the most recent tag
(or the most recent `n`:math: tags, where `n`:math: is fairly small).
Given that restriction, it is possible to use a programming technique
known as `dynamic programming`:dt: (Section sec-algorithm-design_)
to efficiently find the most likely tag sequence.
In particular, for each consecutive word index `i`:math:,
a score is computed for each possible current and previous tag.  

Sentence Segmentation
---------------------

Sentence segmentation can be viewed as a classification task for
punctuation: whenever we encounter a symbol that could possibly end a
sentence, such as a period or a question mark, we have to decide
whether it terminates the preceding sentence.

The first step is to obtain some data that has already been segmented
into sentences and convert it into a form that is suitable for
extracting features:

>>> sents = nltk.corpus.treebank_raw.sents()
>>> tokens = []
>>> offset = 0
>>> for sent in nltk.corpus.treebank_raw.sents():
...     tokens.extend(sent)
...     offset += len(sent)
...     boundaries.add(offset-1)

Here, ``tokens`` is a merged list of tokens from the individual
sentences; and ``boundaries`` is a set containing the indices of all
sentence-boundary tokens.  Next, we need to specify the features of
the data that will be used in order to decide whether punctuation
indicates a sentence-boundary:

>>> def punct_features(tokens, i):
...     return {'next-word-capitalized': tokens[i+1][0].isupper(),
...             'prevword': tokens[i-1].lower(),
...             'punct': tokens[i],
...             'prev-word-is-one-char': len(tokens[i-1]) == 1}

Based on this feature extractor, we can create a list of labeled
featuresets by selecting out all the punctuation tokens, and tagging
whether they are boundary tokens or not:

>>> featuresets = [(punct_features(tokens, i), (i in boundaries))
...              for i in range(1, len(tokens)-1)
...              if tokens[i] in '.?!']

Using that list of featuresets, we can train and evaluate a
punctuation classifier:

>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> nltk.classify.accuracy(classifier, test_set)
0.971

To use this classifier to perform sentence segmentation, we simply
check each punctuation mark to see whether it's labeled as a boundary;
and divide the list of words at the boundary marks.  Listing
classification-based-segmenter_ shows how this can be done.

.. pylisting:: classification-based-segmenter
   :caption: Missing Title

   def segment_sentences(words):
       start = 0
       sents = []
       for i, word in words:
           if word in '.?!' and classifier.classify(words, i) == True:
               sents.append(words[start:i+1])
               start = i+1
       if start < len(words):
           sents.append(words[start:])

Identifying Dialog Act Types
----------------------------

When processing conversational dialog, it can be useful to think of
utterances as a type of *action* performed by the speaker.  This
interpretation is most straightforward for performative statements
such as "I forgive you" or "I bet you can't climb that hill."  But
greetings, questions, answers, assertions, and clarifications can all
be thought of as types of speech-based actions.  Recognizing the
`dialog acts`:dt: underlying the utterances in a dialog can be an
important first step in understanding the conversation.

The ``nps_chat`` corpus, which was demonstrated in Section
sec-extracting-text-from-corpora_, consists of over 10,000 posts from
instant messaging sessions.  These posts have all been labeled with
one of 15 dialog act types, such as "Statement," "Emotion,"
"ynQuestion", and "Continuer."  We can therefore use this data to
build a classifier that can identify the dialog act types for new
instant messaging posts.  The first step is to extract the basic
messaging data.  We will use ``xml_posts()`` to get a data structure
representing the XML annotation for each post:

>>> posts = nps_chat.xml_posts()[:10000]

Next, we'll define a simple feature extractor that checks what words
the post contains:

>>> def dialog_act_features(post):
...     features = {}
...     for word in nltk.word_tokenize(post):
...         features['contains(%s)' % word.lower()] = True
...     return features

Finally, we construct the training and testing data by applying the
feature extractor to each post (using ``post.get('class')`` to get
a post's dialog act type), and create a new classifier:

>>> featuresets = [(dialog_act_features(post.text), post.get('class'))
...                for post in posts]
>>> size = int(len(featuresets) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]
>>> classifier = nltk.NaiveBayesClassifier.train(train_set)
>>> print nltk.classify.accuracy(classifier, test_set)
0.61

.. note:: Authors' note:
   *We plan to extend this example by showing how a sequence
   classifier can be used to take advantage of the fact that 
   some sequences (eg ynquestion->yanswer) are more likely than
   others.*


Recognizing Textual Entailment
------------------------------

Recognizing textual entailment (RTE) is the task of determining
whether a given piece of text *T* entails another text called the
"hypothesis". To date, there have been four RTE Challenges, where
shared development and test data is made available to competing teams.
Here are a couple of examples of Text/Hypothesis pairs from the
Challenge 3 development dataset. The label *True* indicates that the
entailment holds, and *False*, that it fails to hold.

    Challenge 3, Pair 34 (True)
    
      **T**: Parviz Davudi was representing Iran at a meeting of the Shanghai
      Co-operation Organisation (SCO), the fledgling association that
      binds Russia, China and four former Soviet republics of central
      Asia together to fight terrorism.
    
      **H**: China is a member of SCO.

    Challenge 3, Pair 81 (False)

      **T**: According to NC Articles of Organization, the members of LLC
      company are H. Nelson Beavers, III, H. Chester Beavers and Jennie
      Beavers Stewart.

      **H**: Jennie Beavers Stewart is a share-holder of Carolina Analytical
      Laboratory.

It should be emphasized that the relationship between Text and
Hypothesis is not intended to be logical entailment, but rather
whether a human would conclude that the Text provides reasonable
evidence for taking the Hypothesis to be true.

We can treat RTE as a classification task, in which we try to
predict the *True*\ /*False* label for each pair. Although it seems
likely that successful approaches to this task will involve a
combination of parsing, compositional and lexical semantics and real world
knowledge, many early attempts at RTE achieved reasonably good results
with shallow analysis, based on similarity between the Text and
Hypothesis at the word level. In the ideal case, we would expect that if
there is an entailment, then all the information expressed by the Hypothesis
should also be present in the Text. Conversely, if there is information
found in the Hypothesis that is absent from the Text, then there
will be no entailment. We let words (i.e., word types) go proxy for information, and
our features count the degree of word overlap, and the degree to which
there are words in the Hypothesis but not in the Text (captured by the
method ``hyp_extra()``). Not all words are equally important |mdash|
Named Entity mentions such as the names of people, organizations and
places are likely to be more significant, which motivates us to
extract distinct information for ``word``\ s  and ``ne``\ s (Named
Entities). In addition, some high frequency function words are
filtered out as "stopwords".

.. pylisting:: rte_features
   :caption: Missing Title

    def rte_features(rtepair):
	extractor = RTEFeatureExtractor(rtepair)
	features['word_overlap'] = len(extractor.overlap('word'))
	features['word_hyp_extra'] = len(extractor.hyp_extra('word'))
	features['ne_overlap'] = len(extractor.overlap('ne'))
	features['ne_hyp_extra'] = len(extractor.hyp_extra('ne'))
	return features 

To illustrate the content of these features, we examine some
attributes of the Text/Hypothesis Pair 34 shown earlier:

    >>> rtepair = nltk.corpus.rte.pairs(['rte3_dev.xml'])[33]
    >>> extractor = RTEFeatureExtractor(rtepair)
    >>> print extractor.text_words
    set(['Russia', 'Organisation', 'Shanghai', 'Asia', 'four', 'at',
    'operation', 'SCO', ...])
    >>> print extractor.hyp_words
    set(['member', 'SCO', 'China'])
    >>> print extractor.overlap('word')
    set([])
    >>> print extractor.overlap('ne')
    set(['SCO', 'China'])
    >>> print extractor.hyp_extra('word')
    set(['member'])  

These features indicate that all important words in the Hypothesis are
contained in the Text, and thus there is some evidence for labeling this
as *True*.

.. need to decide how to import RTEFeatureExtractor

The module ``nltk.classify.rte_classify`` reaches just over 58%
accuracy on the combined RTE test data using methods like these. Although
this figure is not very impressive, it is surprisingly hard to achieve
much better results without considerable effort, and the performance
is comparable with that achieved by systems submitted for RTE
Challenge 1.

.. check the official challenge results

Scaling Up to Large Data Sets
-----------------------------

Python provides an excellent environment for performing basic text
processing and feature extraction.  However, it is not able to perform
the numerically intensive calculations required by machine learning
methods nearly as quickly as lower-level languages such as C.  Thus,
if you attempt to use the pure-Python machine learning implementations
(such as ``nltk.NaiveBayesClassifier``) on large data sets, you may
find that the learning algorithm takes an unreasonable amount of time
and memory to complete.

If you plan to train classifiers with large amounts of training data
or a large number of features, we recommend that you make use of
|NLTK|\ 's facilities for interfacing with external machine learning
packages.  Once these packages have been installed, |NLTK| can
transparently make use of them (via system calls) to train classifier
models significantly faster than the pure-Python classifier
implementations.  See the |NLTK| webpage or a list of recommended
machine learning packages that are supported by |NLTK|.

.. SB: I think numpy and other numerical libraries do their work in C, and
       are probably quite efficient.  Not clear about support for sparse arrays.
.. EL: Yes, but even with numpy etc, python is *slow* for this stuff.
       I speak from experience. :)

.. _sec-evaluation:

----------
Evaluation
----------

In order to decide whether a classification model is accurately
capturing a pattern, we must evaluate that model.  The result of this
evaluation is important for deciding how trustworthy the model is, and
for what purposes we can use it.  Evaluation can also be a useful tool
for guiding us in making future improvements to the model.

The Evaluation Set
------------------

Most evaluation techniques calculate a score for a model by comparing
the labels that it generates for the inputs in an `evaluation set`:dt:
with the correct labels for those inputs.  This evaluation set
typically has the same format as the training corpus.  However, it is
very important that the evaluation set be distinct from the training
corpus: if we simply re-used the training corpus as the evaluation
set, then a model that simply memorized its input, without learning
how to generalize to new examples, would receive misleadingly high
scores.

When building the evaluation set, there is often a trade-off between
the amount of data that can be used for testing and the amount of data
can be used for training.  For classification tasks that have a small
number of well-balanced labels and a diverse evaluation set, a
meaningful evaluation can be performed with as few as 100 evaluation
instances.  But if a classification task has a large number of labels,
or includes very infrequent labels, then the size of the evaluation
set should be chosen to ensure that the least frequent label occurs at
least 50 times.  Additionally, if the evaluation set contains many
closely related instances |mdash| such as instances drawn from a single
document |mdash| then the size of the evaluation set should be increased to
ensure that this lack of diversity does not skew the evaluation
results.  When large amounts of annotated data are available, it is
common to err on the side of safety by using 10% of the overall data
for evaluation.

Another consideration when choosing the evaluation set is the degree
of similarity between instances in the evaluation set and those in the
development set.  The more similar these two data sets are, the less
confident we can be that evaluation results will generalize to other
data sets.  For example, consider the part-of-speech tagging task.  At
one extreme, we could create the training set and evaluation set by
randomly assigning sentences from a data source that reflects a single
genre (news):

>>> tagged_sents = nltk.corpus.brown.tagged_sents(categories='news')
>>> random.shuffle(tagged_sents)
>>> size = int(len(tagged_sents) * 0.1)
>>> train_set, test_set = featuresets[size:], featuresets[:size]

In this case, our evaluation set will be *very* similar to our test
set.  The training set and evaluation set are taken from the same
genre, and so we can not be confident that evaluation results would
generalize to other genres.  What's worse, because of the call to
``random.shuffle()``, the evaluation set contains sentences that are
taken from the same documents that were used for training.  If there
is any consistent pattern within a document (say, if a given word gets
used with a particular part-of-speech tag especially frequently), then
that difference will be reflected in both the development set and the
evaluation set.  A somewhat better approach is to at least ensure that
the training set and test set are taken from different documents:

>>> file_ids = nltk.corpus.brown.files(categories='news')
>>> size = int(len(tagged_sents) * 0.1)
>>> train_set = nltk.corpus.brown.tagged_sents(file_ids[size:])
>>> test_set = nltk.corpus.brown.tagged_sents(file_ids[size:])

If we want to perform a more stringent evaluation, we can draw the
evaluation set from documents that are less strongly related to those
in the training set:

>>> train_set = nltk.corpus.brown.tagged_sents(categories='news')
>>> test_set = nltk.corpus.brown.tagged_sents(categories='fiction')

If we build a classifier that performs well on this evaluation set,
then we can be confident that it has the power to generalize well
beyond the data that it was trained on.

The Development Set
-------------------

When actively developing a model by adjusting the features that it
uses or by hand-tuning any parameters, it is advisable to make use of
a second test set.  The test set, known as the `development set`:dt:,
is used while developing the model, to evaluate whether specific
changes to the model are beneficial.  However, once we've made use of
the development set to help develop the model, we can no longer trust
that it will give us an accurate idea of how well the model would
perform on new data.  We therefore save the evaluation set until our
model development is complete, at which point we can use it to check
how well our model will perform on new input values.

Accuracy
--------

The simplest metric that can be used to evaluate a classifier,
`accuracy`:dt:, measures the percentage of inputs in the evaluation
set that the classifier correctly labeled.  For example, a name gender
classifier that predicts the correct name 60 times in an evaluation
set containing 80 names would have an accuracy of 60/80 = 75%.  The
function ``nltk.classify.accuracy`` can be used to calculate the
accuracy of a classifier model on a given evaluation set:

    >>> classifier = nltk.NaiveBayesClassifier.train(train)
    >>> print 'Accuracy: %4.2f' % nltk.classify.accuracy(classifier, test)
    0.75

When interpreting the accuracy score of a classifier, it is important
to take into consideration the frequencies of the individual class
labels in the evaluation set.  For example, consider a classifier that
determines the correct word sense for each occurrence of the word
"bank."  If we evaluate this classifier on financial newswire text,
then we may find that the ``financial-institution`` sense is used 19
times out of 20.  In that case, an accuracy of 95% would hardly be
impressive, since we could achieve that accuracy with a model that
always returns the ``financial-institution`` sense.  However, if we
instead evaluate the classifier on a more balanced corpus, where the
most frequent word sense has a frequency of 40%, then a 95% accuracy
score would be a much more positive result.

Precision and Recall
--------------------

Another instance where accuracy scores can be misleading is in
"search" tasks, such as information retrieval, where we are attempting
to find documents that are relevant to a particular task.  Since the
number of irrelevant documents far outweighs the number of relevant
documents, the accuracy score for a model that labels every document
as irrelevant would be very close to 100%.  

.. _fig-precision-recall:
.. figure:: ../images/precision-recall.png
   :scale: 25:120:30

   True and False Positives and Negatives

It is therefore conventional to use a different set of measures for
search tasks, based on the number of items in each of the four
categories shown in Figure fig-precision-recall_:

- `True positives`:dt: are relevant items that we correctly identified
  as relevant.
- `True negatives`:dt: are relevant items that we correctly identified
  as irrelevant.
- `False positives`:dt: (or `Type I errors`:dt:) are irrelevant items
  that we incorrectly identified as relevant.
- `False negatives`:dt: (or `Type II errors`:dt:) are relevant items
  that we incorrectly identified as irrelevant.

Given these four numbers, we can define the following metrics:

- `Precision`:dt:, which indicates how many of the items that we
  identified were relevant, is `TP/(TP+FP)`:math:.
- `Recall`:dt:, which indicates how many of the relevant items that we
  identified, is `TP/(TP+FN)`:math:.
- The `F-Measure`:dt: (or `F-Score`:dt:), which combines the precision
  and recall to give a single score, is defined to be the harmonic
  mean of the precision and recall: `2/(1/Precision+1/Recall)`:math:.

Confusion Matrices
------------------

When performing classification tasks with three or more labels, it can
be informative to subdivide the errors made by the model based on
which types of mistake it made.  A `confusion matrix`:dt: is a table
where each cell `[i,j]`:math: indicates how often label `j`:math: was
predicted when the correct label was `i`:math:.  Thus, the diagonal
entries (i.e., cells `[i,i]`:math:) indicate labels that were
correctly predicted; and the off-diagonal entries indicate errors.  In
the following example, we generate a confusion matrix for the unigram
tagger developed in Section sec-automatic-tagging_:

>>> def tag_list(tagged_sents):
...     return [tag for sent in tagged_sents for (word, tag) in sent]
>>> def apply_tagger(tagger, corpus):
...     return [tagger.tag(tag.untag(sent)) for sent in corpus]
>>> gold = tag_list(brown.tagged_sents(categories='editorial'))
>>> test = tag_list(apply_tagger(t2, brown.tagged_sents(categories='editorial')))
>>> cm = nltk.ConfusionMatrix(gold, test)
>>> print cm.pp(sort_by_count=True, show_percents=True, truncate=9)
    |                                                N                 
    |      N      I      A      J                    N      V      C   
    |      N      N      T      J      .      ,      S      B      C   
----+------------------------------------------------------------------
 NN | <11.8%>  0.0%      .   0.2%      .      .   0.0%   0.3%      .   
 IN |   0.0%  <9.0%>     .      .      .      .   0.0%      .   0.0%   .
 AT |      .      .  <8.6%>     .      .      .      .      .      .   
 JJ |   1.6%      .      .  <4.0%>     .      .      .   0.0%      .   
  . |      .      .      .      .  <4.8%>     .      .      .      .   .
  , |      .      .      .      .      .  <4.4%>     .      .      .   
NNS |   1.5%      .      .      .      .      .  <3.2%>     .      .   
 VB |   0.9%      .      .   0.0%      .      .      .  <2.4%>     .   .
 CC |   0.0%   0.0%      .      .      .      .      .      .  <3.0%>  
 ...|                    .         .        .
----+-----------------------------------------------------------------
(row = reference; col = test)

The confusion matrix indicates that common errors include a
substitution of NN for JJ (for 1.6% of words), and of NN for NNS (for
1.5% of words).  Note that periods (``.``) are used to indicate cells
whose value is 0; and that the diagonal entries, which correspond to
correct classifications, are marked with angle brackets.

Cross-Validation
----------------

In order to evaluate our models, we must reserve a portion of the
annotated data for the test set.  If the test set is too small, then
our evaluation may not be accurate.  However, making the test set
larger usually means making the training set smaller, which can have a
significant impact on performance if a limited amount of annotated
data is available.  

One solution to this problem is to perform multiple evaluations on
different test sets, and then to combine the scores from those
evaluations.  This technique is known as `cross-validation`:dt:.  In
particular, we subdivide the original corpus into `N`:math:
sub-corpora.  For each of these sub-corpora, we train a model using all
of the data *except* the data in that sub-corpus; and then test that
model on the sub-corpus.  Even though the individual sub-corpora might
be too small to give accurate evaluation scores on their own, the
combined evaluation score is based on a large amount of data, and is
therefore quite reliable.

A second, and equally important, advantage of using cross-validation
is that it allows us to examine how widely the performance varies
across different training sets.  If we get very similar scores for all
`N`:math: training sets, then we can be fairly confident that the
score is accurate.  On the other hand, if the scores of the `N`:math:
training sets varies wildly, then we should probably be skeptical
about the accuracy of the evaluation score.

.. mention statistical significance tests explicitly here?

Error Analysis
--------------

The metrics above give us a general feel for how well a system does,
but doesn't tell us much about why it gets that performance |mdash| are
there patterns in what it gets wrong?  If so, that can help us to
improve the system, or if we can't improve it, then at least make us
more aware of what the limitations of the system are, and what kind of
data it will produce more reliable or less reliable results for.

Error analysis should always be performed on a development sub-corpus,
separate from the evaluation set, to ensure that the test set can
still be used to evaluate the model.  The first step in performing
error analysis is to run the model on the development sub-corpus, and
print out information about the inputs where the model generated the
incorrect label.  This listing can then be examined, in an attempt to
find patterns in the errors made by the model, to determine what
additional pieces of information would allow it to make the right
decision, or which existing pieces of information are tricking it into
making the wrong decision.  The feature set can then be adjusted
accordingly, and the error analysis repeated until no more progress is
made.

.. note:: Authors' note: *We plan to add an explicit example of error
    analysis here, based on the part-of-speech tagging classifier.*

.. _classification-methods:

----------------------
Classification Methods
----------------------

In the next three sections, we'll take a closer took at three machine
learning methods that can be used to automatically build
classification models: Decision Trees, Naive Bayes classifiers, and
Maximum Entropy classifiers.  As we've seen, it's possible treat these
learning methods as black boxes, simply training models and using them
for prediction without understanding how they work.  But there's a lot
to be learned from taking a closer look at how these learning methods
select models based on the data in a training corpus.  An
understanding of these methods can help guide our selection of
appropriate features, and especially our decisions about how those
features should be encoded.  And an understanding of the generated
models can allow us to extract useful information about which features
are most informative, and how those features relate to one another.

--------------
Decision Trees
--------------

.. Note that they haven't necessarily seen syntax trees before this, so
   it may seem odd to them (or at least not obvious) that these "trees"
   are upside down.

A `decision tree`:dt: is a tree-structured flowchart used to choose
labels for input values.  This flowchart consists of `decision
nodes`:dt:, which check feature values, and `leaf nodes`:dt:, which
assign labels.  To choose the label for an input value, we begin at
the flowchart's initial decision node, known as its `root node`:dt:.
This node contains a condition that checks one of the input value's
features, and selects a branch based on that feature's value.
Following the branch that describes our input value, we arrive at a
new decision node, with a new condition on the input value's features.
We continue following the branch selected by each node's condition,
until we arrive at a leaf node, which provides a label for the input
value.  Figure fig-decision-tree_ shows an example decision tree model for
the name gender task.

.. _fig-decision-tree:
.. figure:: ../images/decision-tree.png
   :scale: 50:120:50

   Decision Tree model for the name gender task.  Note that tree
   diagrams are conventionally drawn "upside down," with the root at the
   top, and the leaves at the bottom.

..   XXX add yes/no labels to this figure

.. XXX show: train decision tree, then print it as pseudocode & as more
   compact tree.

Once we have a decision tree, it is thus fairly straightforward to
use it to assign labels to new input values.  What's less straightforward
is how we can build a decision tree that models a given
training corpus.  But before we look at the learning algorithm for
building decision trees, we'll consider a simpler task: picking the
best "decision stump" for a corpus.  A `decision stump`:dt: is a
decision tree with a single node that decides how to classify inputs
based on a single feature.  It contains one leaf for each possible
feature value, specifying the class label that should be assigned to
inputs whose features have that value.  In order to build a decision
stump, we must first decide which feature should be used.  The
simplest method is to just build a decision stump for each possible
feature, and see which one achieves the highest accuracy on the
training data; but we'll discuss some other alternatives below.  Once
we've picked a feature, we can build the decision stump by assigning a
label to each leaf based on the most frequent label for the selected
examples in the training corpus (i.e., the examples where the selected
feature has that value).

.. XX show building a decision stump
.. XX show refining a decision stump's leaf

Given the algorithm for choosing decision stumps, the algorithm for
growing larger decision trees is straightforward.  We
begin by selecting the overall best decision stump for the corpus.  We
then check the accuracy of each of the leaves on the training corpus.
Any leaves that do not achieve sufficiently good accuracy are then
replaced by new decision stumps, trained on the subset of the training
corpus that is selected by the path to the leaf.  For example, we
could grow the decision tree in Figure fig-decision-tree_ by replacing the
leftmost leaf with a new decision stump, trained on the subset of the
training corpus names that do not start with a "k" or end with a vowel
or an "l."

Entropy and Information Gain
----------------------------

As was mentioned before, there are a number of methods that can be
used to select the most informative feature for a decision stump.  One
popular alternative is to use `information gain`:dt:, a measure of how
much more organized the input values become when we divide them up
using a given feature.  To measure how disorganized the original set
of input values are, we calculate entropy of their labels, which will
be high if the input values have highly varied labels, and low if many
input values all have the same label.  In particular, entropy is
defined as the sum of the probability of each label times the log
probability of that same label:

:math:`H` = :math:`|sum|`\ :sub:`l |in| labels`\
:math:`P(l)` |times| :math:`log`\ :sub:`2`\ :math:`P(l)`.

.. _fig-entropy:
.. figure:: ../images/Binary_entropy_plot.png
   :scale: 50:150:150

   The entropy of labels in the name gender prediction task, as a
   function of the percentage of names in a given set that are male.

For example, Figure fig-entropy_ shows how the entropy of labels in
the name gender prediction task depends on the ratio of male to female
names.  Note that if most input values have the same label (e.g., if
P(male) is near 0 or near 1), then entropy is be low.  In particular,
labels that have low frequency do not contribute much to the entropy
(since :math:`P(l)` is low); and labels with high frequency also do
not contribute much to the entropy (since :math:`log`\ :sub:`2`\
:math:`P(l)` is be low).  On the other hand, if the input values have
a wide variety of labels, then there are many labels with a "medium"
frequency, where neither :math:`P(l)` nor :math:`log`\ :sub:`2`\
:math:`P(l)` is low, so the entropy is high.

.. pylisting:: entropy
   :caption: Missing Title

    import math
    def entropy(labels):
       probs = [freqdist.freq(l) for l in nltk.FreqDist(labels)]
       return -sum([p * math.log(p,2) for p in probs])

    >>> entropy(['male', 'male', 'male', 'male'])
    0.0
    >>> entropy(['male', 'female', 'male', 'male'])
    0.811278124459
    >>> entropy(['female', 'male', 'female', 'male'])
    1.0
    >>> entropy(['female', 'female', 'male', 'female'])
    0.811278124459
    >>> entropy(['female', 'female', 'female', 'female'])
    0.0

.. XX show: print entropy of decision stump etc

Once we have calculated the entropy of the original set of input
values' labels, we can figure out how much more organized the labels
become once we apply the decision stump.  To do so, we calculate the
entropy for each of the decision stump's leaves, and take the average
of those leaf entropy values (weighted by the number of samples in
each leaf).  The information gain is then equal to the original
entropy minus this new, reduced entropy.  The higher the information
gain, the better job the decision stump does of dividing the input
values into coherent groups, so we can build decision trees by
selecting the decision stumps with the highest information gain.

Another consideration for decision trees is efficiency.  The simple
algorithm for selecting decision stumps described above must construct
a candidate decision stump for every possible feature; and this
process must be repeated for every node in the constructed decision
tree.  A number of algorithms have been developed to cut down on the
training time by storing and reusing information about previously
evaluated examples.  <<references>>.

Decision trees have a number of useful qualities.  To begin with,
they're simple to understand, and easy to interpret.  This is
especially true near the top of the decision tree, where it is usually
possible for the learning algorithm to find very useful features.
Decision trees are especially well suited to cases where many
hierarchical categorical distinctions can be made.  For example,
decision trees can be very effective at modeling phylogeny trees.

However, decision trees also have a few disadvantages.  One problem is
that, since each branch in the decision tree splits the training data,
the amount of training data available to train nodes lower in the tree
can become quite small.  As a result, these lower decision nodes may
`overfit`:dt: the training corpus, learning patterns that reflect
idiosyncrasies of the training corpus, rather than genuine patterns in
the underlying problem.  One solution to this problem is to stop
dividing nodes once the amount of training data becomes too small.
Another solution is to grow a full decision tree, but then to
`prune`:dt: decision nodes that do not improve performance on a
development corpus.

A second problem with decision trees is that they force features to be
checked in a specific order, even when features may act relatively
independently of one another.  For example, when classifying documents
into topics (such as sports, automotive, or murder mystery), features
such as ``hasword(football)`` are highly indicative of a specific
label, regardless of what other the feature values are.  Since there
is limited space near the top of the decision tree, most of these
features will need to be repeated on many different branches in the
tree.  And since the number of branches increases exponentially as we
go down the tree, the amount of repetition can be very large.

A related problem is that decision trees are not good at making use of
features that are weak predictors of the correct label.  Since these
features make relatively small incremental improvements, they tend to
occur very low in the decision tree.  But by the time the decision
tree learner has descended far enough to use these features, there is
not enough training data left to reliably determine what effect they
should have.  If we could instead look at the effect of these features
across the entire training corpus, then we might be able to make some
conclusions about how they should affect the choice of label.

The fact that decision trees require that features be checked in a
specific order limits their ability to make use of features that are
relatively independent of one another.  The Naive Bayes classification
method, which we'll discuss next, overcomes this limitation by
allowing all features to act "in parallel."

-----------------------
Naive Bayes Classifiers
-----------------------

In `Naive Bayes`:dt: classifiers, every feature gets a say in
determining which label should be assigned to a given input value.  To
choose a label for an input value, the Naive Bayes classifier begins
by calculating the `prior probability`:dt: of each label, which is
determined by checking frequency of each label in the training corpus.
The contribution from each feature is then combined with this prior
probability, to arrive at a likelihood estimate for each label.  The
label whose likelihood estimate is the highest is then assigned to the
input value.  Figure fig-naive-bayes-triangle_ illustrates this process.

.. I go back and forth on whether we should include a figure like this
   one.  I think it gives a good high-level feeling of what's going
   on, but the details don't really line up with the algorithm's 
   specifics, and it takes a good amount of work to explain the figure.

.. _fig-naive-bayes-triangle:
.. figure:: ../images/naive-bayes-triangle.png
   :scale: 30:100:30

   An abstract illustration of the procedure used by the Naive Bayes
   classifier to choose the topic for a document.  In the training
   corpus, most documents are automotive, so the classifier starts out
   at a pointer closer to the "automotive" label.  But it then
   considers the effect of each feature.  In this example, the input
   document contains the word "dark," which is a weak indicator for
   murder mysteries; but it also contains the word "football," which
   is a strong indicator for sports documents.  After every feature
   has made its contribution, the classifier checks which label it is
   closest to, and assigns that label to the input.

Individual features make their contribution to the overall decision by
"voting against" labels that don't occur with that feature very often.
In particular, the likelihood score for each label is reduced by
multiplying it by the probability that an input value with that label
would have the feature.  For example, if the word "run" occurs in 12%
of the sports documents, 10% of the murder mystery documents, and 2%
of the automotive documents, then the likelihood score for the sports
label will be multiplied by 0.12; the likelihood score for the murder
mystery label will be multiplied by 0.1; and the likelihood score for
the automotive label will be multiplied by 0.02.  The overall effect
will be to reduce the score of the murder mystery label slightly more
than the score of the sports label; and to significantly reduce the
automotive label with respect to the other two labels.  This overall
process is illustrated in Figure fig-naive-bayes-bargraph_.

.. _fig-naive-bayes-bargraph:
.. figure:: ../images/naive_bayes_bargraph.png
   :scale: 30:120:30

   Calculating label likelihoods with Naive Bayes.  Naive Bayes begins
   by calculating the prior probability of each label, based on how
   frequently each label occurs in the training data.  Every feature
   then contributes to the likelihood estimate for each label, by
   multiplying it by the probability that input values with that label
   will have that feature.  The resulting likelihood score can be
   thought of as an estimate of the probability that a randomly
   selected value from the training corpus would have both the given
   label and the set of features, assuming that the feature
   probabilities are all independent.

Underlying Probabilistic Model
------------------------------

Another way of understanding the Naive Bayes classifier is that it
chooses the most likely label for an input, under the assumption that
every input value is generated by first choosing a class label for
that input value, and then generating each feature, entirely
independent of every other feature.  Of course, this assumption is
unrealistic: features are often highly dependent on one another in
ways that don't just reflect differences in the class label.  We'll
return to some of the consequences of this assumption at the end of
this section.  But making this simplifying assumption makes it much
easier to combine the contributions of the different features, since
we don't need to worry about how they should interact with one
another.

.. _fig-naive-bayes-graph:
.. figure:: ../images/naive_bayes_graph.png
   :scale: 30:120:30

   A `Bayesian Network Graph`:dt: illustrating the generative process
   that is assumed by the Naive Bayes classifier.  To generate a
   labeled input, the model first chooses a label for the input; and
   then it generates each of the input's features based on that label.
   Every feature is assumed to be entirely independent of every other
   feature, given the label.

Based on this assumption, we can calculate an expression for
:math:`P(label|features)`, the probability that an input will have a
particular label, given that it has a particular set of features.  To
choose a label for a new input, we can then simply pick the label
:math:`l` that maximizes :math:`P(l|features)`.

To begin, we note that :math:`P(label|features)` is equal to the
probability that an input has a particular label *and* the specified
set of features, divided by the probability that it has the specified
set of features:

:math:`P(label|features) = P(features, label)/P(features)`

Next, we note that :math:`P(features)` will be the same for every
choice of label, so if we are simply interested in finding the most
likely label, it suffices to calculate :math:`P(features, label)`,
which we'll call the label likelihood.

.. Note:: If we want to generate a probability estimate for each
   label, rather than just choosing the most likely label, then the
   easiest way to compute P(features) is to simply calculate the sum
   over labels of P(features, label):

   :math:`P(features)` =
   :math:`|sum|`\ 
   :sub:`l \in| labels` \
   :math:`P(features, label)``

The label likelihood can be expanded out as the probability of the
label times the probability of the features given the label:

`P(features, label) = P(label)`:math: |times| `P(features|label)`:math:

Furthermore, since the features are all independent of one another
(given the label), we can separate out the probability of each
individual feature:

:math:`P(features, label)` = :math:`P(label)` |times| |prod|\ :sub:`f \in| features`\ :math:`P(f|label)``

This is exactly the equation we discussed above for calculating the
label likelihood: :math:`P(label)` is the prior probability for a
given label, and each :math:`P(f|label)` is the contribution of a single
feature to the label likelihood.

Zero Counts and Smoothing
-------------------------

The simplest way to calculate :math:`P(f|label)`, the contribution of a
feature `f` toward the label likelihood for a label `label`, is to
take the percentage of training instances with the given label that
also have the given feature:

:math:`P(f|label) = count(f, label) / count(label)`

However, this simple approach can become problematic when a feature
*never* occurs with a given label in the training corpus.  In this
case, our calculated value for :math:`P(f|label)` will be zero, which will
cause the label likelihood for the given label to be zero.  Thus, the
input will never be assigned this label, regardless of how well the
other features fit the label.

The basic problem here is with our calculation of :math:`P(f|label)`, the
probability that an input will have a feature, given a label.  In
particular, just because we haven't seen a feature/label combination
occur in the training corpus, doesn't mean it's impossible for that
combination to occur.  For example, we may not have seen any murder
mystery documents that contained the word "football," but we wouldn't
want to conclude that it's completely impossible for such documents to
exist.  

Thus, although :math:`count(f,label)/count(label)` is a good estimate for
:math:`P(f|label)` when :math:`count(f, label)` is relatively high, this
estimate becomes less reliable when :math:`count(f)` becomes smaller.
Therefore, when building Naive Bayes models, we usually make use of
more sophisticated techniques, known as `smoothing`:dt: techniques,
for calculating :math:`P(f|label)`, the probability of a feature given a
label.  For example, the "Expected Likelihood Estimation" for the
probability of a feature given a label basically adds 0.5 to each
:math:`count(f,label)` value; and the "Heldout Estimation" uses a heldout
corpus to calculate the relationship between feature frequencies and
feature probabilities.  For more information on smoothing techniques,
see <<ref -- manning & schutze?>>.

Non-Binary Features
-------------------

We have assumed here that each feature is binary -- in other words
that each input either has a feature or does not.  Label-valued
features (e.g., a color feature which could be red, green, blue,
white, or orange) can be converted to binary features by replacing
them with features such as "color-is-red".  Numeric features can be
converted to binary features by `binning`:dt:, which replaces them with
features such as "4<x<6".  

Another alternative is to use regression methods to model the
probabilities of numeric features.  For example, if we assume that the
height feature has a bell curve distribution, then we could estimate
P(height|label) by finding the mean and variance of the heights of the
inputs with each label.  In this case, :math:`P(f=v|label)` would not
be a fixed value, but would vary depending on the value of `v`.

The Naivete of Independence
---------------------------

The reason that Naive Bayes classifiers are called "naive" is that
it's unreasonable to assume that all features are independent of one
another (given the label).  In particular, almost all real-world
problems contain features with varying degrees of dependence on one
another.  If we had to avoid any features that were dependent on one
another, it would be very difficult to construct good feature sets
that provide the required information to the machine learning
algorithm.

So what happens when we ignore the independence assumption, and use
the Naive Bayes classifier with features that are not independent?
One problem that arises is that the classifier can end up
"double-counting" the effect of highly correlated features, pushing
the classifier closer to a given label than is justified.

To see how this can occur, consider a name gender classifier that
contains two identical features, :math:`f`\ :sub:`1` and :math:`f`\
:sub:`2`.  In other words, :math:`f`\ :sub:`2` is an exact copy of
:math:`f`\ :sub:`1`, and contains no new information.  Nevertheless,
when the classifier is considering an input, it will include the
contribution of both :math:`f`\ :sub:`1` and :math:`f`\ :sub:`2` when
deciding which label to choose.  Thus, the information content of
these two features is given more weight than it should be.

Of course, we don't usually build Naive Bayes classifiers that contain
two identical features.  However, we do build classifiers that contain
features which are dependent on one another.  For example, the
features ``ends-with(a)`` and ``ends-with(vowel)`` are dependent on
one another, because if an input value has the first feature, then it
must also have the second feature.  For features like these, the
duplicated information may be given more weight than is justified by
the training corpus.

The Cause of Double-Counting
----------------------------

The basic problem that causes this double-counting issue is that
during training, feature contributions are computed separately; but
when using the classifier to choose labels for new inputs, those
feature contributions are combined.  One solution, therefore, is to
consider the possible interactions between feature contributions
during training.  We could then use those interactions to adjust the
contributions that individual features make.

To make this more precise, we can rewrite the equation used to
calculate out the likelihood of a label, separating out the
contribution made by each feature (or label):

.. .. _parameterized-naive-bayes:

:math:`P(features, label) = w[label]` |times| |prod|\ :sub:`f |in| features` :math:`w[f, label]`

Here, :math:`w[label]` is the "starting score" for a given label; and
:math:`w[f, label]` is the contribution made by a given feature
towards a label's likelihood.  We call these values :math:`w[label]`
and :math:`w[f, label]` the `parameters`:dt: or `weights`:dt: for the
model.  Using the Naive Bayes algorithm, we set each of these
parameters independently:

:math:`w[label] = P(label)`

:math:`w[f, label] = P(f|label)`

However, in the next section, we'll look at a classifier that
considers the possible interactions between these parameters when
choosing their values.

---------------------------
Maximum Entropy Classifiers
---------------------------

The `Maximum Entropy`:dt: classifier uses a model that is very similar
to the model used by the Naive Bayes classifier.  But rather than
using probabilities to set the model's parameters, it uses search
techniques to find a set of parameters that will maximize the
performance of the classifier.  In particular, it looks for the set of
parameters that maximizes the `total likelihood`:dt: of the training
corpus, which is defined as:

``\sum_{(x) in corpus} P(label(x)|features(x))``

Where ``P(label|features)``, the probability that an input whose
features are ``features`` will have class label ``label``, is defined as:
                              
`P(label|features) = P(label, features) / sum_{label} P(label, features)`

Because of the potentially complex interactions between the effects of
related features, there is no way to directly calculate the model
parameters that maximize the likelihood of the training corpus.
Therefore, Maximum Entropy classifiers choose the model parameters
using `iterative optimization`:dt: techniques, which initialize the
model's parameters to random values, and then repeatedly refine those
parameters to bring them closer to the optimal solution.  These
iterative optimization techniques guarantee that each refinement of
the parameters will bring them closer to the optimal values; but do
not necessarily provide a means of determining when those optimal
values have been reached.  Because the parameters for Maximum Entropy
classifiers are selected using iterative optimization techniques, they
can take a long time to learn.  This is especially true when the size
of the training corpus, the number of features, and the number of
labels are all large.

.. note:: Some iterative optimization techniques are much faster than
   others.  When training Maximum Entropy models, avoid the use of
   Generalized Iterative Scaling (GIS) or Improved Iterative Scaling
   (IIS), which are both considerably slower than the Conjugate
   Gradient (CG) and the BFGS optimization methods.

.. For related work section:?
   The use of iterative optimization techniques to find the parameters
   that maximize the performance of a model is quite common in machine
   learning.  

The Maximum Entropy Model
-------------------------

The Maximum Entropy classifier model is a generalization of the model
used by the Naive Bayes classifier.  Like the Naive Bayes model, the
Maximum Entropy classifier calculates the likelihood of each label for
a given input value by multiplying together the parameters that are
applicable for the input value and label.  The Naive Bayes classifier
model defines a parameter for each label, specifying its prior
probability; and a parameter for each (feature, label) pair,
specifying the contribution of individual features towards a label's
likelihood.

In contrast, the Maximum Entropy classifier model leaves it up to the
user to decide what combinations of labels and features should receive
their own parameters.  In particular, it is possible to use a single
parameter to associate a feature with more than one label; or to
associate more than one feature with a given label.  This can
sometimes be used to allow the model to "generalize" over some of the
differences between related labels or features.

Each combination of labels and features that receives its own
parameter is called a `joint-feature`:dt:.  Note that joint-features
are properties of *labeled* values, whereas (simple) features are
properties of *unlabeled* values.  

.. note:: In literature that describes and discusses Maximum Entropy
   models, the term "features" is often used to refer to
   joint-features.  The term `contexts`:dt: is used to refer to what
   we have been calling (simple) features.

Typically, the joint-features that are used to construct Maximum
Entropy models exactly mirror those that are used by the Naive Bayes
model.  In particular, a joint-feature is defined for each label,
corresponding to `w[label]`:math:; and for each combination of
(simple) feature and label, corresponding to `w[f, label]`:math:.
Given the joint-features for a Maximum Entropy model, the score
assigned to a label for a given input is simply the product of the
parameters associated with the joint-features that apply to that input
and label:

.. .. _parameterized-maxent:

:math:`P(input, label) =` |prod|\ :sub:`joint-features(input,label)` :math:`w[joint-feature]`

Maximizing Entropy
------------------

The intuition that motivates Maximum Entropy classification is that we
should build a model that captures the frequencies of individual
joint-features; but that does not make any unwarranted assumptions.
An example will help to illustrate this principle.

Suppose we are assigned the task of picking the correct word sense for
a given word, from a list of ten possible senses (labeled A-J).  At
first, we are not told anything more about the word or the senses.
There are many probability distributions that we could choose for the
ten senses, such as:

.. |d1| replace:: `(i)`
.. |d2| replace:: `(ii)`
.. |d3| replace:: `(iii)`
.. |d4| replace:: `(iv)`
.. |d5| replace:: `(v)`
.. |d6| replace:: `(vi)`
.. |d7| replace:: `(vii)`

.. table::

   ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====
        A     B     C     D     E     F     G     H     I     J  
   ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====
   |d1|  10%   10%   10%   10%   10%   10%   10%   10%   10%   10% 
   |d2|   5%   15%    0%   30%    0%    8%   12%    0%    6%   24%
   |d3|   0%  100%    0%    0%    0%    0%    0%    0%    0%    0%
   ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====

Although any of these distributions *might* be correct, we are likely
to choose distribution |d1|, because without any more information,
there is no reason to believe that any word sense is more likely than
any other.  On the other hand, distributions |d2| and |d3| reflect
assumptions that are not supported by what we know.

One way to capture this intuition that distribution |d1| is more "fair"
than the other two is to make use of the concept of entropy.  In the
discussion of decision trees, we described entropy as a measure of how
"disorganized" a set of labels was.  In particular, if a single label
dominates, then entropy is low; but if the labels are more evenly
distributed, then entropy is high.  In our example, we chose
distribution |d1| because the labels' probabilities are evenly
distributed -- in other words, because its entropy is high.  In
general, the `Maximum Entropy principle`:dt: states that, among the
distributions that are consistent with what we know, we should choose
the distribution whose entropy is highest.

Next, suppose that we are told that sense A is used 55% of the time.
Once again, there are many distributions that are consistent with this
new piece of information, such as:

.. table::

   ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====
        A     B     C     D     E     F     G     H     I     J  
   ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====
   |d4|  55%   45%    0%    0%    0%    0%    0%    0%    0%    0% 
   |d5|  55%    5%    5%    5%    5%    5%    5%    5%    5%    5% 
   |d6|  55%    3%    1%    2%    9%    5%    0%   25%    0%    0%
   ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====

But again, we will likely choose the distribution that makes the
fewest unwarranted assumptions -- in this case, distribution |d5|.

Finally, suppose that we are told that the word "up" appears in the
nearby context 10% of the time; and that when it does appear in the
context, there's an 80% chance that sense A or C will be used.  In
this case, we will have a harder time coming up with an appropriate
distribution by hand; however, we can verify that the following
distribution looks appropriate:

.. table::

   ==== ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====
             A     B     C     D     E     F     G     H     I     J  
   ==== ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====
   |d7| +up   5.1% 0.25%  2.9% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25% 0.25%
   ` `  -up  49.9% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46% 4.46%
   ==== ==== ===== ===== ===== ===== ===== ===== ===== ===== ===== =====

In particular, the distribution is consistent with what we know: if we
add up the probabilities in column A, we get 55%; if we add up the
probabilities of row 1, we get 10%; and if we add up the boxes for
senses A and C in the +up row, we get 8% (or 80% of the +up cases).
Furthermore, the remaining probabilities appear to be "evenly
distributed."

Throughout this example, we have restricted ourselves to distributions
that are consistent with what we know; and among these, we chose the
distribution with the highest entropy.  This is exactly what the
Maximum Classifier does as well.  In particular, for each
joint-feature, the Maximum Entropy model calculates the "empirical
frequency" of that feature -- i.e., the frequency with which it occurs
in the training corpus.  It then searches for the distribution which
maximizes entropy, while still predicting the correct frequency for
each joint-feature.  

Generative vs Conditional Classifiers
-------------------------------------

An important difference between the Naive Bayes classifier and the
Maximum Entropy classifier has to do with what they are attempting to
build a model for from the training corpus.  The Naive Bayes
classifier is an example of a `generative`:dt: classifier, which
builds a model that predict `P(input, label)`:math:, the joint
probability of a `(input, label)`:math: pair.  As a result, generative
models can be used to answer the following questions:

1. What is the most likely label for a given input?
2. How likely is a given label for a given input?
3. What is the most likely input value?
4. How likely is a given input value?
5. How likely is a given input value with a given label?
6. What is the most likely label for an input that might have one
   of two values (but we don't know which)?

The Maximum Entropy classifier, on the other hand, is an example of a
conditional classifier.  `Conditional`:dt: classifiers build models
that predict `P(label|input)`:math:, the probability of a label,
*given* the input value.  Thus, generative models can still be used to
answer questions 1 and 2.  However, generative models can *not* be
used to answer the remaining questions 3-6.

In general, generative models are strictly more powerful than
conditional models, since we can calculate the conditional probability
`P(label|input)`:math: from the joint probability `P(input,
label)`:math:, but not vice versa.  

However, this additional power comes at a price.  Because the model is
more powerful, it has more "free parameters" which need to be learned.
However, the size of the training corpus is fixed.  Thus, when using a
more powerful model, we end up with less data that can be used to
train each parameter's value, making it harder to find the best
parameter values.  As a result, a generative model may not do as good
a job at answering questions 1 and 2 as a conditional model, since the
conditional model can focus its efforts on those two questions.
However, if we do need answers to questions like 3-6, then we have no
choice but to use a generative model.

The difference between a generative model and a conditional model is
analogous to the difference between a topological map and a picture of
a skyline.  Although the topological map can be used to answer a wider
variety of questions, it is significantly more difficult to generate
an accurate topological map than it is to generate an accurate skyline.

.. note:: 
  I want a figure here.  But the images I used for this in the past
  (on powerpoint) are probably copyrighted, so I'll need to draw/find
  some new images.  They should be a side-by-side picture of a
  topological map and a skyline.  The left/right axis of each should
  be labeled as (output value), the up/down axis of each should be
  labeled as (probability), and the forward/back axis of the topo map
  should be (input value).

----------------------------
Modeling Linguistic Patterns
----------------------------

Classifiers can help us to understand the linguistic patterns that
occur in natural language corpora, by allowing us to create explicit
`models`:dt: that capture those patterns.  Typically, these models are
using supervised classification techniques; but it is also possible to
build analytically motivated models.  Either way, these explicit
models serve two important purposes: they help us to understand
linguistic patterns, and they can be used to make predictions about
new language data.

The extent to which explicit models can give us insight into
linguistic patterns depends largely on what kind of model is used.
Some models, such as decision trees, are relatively transparent, and
give us direct information about which factors are important in making
decisions, and about which factors are related to one another.  Other
models, such as multi-level neural networks, are much more opaque --
although it can be possible to gain insight by studying them, it
typically takes a lot more work.

But all explicit models can make predictions about new "`unseen`:dt:"
language data that was not included in the corpus used to build the
model.  These predictions can be evaluated to assess the accuracy of
the model.  Once a model is deemed sufficiently accurate, it can then
be used to automatically predict information about new language
data.  These predictive models can be combined into systems that
perform many useful language processing tasks, such as document
classification, automatic translation, and question answering.

What do models tell us?
-----------------------

It's important to understand what we can learn about language from an
automatically constructed model.  One important consideration when
dealing with models of language is the distinction between descriptive
models and explanatory models.  Descriptive models capture patterns in
the data but they don't provide any information about *why* the data
contains those patterns.  For example, as we saw in Table absolutely_,
the synonyms `absolutely`:lx: and `definitely`:lx: are not
interchangeable: we say `absolutely adore`:lx: not `definitely
adore`:lx:, and `definitely prefer`:lx: not `absolutely prefer`:lx:.
In contrast, explanatory models attempt to capture properties and
relationships that cause the linguistic patterns.  For example, we
might introduce the abstract concept of "polar adjective", as one that
has an extreme meaning, and categorize some adjectives like
`adore`:lx: and `detest`:lx: as polar.  Our explanatory model would
contain the constraint that `absolutely`:lx: can only combine with
polar adjectives, and `definitely`:lx: can only combine with non-polar
adjectives.  In summary, descriptive models provide information about
correlations in the data, while explanatory models go further to
postulate causal relationships.

Most models that are automatically constructed from a corpus are
descriptive models; in other words, they can tell us what features are
relevant to a given patterns or construction, but they can't
necessarily tell us how those features and patterns relate to one
another.  If our goal is to understand the linguistic patterns, then
we can use this information about which features are related as a
starting point for further experiments designed to tease apart the
relationships between features and patterns.  On the other hand, if
we're just interested in using the model to make predictions (e.g., as
part of a language processing system), then we can use the model to
make predictions about new data, without worrying about the precise
nature of the underlying causal relationships.


..       
  ------------
  Data Sources
  ------------

  Publishers: LDC, ELRA

  Annual competitions: CoNLL, TREC, CLEF


-------
Summary
-------

* Intuitions about how language works are unreliable.

* Data modeling helps us to understand the linguistic patterns, and
  can be used to make predictions about new language data.

* Automatically constructed models are typically descriptive, not
  explanatory.

* Supervised classification: training data, dev set, test set.

* Feature extraction

* Evaluation [don't test on train/train on test!]

* Decision trees

* Naive bayes

* Maxent

---------------
Further Reading
---------------

Consult [URL] for further materials on this chapter.

- choosing prepositions http://itre.cis.upenn.edu/~myl/languagelog/archives/002003.html
- Jurafsky and Martin
- publications describing some of the corpora that were created for the
  purpose of training and testing classifiers 

Xin Li, Dan Roth, Learning Question Classifiers: The Role of Semantic Information
http://l2r.cs.uiuc.edu/~danr/Papers/LiRo05a.pdf

[Abney2008]_

Current challenge: The problem of applying models trained on one genre
to another (portability across domains; domain adaptation);
Use of unsupervised or weakly supervised approaches to exploit a small
amount of in-domain training data, or of unlabeled in-domain training data
[REFS]

---------
Exercises
---------

#. |easy| Read up on one of the language technologies mentioned in this section, such as
   word sense disambiguation, semantic role labeling, question answering, machine translation,
   named entity detection.
   Find out what type and quantity of annotated data is required for developing such systems.
   Why do you think a large amount of data is required?

#. |easy| word sense disambiguation.

#. |easy|
   Exercise: compare the performance of different machine learning
   methods.
   
#. |easy|
   The synonyms `strong`:lx: and `powerful`:lx: pattern
   differently (try combining them with `chip`:lx: and `sales`:lx:).

#. |soso|
   Accessing extra features from WordNet to augment those that appear
   directly in the text (e.g. hypernym of any monosemous word)

#. |hard|
   Task involving PP Attachment data; predict choice of preposition
   from the nouns.

#. |hard| Suppose you wanted to automatically generate a prose description of a scene,
   and already had a word to uniquely describe each entity, such as `the jar`:lx:,
   and simply wanted to decide whether to use `in`:lx: or `on`:lx: in relating
   various items, e.g. `the book is in the cupboard`:lx: vs `the book is on the shelf`:lx:.
   Explore this issue by looking at corpus data; writing programs as needed.

.. _prepositions:
.. ex::
   .. ex:: in the car *vs* on the train
   .. ex:: in town *vs* on campus
   .. ex:: in the picture *vs* on the screen
   .. ex:: in Macbeth *vs* on Letterman


.. include:: footer.rst

