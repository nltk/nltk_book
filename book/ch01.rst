.. -*- mode: rst -*-
.. include:: ../definitions.rst

.. TODO: update cspy reference to more recent book
.. TODO: add some literature references (esp to other intro linguistics textbooks)
.. TODO: adopt simpler hacker example with only single character transpositions;
   move hacker example to later section (later chapter?)
.. TODO: get URL hyperlinks to be fixed width
.. TODO: websites with automatically generated language -- lobner prize... 

.. _chap-introduction:

=================================
1. Language Processing and Python
=================================

-----------------------------------
Natural Language Processing Systems
-----------------------------------

Translation
-----------

For decades, automatic translation between languages has been held up as the ultimate challenge for |NLP|.
If a system can translate arbitrary expressions and concepts between languages, it surely must have gained human-level understanding of language.
The roots of "mechanical translation", now known as `machine translation`:dt: (MT), go back to the early days of the Cold War, when governments were keen to exploit computational methods for sourcing intelligence in other languages.
For the past two decades, there has been a resurgence of interest in this area, in the sub-field known as `statistical machine translation`:dt: (SMT).

Today, machine translation services such as "Google Translate" support about 100 languages, and translation is integrated into Web search engines.
However, these systems have some serious shortcomings.
Consider the following example, where the input phrase is translated into Japanese, then back to English, and so forth, until equilibrium has been reached:

| how long before the next flight to Alice Springs
| どのくらいåにアリス ・ スプリングスにæのä
| How long before Alice Springs for the next flight
| æのフライトのアリス ・ スプリングスのどのくらいåに
| How much of Alice Springs on the next flight before
| アリスのどのくらいåにæのäにææします。
| Ago Alice how much hot springs on the next flight.
| åæのäにどのくらいçいアリススプリングスします。
| Before the next flight how hot Alice Springs the.
| æはどのようにホット アリス ・ スプリングスをéèするåに。
| Before the next how to fly hot Alice Springs.
| æのåにどのようにホット アリス ・ スプリングスをéぶ。
| Ago the next how to fly hot Alice Springs.
| 前次温泉アリスの操縦方法。
| How to fly before the next hot Alice.
| 次のホット アリスの前に飛ぶ方法。
| How to fly in front of the next hot Alice.
| 次のホット アリスの前に飛ぶ方法。
| How to fly in front of the next hot Alice.

Observe that `springs`:lx: becomes `hot springs`:lx: and eventually `springs`:lx: is lost entirely;
`long before`:lx: becomes `ago`:lx:, then `before`:lx: and finally `in front of`:lx:.

.. note:: |TRY| Try this yourself using ``http://translationparty.com/``

Machine translation is difficult because a given word could have several possible translations (depending on its meaning), and because word order must be changed in keeping with the grammatical structure of the target language.
These difficulties are being addressed with the help of massive quantities of parallel texts, sourced from news and government websites that publish documents in two or more languages.
Given a pair of such documents,
we can automatically find corresponding sentences using two heuristics:
(a) sentences tend to be translated in order even if they are sometimes split or merged in the process;
and (b) short sentences tend to be translated into short sentences, and long sentences tend to be translated into long sentences.
Once we have found corresponding sentences, we can identify corresponding words using the following heuristic:
find all sentences containing a particular word in the source language, then look for the most frequent word in the translation of those sentences.
Once we have a million or more sentence pairs, we can detect corresponding words and phrases, and build a model that can be used for translating new text.

There are two main components in an |SMT| system, namely a translation model and a language model.
The translation model is responsible for representing the likely translations for each word in context, and can be thought of as a mapping from source language expressions to target language expressions.
Source expressions can include idioms, e.g. French `la goutte d'eau qui fait déborder le vase`:lx:, literally the drop of water that made the vase overflow, becomes `the straw the broke the camel's back`:lx: in English).
The language model is responsible for representing the likely words and expressions in the target language, and can be thought of as a mapping from target language expressions to a list of the most likely following words.
For example, after `I absolutely`:lx: we would expect words such as `adore`:lx: and `detest`:lx:, but not `like`:lx: or `dislike`:lx:.

| ARCHITECTURE DIAGRAM:
| Source text -> translation model p(f|e) -> language model p(e) -> target text

The translation model and language model act together to produce a translation that is both faithful to the source language and coherent in the target language.
Proper names need to be handled specially, usually by some combination of lookup (e.g. German `München`:lx: becomes `Munich`:lx: in English) or transliteration (e.g. Arabic `القذافي`:lx: becomes `Gaddafi`:lx: or `Qadhafi`:lx: in English).
Another challenge is posed by languages with complex morphology, including inflectional affixes (EXAMPLE).



* dialogue
* summarisation
* web search

* recommendations (similarity metrics over document collections)
* pointers to demonstrations online (links hosted at nltk.org to avoid link rot)
* discussion to highlight the non-trivial NLP involved


Question Answering
------------------

Humans like asking questions. But we don't always get good answers!
One of the challengs of Question Answering as a research topic in
|NLP| is figuring out what knowledge can be drawn on to satisfy our curiosity.

An early strand of work on this topic was arguably driven by a
slightly different perception: lots of useful information is stored as
structured data models, but it's hard for the ordinary user to get at that
information. For example, querying a relational database requires that you know how
to formulate your questions in a special language such as SQL
[ref]. Wouldn't it be convenient if we could make those queries by
asking natural language questions instead! Work on natural language
interfaces to databases has a long history, dating back to at least
the 1970s, and flourished particularly in the following decade.


Sentiment Analysis
------------------

When we talk about understanding natural language, we often focus on
'who did what to whom'. Yet in many situations, we are more interested
in attitudes and opinions. When someone writes about a movie, did they
like it or hate it? Is a product review for a water bottle on Amazon
positive or negative? Is this Tweet about the US President supportive
or critical? We might also care about the intensity of the views
expressed: `this is a fine movie`:lx: is different from `WOW!
This movie is soooooo great!!!!`:lx: even though both are
positive.

`Sentiment analysis`:dt: (or `opinion mining`:dt:) is a broad term for
a range of techniques that try to identify the subjective views
expressed in texts. Many organisations care deeply about public
opinion |mdash| whether these concern commercial products, creative
works, or political parties and policies |mdash| and have consequently
turned to sentiment analysis as a way of gleaning valuable insights
from voluminous bodies of online text. This in turn has stimulated
much activity in the area, ranging from academic research to
commercial applications and industry-focussed conferences.

We'll look at sentiment analysis in more detail later in the book [NB
XREF]. For the time being, let's say that our task is to classify a
sentence into one of three categories: positive, negative or
neutral. Each of these can be illustrated by posts on Twitter collected during
the UK General Election in 2015.

.. _ex-twitter-sa1:
.. ex::
   .. ex:: [positive] Good stuff from Clegg. Clear, passionate & \
           honest about the difficulties of govt but also the difference @LibDems have made. 
   .. ex:: [negative] Hmm. Ed Miliband being against SNP is a bad move \
           I think. It'll cost him n it is a dumb choice.
   .. ex:: [neutral] Why is Ed Milliband trending when him name is Ed Miliband?

The easiest approach to classifying examples like these is to get hold
of two lists of words, positive ones such as `good`:lx:,
`excellent`:lx:, `fine`:lx:, `triumph`:lx:, `well`:lx:, `succeed`:lx:
|DOTS| and negative ones such as `bad`:lx:, `poor`:lx:, `dismal`:lx:,
`lying`:lx:, `fail`:lx:, `disaster`:lx: |DOTS|. We then base our
polarity assignment on the ratio of positive tokens to negative ones
in a given string. A string with neither positive or negative tokens
(or possibly an equal number of each) will be categorised as
neutral. This simple approach is likely to yield the intuitively
correct results for ex-twitter-sa1_.

Things become more complicated when negation enter into the
picture. ex-twitter-sa1_ is mildly positive (at least in British
English), so we need to ensure that `not`:lx: flips the
polarity of `bad`:lx: in appropriate contexts,

.. _ex-twitter-sa1:

.. ex::  Given Miliband personal ratings still 20 points behind \
         Cameron, I'd say that not a bad margin for Labour leader  https://t.co/ILQP93VYLF

Relatively 'shallow' techniques can deal fairly effectively
with the way in which the prior polarity of a word is modified by the
contextual effects of negation and other semantic operators.
Nevertheless it's not hard to find examples where something close to
full natural language understanding is required to determine the
correct polarity.

.. _ex-twitter-sa2:
.. ex::
   .. ex:: David Cameron doesn't seem to have done too badly until \
      	   now. Otherwise #milifandom and #cleggers would be attacking \
	   him for these bad things 
   .. ex:: Even though I don't like UKIP I'm hating them less and less \
           every day, they do actually have very some good policies 

This has led some researchers to develop approaches where syntactic
structure is also factored into sentiment analysis,

A further challenge in sentiment analysis is deciding the right level
of granularity for the topic under discussion. Often, we can agree in the overall polarity of a
sentence (or even of larger texts) because there is a single dominant topic. But in a list-like construction
such as ex-twitter-sa3_, different sentiments are associated with different
entities, and there is no sensible way of aggregating this into a combined
polarity score for the text as a whole:

.. _ex-twitter-sa3:
.. ex::  @hugorifkind Audience - good. Mili - bad. Clegg - a bit sad. Cam - unscathed

Finally, our current approaches to language processing struggle with
sarcasm, irony and satire, which again lead to polarity reversals.

.. _ex-twitter-sa4:
.. ex::
   .. ex:: LOVE being sat on a plane for 4 hours after a 10 hour flight !! Soooo fun !
   .. ex:: The wrong spelling of Ed Miliband is trending, but not the \
           correct one. Good job, Britain.



---------
Sub-Tasks
---------

* WSD
* pronoun resolution
* entailment
* finding things in text
* language modeling
* collocations


Sentence Segmentation
---------------------

Many NLP operations are performed on the sentence level, e.g. we parse sentences, not phrases or paragraphs.
A declarative sentence, like `Today is Tuesday,`:lx: is the smallest linguistic unit that can be assigned a truth value.
Sentences are terminated with a period, question mark, or exclamation mark, though we cannot merely search for these characters in continuous text in order to locate sentence boundaries.
In some cases, periods occur inside a sentence, e.g. `The work is being done by Mr. W.E.B. Du Bois, Ph. D.`:lx:
In some cases, a period serves dual purposes as marking an abbreviation and the sentence boundary, as in the previous sentence.
(In some cases a following quotation mark or parenthesis is part of the sentence.)

An initial capital is a further clue to a sentence boundary.
However, some sentences can begin with a lowercase letter, e.g. `i is a common name for a variable.`:lx:

A simple heuristic is to segment a sentence at a period, unless the previous word is from a known list of abbreviations, in which case we check whether the following word is capitalised.
This heuristic works in about 95% of cases (at least, for English).
However, some texts are extra challenging, such as the following fragment from an article in the opinion pages of the New York Times:

.. ex::
   .. ex:: When I rented my first city apartment, I discovered it came with a roommate. We did better than make do: We bonded.
   .. ex:: My roommate cheered and comforted me, although I only ever glimpsed her — a blur — from the corner of my eye. (The "her" was a guess — I thought of us as two girls on our own.)
   .. ex:: "She’s a runaway hamster," I told people at work. Not a pet. My friend.
   .. ex:: My bosses stared at me. "Alison. That’s. Not. A. Hamster." They warned me that my roommate would chew my eyelids while I slept.

For better sentence segmentation, it is necessary to apply automatic classification methods that learn whether a given period (or question mark or exclamation mark) marks a sentence boundary.
These methods are able to weigh up a variety of contributing factors.








----------------------------------------
Automatic Natural Language Understanding
----------------------------------------

..
    >>> from nltk.misc import babelize_shell

We have been exploring language bottom-up, with the help of texts and
the Python programming
language. However, we're also interested in exploiting our knowledge of language and computation
by building useful language technologies. We'll take the opportunity
now to step back from the nitty-gritty of code in order to paint a
bigger picture of natural language processing.

At a purely practical level, we all need help to navigate the universe of information
locked up in text on the Web. Search engines have been crucial to the
growth and popularity of the Web, but have some shortcomings.
It takes skill, knowledge, and some luck,
to extract answers to such questions as: `What tourist sites can I
visit between Philadelphia and Pittsburgh on a limited budget?`:lx:
`What do experts say about digital SLR cameras?`:lx: `What
predictions about the steel market were made by credible commentators
in the past week?`:lx: Getting a computer to answer them automatically
involves a range of language processing tasks, including information extraction,
inference, and summarization, and would need to be carried out on a scale
and with a level of robustness that is still beyond our current capabilities.

On a more philosophical level, a long-standing challenge within artificial intelligence
has been to build intelligent machines, and a major part of intelligent behaviour is understanding
language. For many years this goal has been seen as too difficult.
However, as |NLP| technologies become more mature, and robust methods for
analyzing unrestricted text become more widespread, the prospect of
natural language understanding has re-emerged as a plausible goal.

In this section we describe some language understanding technologies,
to give you a sense of the interesting challenges that are waiting for you.

Word Sense Disambiguation
-------------------------

In `word sense disambiguation`:dt: we want to work out
which sense of a word was intended in a given context. Consider the
ambiguous words `serve`:lx: and `dish`:lx:\ :

.. ex::
    .. ex:: `serve`:lx:\ : help with food or drink; hold an office; put ball into play
    .. ex:: `dish`:lx:\ : plate; course of a meal; communications device

In a sentence containing the phrase: `he served the dish`:lx:, you
can detect that both `serve`:lx: and `dish`:lx: are being used with
their food meanings. It's unlikely that the topic of discussion
shifted from sports to crockery in the space of three words.
This would force you to invent bizarre images, like a tennis pro
taking out his or her frustrations on a china tea-set laid out beside the court. 
In other words, we automatically disambiguate words using context, exploiting
the simple fact that nearby words have closely related meanings.
As another example of this contextual effect, consider the word
`by`:lx:, which has several meanings, e.g.: `the book by
Chesterton`:lx: (agentive |mdash| Chesterton was the author of the book);
`the cup by the stove`:lx: (locative |mdash| the stove is where the
cup is); and `submit by Friday`:lx: (temporal |mdash| Friday is the
time of the submitting).
Observe in ex-lost-children_ that the meaning of the italicized word helps us
interpret the meaning of `by`:lx:.

.. _ex-lost-children:
.. ex::
   .. ex:: The lost children were found by the `searchers`:em:  (agentive)
   .. ex:: The lost children were found by the `mountain`:em:   (locative)
   .. ex:: The lost children were found by the `afternoon`:em:  (temporal)

Pronoun Resolution
------------------

A deeper kind of language understanding is to work out "who did what to whom" |mdash|
i.e., to detect the subjects and objects of verbs. You learnt to do this in
elementary school, but it's harder than you might think.
In the sentence `the thieves stole the paintings`:lx:
it is easy to tell who performed the stealing action.
Consider three possible following sentences in ex-thieves_, and try to determine
what was sold, caught, and found (one case is ambiguous).
  
.. _ex-thieves:
.. ex::
   .. ex:: The thieves stole the paintings. They were subsequently `sold`:em:.
   .. ex:: The thieves stole the paintings. They were subsequently `caught`:em:.
   .. ex:: The thieves stole the paintings. They were subsequently `found`:em:.

Answering this question involves finding the `antecedent`:dt: of the pronoun `they`:lx:,
either thieves or paintings. Computational techniques for tackling this problem
include `anaphora resolution`:dt: |mdash| identifying what a pronoun or noun phrase
refers to |mdash| and `semantic role labeling`:dt: |mdash| identifying how a noun phrase
relates to the verb (as agent, patient, instrument, and so on).

Generating Language Output
--------------------------

If we can automatically solve such problems of language understanding, we will
be able to move on to tasks that involve generating language output, such as
`question answering`:dt: and `machine translation`:dt:. In the first case,
a machine should be able to answer a user's questions relating to collection of texts:

.. _ex-qa-application:
.. ex::
   .. ex:: *Text:* ... The thieves stole the paintings. They were subsequently sold. ...
   .. ex:: *Human:* Who or what was sold?
   .. ex:: *Machine:* The paintings.

The machine's answer demonstrates that it has correctly worked out that `they`:lx:
refers to paintings and not to thieves. In the second case, the machine should
be able to translate the text into another language, accurately
conveying the meaning of the original text. In translating the example text into French,
we are forced to choose the gender of the pronoun in the second sentence:
`ils`:lx: (masculine) if the thieves are found, and `elles`:lx: (feminine) if
the paintings are found. Correct translation actually depends on correct understanding of
the pronoun.

.. _ex-mt-application:
.. ex::
   .. ex:: The thieves stole the paintings. They were subsequently found.
   .. ex:: Les voleurs ont vol\ |eacute| les peintures. Ils ont |eacute|\ t\ |eacute| trouv\ |eacute|\ s plus tard. (the thieves)
   .. ex:: Les voleurs ont vol\ |eacute| les peintures. Elles ont |eacute|\ t\ |eacute| trouv\ |eacute|\ es plus tard. (the paintings)
    
In all of these examples, working out the sense of a word, the subject of a verb, and the 
antecedent of a pronoun are steps in establishing the meaning of a sentence, things
we would expect a language understanding system to be able to do.

Spoken Dialog Systems
---------------------

In the history of artificial intelligence, the chief measure of intelligence
has been a linguistic one, namely the `Turing Test`:dt:\ : can a dialogue system,
responding to a user's text input, perform so naturally that we cannot distinguish
it from a human-generated response?  In contrast, today's commercial dialogue systems
are very limited, but still perform useful functions in narrowly-defined domains,
as we see here:

| S: How may I help you?
| U: When is Saving Private Ryan playing?
| S: For what theater?
| U: The Paramount theater.
| S: Saving Private Ryan is not playing at the Paramount theater, but
| it's playing at the Madison theater at 3:00, 5:30, 8:00, and 10:30. 

You could not ask this system to provide driving instructions or
details of nearby restaurants unless the required information
had already been stored and suitable question-answer pairs
had been incorporated into the language processing system.

Observe that this system seems to understand the user's goals:
the user asks when a movie is showing and the system
correctly determines from this that the user wants to see
the movie. This inference seems so obvious that you probably
didn't notice it was made, yet a natural language system
needs to be endowed with this capability in order to interact
naturally. Without it, when asked `Do you know when Saving Private
Ryan is playing?`:lx:, a system might unhelpfully respond with a cold `Yes`:lx:.
However, the developers of commercial dialogue systems use
contextual assumptions and business logic to ensure that the different ways in which a user might
express requests or provide information are handled in a way that
makes sense for the particular application. So, if you type
`When is ...`:lx:, or `I want to know when ...`:lx:, or `Can you tell me
when ...`:lx:, simple rules will always yield screening times. This is
enough for the system to provide a useful service.

.. _fig-sds:
.. figure:: ../images/dialogue.png
   :scale: 25:32:30

   Simple Pipeline Architecture for a Spoken Dialogue System:
   Spoken input (top left) is analyzed, words are recognized, sentences are parsed and
   interpreted in context, application-specific actions take place (top right);
   a response is planned, realized as a syntactic structure, then to suitably
   inflected words, and finally to spoken output; different types of
   linguistic knowledge inform each stage of the process.

Dialogue systems give us an opportunity to mention the
commonly assumed pipeline for |NLP|.
fig-sds_ shows the architecture of a simple dialogue system.
Along the top of the diagram, moving from left to right, is a
"pipeline" of some language understanding `components`:dt:.
These map from speech input via syntactic parsing
to some kind of meaning representation. Along the middle, moving from
right to left, is the reverse pipeline of components for converting
concepts to speech. These components make up the dynamic aspects of the system.
At the bottom of the diagram are some representative bodies of
static information: the repositories of language-related data that
the processing components draw on to do their work.

.. note:: |TRY|
   For an example of a primitive dialogue system, try having
   a conversation with an NLTK chatbot. To see the available chatbots,
   run ``nltk.chat.chatbots()``.
   (Remember to ``import nltk`` first.) 

Textual Entailment
------------------

The challenge of language understanding has been brought into focus in recent years by a public
"shared task" called Recognizing Textual Entailment (RTE). The basic
scenario is simple. Suppose you want to find evidence to support
the hypothesis: `Sandra Goudie was defeated by Max Purnell`:lx:, and
that you have another short text that seems to be relevant, for example,
`Sandra Goudie was first elected to Parliament in the 2002 elections,
narrowly winning the seat of Coromandel by defeating Labour candidate
Max Purnell and pushing incumbent Green MP Jeanette Fitzsimons into
third place`:lx:. Does the text provide enough evidence for you to
accept the hypothesis?  In this particular case, the answer will be "No."
You can draw this conclusion easily, but it is very hard to come up with 
automated methods for making the right decision. The RTE
Challenges provide data that allow competitors to develop their
systems, but not enough data for "brute force" machine learning techniques (a topic
we will cover in chap-data-intensive_\ ). Consequently, some
linguistic analysis is crucial. In the previous example, it is important
for the system to note that `Sandra Goudie`:lx: names the person being
defeated in the hypothesis, not the person doing the defeating in the
text. As another illustration of the difficulty of the task, consider
the following text-hypothesis pair:

.. ex::
   .. ex:: Text: David Golinkin is the editor or author of eighteen books, and over 150 responsa, articles, sermons and books 
   .. ex:: Hypothesis: Golinkin has written eighteen books

In order to determine whether the hypothesis is supported by the
text, the system needs the following background knowledge:
(i) if someone is an author of a book, then he/she has written that
book; (ii) if someone is an editor of a book, then he/she has not
written (all of) that book; (iii) if someone is editor or author of eighteen
books, then one cannot conclude that he/she is author of eighteen books.


Limitations of |NLP|
--------------------

Despite the research-led advances in tasks like RTE, natural language
systems that have been deployed for real-world applications still cannot perform
common-sense reasoning or draw on world knowledge in a general and
robust manner. We can wait for these difficult artificial
intelligence problems to be solved, but in the meantime it is
necessary to live with some severe limitations on the reasoning and
knowledge capabilities of natural language systems. Accordingly, right
from the beginning, an important goal of |NLP| research has been to
make progress on the difficult task of building technologies that
"understand language," using superficial yet powerful techniques instead of
unrestricted knowledge and reasoning capabilities.
Indeed, this is one of the goals of this book, and we hope to equip you with
the knowledge and skills to build useful |NLP| systems, and to
contribute to the long-term aspiration of building intelligent machines.


.. _sec-overview-of-nltk:

----------------
Overview of NLTK
----------------

Getting Started
---------------

If you already have Python and Pip installed, you can install |NLTK| by running ``pip install nltk``.
If you experience any difficulties with installation, you can consult |NLTK-URL| for more detailed instructions.

Once you've installed |NLTK|, you can install the datasets required for the book by running ``python -m nltk.downloader book``.
Alternatively, you can start up the Python interpreter and install the data by
typing the following two commands at Python's ``>>>`` prompt, then selecting
the ``book`` collection as shown in fig-nltk-downloader_.
   
.. doctest-ignore::
    >>> import nltk
    >>> nltk.download()
   
.. _fig-nltk-downloader:
.. figure:: ../images/nltk-downloader.png
   :scale: 100:100:100

   Downloading the NLTK Book Collection: browse the available packages
   using ``nltk.download()``. The **Collections** tab on the downloader
   shows how the packages are grouped into sets, and you should select the line labeled
   **book** to obtain all
   data required for the examples and exercises in this book. It consists
   of about 40 compressed files requiring about 120Mb disk space.
   The full collection, option **all** in the downloader, is about five times this size.

Once the data is downloaded to your machine, you can load some of it using the Python interpreter as follows:

    >>> from nltk.book import *
    *** Introductory Examples for the NLTK Book ***
    Loading text1, ..., text9 and sent1, ..., sent9
    Type the name of the text or sentence to view it.
    Type: 'texts()' or 'sents()' to list the materials.
    text1: Moby Dick by Herman Melville 1851
    text2: Sense and Sensibility by Jane Austen 1811
    text3: The Book of Genesis
    text4: Inaugural Address Corpus
    text5: Chat Corpus
    text6: Monty Python and the Holy Grail
    text7: Wall Street Journal
    text8: Personals Corpus
    text9: The Man Who Was Thursday by G . K . Chesterton 1908

Any time we want to find out about these texts, we just have
to enter their names at the Python prompt:

    >>> text1
    <Text: Moby Dick by Herman Melville 1851>
    >>> text2
    <Text: Sense and Sensibility by Jane Austen 1811>

If you have your own collection of text, stored in local files, you can load
it and "tokenize" it into a list of words and punctuation
(see chap_words_ for more details about tokenization).

    >>> from nltk import word_tokenize
    >>> raw = open('melville-moby_dick.txt').read()
    >>> tokens = word_tokenize(raw)
    >>> tokens[:100]
    ['[', 'Moby', 'Dick', 'by', 'Herman', 'Melville', '1851', ']', 'ETYMOLOGY', '.',
    '(', 'Supplied', 'by', 'a', 'Late', 'Consumptive', 'Usher', 'to', 'a', 'Grammar',
    'School', ')', 'The', 'pale', 'Usher', '--', 'threadbare', 'in', 'coat', ',',
    'heart', ',', 'body', ',', 'and', 'brain', ';', 'I', 'see', 'him', 'now', '.', ...]
    >>> len(tokens)
    254989

You can convert this list of tokens into an |NLTK| "Text" object in order to
perform the simple text processing operations described in the following sections,
using: ``from nltk import Text; text1 = Text(tokens)``.

Searching Text
--------------

There are many ways to examine the context of a text apart from simply reading it.
A concordance view shows us every occurrence of a given word, together with some context.
Here we look up the word `monstrous`:lx: in *Moby Dick*:

    >>> text1.concordance("monstrous")
    Displaying 11 of 11 matches:
    ong the former , one was of a most monstrous size . ... This came towards us ,
    ON OF THE PSALMS . " Touching that monstrous bulk of the whale or ork we have r
    ll over with a heathenish array of monstrous clubs and spears . Some were thick
    d as you gazed , and wondered what monstrous cannibal and savage could ever hav
    that has survived the flood ; most monstrous and most mountainous ! That Himmal
    they might scout at Moby Dick as a monstrous fable , or still worse and more de
    th of Radney .'" CHAPTER 55 Of the monstrous Pictures of Whales . I shall ere l
    ing Scenes . In connexion with the monstrous pictures of whales , I am strongly
    ere to enter upon those still more monstrous stories of them which are to be fo
    ght have been rummaged out of this monstrous cabinet there is no telling . But
    of Whale - Bones ; for Whales of a monstrous size are oftentimes cast up dead u

The first time you use a concordance on a particular text, it takes a
few extra seconds to build an index so that subsequent searches are fast.

.. note:: |TRY|
   Try searching for other words; to save re-typing, you might be able to
   use up-arrow, Ctrl-up-arrow or Alt-p to access the previous command and modify the word being searched.
   You can also try searches on some of the other texts we have included.
   For example, search *Sense and Sensibility* for the word
   `affection`:lx:, using ``text2.concordance("affection")``.
   Search the book of Genesis to find out how long some people lived, using
   ``text3.concordance("lived")``. You could look at ``text4``, the
   *Inaugural Address Corpus*, to see examples of English going
   back to 1789, and search for words like `nation`:lx:, `terror`:lx:, `god`:lx:
   to see how these words have been used differently over time.
   We've also included ``text5``, the *NPS Chat Corpus*: search this for
   unconventional words like `im`:lx:, `ur`:lx:, `lol`:lx:.
   (Note that this corpus is uncensored!)

Once you've examined these texts, we hope you have a new
sense of the richness and diversity of language. In the next chapter
you will learn how to access a broader range of text, including text in
languages other than English.

A concordance permits us to see words in context. For example, we saw that
`monstrous`:lx: occurred in contexts such as `the ___ pictures`:lx:
and `a ___ size`:lx: . What other words appear in a similar range
of contexts?

    >>> text1.similar("monstrous")
    mean part maddens doleful gamesome subtly uncommon careful untoward
    exasperate loving passing mouldy christian few true mystifying
    imperial modifies contemptible
    >>> text2.similar("monstrous")
    very heartily so exceedingly remarkably as vast a great amazingly
    extremely good sweet

Observe that we get different results for different texts. 
Austen uses this word quite differently from Melville; for her, `monstrous`:lx: has
positive connotations, and sometimes functions as an intensifier like the word
`very`:lx:. 

The ``common_contexts`` method allows us to examine just the
contexts that are shared by two or more words, such as `monstrous`:lx:
and `very`:lx::

    >>> text2.common_contexts(["monstrous", "very"])
    a_pretty is_pretty am_glad be_glad a_lucky

.. note:: |TRY|
   Pick another pair of words and compare their usage in two different texts, using
   the ``similar()`` and ``common_contexts()`` functions.

It is one thing to automatically detect that a particular word occurs in a text,
and to display some words that appear in the same context. However, we can also determine
the *location* of a word in the text: how many words from the beginning it appears.
This positional information can be displayed using a `dispersion plot`:dt:.
Each stripe represents an instance
of a word, and each row represents the entire text. In fig-inaugural_ we
see some striking patterns of word usage over the last 220 years
(in an artificial text constructed by joining
the texts of the Inaugural Address Corpus end-to-end). 
You can produce this plot as shown below.
You might like to try more words (e.g., `liberty`:lx:, `constitution`:lx:),
and different texts. Can you predict the
dispersion of a word before you view it?

.. doctest-ignore::
    >>> text4.dispersion_plot(["citizens", "democracy", "freedom", "duties", "America"])

.. _fig-inaugural:
.. figure:: ../images/inaugural.png
   :scale: 90:90:90

   Lexical Dispersion Plot for Words in U.S. Presidential Inaugural Addresses:
   This can be used to investigate changes in language use over time.

.. note:: |IMPORTANT|
   You need to have Python's NumPy and Matplotlib packages installed
   in order to produce the graphical plots used in this book.

.. note::
   You can also plot the frequency of word usage through time using
   |GOOGLE-NGRAM|

Let's try generating some random text in the various
styles we have just seen, e.g.:

    >>> text3.generate()
    In the beginning of his brother is a hairy man , whose top may reach
    unto heaven ; and ye shall sow the land of Egypt there was no bread in
    all that he was taken out of the month , upon the earth . So shall thy
    wages be ? And they made their father ; and Isaac was old , and kissed
    him : and Laban with his cattle in the midst of the hands of Esau thy
    first born , and Phichol the chief butler unto his son Isaac , she

.. note::
   The ``generate()`` method is not available in NLTK 3.0 but will be
   reinstated in a subsequent version.

..
   Note that the first time you run this command, it is slow because it gathers statistics
   about word sequences. Each time you run it, you will get different output text.
   Now try generating random text in the style of an inaugural address or an
   Internet chat room. Although the text is random, it re-uses common words and
   phrases from the source text and gives us a sense of its style and content.
   (What is lacking in this randomly generated text?)

.. note
   When ``generate`` produces its output, punctuation is split off
   from the preceding word. While this is not correct formatting
   for English text, we do it to make clear that words and
   punctuation are independent of one another. You will learn
   more about this in chap-words_.


Frequency Distributions
-----------------------

How can we automatically identify the words of a text that are most
informative about the topic and genre of the text?  Imagine how you might
go about finding the 50 most frequent words of a book. One method
would be to keep a tally for each vocabulary item, like that shown in fig-tally_.
The tally would need thousands of rows, and it would be an exceedingly
laborious process |mdash| so laborious that we would rather assign the task to a machine.

.. _fig-tally:
.. figure:: ../images/tally.png
   :scale: 20:100:25

   Counting Words Appearing in a Text (a frequency distribution)

The table in fig-tally_ is known as a `frequency distribution`:dt:,
and it tells us the frequency of each vocabulary item in the text.
(In general, it could count any kind of observable event.)
It is a "distribution"
because it tells us how the total number of word tokens in the text
are distributed across the vocabulary items.
Since we often need frequency distributions in language processing, NLTK
provides built-in support for them. Let's use a ``FreqDist`` to find the
50 most frequent words of *Moby Dick*:

    >>> from nltk import FreqDist
    >>> fdist1 = FreqDist(text1) # [_freq-dist-call]
    >>> print(fdist1) # [_freq-dist-inspect]
    <FreqDist with 19317 samples and 260819 outcomes>
    >>> fdist1.most_common(50) # [_freq-dist-most-common]
    [(',', 18713), ('the', 13721), ('.', 6862), ('of', 6536), ('and', 6024),
    ('a', 4569), ('to', 4542), (';', 4072), ('in', 3916), ('that', 2982),
    ("'", 2684), ('-', 2552), ('his', 2459), ('it', 2209), ('I', 2124),
    ('s', 1739), ('is', 1695), ('he', 1661), ('with', 1659), ('was', 1632),
    ('as', 1620), ('"', 1478), ('all', 1462), ('for', 1414), ('this', 1280),
    ('!', 1269), ('at', 1231), ('by', 1137), ('but', 1113), ('not', 1103),
    ('--', 1070), ('him', 1058), ('from', 1052), ('be', 1030), ('on', 1005),
    ('so', 918), ('whale', 906), ('one', 889), ('you', 841), ('had', 767),
    ('have', 760), ('there', 715), ('But', 705), ('or', 697), ('were', 680),
    ('now', 646), ('which', 640), ('?', 637), ('me', 627), ('like', 624)]
    >>> fdist1['whale']
    906

When we first invoke ``FreqDist``, we pass the name of the text as an
argument freq-dist-call_. We can inspect the total number of words ("outcomes")
that have been counted up freq-dist-inspect_ |mdash| 260,819 in the
case of *Moby Dick*. The expression ``most_common(50)`` gives us a list of
the 50 most frequently occurring types in the text freq-dist-most-common_.

.. note:: |TRY|
   Try the preceding frequency distribution example for yourself, for
   ``text2``.  Be careful to use the correct parentheses and uppercase letters.
   If you get an error message ``NameError: name 'FreqDist' is not defined``,
   you need to start your work with ``from nltk.book import *``

.. SB: no period after the above import statement

Do any words produced in the last example help us grasp the topic or genre of this text?
Only one word, `whale`:lx:, is slightly informative!  It occurs over 900 times.
The rest of the words tell us nothing about the text; they're just English "plumbing."
What proportion of the text is taken up with such words?
We can generate a cumulative frequency plot for these words,
using ``fdist1.plot(50, cumulative=True)``, to produce the graph in fig-fdist-moby_.
These 50 words account for nearly half the book!

.. _fig-fdist-moby:
.. figure:: ../images/fdist-moby.png
   :scale: 20:25:25

   Cumulative Frequency Plot for 50 Most Frequently Words in *Moby Dick*:
   these account for nearly half of the tokens.

If the frequent words don't help us, how about the words that occur once
only, the so-called `hapaxes`:dt:?  View them by typing ``fdist1.hapaxes()``.
This list contains `lexicographer`:lx:, `cetological`:lx:,
`contraband`:lx:, `expostulations`:lx:, and about 9,000 others.
It seems that there are too many rare words, and without seeing the
context we probably can't guess what half of the hapaxes mean in any case!
Since neither frequent nor infrequent words help, we need to try
something else.

Fine-grained Selection of Words
-------------------------------

Next, let's look at the *long* words of a text; perhaps these will be
more characteristic and informative.  For this we adapt some notation
from set theory.  We would like to find the words from the vocabulary
of the text that are more than 15 characters long.  Let's call
this property `P`:math:, so that `P(w)`:math: is true
if and only if `w`:math: is more than 15 characters long.
Now we can express the words of interest using mathematical 
set notation as shown in ex-set-comprehension-math_.
This means "the set of all `w`:math: such that `w`:math: is an
element of `V`:math: (the vocabulary) and `w`:math: has property `P`:math:".

.. _ex-set-comprehension:
.. ex::
   .. _ex-set-comprehension-math:
   .. ex:: {`w`:math: | `w`:math: |element| `V`:math: & `P(w)`:math:\ }
   .. _ex-set-comprehension-python:
   .. ex:: ``[w for w in V if p(w)]``

The corresponding Python expression is given in ex-set-comprehension-python_.
(Note that it produces a list, not a set, which means that duplicates are possible.)
Observe how similar the two notations are.  Let's go one more step and
write executable Python code:

    >>> V = set(text1)
    >>> long_words = [w for w in V if len(w) > 15]
    >>> sorted(long_words)
    ['CIRCUMNAVIGATION', 'Physiognomically', 'apprehensiveness', 'cannibalistically',
    'characteristically', 'circumnavigating', 'circumnavigation', 'circumnavigations',
    'comprehensiveness', 'hermaphroditical', 'indiscriminately', 'indispensableness',
    'irresistibleness', 'physiognomically', 'preternaturalness', 'responsibilities',
    'simultaneousness', 'subterraneousness', 'supernaturalness', 'superstitiousness',
    'uncomfortableness', 'uncompromisedness', 'undiscriminating', 'uninterpenetratingly']
    >>>

For each word ``w`` in the vocabulary ``V``, we check whether
``len(w)`` is greater than 15; all other words will
be ignored.  We will discuss this syntax more carefully later.

.. note:: |TRY|
   Try out the previous statements in the Python interpreter,
   and experiment with changing the text and changing the length condition.
   Does it make a difference to your results if you change the
   variable names, e.g., using ``[word for word in vocab if ...]``? 

Let's return to our task of finding words that characterize a text.
Notice that the long words in ``text4`` reflect its national focus
|mdash| `constitutionally`:lx:, `transcontinental`:lx: |mdash|
whereas those in ``text5`` reflect its informal content:
`boooooooooooglyyyyyy`:lx: and `yuuuuuuuuuuuummmmmmmmmmmm`:lx:.
Have we succeeded in automatically extracting words that typify
a text?  Well, these very long words are often hapaxes (i.e., unique)
and perhaps it would be better to find *frequently occurring*
long words. This seems promising since it eliminates
frequent short words (e.g., `the`:lx:) and infrequent long words
(e.g. `antiphilosophists`:lx:).
Here are all words from the chat corpus
that are longer than seven characters, that occur more than seven times:

    >>> fdist5 = FreqDist(text5)
    >>> sorted(w for w in set(text5) if len(w) > 7 and fdist5[w] > 7)
    ['#14-19teens', '#talkcity_adults', '((((((((((', '........', 'Question',
    'actually', 'anything', 'computer', 'cute.-ass', 'everyone', 'football',
    'innocent', 'listening', 'remember', 'seriously', 'something', 'together',
    'tomorrow', 'watching']

Notice how we have used two conditions: ``len(w) > 7`` ensures that the
words are longer than seven letters, and ``fdist5[w] > 7`` ensures that
these words occur more than seven times. At last we have managed to
automatically identify the frequently-occurring content-bearing
words of the text. It is a modest but important milestone: a tiny piece of code,
processing tens of thousands of words, produces some informative output.

Collocations and Bigrams
------------------------

A `collocation`:dt: is a sequence of words that occur together
unusually often. Thus `red wine`:lx: is a collocation, whereas `the
wine`:lx: is not. A characteristic of collocations is that they are
resistant to substitution with words that have similar senses;
for example, `maroon wine`:lx: sounds definitely odd. 

To get a handle on collocations, we start off by extracting from a text
a list of word pairs, also known as `bigrams`:dt:. This is easily
accomplished with the function ``bigrams()``:

    >>> from nltk import bigrams
    >>> list(bigrams(['more', 'is', 'said', 'than', 'done']))
    [('more', 'is'), ('is', 'said'), ('said', 'than'), ('than', 'done')]

.. note::
   If you omitted ``list()`` above, and just typed ``bigrams(['more', ...])``,
   you would have seen output of the form ``<generator object bigrams at 0x10fb8b3a8>``.
   This is Python's way of saying that it is ready to compute
   a sequence of items, in this case, bigrams. For now, you just need
   to know to tell Python to convert it into a list, using ``list()``.

Here we see that the pair of words `than-done`:lx: is a bigram, and we write
it in Python as ``('than', 'done')``. Now, collocations are essentially
just frequent bigrams, except that we want to pay more attention to the
cases that involve rare words. In particular, we want to find
bigrams that occur more often than we would expect based on
the frequency of the individual words. The ``collocations()`` function
does this for us. We will see how it works later.

    >>> text4.collocations()
    United States; fellow citizens; four years; years ago; Federal
    Government; General Government; American people; Vice President; Old
    World; Almighty God; Fellow citizens; Chief Magistrate; Chief Justice;
    God bless; every citizen; Indian tribes; public debt; one another;
    foreign nations; political parties
    >>> text8.collocations()
    would like; medium build; social drinker; quiet nights; non smoker;
    long term; age open; Would like; easy going; financially secure; fun
    times; similar interests; Age open; weekends away; poss rship; well
    presented; never married; single mum; permanent relationship; slim
    build

The collocations that emerge are very specific to the genre of the
texts. In order to find  `red wine`:lx: as a collocation, we would
need to process a much larger body of text.

Counting Other Things
---------------------

Counting words is useful, but we can count other things too. For example, we can
look at the distribution of word lengths in a text, by creating a ``FreqDist``
out of a long list of numbers, where each number is the length of the corresponding
word in the text: 

    >>> [len(w) for w in text1] # [_word-lengths]
    [1, 4, 4, 2, 6, 8, 4, 1, 9, 1, 1, 8, 2, 1, 4, 11, 5, 2, 1, 7, 6, 1, 3, 4, 5, 2, ...]
    >>> fdist = FreqDist(len(w) for w in text1)  # [_freq-word-lengths]
    >>> print(fdist)  # [_freq-word-lengths-size]
    <FreqDist with 19 samples and 260819 outcomes>
    >>> fdist
    FreqDist({3: 50223, 1: 47933, 4: 42345, 2: 38513, 5: 26597, 6: 17111, 7: 14399,
      8: 9966, 9: 6428, 10: 3528, ...})

We start by deriving a list of the lengths of words in ``text1``
word-lengths_,
and the ``FreqDist`` then counts the number of times each of these
occurs freq-word-lengths_. The result freq-word-lengths-size_ is a distribution containing
a quarter of a million items, each of which is a number corresponding to a
word token in the text. But there are at most only 20 distinct
items being counted, the numbers 1 through 20, because there are only 20
different word lengths. I.e., there are words consisting of just one character,
two characters, ..., twenty characters, but none with twenty one or more
characters. One might wonder how frequent the different lengths of word are
(e.g., how many words of length four appear in the text, are there more words of length five
than length four, etc). We can do this as follows:

    >>> fdist.most_common()
    [(3, 50223), (1, 47933), (4, 42345), (2, 38513), (5, 26597), (6, 17111), (7, 14399),
    (8, 9966), (9, 6428), (10, 3528), (11, 1873), (12, 1053), (13, 567), (14, 177),
    (15, 70), (16, 22), (17, 12), (18, 1), (20, 1)]
    >>> fdist.max()
    3
    >>> fdist[3]
    50223
    >>> fdist.freq(3)
    0.19255882431878046

From this we see that the most frequent word length is 3, and that
words of length 3 account for roughly 50,000 (or 20%) of the words making up the
book. Although we will not pursue it here, further analysis of word
length might help us understand differences between authors, genres, or
languages.

tab-freqdist_ summarizes the functions defined in frequency distributions.

.. table:: tab-freqdist

   ===============================  ======================================================================
   Example                          Description
   ===============================  ======================================================================
   ``fdist = FreqDist(samples)``    create a frequency distribution containing the given samples
   ``fdist[sample] += 1``           increment the count for this sample
   ``fdist['monstrous']``           count of the number of times a given sample occurred
   ``fdist.freq('monstrous')``      frequency of a given sample
   ``fdist.N()``                    total number of samples
   ``fdist.most_common(n)``         the ``n`` most common samples and their frequencies
   ``for sample in fdist:``         iterate over the samples
   ``fdist.max()``                  sample with the greatest count
   ``fdist.tabulate()``             tabulate the frequency distribution
   ``fdist.plot()``                 graphical plot of the frequency distribution
   ``fdist.plot(cumulative=True)``  cumulative plot of the frequency distribution
   ``fdist1 |= fdist2``             update ``fdist1`` with counts from ``fdist2``
   ``fdist1 < fdist2``              test if samples in ``fdist1`` occur less frequently than in ``fdist2``
   ===============================  ======================================================================

   Functions Defined for |NLTK|\ 's Frequency Distributions


Tagging and Parsing
-------------------

|NLTK| includes several "Part-of-speech taggers" (see chap_tag_),
which automatically associate lexical categories with words, e.g.:

    >>> from nltk import pos_tag
    >>> pos_tag(text1[:100], tagset='universal')
    [('[', 'NOUN'), ('Moby', 'NOUN'), ('Dick', 'NOUN'), ('by', 'ADP'), ('Herman', 'NOUN'),
    ('Melville', 'NOUN'), ('1851', 'NUM'), (']', 'NUM'), ('ETYMOLOGY', 'NOUN'), ('.', '.'),
    ('(', '.'), ('Supplied', 'NOUN'), ('by', 'ADP'), ('a', 'DET'), ('Late', 'NOUN'),
    ('Consumptive', 'NOUN'), ('Usher', 'NOUN'), ('to', 'PRT'), ('a', 'DET'),
    ('Grammar', 'NOUN'), ('School', 'NOUN'), (')', 'NOUN'), ('The', 'NOUN'),
    ('pale', 'NOUN'), ('Usher', 'NOUN'), ('--', '.'), ('threadbare', 'NOUN'),
    ('in', 'ADP'), ('coat', 'ADJ'), (',', '.'), ('heart', 'NOUN'), (',', '.'),
    ('body', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('brain', 'NOUN'), (';', '.'), ...]


* parsing



---------------------
Overview of this Book
---------------------

-------
Summary
-------

---------------
Further Reading
---------------

* Introductions to NLP

You may also like to read up on
some linguistics and |NLP|\ -related concepts in Wikipedia (e.g., collocations,
the Turing Test, the type-token distinction).

As you delve into |NLTK|, you might want to subscribe to the mailing list where new
releases of the toolkit are announced. There is also an NLTK-Users mailing list,
where users help each other as they learn how to use Python and |NLTK| for
language analysis work. Details of these lists are available at |NLTK-URL|.

For more information on the topics covered in sec-automatic-natural-language-understanding_,
and on |NLP| more generally, you might like to consult one of the following excellent
books:

* Indurkhya, Nitin and Fred Damerau (eds, 2010) *Handbook of Natural Language Processing*
  (Second Edition) Chapman & Hall/CRC. 2010. [IndurkhyaDamerau2010]_ [Dale00handbook]_

* Jurafsky, Daniel and James Martin (2008) *Speech and Language Processing* (Second Edition). Prentice Hall.
  [JurafskyMartin2008]_

* Mitkov, Ruslan (ed, 2003) *The Oxford Handbook of Computational Linguistics*. Oxford University Press.
  (second edition expected in 2010). [Mitkov02handbook]_

The Association for Computational Linguistics is the international organization that
represents the field of |NLP|. The ACL website (``http://www.aclweb.org/``) hosts many useful resources, including:
information about international and regional conferences and workshops;
the `ACL Wiki`:em: with links to hundreds of useful resources;
and the `ACL Anthology`:em:, which contains most of the |NLP| research literature
from the past 50+ years, fully indexed and freely downloadable.

Some excellent introductory Linguistics textbooks are:
[Finegan2007]_, [OGrady2004]_, [OSU2007]_. You might like to consult
`LanguageLog`:em:, a popular linguistics blog with occasional posts that
use the techniques described in this book.

.. Natural Language Interfaces to Databases – An Introduction
.. I. Androutsopoulos, G.D. Ritchie and P. Thanisch, Natural Language
.. Engineering, 1(1):29-81, 1995. 

.. include:: footer.rst
